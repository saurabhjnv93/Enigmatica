Artiﬁcial Intelligence
A Modern Approach
Third EditionPRENTICE HALL SERIES
IN ARTIFICIAL INTELLIGENCE
Stuart Russell and Peter Norvig, Editors
FORSYTH & PONCE
Computer Vision: A Modern Approach
GRAHAM
ANSI Common Lisp
JURAFSKY & MARTIN
Speech and Language Processing, 2nd ed.
NEAPOLITAN
Learning Bayesian Networks
RUSSELL & NORVIG
Artiﬁcial Intelligence: A Modern Approach, 3rd ed.Artiﬁcial Intelligence
A Modern Approach
Third Edition
Stuart J. Russell and Peter Norvig
Contributing writers:
Ernest Davis
Douglas D. Edwards
David Forsyth
Nicholas J. Hay
Jitendra M. Malik
Vibhu Mittal
Mehran Sahami
Sebastian Thrun
Upper Saddle River
Boston
Columbus
San Francisco
New York
Indianapolis
London
Toronto
Sydney
Singapore
Tokyo
Montreal
Dubai
Madrid
Hong Kong
Mexico City
Munich
Paris
Amsterdam
Cape TownVice President and Editorial Director, ECS: Marcia J. Horton
Executive Editor: Tracy Dunkelberger
Assistant Editor: Melinda Haggerty
Editorial Assistant: Allison Michael
Vice President, Production: Vince O’Brien
Senior Managing Editor: Scott Disanno
Production Editor: Jane Bonnell
Senior Operations Supervisor: Alan Fischer
Operations Specialist: Lisa McDowell
Marketing Manager: Erin Davis
Marketing Assistant: Mack Patterson
Cover Designers: Kirsten Sims and Geoffrey Cassar
Interior Designers: Stuart Russell and Peter Norvig
Copy Editor: Mary Lou Nohr
Art Editor: Greg Dulles
Media Editor: Daniel Sandin
Media Project Manager: Danielle Leone
All rights reserved. Manufactured in the United States of America. This publication is protected by
Copyright and permissions should be obtained from the publisher prior to any prohibited reproduction,
storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical,
photocopying, recording, or likewise. To obtain permission(s) to use materials from this work, please
The author and publisher of this book have used their best efforts in preparing this book. These
efforts include the development, research, and testing of the theories and programs to determine their
effectiveness. The author and publisher make no warranty of any kind, expressed or implied, with
regard to these programs or the documentation contained in this book. The author and publisher shall
not be liable in any event for incidental or consequential damages in connection with, or arising out
of, the furnishing, performance, or use of these programs.For Loy, Gordon, Lucy, George, and Isaac — S.J.R.
For Kris, Isabella, and Juliet — P.N.This page intentionally left blank Preface
Artiﬁcial Intelligence (AI) is a big ﬁeld, and this is a big book. We have tried to explore the
full breadth of the ﬁeld, which encompasses logic, probability, and continuous mathematics;
perception, reasoning, learning, and action; and everything from microelectronic devices to
robotic planetary explorers. The book is also big because we go into some depth.
The subtitle of this book is “A Modern Approach.” The intended meaning of this rather
work, rather than trying to explain each subﬁeld of AI in its own historical context. We
apologize to those whose subﬁelds are, as a result, less recognizable.
New to this edition
hold robotics. There have been algorithmic landmarks, such as the solution of the game of
checkers. And there has been a great deal of theoretical progress, particularly in areas such
as probabilistic reasoning, machine learning, and computer vision. Most important from our
point of view is the continued evolution in how we think about the ﬁeld, and thus how we
organize the book. The major changes are as follows:
• We place more emphasis on partially observable and nondeterministic environments,
especially in the nonprobabilistic settings of search and planning. The concepts of
belief state (a set of possible worlds) and state estimation (maintaining the belief state)
are introduced in these settings; later in the book, we add probabilities.
• In addition to discussing the types of environments and types of agents, we now cover
in more depth the types of representations that an agent can use. We distinguish among
atomic representations (in which each state of the world is treated as a black box),
representations (in which the world consists of objects and relations between them).
• Our coverage of planning goes into more depth on contingent planning in partially
observable environments and includes a new approach to hierarchical planning.
models for cases where there is uncertainty as to what objects exist.
retical footing.
niques for learning from very large data sets.
older work but has been largely rewritten to present a more uniﬁed picture of the ﬁeld.
viiviii
Preface
Overview of the book
The main unifying theme is the idea of an intelligent agent. We deﬁne AI as the study of
plements a function that maps percept sequences to actions, and we cover different ways to
systems. We explain the role of learning as extending the reach of the designer into unknown
edge representation and reasoning. We treat robotics and vision not as independently deﬁned
problems, but as occurring in the service of achieving goals. We stress the importance of the
task environment in determining the appropriate agent design.
Our primary aim is to convey the ideas that have emerged over the past ﬁfty years of AI
ity in the presentation of these ideas while retaining precision. We have included pseudocode
algorithms to make the key ideas concrete; our pseudocode is described in Appendix B.
This book is primarily intended for use in an undergraduate course or course sequence.
selected chapters to suit the interests of the instructor and students. The book can also be
suggested in the bibliographical notes). Sample syllabi are available at the book’s Web site,
aima.cs.berkeley.edu. The only prerequisite is familiarity with basic concepts of
computer science (algorithms, data structures, complexity) at a sophomore level. Freshman
ground is supplied in Appendix A.
gramming are marked with a keyboard icon. These exercises can best be solved by taking
advantage of the code repository at aima.cs.berkeley.edu. Some of them are large
enough to be considered term projects. A number of exercises require some investigation of
the literature; these are marked with a book icon.
Wherever a new term is ﬁrst deﬁned, it is also marked in the margin.
NEW TERM
About the Web site
aima.cs.berkeley.edu, the Web site for the book, contains
• implementations of the algorithms in the book in several programming languages,
materials and syllabi,
• instructions on how to join a discussion group for the book,Preface
ix
• instructions on how to contact the authors with questions or comments,
• instructions on how to report errors in the book, in the likely event that some exist, and
• slides and other materials for instructors.
About the cover
chess champion Garry Kasparov and program DEEP BLUE. Kasparov, playing Black, was
forced to resign, making this the ﬁrst time a computer had beaten a world champion in a
chess match. Kasparov is shown at the top. To his left is the Asimo humanoid robot and
belief underlie much of modern AI technology. Below that we see a Mars Exploration Rover,
century (copy of a bust by Lysippos). At the bottom left, lightly screened behind the authors’
names, is a planning algorithm by Aristotle from De Motu Animalium in the original Greek.
Behind the title is a portion of the CPSC Bayesian network for medical diagnosis (Pradhan
explosions from seismic signals.
rover), National Museum of Rome (Aristotle), Peter Norvig (book), Ian Parker (Berkeley
Acknowledgments
This book would not have been possible without the many contributors whose names did not
(natural language). Nick Hay, Mehran Sahami, and Ernest Davis wrote some of the exercises.
Zoran Duric (George Mason), Thomas C. Henderson (Utah), Leon Reznik (RIT), Michael
Gourley (Central Oklahoma) and Ernest Davis (NYU) reviewed the manuscript and made
helpful suggestions. We thank Ernie Davis in particular for his tireless ability to read multiple
drafts and help improve the book. Nick Hay whipped the bibliography into shape and on
and improved the diagrams in this edition, while Tim Huang, Mark Paskin, and Cynthia
Bruyns helped with diagrams and algorithms in previous editions. Ravi Mohan and Ciaran
O’Reilly wrote and maintain the Java code examples on the Web site. John Canny wrote
the robotics chapter for the ﬁrst edition and Douglas Edwards researched the historical notes.
Tracy Dunkelberger, Allison Michael, Scott Disanno, and Jane Bonnell at Pearson tried their
best to keep us on schedule and made many helpful suggestions. Most helpful of all hasx
Preface
been Julie Sussman, P.P.A., who read every chapter and provided extensive improvements. In
previous editions we had proofreaders who would tell us when we left out a comma and said
which when we meant that; Julie told us when we left out a minus sign and said xi when we
meant xj. For every typo or confusing explanation that remains in the book, rest assured that
Julie has ﬁxed at least ﬁve. She persevered even when a power failure forced her to work by
lantern light rather than LCD glow.
Stuart would like to thank his parents for their support and encouragement and his
wife, Loy Sheﬂott, for her endless patience and boundless wisdom. He hopes that Gordon,
Lucy, George, and Isaac will soon be reading this book after they have forgiven him for
working so long on it. RUGS (Russell’s Unusual Group of Students) have been unusually
helpful, as always.
Peter would like to thank his parents (Torsten and Gerda) for getting him started,
and his wife (Kris), children (Bella and Juliet), colleagues, and friends for encouraging and
tolerating him through the long hours of writing and longer hours of rewriting.
We both thank the librarians at Berkeley, Stanford, and NASA and the developers of
CiteSeer, Wikipedia, and Google, who have revolutionized the way we do research. We can’t
acknowledge all the people who have used the book and made suggestions, but we would like
los, Krzysztof Apt, Warren Haley Armstrong, Ellery Aziel, Jeff Van Baalen, Darius Bacon,
Brian Baker, Shumeet Baluja, Don Barker, Tony Barrett, James Newton Bass, Don Beal,
man, John Bresina, Gerhard Brewka, Selmer Bringsjord, Carla Brodley, Chris Brown, Emma
bell, Norman Carver, Emmanuel Castro, Anil Chakravarthy, Dan Chisarick, Berthe Choueiry,
Roberto Cipolla, David Cohen, James Coleman, Julie Ann Comparini, Corinna Cortes, Gary
Cottrell, Ernest Davis, Tom Dean, Rina Dechter, Tom Dietterich, Peter Drake, Chuck Dyer,
John Fosler, Jeremy Frank, Alex Franz, Bob Futrelle, Marek Galecki, Stefan Gerberding,
bara Grosz, Larry Hall, Steve Hanks, Othar Hansson, Ernst Heinz, Jim Hendler, Christoph
Herrmann, Paul Hilﬁnger, Robert Holte, Vasant Honavar, Tim Huang, Seth Hutchinson, Joost
Jacob, Mark Jelasity, Magnus Johansson, Istvan Jonyer, Dan Jurafsky, Leslie Kaelbling, Keiji
Kanazawa, Surekha Kasibhatla, Simon Kasif, Henry Kautz, Gernot Kerschbaumer, Max
Khesin, Richard Kirby, Dan Klein, Kevin Knight, Roland Koenig, Sven Koenig, Daphne
son, John Lazzaro, Jon LeBlanc, Jason Leatherman, Frank Lee, Jon Lehto, Edward Lim,
Phil Long, Pierre Louveaux, Don Loveland, Sridhar Mahadevan, Tony Mancill, Jim Martin,
Andy Mayer, John McCarthy, David McGrane, Jay Mendelsohn, Risto Miikkulanien, Brian
Milch, Steve Minton, Vibhu Mittal, Mehryar Mohri, Leora Morgenstern, Stephen Muggleton,
Kevin Murphy, Ron Musick, Sung Myaeng, Eric Nadeau, Lee Naish, Pandu Nayak, Bernhard
Nebel, Stuart Nelson, XuanLong Nguyen, Nils Nilsson, Illah Nourbakhsh, Ali Nouri, ArthurPreface
xi
raine Prior, Greg Provan, William Rapaport, Deepak Ravichandran, Ioannis Refanidis, Philip
Resnik, Francesca Rossi, Sam Roweis, Richard Russell, Jonathan Schaeffer, Richard Scherl,
lik, Yoram Singer, Satinder Singh, Daniel Sleator, David Smith, Bryan So, Robert Sproull,
Lynn Stein, Larry Stephens, Andreas Stolcke, Paul Stradling, Devika Subramanian, Marek
Suchenek, Rich Sutton, Jonathan Tash, Austin Tate, Bas Terwijn, Olivier Teytaud, Michael
Thielscher, William Thompson, Sebastian Thrun, Eric Tiedemann, Mark Torrance, Randall
Upham, Paul Utgoff, Peter van Beek, Hal Varian, Paulina Varshavskaya, Sunil Vemuri, Vandi
Verma, Ubbo Visser, Jim Waldo, Toby Walsh, Bonnie Webber, Dan Weld, Michael Wellman,
Kamin Whitehouse, Michael Dean White, Brian Williams, David Wolfe, Jason Wolfe, Bill
Woods, Alden Wright, Jay Yagnik, Mark Yasuda, Richard Yen, Eliezer Yudkowsky, Weixiong
Zhang, Ming Zhao, Shlomo Zilberstein, and our esteemed colleague Anonymous Reviewer.About the Authors
where he is a professor of computer science, director of the Center for Intelligent Systems,
Forsythe Memorial Lectures at Stanford University. He is a Fellow and former Executive
Council member of the American Association for Artiﬁcial Intelligence. He has published
The Use of Knowledge in Analogy and Induction and (with Eric Wefald) Do the Right Thing:
Studies in Limited Rationality.
ously, he was head of the Computational Sciences Division at NASA Ames Research Center,
where he oversaw NASA’s research and development in artiﬁcial intelligence and robotics,
and chief scientist at Junglee, where he helped develop one of the ﬁrst Internet information
extraction services. He received a B.S. in applied mathematics from Brown University and
a Ph.D. in computer science from the University of California at Berkeley. He received the
Distinguished Alumni and Engineering Innovation awards from Berkeley and the Exceptional
ifornia and a research faculty member at Berkeley. His other books are Paradigms of AI
xiiINTRODUCTION
In which we try to explain why we consider artiﬁcial intelligence to be a subject
most worthy of study, and in which we try to decide what exactly it is, this being a
good thing to decide before embarking.
We call ourselves Homo sapiens—man the wise—because our intelligence is so important
INTELLIGENCE
to us. For thousands of years, we have tried to understand how we think; that is, how a mere
handful of matter can perceive, understand, predict, and manipulate a world far larger and
more complicated than itself. The ﬁeld of artiﬁcial intelligence, or AI, goes further still: it
ARTIFICIAL
INTELLIGENCE
attempts not just to understand but also to build intelligent entities.
AI is one of the newest ﬁelds in science and engineering. Work started in earnest soon
AI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other disciplines.
A student in physics might reasonably feel that all the good ideas have already been taken by
Galileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings for several
AI currently encompasses a huge variety of subﬁelds, ranging from the general (learning
and perception) to the speciﬁc, such as playing chess, proving mathematical theorems, writing
poetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any
intellectual task; it is truly a universal ﬁeld.
WHAT IS AI?
eight deﬁnitions of AI, laid out along two dimensions. The deﬁnitions on top are concerned
with thought processes and reasoning, whereas the ones on the bottom address behavior. The
deﬁnitions on the left measure success in terms of ﬁdelity to human performance, whereas
the ones on the right measure against an ideal performance measure, called rationality. A
RATIONALITY
system is rational if it does the “right thing,” given what it knows.
Historically, all four approaches to AI have been followed, each by different peopleChapter
Introduction
Thinking Humanly
Thinking Rationally
ers think . . . machines with minds, in the
“The study of mental faculties through the
use of computational models.”
“[The automation of] activities that we
associate with human thinking, activities
“The study of the computations that make
it possible to perceive, reason, and act.”
Acting Humanly
Acting Rationally
form functions that require intelligence
when performed by people.” (Kurzweil,
“Computational Intelligence is the study
of the design of intelligent agents.” (Poole
“The study of how to make computers do
things at which, at the moment, people are
Some deﬁnitions of artiﬁcial intelligence, organized into four categories.
volving observations and hypotheses about human behavior. A rationalist1 approach involves
a combination of mathematics and engineering. The various group have both disparaged and
helped each other. Let us look at the four approaches in more detail.
Acting humanly: The Turing Test approach
TURING TEST
operational deﬁnition of intelligence. A computer passes the test if a human interrogator, after
posing some written questions, cannot tell whether the written responses come from a person
really be intelligent if it passed. For now, we note that programming a computer to pass a
rigorously applied test provides plenty to work on. The computer would need to possess the
following capabilities:
• natural language processing to enable it to communicate successfully in English;
NATURAL LANGUAGE
PROCESSING
• knowledge representation to store what it knows or hears;
KNOWLEDGE
REPRESENTATION
• automated reasoning to use the stored information to answer questions and to draw
AUTOMATED
REASONING
new conclusions;
• machine learning to adapt to new circumstances and to detect and extrapolate patterns.
MACHINE LEARNING
“irrational” in the sense of “emotionally unstable” or “insane.” One merely need note that we are not perfect:
not all chess players are grandmasters; and, unfortunately, not everyone gets an A on the exam. Some systematicWhat Is AI?
Turing’s test deliberately avoided direct physical interaction between the interrogator and the
computer, because physical simulation of a person is unnecessary for intelligence. However,
TOTAL TURING TEST
subject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical
objects “through the hatch.” To pass the total Turing Test, the computer will need
• computer vision to perceive objects, and
COMPUTER VISION
• robotics to manipulate objects and move about.
ROBOTICS
These six disciplines compose most of AI, and Turing deserves credit for designing a test
telligence than to duplicate an exemplar. The quest for “artiﬁcial ﬂight” succeeded when the
ing about aerodynamics. Aeronautical engineering texts do not deﬁne the goal of their ﬁeld
as making “machines that ﬂy so exactly like pigeons that they can fool even other pigeons.”
Thinking humanly: The cognitive modeling approach
If we are going to say that a given program thinks like a human, we must have some way of
determining how humans think. We need to get inside the actual workings of human minds.
There are three ways to do this: through introspection—trying to catch our own thoughts as
they go by; through psychological experiments—observing a person in action; and through
brain imaging—observing the brain in action. Once we have a sufﬁciently precise theory of
the mind, it becomes possible to express the theory as a computer program. If the program’s
input–output behavior matches corresponding human behavior, that is evidence that some of
the program’s mechanisms could also be operating in humans. For example, Allen Newell
and Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon,
more concerned with comparing the trace of its reasoning steps to traces of human subjects
solving the same problems. The interdisciplinary ﬁeld of cognitive science brings together
COGNITIVE SCIENCE
computer models from AI and experimental techniques from psychology to construct precise
and testable theories of the human mind.
Cognitive science is a fascinating ﬁeld in itself, worthy of several textbooks and at least
differences between AI techniques and human cognition. Real cognitive science, however, is
necessarily based on experimental investigation of actual humans or animals. We will leave
that for other books, as we assume the reader has only a computer for experimentation.
In the early days of AI there was often confusion between the approaches: an author
would argue that an algorithm performs well on a task and that it is therefore a good model
of human performance, or vice versa. Modern authors separate the two kinds of claims;
this distinction has allowed both AI and cognitive science to develop more rapidly. The two
ﬁelds continue to fertilize each other, most notably in computer vision, which incorporates
neurophysiological evidence into computational models.Chapter
Introduction
Thinking rationally: The “laws of thought” approach
The Greek philosopher Aristotle was one of the ﬁrst to attempt to codify “right thinking,” that
is, irrefutable reasoning processes. His syllogisms provided patterns for argument structures
SYLLOGISM
that always yielded correct conclusions when given correct premises—for example, “Socrates
is a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought were
supposed to govern the operation of the mind; their study initiated the ﬁeld called logic.
LOGIC
Logicians in the 19th century developed a precise notation for statements about all kinds
of objects in the world and the relations among them. (Contrast this with ordinary arithmetic
that could, in principle, solve any solvable problem described in logical notation. (Although
LOGICIST
artiﬁcial intelligence hopes to build on such programs to create intelligent systems.
There are two main obstacles to this approach. First, it is not easy to take informal
knowledge and state it in the formal terms required by logical notation, particularly when
a problem “in principle” and solving it in practice. Even problems with just a few hundred
facts can exhaust the computational resources of any computer unless it has some guidance
as to which reasoning steps to try ﬁrst. Although both of these obstacles apply to any attempt
to build computational reasoning systems, they appeared ﬁrst in the logicist tradition.
Acting rationally: The rational agent approach
An agent is just something that acts (agent comes from the Latin agere, to do). Of course,
AGENT
all computer programs do something, but computer agents are expected to do more: operate
autonomously, perceive their environment, persist over a prolonged time period, adapt to
change, and create and pursue goals. A rational agent is one that acts so as to achieve the
RATIONAL AGENT
best outcome or, when there is uncertainty, the best expected outcome.
ing correct inferences is sometimes part of being a rational agent, because one way to act
rationally is to reason logically to the conclusion that a given action will achieve one’s goals
ality; in some situations, there is no provably correct thing to do, but something must still be
done. There are also ways of acting rationally that cannot be said to involve inference. For
example, recoiling from a hot stove is a reﬂex action that is usually more successful than a
slower action taken after careful deliberation.
All the skills needed for the Turing Test also allow an agent to act rationally. Knowledge
representation and reasoning enable agents to reach good decisions. We need to be able to
generate comprehensible sentences in natural language to get by in a complex society. We
need learning not only for erudition, but also because it improves our ability to generate
effective behavior.
is more general than the “laws of thought” approach because correct inference is just one
of several possible mechanisms for achieving rationality. Second, it is more amenable toThe Foundations of Artiﬁcial Intelligence
scientiﬁc development than are approaches based on human behavior or human thought. The
standard of rationality is mathematically well deﬁned and completely general, and can be
“unpacked” to generate agent designs that provably achieve it. Human behavior, on the other
hand, is well adapted for one speciﬁc environment and is deﬁned by, well, the sum total
of all the things that humans do. This book therefore concentrates on general principles
of rational agents and on components for constructing them. We will see that despite the
apparent simplicity with which the problem can be stated, an enormous variety of issues
One important point to keep in mind: We will see before too long that achieving perfect
rationality—always doing the right thing—is not feasible in complicated environments. The
computational demands are just too high. For most of the book, however, we will adopt the
working hypothesis that perfect rationality is a good starting point for analysis. It simpliﬁes
the problem and provides the appropriate setting for most of the foundational material in
LIMITED
RATIONALITY
appropriately when there is not enough time to do all the computations one might like.
THE FOUNDATIONS OF ARTIFICIAL INTELLIGENCE
In this section, we provide a brief history of the disciplines that contributed ideas, viewpoints,
and techniques to AI. Like any history, this one is forced to concentrate on a small number
of people, events, and ideas and to ignore others that also were important. We organize the
history around a series of questions. We certainly would not wish to give the impression that
these questions are the only ones the disciplines address or that the disciplines have all been
working toward AI as their ultimate fruition.
Philosophy
• Can formal rules be used to draw valid conclusions?
• How does the mind arise from a physical brain?
• Where does knowledge come from?
• How does knowledge lead to action?
to formulate a precise set of laws governing the rational part of the mind. He developed an
the idea that useful reasoning could actually be carried out by a mechanical artifact. Thomas
and subtract in our silent thoughts.” The automation of computation itself was already well
chanical calculator; recent reconstructions have shown the design to be functional. The ﬁrstChapter
Introduction
is more famous. Pascal wrote that “the arithmetical machine produces effects which appear
built a mechanical device intended to carry out operations on concepts rather than numbers,
but its scope was rather limited. Leibniz did surpass Pascal by building a calculator that
tract. Some speculated that machines might not just do calculations but actually be able to
of an “artiﬁcial animal,” arguing “For what is the heart but a spring; and the nerves, but so
many strings; and the joints, but so many wheels.”
It’s one thing to say that the mind operates, at least in part, according to logical rules, and
to build physical systems that emulate some of those rules; it’s another to say that the mind
of the distinction between mind and matter and of the problems that arise. One problem with
a purely physical conception of the mind is that it seems to leave little room for free will:
if the mind is governed entirely by physical laws, then it has no more free will than a rock
“deciding” to fall toward the center of the earth. Descartes was a strong advocate of the power
of reasoning in understanding the world, a philosophy now called rationalism, and one that
RATIONALISM
counts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.
DUALISM
He held that there is a part of the human mind (or soul or spirit) that is outside of nature,
exempt from physical laws. Animals, on the other hand, did not possess this dual quality;
they could be treated as machines. An alternative to dualism is materialism, which holds
MATERIALISM
that the brain’s operation according to the laws of physics constitutes the mind. Free will is
simply the way that the perception of available choices appears to the choosing entity.
Given a physical mind that manipulates knowledge, the next problem is to establish
EMPIRICISM
induction: that general rules are acquired by exposure to repeated associations between their
INDUCTION
doctrine of logical positivism. This doctrine holds that all knowledge can be characterized by
LOGICAL POSITIVISM
logical theories connected, ultimately, to observation sentences that correspond to sensory
OBSERVATION
SENTENCES
CONFIRMATION
THEORY
explicit computational procedure for extracting knowledge from elementary experiences. It
was probably the ﬁrst theory of mind as a computational process.
seen as both an empiricist and a rationalist.
of the meaning of the words. Because this rules out most of metaphysics, as was the intention, logical positivism
was unpopular in some circles.The Foundations of Artiﬁcial Intelligence
The ﬁnal element in the philosophical picture of the mind is the connection between
knowledge and action. This question is vital to AI because intelligence requires action as well
as reasoning. Moreover, only by understanding how actions are justiﬁed can we understand
how to build an agent whose actions are justiﬁable (or rational). Aristotle argued (in De Motu
Animalium) that actions are justiﬁed by a logical connection between goals and knowledge of
the action’s outcome (the last part of this extract also appears on the front cover of this book,
in the original Greek):
But how does it happen that thinking is sometimes accompanied by action and sometimes
not, sometimes by motion, and sometimes not? It looks as if almost the same thing
happens as in the case of reasoning and making inferences about unchanging objects. But
in that case the end is a speculative proposition . . . whereas here the conclusion which
results from the two premises is an action. . . . I need covering; a cloak is a covering. I
need a cloak. What I need, I have to make; I need a cloak. I have to make a cloak. And
the conclusion, the “I have to make a cloak,” is an action.
suggesting an algorithm:
We deliberate not about ends, but about means. For a doctor does not deliberate whether
he shall heal, nor an orator whether he shall persuade, . . . They assume the end and
consider how and by what means it is attained, and if it seems easily and best produced
thereby; while if it is achieved by one means only they consider how it will be achieved
by this and by what means this will be achieved, till they come to the ﬁrst cause, . . . and
what is last in the order of analysis seems to be ﬁrst in the order of becoming. And if we
come on an impossibility, we give up the search, e.g., if we need money and this cannot
be got; but if a thing appears possible we try to do it.
correctly described a quantitative formula for deciding what action to take in cases like this
the idea of rational decision criteria in all spheres of human activity. The more formal theory
of decisions is discussed in the following section.
Mathematics
• What are the formal rules to draw valid conclusions?
• What can be computed?
• How do we reason with uncertain information?
Philosophers staked out some of the fundamental ideas of AI, but the leap to a formal science
tion, and probability.
The idea of formal logic can be traced back to the philosophers of ancient Greece, butChapter
Introduction
that shows how to relate the objects in a logic to objects in the real world.
putation. The ﬁrst nontrivial algorithm is thought to be Euclid’s algorithm for computing
ALGORITHM
greatest common divisors. The word algorithm (and the idea of studying them) comes from
Arabic numerals and algebra to Europe. Boole and others discussed algorithms for logical
tion do exist. His incompleteness theorem showed that in any formal theory as strong as
INCOMPLETENESS
THEOREM
Peano arithmetic (the elementary theory of natural numbers), there are true statements that
are undecidable in the sense that they have no proof within the theory.
This fundamental result can also be interpreted as showing that some functions on the
integers cannot be represented by an algorithm—that is, they cannot be computed. This
putable—capable of being computed. This notion is actually slightly problematic because
COMPUTABLE
the notion of a computation or effective procedure really cannot be given a formal deﬁnition.
capable of computing any computable function, is generally accepted as providing a sufﬁcient
deﬁnition. Turing also showed that there were some functions that no Turing machine can
compute. For example, no machine can tell in general whether a given program will return
an answer on a given input or run forever.
tion, the notion of tractability has had an even greater impact. Roughly speaking, a problem
TRACTABILITY
is called intractable if the time required to solve instances of the problem grows exponentially
with the size of the instances. The distinction between polynomial and exponential growth
important because exponential growth means that even moderately large instances cannot be
solved in any reasonable time. Therefore, one should strive to divide the overall problem of
generating intelligent behavior into tractable subproblems rather than intractable ones.
never became popular.The Foundations of Artiﬁcial Intelligence
problems are necessarily intractable, most theoreticians believe it.) These results contrast
with the optimism with which the popular press greeted the ﬁrst computers—“Electronic
careful use of resources will characterize intelligent systems. Put crudely, the world is an
extremely large problem instance! Work in AI has helped explain why some instances of
Besides logic and computation, the third great contribution of mathematics to AI is the
PROBABILITY
dict the future of an unﬁnished gambling game and assign average payoffs to the gamblers.
Probability quickly became an invaluable part of all the quantitative sciences, helping to deal
a rule for updating probabilities in the light of new evidence. Bayes’ rule underlies most
modern approaches to uncertain reasoning in AI systems.
Economics
• How should we make decisions so as to maximize payoff?
• How should we do this when others may not go along?
• How should we do this when the payoff may be far in the future?
While the ancient Greeks and others had made contributions to economic thought, Smith was
economics as being about money, but economists will say that they are really studying how
people make choices that lead to preferred outcomes. When McDonald’s offers a hamburger
for a dollar, they are asserting that they would prefer the dollar and hoping that customers will
prefer the hamburger. The mathematical treatment of “preferred outcomes” or utility was
UTILITY
DECISION THEORY
mal and complete framework for decisions (economic or otherwise) made under uncertainty—
that is, in cases where probabilistic descriptions appropriately capture the decision maker’s
environment. This is suitable for “large” economies where each agent need pay no attention
to the actions of other agents as individuals. For “small” economies, the situation is much
more like a game: the actions of one player can signiﬁcantly affect the utility of another
(either positively or negatively). Von Neumann and Morgenstern’s development of game
GAME THEORYChapter
Introduction
cision theory, game theory does not offer an unambiguous prescription for selecting actions.
For the most part, economists did not address the third question listed above, namely,
sult from several actions taken in sequence. This topic was pursued in the ﬁeld of operations
OPERATIONS
RESEARCH
tions, and later found civilian applications in complex management decisions. The work of
tional agents, yet for many years AI research developed along entirely separate paths. One
work showing that models based on satisﬁcing—making decisions that are “good enough,”
SATISFICING
rather than laboriously calculating an optimal decision—gave a better description of actual
Neuroscience
• How do brains process information?
Neuroscience is the study of the nervous system, particularly the brain. Although the exact
NEUROSCIENCE
way in which the brain enables thought is one of the great mysteries of science, the fact that it
does enable thought has been appreciated for thousands of years because of the evidence that
strong blows to the head can lead to mental incapacitation. It has also long been known that
18th century that the brain was widely recognized as the seat of consciousness. Before then,
candidate locations included the heart and the spleen.
cognitive functions. In particular, he showed that speech production was localized to the
NEURON
which neurons were embedded, whereas Cajal propounded the “neuronal doctrine.” The two shared the NobelThe Foundations of Artiﬁcial Intelligence
Axon
Cell body or Soma
Nucleus
Dendrite
Synapses
Axonal arborization
Axon from another cell
Synapse
The parts of a nerve cell or neuron. Each neuron consists of a cell body,
or soma, that contains a cell nucleus. Branching out from the cell body are a number of
ﬁbers called dendrites and a single long ﬁber called the axon. The axon stretches out for a
long distance, much longer than the scale in this diagram indicates. Typically, an axon is
propagated from neuron to neuron by a complicated electrochemical reaction. The signals
of neurons. These mechanisms are thought to form the basis for learning in the brain. Most
information processing goes on in the cerebral cortex, the outer layer of the brain. The basic
We now have some data on the mapping between areas of the brain and the parts of the
body that they control or from which they receive sensory input. Such mappings are able to
change radically over the course of a few weeks, and some animals seem to have multiple
maps. Moreover, we do not fully understand how other areas can take over functions when
one area is damaged. There is almost no theory on how an individual memory is stored.
Berger of the electroencephalograph (EEG). The recent development of functional magnetic
roscientists unprecedentedly detailed images of brain activity, enabling measurements that
correspond in interesting ways to ongoing cognitive processes. These are augmented by
output relationships to be mapped. Despite these advances, we are still a long way from
understanding how cognitive processes actually work.
The truly amazing conclusion is that a collection of simple cells can lead to thought,Chapter
Introduction
Supercomputer
Personal Computer
Human Brain
Computational units
Storage units
Cycle time
A crude comparison of the raw computational resources available to the IBM
brain. The personal computer lags behind on all metrics except cycle time.
The only real alternative theory is mysticism: that minds operate in some mystical realm that
is beyond physical science.
computers have a cycle time that is a million times faster than a brain. The brain makes up
although the largest supercomputers have a capacity that is similar to the brain’s. (It should
be noted, however, that the brain does not seem to use all of its neurons simultaneously.)
Futurists make much of these numbers, pointing to an approaching singularity at which
SINGULARITY
raw comparisons are not especially informative. Even with a computer of virtually unlimited
capacity, we still would not know how to achieve the brain’s level of intelligence.
Psychology
• How do humans and animals think and act?
Helmholtz applied the scientiﬁc method to the study of human vision, and his Handbook
of Physiological Optics is even now described as “the single most important treatise on the
ﬁrst laboratory of experimental psychology, at the University of Leipzig. Wundt insisted
sociative task while introspecting on their thought processes. The careful controls went a
long way toward making psychology a science, but the subjective nature of the data made
it unlikely that an experimenter would ever disconﬁrm his or her own theories. Biologists
the Lower Organisms. Applying this viewpoint to humans, the behaviorism movement, led
BEHAVIORISMThe Foundations of Artiﬁcial Intelligence
that introspection could not provide reliable evidence. Behaviorists insisted on studying only
objective measures of the percepts (or stimulus) given to an animal and its resulting actions
(or response). Behaviorism discovered a lot about rats and pigeons but had less success at
understanding humans.
COGNITIVE
PSYCHOLOGY
insisted that perception involved a form of unconscious logical inference. The cognitive
able to ﬂourish. The Nature of Explanation, by Bartlett’s student and successor Kenneth
goals, arguing that they are just as scientiﬁc as, say, using pressure and temperature to talk
about gases, despite their being made of molecules that have neither. Craik speciﬁed the
explained why this was a good design for an agent:
actions within its head, it is able to try out various alternatives, conclude which is the best
of them, react to future situations before they arise, utilize the knowledge of past events
in dealing with the present and future, and in every way to react in a much fuller, safer,
psychological phenomena as information processing. Meanwhile, in the United States, the
development of computer modeling led to the creation of the ﬁeld of cognitive science. The
this is just two months after the conference at which AI itself was “born.”) At the workshop,
George Miller presented The Magic Number Seven, Noam Chomsky presented Three Models
of Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine.
These three inﬂuential papers showed how computer models could be used to address the
psychology of memory, language, and logical thinking, respectively. It is now a common
(although far from universal) view among psychologists that “a cognitive theory should be
processing mechanism whereby some cognitive function might be implemented.
Computer engineering
• How can we build an efﬁcient computer?
For artiﬁcial intelligence to succeed, we need two things: intelligence and an artifact. The
vented independently and almost simultaneously by scientists in three countries embattled inChapter
Introduction
at Iowa State University. Atanasoff’s research received little support or recognition; it was
the ENIAC, developed as part of a secret military project at the University of Pennsylvania
by a team including John Mauchly and John Eckert, that proved to be the most inﬂuential
forerunner of modern computers.
Since that time, each generation of computer hardware has brought an increase in speed
CPU cores rather than the clock speed. Current expectations are that future increases in power
will come from massive parallelism—a curious convergence with the properties of the brain.
Of course, there were calculating devices before the electronic computer. The earliest
pleted. The Difference Engine was intended to compute mathematical tables for engineering
ble of universal computation. Babbage’s colleague Ada Lovelace, daughter of the poet Lord
Byron, was perhaps the world’s ﬁrst programmer. (The programming language Ada is named
after her.) She wrote programs for the unﬁnished Analytical Engine and even speculated that
the machine could play chess or compose music.
AI also owes a debt to the software side of computer science, which has supplied the
operating systems, programming languages, and tools needed to write modern programs (and
neered many ideas that have made their way back to mainstream computer science, including
velopment environments, the linked list data type, automatic storage management, and key
tions for everyday tasks such as buttering toast.The Foundations of Artiﬁcial Intelligence
Control theory and cybernetics
• How can artifacts operate under their own control?
with a regulator that maintained a constant ﬂow rate. This invention changed the deﬁnition
of what an artifact could do. Previously, only living things could modify their behavior in
The mathematical theory of stable feedback systems was developed in the 19th century.
The central ﬁgure in the creation of what is now called control theory was Norbert
CONTROL THEORY
sell, among others, before developing an interest in biological and mechanical control systems
and their connection to cognition. Like Craik (who also used control systems as psychological
models), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the
ing from a regulatory mechanism trying to minimize “error”—the difference between current
state and goal state. In the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts,
and John von Neumann, organized a series of inﬂuential conferences that explored the new
CYBERNETICS
came a bestseller and awoke the public to the possibility of artiﬁcially intelligent machines.
Turing, Grey Walter, and others formed the Ratio Club for “those who had Wiener’s ideas
HOMEOSTATIC
priate feedback loops to achieve stable adaptive behavior.
Modern control theory, especially the branch known as stochastic optimal control, has
as its goal the design of systems that maximize an objective function over time. This roughly
OBJECTIVE
FUNCTION
matches our view of AI: designing systems that behave optimally. Why, then, are AI and
control theory two different ﬁelds, despite the close connections among their founders? The
answer lies in the close coupling between the mathematical techniques that were familiar to
the participants and the corresponding sets of problems that were encompassed in each world
view. Calculus and matrix algebra, the tools of control theory, lend themselves to systems that
are describable by ﬁxed sets of continuous variables, whereas AI was founded in part as a way
to escape from the these perceived limitations. The tools of logical inference and computation
allowed AI researchers to consider problems such as language, vision, and planning that fell
completely outside the control theorist’s purview.
Linguistics
• How does language relate to thought?
count of the behaviorist approach to language learning, written by the foremost expert inChapter
Introduction
the ﬁeld. But curiously, a review of the book became as well known as the book itself, and
served to almost kill off interest in behaviorism. The author of the review was the linguist
Noam Chomsky, who had just published a book on his own theory, Syntactic Structures.
Chomsky pointed out that the behaviorist theory did not address the notion of creativity in
language—it did not explain how a child could understand and make up sentences that he or
she had never heard before. Chomsky’s theory—based on syntactic models going back to the
formal enough that it could in principle be programmed.
Modern linguistics and AI, then, were “born” at about the same time, and grew up
together, intersecting in a hybrid ﬁeld called computational linguistics or natural language
COMPUTATIONAL
LINGUISTICS
processing. The problem of understanding language soon turned out to be considerably more
subject matter and context, not just an understanding of the structure of sentences. This might
seem obvious, but it was not widely appreciated until the 1960s. Much of the early work in
knowledge representation (the study of how to put knowledge into a form that a computer
can reason with) was tied to language and informed by research in linguistics, which was
connected in turn to decades of work on the philosophical analysis of language.
THE HISTORY OF ARTIFICIAL INTELLIGENCE
With the background material behind us, we are ready to cover the development of AI itself.
The ﬁrst work that is now generally recognized as AI was done by Warren McCulloch and
function of neurons in the brain; a formal analysis of propositional logic due to Russell and
Whitehead; and Turing’s theory of computation. They proposed a model of artiﬁcial neurons
in which each neuron is characterized as being “on” or “off,” with a switch to “on” occurring
in response to stimulation by a sufﬁcient number of neighboring neurons. The state of a
neuron was conceived of as “factually equivalent to a proposition which proposed its adequate
stimulus.” They showed, for example, that any computable function could be computed by
some network of connected neurons, and that all the logical connectives (and, or, not, etc.)
could be implemented by simple net structures. McCulloch and Pitts also suggested that
rule for modifying the connection strengths between neurons. His rule, now called Hebbian
learning, remains an inﬂuential model to this day.
HEBBIAN LEARNING
Two undergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the
His Ph.D. committee was skeptical about whether this kind of work should be consideredThe History of Artiﬁcial Intelligence
mathematics, but von Neumann reportedly said, “If it isn’t now, it will be someday.” Minsky
was later to prove inﬂuential theorems showing the limitations of neural network research.
There were a number of early examples of work that can be characterized as AI, but
Alan Turing’s vision was perhaps the most inﬂuential. He gave lectures on the topic as early
article “Computing Machinery and Intelligence.” Therein, he introduced the Turing Test,
machine learning, genetic algorithms, and reinforcement learning. He proposed the Child
Programme idea, explaining “Instead of trying to produce a programme to simulate the adult
mind, why not rather try to produce one which simulated the child’s?”
Princeton was home to another inﬂuential ﬁgure in AI, John McCarthy. After receiving his
ford and then to Dartmouth College, which was to become the ofﬁcial birthplace of the ﬁeld.
McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring
shire. The study is to proceed on the basis of the conjecture that every aspect of
scribed that a machine can be made to simulate it. An attempt will be made to ﬁnd
how to make machines use language, form abstractions and concepts, solve kinds
of problems now reserved for humans, and improve themselves. We think that a
signiﬁcant advance can be made in one or more of these problems if a carefully
selected group of scientists work on it together for a summer.
from IBM, and Ray Solomonoff and Oliver Selfridge from MIT.
cations such as checkers, Newell and Simon already had a reasoning program, the Logic
Theorist (LT), about which Simon claimed, “We have invented a computer program capable
would have been more precise and less threatening, but “AI” has stuck. At the 50th anniversary of the Dartmouth
conference, McCarthy stated that he resisted the terms “computer” or “computational” in deference to Norbert
Weiner, who was promoting analog cybernetic devices rather than digital computers.
translated it into machine code by hand. To avoid errors, they worked in parallel, calling out binary numbers to
each other as they wrote each instruction to make sure they agreed.Chapter
Introduction
sell and Whitehead’s Principia Mathematica. Russell was reportedly delighted when Simon
showed him that the program had come up with a proof for one theorem that was shorter than
the one in Principia. The editors of the Journal of Symbolic Logic were less impressed; they
rejected a paper coauthored by Newell, Simon, and Logic Theorist.
The Dartmouth workshop did not lead to any new breakthroughs, but it did introduce
these people and their students and colleagues at MIT, CMU, Stanford, and IBM.
see why it was necessary for AI to become a separate ﬁeld. Why couldn’t all the work done
in AI have taken place under the name of control theory or operations research or decision
theory, which, after all, have objectives similar to those of AI? Or why isn’t AI a branch
of mathematics? The ﬁrst answer is that AI from the start embraced the idea of duplicating
ﬁelds were addressing these issues. The second answer is methodology. AI is the only one
of these ﬁelds that is clearly a branch of computer science (although operations research does
share an emphasis on computer simulations), and AI is the only ﬁeld to attempt to build
machines that will function autonomously in complex, changing environments.
ers and programming tools of the time and the fact that only a few years earlier computers
puter did anything remotely clever. The intellectual establishment, by and large, preferred to
by Turing.) AI researchers naturally responded by demonstrating one X after another. John
McCarthy referred to this period as the “Look, Ma, no hands!” era.
Newell and Simon’s early success was followed up with the General Problem Solver,
or GPS. Unlike Logic Theorist, this program was designed from the start to imitate human
that the order in which the program considered subgoals and possible actions was similar to
symbol system hypothesis, which states that “a physical symbol system has the necessary and
PHYSICAL SYMBOL
SYSTEM
sufﬁcient means for general intelligent action.” What they meant is that any system (human
or machine) exhibiting intelligence must operate by manipulating data structures composed
of symbols. We will see later that this hypothesis has been challenged from many directions.
grams.
able to prove theorems that many students of mathematics would ﬁnd quite tricky. StartingThe History of Artiﬁcial Intelligence
ers can do only what they are told to: his program quickly learned to play a better game than
impression. Like Turing, Samuel had trouble ﬁnding computer time. Working at night, he
LISP
puting resources was also a serious problem. In response, he and others at MIT invented time
in which he described the Advice Taker, a hypothetical program that can be seen as the ﬁrst
complete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthy’s
program was designed to use knowledge to search for solutions to problems. But unlike the
others, it was to embody general knowledge of the world. For example, he showed how
some simple axioms would enable the program to generate a plan to drive to the airport. The
program was also designed to accept new axioms in the normal course of operation, thereby
allowing it to achieve competence in new areas without being reprogrammed. The Advice
Taker thus embodied the central principles of knowledge representation and reasoning: that
it is useful to have a formal, explicit representation of the world and its workings and to be
able to manipulate that representation with deductive processes. It is remarkable how much
mal logic, whereas Minsky was more interested in getting programs to work and eventually
systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute
complete integration of logical reasoning and physical activity.
Minsky supervised a series of students who chose limited problems that appeared to
require intelligence to solve. These limited domains became known as microworlds. James
MICROWORLD
solved algebra story problems, such as the following:
number of customers Tom gets?Chapter
Introduction
Red
Green
Red
Green
Green
Blue
Blue
Red
the command “Find a block which is taller than the one you are holding and put it in the box.”
The most famous microworld was the blocks world, which consists of a set of solid blocks
A typical task in this world is to rearrange the blocks in a certain way, using a robot hand
that can pick up one block at a time. The blocks world was home to the vision project of
Early work building on the neural networks of McCulloch and Pitts also ﬂourished.
collectively represent an individual concept, with a corresponding increase in robustness and
parallelism. Hebb’s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,
the learning algorithm can adjust the connection strengths of a perceptron to match any input
From the beginning, AI researchers were not shy about making predictions of their coming
It is not my aim to surprise or shock you—but the simplest way I can summarize is to say
that there are now in the world machines that think, that learn and that create. Moreover,The History of Artiﬁcial Intelligence
their ability to do these things is going to increase rapidly until—in a visible future—the
range of problems they can handle will be coextensive with the range to which the human
mind has been applied.
Terms such as “visible future” can be interpreted in various ways, but Simon also made
a signiﬁcant mathematical theorem would be proved by machine. These predictions came
to the promising performance of early AI systems on simple examples. In almost all cases,
however, these early systems turned out to fail miserably when tried out on wider selections
of problems and on more difﬁcult problems.
The ﬁrst kind of difﬁculty arose because most early programs knew nothing of their
subject matter; they succeeded by means of simple syntactic manipulations. A typical story
tional Research Council in an attempt to speed up the translation of Russian scientiﬁc papers
formations based on the grammars of Russian and English, and word replacement from an
electronic dictionary, would sufﬁce to preserve the exact meanings of sentences. The fact is
that accurate translation requires background knowledge in order to resolve ambiguity and
establish the content of the sentence. The famous retranslation of “the spirit is willing but
translation of general scientiﬁc text, and none is in immediate prospect.” All U.S. government
perfect but widely used tool for technical, commercial, government, and Internet documents.
The second kind of difﬁculty was the intractability of many of the problems that AI was
attempting to solve. Most of the early AI programs solved problems by trying out different
combinations of steps until the solution was found. This strategy worked initially because
microworlds contained very few objects and hence very few possible actions and very short
solution sequences. Before the theory of computational complexity was developed, it was
widely thought that “scaling up” to larger problems was simply a matter of faster hardware
and larger memories. The optimism that accompanied the development of resolution theorem
ing more than a few dozen facts. The fact that a program can ﬁnd a solution in principle does
not mean that the program contains any of the mechanisms needed to ﬁnd it in practice.
MACHINE EVOLUTION
GENETIC
ALGORITHM
erate a program with good performance for any particular task. The idea, then, was to try
spite thousands of hours of CPU time, almost no progress was demonstrated. Modern genetic
algorithms use better representations and have shown more success.Chapter
Introduction
decision by the British government to end support for AI research in all but two universities.
(Oral tradition paints a somewhat different and more colorful picture, with political ambitions
and personal animosities whose description is beside the point.)
A third difﬁculty arose because of some fundamental limitations on the basic structures
shown to learn anything they were capable of representing, they could represent very little. In
studied) could not be trained to recognize when its two inputs were different. Although their
The picture of problem solving that had arisen during the ﬁrst decade of AI research was of
ﬁnd complete solutions. Such approaches have been called weak methods because, although
WEAK METHOD
general, they do not scale up to large or difﬁcult problem instances. The alternative to weak
steps and can more easily handle typically occurring cases in narrow areas of expertise. One
might say that to solve a hard problem, you have to almost know the answer already.
It was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon),
Bruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel
laureate geneticist) teamed up to solve the problem of inferring molecular structure from the
mentary formula of the molecule (e.g., C6H13NO2) and the mass spectrum giving the masses
of the various fragments of the molecule generated when it is bombarded by an electron beam.
of a methyl (CH3) fragment.
The naive version of the program generated all possible structures consistent with the
formula, and then predicted what mass spectrum would be observed for each, comparing this
molecules. The DENDRAL researchers consulted analytical chemists and found that they
substructures in the molecule. For example, the following rule is used to recognize a ketone
if there are two peaks at x1 and x2 such thatThe History of Artiﬁcial Intelligence
(d) At least one of x1 and x2 is high.
then there is a ketone subgroup
sible candidates enormously. DENDRAL was powerful because
All the relevant theoretical knowledge to solve these problems has been mapped over from
its general form in the [spectrum prediction component] (“ﬁrst principles”) to efﬁcient
incorporated the main theme of McCarthy’s Advice Taker approach—the clean separation of
the knowledge (in the form of rules) from the reasoning component.
gramming Project (HPP) to investigate the extent to which the new methodology of expert
systems could be applied to other areas of human expertise. The next major effort was in
EXPERT SYSTEMS
the area of medical diagnosis. Feigenbaum, Buchanan, and Dr. Edward Shortliffe developed
as well as some experts, and considerably better than junior doctors. It also contained two
major differences from DENDRAL. First, unlike the DENDRAL rules, no general theoretical
model existed from which the MYCIN rules could be deduced. They had to be acquired from
extensive interviewing of experts, who in turn acquired them from textbooks, other experts,
and direct experience of cases. Second, the rules had to reﬂect the uncertainty associated with
medical knowledge. MYCIN incorporated a calculus of uncertainty called certainty factors
CERTAINTY FACTOR
of evidence on the diagnosis.
The importance of domain knowledge was also apparent in the area of understanding
natural language. Although Winograd’s SHRDLU system for understanding natural language
had engendered a good deal of excitement, its dependence on syntactic analysis caused some
of the same problems as occurred in the early machine translation work. It was able to
overcome ambiguity and understand pronoun references, but this was mainly because it was
designed speciﬁcally for one area—the blocks world. Several researchers, including Eugene
Charniak, a fellow graduate student of Winograd’s at MIT, suggested that robust language
understanding would require general knowledge about the world and a general method for
using that knowledge.
“There is no such thing as syntax,” which upset a lot of linguists but did serve to start a useful
standing natural language. The emphasis, however, was less on language per se and more onChapter
Introduction
crease in the demands for workable knowledge representation schemes. A large number
of different representation and reasoning languages were developed. Some were based on
FRAMES
structured approach, assembling facts about particular object and event types and arranging
the types into a large taxonomic hierarchy analogous to a biological taxonomy.
The ﬁrst successful commercial expert system, R1, began operation at the Digital Equipment
corporation had its own AI group and was either using or investigating expert systems.
tronics and Computer Technology Corporation (MCC) as a research consortium designed to
assure national competitiveness. In both cases, AI was part of a broad effort, including chip
ambitious goals.
and software and hardware specialized for these purposes. Soon after that came a period
called the “AI Winter,” in which many companies fell by the wayside as they failed to deliver
on extravagant promises.
ing problems in computer science and psychology, and the widespread dissemination of the
caused great excitement.
CONNECTIONIST
rect competitors both to the symbolic models promoted by Newell and Simon and to the
at some level humans manipulate symbols—in fact, Terrence Deacon’s book The Symbolic
Artiﬁcial Intelligence had been ofﬁcially canceled.The History of Artiﬁcial Intelligence
dent connectionists questioned whether symbol manipulation had any real explanatory role in
detailed models of cognition. This question remains unanswered, but the current view is that
connectionist and symbolic approaches are complementary, not competing. As occurred with
the separation of AI and cognitive science, modern neural network research has bifurcated
into two ﬁelds, one concerned with creating effective network architectures and algorithms
and understanding their mathematical properties, the other concerned with careful modeling
of the empirical properties of actual neurons and ensembles of neurons.
Recent years have seen a revolution in both the content and the methodology of work in
AI was founded in part as a rebellion against the limitations of existing ﬁelds like control
In the early period of AI it seemed plausible that new forms of symbolic computation,
e.g., frames and semantic networks, made much of classical theory obsolete. This led to
a form of isolationism in which AI became largely separated from the rest of computer
science. This isolationism is currently being abandoned. There is a recognition that
machine learning should not be isolated from information theory, that uncertain reasoning
should not be isolated from stochastic modeling, that search should not be isolated from
classical optimization and control, and that automated reasoning should not be isolated
from formal methods and static analysis.
cepted, hypotheses must be subjected to rigorous empirical experiments, and the results must
experiments by using shared repositories of test data and code.
The ﬁeld of speech recognition illustrates the pattern. In the 1970s, a wide variety of
different architectures and approaches were tried. Many of these were rather ad hoc and
fragile, and were demonstrated on only a few specially selected examples. In recent years,
approaches based on hidden Markov models (HMMs) have come to dominate the area. Two
HIDDEN MARKOV
MODELS
aspects of HMMs are relevant. First, they are based on a rigorous mathematical theory. This
has allowed speech researchers to build on several decades of mathematical results developed
in other ﬁelds. Second, they are generated by a process of training on a large corpus of
real speech data. This ensures that the performance is robust, and in rigorous blind tests the
HMMs have been improving their scores steadily. Speech technology and the related ﬁeld of
handwritten character recognition are already making the transition to widespread industrial
grounded in mathematical rigor—over the scrufﬁes—those who would rather try out lots of ideas, write some
programs, and then assess what seems to be working. Both approaches are important. A shift toward neatness
implies that the ﬁeld has reached a level of stability and maturity. Whether that stability will be disrupted by a
new scruffy idea is another question.Chapter
Introduction
and consumer applications. Note that there is no scientiﬁc claim that humans use HMMs to
recognize speech; rather, HMMs provide a mathematical framework for understanding the
problem and support the engineering claim that they work well in practice.
Machine translation follows the same course as speech recognition. In the 1950s there
was initial enthusiasm for an approach based on sequences of words, with models learned
according to the principles of information theory. That approach fell out of favor in the
1960s, but returned in the late 1990s and now dominates the ﬁeld.
Neural networks also ﬁt this trend. Much of the work on neural nets in the 1980s was
done in an attempt to scope out what could be done and to learn how neural nets differ from
“traditional” techniques. Using improved methodology and theoretical frameworks, the ﬁeld
arrived at an understanding in which neural nets can now be compared with corresponding
techniques from statistics, pattern recognition, and machine learning, and the most promising
data mining technology has spawned a vigorous new industry.
DATA MINING
tance of probability and decision theory in AI, following a resurgence of interest epitomized
BAYESIAN NETWORK
formalism was invented to allow efﬁcient representation of, and rigorous reasoning with,
uncertain knowledge. This approach largely overcomes many problems of the probabilistic
ing and expert systems. The approach allows for learning from experience, and it combines
the best of classical AI and neural nets. Work by Judea Pearl (1982a) and by Eric Horvitz and
normative expert systems: ones that act rationally according to the laws of decision theory
tem includes several normative diagnostic expert systems for correcting problems. Chapters
Similar gentle revolutions have occurred in robotics, computer vision, and knowledge
bined with increased mathematical sophistication, has led to workable research agendas and
robust methods. Although increased formalization and specialization led ﬁelds such as vision
and robotics to become somewhat isolated from “mainstream” AI in the 1990s, this trend has
reversed in recent years as tools from machine learning in particular have proved effective for
many problems. The process of reintegration is already yielding signiﬁcant beneﬁts
Perhaps encouraged by the progress in solving the subproblems of AI, researchers have also
started to look at the “whole agent” problem again. The work of Allen Newell, John Laird,
of a complete agent architecture. One of the most important environments for intelligent
the “-bot” sufﬁx has entered everyday language. Moreover, AI technologies underlie manyThe History of Artiﬁcial Intelligence
Internet tools, such as search engines, recommender systems, and Web site aggregators.
One consequence of trying to build complete agents is the realization that the previously
isolated subﬁelds of AI might need to be reorganized somewhat when their results are to be
tied together. In particular, it is now widely appreciated that sensory systems (vision, sonar,
speech recognition, etc.) cannot deliver perfectly reliable information about the environment.
Hence, reasoning and planning systems must be able to handle uncertainty. A second major
consequence of the agent perspective is that AI has been drawn into much closer contact
with other ﬁelds, such as control theory and economics, that also deal with agents. Recent
progress in the control of robotic cars has derived from a mixture of approaches ranging from
Despite these successes, some inﬂuential founders of AI, including John McCarthy
ciﬁc task, such as driving a car, playing chess, or recognizing speech. Instead, they believe
AI should return to its roots of striving for, in Simon’s words, “machines that think, that learn
A related idea is the subﬁeld of Artiﬁcial General Intelligence or AGI (Goertzel and
ARTIFICIAL GENERAL
INTELLIGENCE
FRIENDLY AI
as the main subject of study. But some recent work in AI suggests that for many problems, it
makes more sense to worry about the data and be less picky about what algorithm to apply.
This is true because of the increasing availability of very large data sources: for example,
trillions of words of English and billions of images from the Web (Kilgarriff and Grefenstette,
biguation: given the use of the word “plant” in a sentence, does that refer to ﬂora or factory?
machine learning algorithms. Yarowsky showed that the task can be done, with accuracy
tated text and just the dictionary deﬁnitions of the two senses—“works, industrial plant” and
“ﬂora, plant life”—one can label examples in the corpus, and from there bootstrap to learnChapter
Introduction
like this perform even better as the amount of available text goes from a million words to a
billion and that the increase in performance from using more data exceeds any difference in
now you need to ﬁll in the masked area with something that matches the background. Hays
and Efros deﬁned an algorithm that searches through a collection of photos to ﬁnd something
that will match. They found the performance of their algorithm was poor when they used
a collection of only ten thousand photos, but crossed a threshold into excellent performance
when they grew the collection to two million photos.
Work like this suggests that the “knowledge bottleneck” in AI—the problem of how to
plications and have written that “AI Winter” may be yielding to a new Spring (Havenstein,
embedded in the infrastructure of every industry.”
THE STATE OF THE ART
What can AI do today? A concise answer is difﬁcult because there are so many activities in
so many subﬁelds. Here we sample a few applications; others appear throughout the book.
Robotic vehicles: A driverless robotic car named STANLEY sped through the rough
DARPA Grand Challenge. STANLEY is a Volkswagen Touareg outﬁtted with cameras, radar,
ban Challenge, safely driving in trafﬁc through the streets of a closed Air Force base, obeying
trafﬁc rules and avoiding pedestrians and other vehicles.
tire conversation guided by an automated speech recognition and dialog management system.
Autonomous planning and scheduling: A hundred million miles from Earth, NASA’sSummary
Game playing: IBM’s DEEP BLUE became the ﬁrst computer program to defeat the
intelligence” across the board from him. Newsweek magazine described the match as “The
studied Kasparov’s loss and were able to draw a few matches in subsequent years, but the
Spam ﬁghting: Each day, learning algorithms classify over a billion messages as spam,
saving the recipient from having to waste time deleting what, for many users, could comprise
continually updating their tactics, it is difﬁcult for a static programmed approach to keep up,
cargo, and people at a time, and had to account for starting points, destinations, routes, and
conﬂict resolution among all parameters. The AI planning techniques generated in hours
a plan that would have taken weeks with older methods. The Defense Advanced Research
Project Agency (DARPA) stated that this single application more than paid back DARPA’s
Robotics: The iRobot Corporation has sold over two million Roomba robotic vacuum
cleaners for home use. The company also deploys the more rugged PackBot to Iraq and
Afghanistan, where it is used to handle hazardous materials, clear explosives, and identify
the location of snipers.
Machine Translation: A computer program automatically translates from Arabic to
English, allowing an English speaker to see the headline “Ardogan Conﬁrms That Turkey
Would Not Accept Any Pressure, Urging Them to Recognize Cyprus.” The program uses a
on the team speak Arabic, but they do understand statistics and machine learning algorithms.
These are just a few examples of artiﬁcial intelligence systems that exist today. Not
magic or science ﬁction—but rather science, engineering, and mathematics, to which this
book provides an introduction.
SUMMARY
oped. Some of the important points are as follows:
• Different people approach AI with different goals in mind. Two important questions to
ask are: Are you concerned with thinking or behavior? Do you want to model humans
or work from an ideal standard?Chapter
Introduction
• In this book, we adopt the view that intelligence is concerned mainly with rational
action. Ideally, an intelligent agent takes the best possible action in a situation. We
study the problem of building agents that are intelligent in this sense.
that the mind is in some ways like a machine, that it operates on knowledge encoded in
some internal language, and that thought can be used to choose what actions to take.
• Mathematicians provided the tools to manipulate statements of logical certainty as well
as uncertain, probabilistic statements. They also set the groundwork for understanding
computation and reasoning about algorithms.
• Economists formalized the problem of making decisions that maximize the expected
outcome to the decision maker.
• Neuroscientists discovered some facts about how the brain works and the ways in which
it is similar to and different from computers.
processing machines. Linguists showed that language use ﬁts into this model.
tions possible.
• Control theory deals with designing devices that act optimally on the basis of feedback
from the environment. Initially, the mathematical tools of control theory were quite
different from AI, but the ﬁelds are coming closer together.
• The history of AI has had cycles of success, misplaced optimism, and resulting cutbacks
in enthusiasm and funding. There have also been cycles of introducing new creative
approaches and systematically reﬁning the best ones.
• AI has advanced more rapidly in the past decade because of greater use of the scientiﬁc
method in experimenting with and comparing approaches.
• Recent progress in understanding the theoretical basis for intelligence has gone hand in
hand with improvements in the capabilities of real systems. The subﬁelds of AI have
become more integrated, and AI has found common ground with other disciplines.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The methodological status of artiﬁcial intelligence is investigated in The Sciences of the Artiﬁ-
overview of experimental methodology within AI.
the usefulness of its instantiation in the Loebner Prize competition, and by Ford and HayesChapter
Introduction
How could introspection—reporting on one’s inner thoughts—be inaccurate? Could I
be wrong about what I’m thinking? Discuss.
To what extent are the following computer systems instances of artiﬁcial intelligence:
• Supermarket bar code scanners.
• Web search engines.
• Internet routing algorithms that respond dynamically to the state of the network.
volve quite complex mathematical operations, such as convolving an image with a Gaussian
or ﬁnding a minimum of the entropy function. Most humans (and certainly all animals) never
learn this kind of mathematics at all, almost no one learns it before college, and almost no
one can compute the convolution of a function with a Gaussian in their head. What sense
does it make to say that the “vision system” is doing this kind of mathematics, whereas the
actual person has no idea how to do it?
Why would evolution tend to result in systems that act rationally? What goals are such
systems designed to achieve?
Is AI a science, or is it engineering? Or neither or both? Explain.
“Surely computers cannot be intelligent—they can do only what their programmers
tell them.” Is the latter statement true, and does it imply the former?
“Surely animals cannot be intelligent—they can do only what their genes tell them.”
Is the latter statement true, and does it imply the former?
“Surely animals, humans, and computers cannot be intelligent—they can do only what
their constituent atoms are told to do by the laws of physics.” Is the latter statement true, and
does it imply the former?
Examine the AI literature to discover whether the following tasks can currently be
solved by computers:
b. Driving in the center of Cairo, Egypt.
c. Driving in Victorville, California.
d. Buying a week’s worth of groceries at the market.
e. Buying a week’s worth of groceries on the Web.
f. Playing a decent game of bridge at a competitive level.
g. Discovering and proving new mathematical theorems.
h. Writing an intentionally funny story.
i. Giving competent legal advice in a specialized area of law.
j. Translating spoken English into spoken Swedish in real time.
k. Performing a complex surgical operation.INTELLIGENT AGENTS
In which we discuss the nature of agents, perfect or otherwise, the diversity of
environments, and the resulting menagerie of agent types.
intelligence. In this chapter, we make this notion more concrete. We will see that the concept
ment. Our plan in this book is to use this concept to develop a small set of design principles
for building successful agents—systems that can reasonably be called intelligent.
We begin by examining agents, environments, and the coupling between them. The
observation that some agents behave better than others leads naturally to the idea of a rational
agent—one that behaves as well as possible. How well an agent can behave depends on
the nature of the environment; some environments are more difﬁcult than others. We give a
crude categorization of environments and show how properties of an environment inﬂuence
the design of suitable agents for that environment. We describe a number of basic “skeleton”
agent designs, which we ﬂesh out in the rest of the book.
AGENTS AND ENVIRONMENTS
An agent is anything that can be viewed as perceiving its environment through sensors and
ENVIRONMENT
SENSOR
ACTUATOR
A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so
on for actuators. A robotic agent might have cameras and infrared range ﬁnders for sensors
and various motors for actuators. A software agent receives keystrokes, ﬁle contents, and
network packets as sensory inputs and acts on the environment by displaying on the screen,
writing ﬁles, and sending network packets.
We use the term percept to refer to the agent’s perceptual inputs at any given instant. An
PERCEPT
agent’s percept sequence is the complete history of everything the agent has ever perceived.
PERCEPT SEQUENCE
In general, an agent’s choice of action at any given instant can depend on the entire percept
sequence observed to date, but not on anything it hasn’t perceived. By specifying the agent’s
choice of action for every possible percept sequence, we have said more or less everythingAgents and Environments
Agent
Sensors
Actuators
Environment
Percepts
Actions
?
Agents interact with environments through sensors and actuators.
there is to say about the agent. Mathematically speaking, we say that an agent’s behavior is
described by the agent function that maps any given percept sequence to an action.
AGENT FUNCTION
We can imagine tabulating the agent function that describes any given agent; for most
agents, this would be a very large table—inﬁnite, in fact, unless we place a bound on the
length of percept sequences we want to consider. Given an agent to experiment with, we can,
in principle, construct this table by trying out all possible percept sequences and recording
of the agent. Internally, the agent function for an artiﬁcial agent will be implemented by an
agent program. It is important to keep these two ideas distinct. The agent function is an
AGENT PROGRAM
abstract mathematical description; the agent program is a concrete implementation, running
within some physical system.
locations: squares A and B. The vacuum agent perceives which square it is in and whether
there is dirt in the square. It can choose to move left, move right, suck up the dirt, or do
nothing. One very simple agent function is the following: if the current square is dirty, then
suck; otherwise, move to the other square. A partial tabulation of this agent function is shown
is the right way to ﬁll out the table? In other words, what makes an agent good or bad,
intelligent or stupid? We answer these questions in the next section.
times to identify the probability of each action. One might imagine that acting randomly is rather silly, but we
show later in this chapter that it can be very intelligent.Chapter
Intelligent Agents
A
B
Percept sequence
Action
[A, Clean]
Right
[A, Dirty]
Suck
[B, Clean]
Left
[B, Dirty]
Suck
[A, Clean], [A, Clean]
Right
[A, Clean], [A, Dirty]
Suck
...
...
[A, Clean], [A, Clean], [A, Clean]
Right
[A, Clean], [A, Clean], [A, Dirty]
Suck
...
...
Before closing this section, we should emphasize that the notion of an agent is meant to
be a tool for analyzing systems, not an absolute characterization that divides the world into
would hardly aid our understanding of the calculator. In a sense, all areas of engineering can
be seen as designing artifacts that interact with the world; AI operates at (what the authors
consider to be) the most interesting end of the spectrum, where the artifacts have signiﬁcant
computational resources and the task environment requires nontrivial decision making.
GOOD BEHAVIOR: THE CONCEPT OF RATIONALITY
A rational agent is one that does the right thing—conceptually speaking, every entry in the
RATIONAL AGENT
table for the agent function is ﬁlled out correctly. Obviously, doing the right thing is better
than doing the wrong thing, but what does it mean to do the right thing?Good Behavior: The Concept of Rationality
of the agent’s behavior. When an agent is plunked down in an environment, it generates a
sequence of actions according to the percepts it receives. This sequence of actions causes the
environment to go through a sequence of states. If the sequence is desirable, then the agent
has performed well. This notion of desirability is captured by a performance measure that
PERFORMANCE
MEASURE
evaluates any given sequence of environment states.
Notice that we said environment states, not agent states. If we deﬁne success in terms
of agent’s opinion of its own performance, an agent could achieve perfect rationality simply
by deluding itself that its performance was perfect. Human agents in particular are notorious
for “sour grapes”—believing they did not really want something (e.g., a Nobel Prize) after
not getting it.
Obviously, there is not one ﬁxed performance measure for all tasks and agents; typically,
a designer will devise one appropriate to the circumstances. This is not as easy as it sounds.
With a rational agent, of course, what you ask for is what you get. A rational agent can
maximize this performance measure by cleaning up the dirt, then dumping it all on the ﬂoor,
then cleaning it up again, and so on. A more suitable performance measure would reward the
agent for having a clean ﬂoor. For example, one point could be awarded for each clean square
at each time step (perhaps with a penalty for electricity consumed and noise generated). As
a general rule, it is better to design performance measures according to what one actually
wants in the environment, rather than according to how one thinks the agent should behave.
Even when the obvious pitfalls are avoided, there remain some knotty issues to untangle.
For example, the notion of “clean ﬂoor” in the preceding paragraph is based on average
cleanliness over time. Yet the same average cleanliness can be achieved by two different
agents, one of which does a mediocre job all the time while the other cleans energetically but
takes long breaks. Which is preferable might seem to be a ﬁne point of janitorial science, but
a reckless life of highs and lows, or a safe but humdrum existence? Which is better—an
economy where everyone lives in moderate poverty, or one in which some live in plenty
while others are very poor? We leave these questions as an exercise for the diligent reader.
Rationality
What is rational at any given time depends on four things:
• The performance measure that deﬁnes the criterion of success.
• The agent’s prior knowledge of the environment.
• The actions that the agent can perform.
• The agent’s percept sequence to date.
This leads to a deﬁnition of a rational agent:
DEFINITION OF A
RATIONAL AGENT
pected to maximize its performance measure, given the evidence provided by the perceptChapter
Intelligent Agents
That depends! First, we need to say what the performance measure is, what is known about
the environment, and what sensors and actuators the agent has. Let us assume the following:
• The performance measure awards one point for each clean square at each time step,
bution and the initial location of the agent are not. Clean squares stay clean and sucking
cleans the current square. The Left and Right actions move the agent left and right
except when this would take the agent outside the environment, in which case the agent
remains where it is.
• The only available actions are Left, Right, and Suck.
• The agent correctly perceives its location and whether that location contains dirt.
stances. For example, once all the dirt is cleaned up, the agent will oscillate needlessly back
and forth; if the performance measure includes a penalty of one point for each movement left
or right, the agent will fare poorly. A better agent for this case would do nothing once it is
sure that all the squares are clean. If clean squares can become dirty again, the agent should
asks you to design agents for these cases.
Omniscience, learning, and autonomy
We need to be careful to distinguish between rationality and omniscience. An omniscient
OMNISCIENCE
agent knows the actual outcome of its actions and can act accordingly; but omniscience is
impossible in reality. Consider the following example: I am walking along the Champs
Elys´ees one day and I see an old friend across the street. There is no trafﬁc nearby and I’m
street I am ﬂattened. Was I irrational to cross the street? It is unlikely that my obituary would
read “Idiot attempts to cross street.”
imizes expected performance, while perfection maximizes actual performance. Retreating
from a requirement of perfection is not just a question of being fair to agents. The point is
that if we expect an agent to do what turns out to be the best action after the fact, it will be
impossible to design an agent to fulﬁll this speciﬁcation—unless we improve the performance
of crystal balls or time machines.Good Behavior: The Concept of Rationality
Our deﬁnition of rationality does not require omniscience, then, because the rational
choice depends only on the percept sequence to date. We must also ensure that we haven’t
ple, if an agent does not look both ways before crossing a busy road, then its percept sequence
will not tell it that there is a large truck approaching at high speed. Does our deﬁnition of
rationality say that it’s now OK to cross the road? Far from it! First, it would not be rational
ing without looking is too great. Second, a rational agent should choose the “looking” action
before stepping into the street, because looking helps maximize the expected performance.
INFORMATION
GATHERING
example of information gathering is provided by the exploration that must be undertaken by
EXPLORATION
Our deﬁnition requires a rational agent not only to gather information but also to learn
LEARNING
as much as possible from what it perceives. The agent’s initial conﬁguration could reﬂect
some prior knowledge of the environment, but as the agent gains experience this may be
modiﬁed and augmented. There are extreme cases in which the environment is completely
known a priori. In such cases, the agent need not perceive or learn; it simply acts correctly.
Of course, such agents are fragile. Consider the lowly dung beetle. After digging its nest and
laying its eggs, it fetches a ball of dung from a nearby heap to plug the entrance. If the ball of
ging the nest with the nonexistent dung ball, never noticing that it is missing. Evolution has
built an assumption into the beetle’s behavior, and when it is violated, unsuccessful behavior
results. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go
out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is
well, drag the caterpillar inside, and lay its eggs. The caterpillar serves as a food source when
the eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches
away while the sphex is doing the check, it will revert to the “drag” step of its plan and will
The sphex is unable to learn that its innate plan is failing, and thus will not change it.
To the extent that an agent relies on the prior knowledge of its designer rather than
on its own percepts, we say that the agent lacks autonomy. A rational agent should be
AUTONOMY
quires complete autonomy from the start: when the agent has had little or no experience, it
would have to act randomly unless the designer gave some assistance. So, just as evolution
it would be reasonable to provide an artiﬁcial intelligent agent with some initial knowledge
as well as an ability to learn. After sufﬁcient experience of its environment, the behavior
of a rational agent can become effectively independent of its prior knowledge. Hence, the
incorporation of learning allows one to design a single rational agent that will succeed in a
vast variety of environments.Chapter
Intelligent Agents
THE NATURE OF ENVIRONMENTS
Now that we have a deﬁnition of rationality, we are almost ready to think about building
TASK ENVIRONMENT
tially the “problems” to which rational agents are the “solutions.” We begin by showing how
to specify a task environment, illustrating the process with a number of examples. We then
show that task environments come in a variety of ﬂavors. The ﬂavor of the task environment
directly affects the appropriate design for the agent program.
Specifying the task environment
the performance measure, the environment, and the agent’s actuators and sensors. We group
all these under the heading of the task environment. For the acronymically minded, we call
this the PEAS (Performance, Environment, Actuators, Sensors) description. In designing an
PEAS
agent, the ﬁrst step must always be to specify the task environment as fully as possible.
The vacuum world was a simple example; let us consider a more complex problem: an
automated taxi driver. We should point out, before the reader becomes alarmed, that a fully
no limit to the novel combinations of circumstances that can arise—another reason we chose
environment. We discuss each element in more detail in the following paragraphs.
Agent Type
Performance
Measure
Environment
Actuators
Sensors
Taxi driver
Safe, fast, legal,
comfortable trip,
maximize proﬁts
Roads, other
trafﬁc,
pedestrians,
customers
Steering,
accelerator,
brake, signal,
horn, display
Cameras, sonar,
speedometer,
GPS, odometer,
accelerometer,
engine sensors,
keyboard
PEAS description of the task environment for an automated taxi.
First, what is the performance measure to which we would like our automated driver
sumption and wear and tear; minimizing the trip time or cost; minimizing violations of trafﬁc
ing proﬁts. Obviously, some of these goals conﬂict, so tradeoffs will be required.
Next, what is the driving environment that the taxi will face? Any taxi driver must
The roads contain other trafﬁc, pedestrians, stray animals, road works, police cars, puddles,The Nature of Environments
and potholes. The taxi must also interact with potential and actual passengers. There are also
some optional choices. The taxi might need to operate in Southern California, where snow
is seldom a problem, or in Alaska, where it seldom is not. It could always be driving on the
right, or we might want it to be ﬂexible enough to drive on the left when in Britain or Japan.
Obviously, the more restricted the environment, the easier the design problem.
The actuators for an automated taxi include those available to a human driver: control
over the engine through the accelerator and control over steering and braking. In addition, it
will need output to a display screen or voice synthesizer to talk back to the passengers, and
perhaps some way to communicate with other vehicles, politely or otherwise.
The basic sensors for the taxi will include one or more controllable video cameras so
ter, and to control the vehicle properly, especially on curves, it should have an accelerometer.
To determine the mechanical state of the vehicle, it will need the usual array of engine, fuel,
and electrical system sensors. Like many human drivers, it might want a global positioning
system (GPS) so that it doesn’t get lost. Finally, it will need a keyboard or microphone for
the passenger to request a destination.
ers that our list of agent types includes some programs that operate in the entirely artiﬁcial
environment deﬁned by keyboard input and character output on a screen. “Surely,” one might
say, “this is not a real environment, is it?” In fact, what matters is not the distinction between
ior of the agent, the percept sequence generated by the environment, and the performance
measure. Some “real” environments are actually quite simple. For example, a robot designed
to inspect parts as they come by on a conveyor belt can make use of a number of simplifying
assumptions: that the lighting is always just so, that the only thing on the conveyor belt will
be parts of a kind that it knows about, and that only two actions (accept or reject) are possible.
SOFTWARE AGENT
SOFTBOT
ited domains. Imagine a softbot Web site operator designed to scan Internet news sources and
show the interesting items to its users, while selling advertising space to generate revenue.
To do well, that operator will need some natural language processing abilities, it will need
to learn what each user and advertiser is interested in, and it will need to change its plans
dynamically—for example, when the connection for one news source goes down or when a
new one comes online. The Internet is an environment whose complexity rivals that of the
physical world and whose inhabitants include many artiﬁcial and human agents.
Properties of task environments
The range of task environments that might arise in AI is obviously vast. We can, however,
rized. These dimensions determine, to a large extent, the appropriate agent design and the
applicability of each of the principal families of techniques for agent implementation. First,Chapter
Intelligent Agents
Agent Type
Performance
Measure
Environment
Actuators
Sensors
Medical
diagnosis system
Healthy patient,
reduced costs
Patient, hospital,
staff
Display of
questions, tests,
diagnoses,
treatments,
referrals
Keyboard entry
of symptoms,
ﬁndings, patient’s
answers
Satellite image
analysis system
Correct image
categorization
Downlink from
orbiting satellite
Display of scene
categorization
Color pixel
arrays
robot
Percentage of
parts in correct
bins
Conveyor belt
with parts; bins
Jointed arm and
hand
Camera, joint
angle sensors
Reﬁnery
controller
Purity, yield,
safety
Reﬁnery,
operators
Valves, pumps,
heaters, displays
Temperature,
pressure,
chemical sensors
Interactive
English tutor
Student’s score
on test
Set of students,
testing agency
Display of
exercises,
suggestions,
corrections
Keyboard entry
Examples of agent types and their PEAS descriptions.
we list the dimensions, then we analyze several task environments to illustrate the ideas. The
deﬁnitions here are informal; later chapters provide more precise statements and examples of
each kind of environment.
Fully observable vs. partially observable: If an agent’s sensors give it access to the
FULLY OBSERVABLE
PARTIALLY
OBSERVABLE
ment is fully observable. A task environment is effectively fully observable if the sensors
detect all aspects that are relevant to the choice of action; relevance, in turn, depends on the
performance measure. Fully observable environments are convenient because the agent need
not maintain any internal state to keep track of the world. An environment might be partially
observable because of noisy and inaccurate sensors or because parts of the state are simply
missing from the sensor data—for example, a vacuum agent with only a local dirt sensor
cannot tell whether there is dirt in other squares, and an automated taxi cannot see what other
able. One might think that in such cases the agent’s plight is hopeless, but, as we discuss in
UNOBSERVABLE
SINGLE AGENT
MULTIAGENTThe Nature of Environments
vironments may seem simple enough. For example, an agent solving a crossword puzzle by
agent environment. There are, however, some subtle issues. First, we have described how an
entity may be viewed as an agent, but we have not explained which entities must be viewed
as agents. Does an agent A (the taxi driver for example) have to treat an object B (another
vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of
physics, analogous to waves at the beach or leaves blowing in the wind? The key distinction
is whether B’s behavior is best described as maximizing a performance measure whose value
depends on agent A’s behavior. For example, in chess, the opponent entity B is trying to
COMPETITIVE
environment, on the other hand, avoiding collisions maximizes the performance measure of
COOPERATIVE
vironments; for example, communication often emerges as a rational behavior in multiagent
environments; in some competitive environments, randomized behavior is rational because
it avoids the pitfalls of predictability.
DETERMINISTIC
STOCHASTIC
mined by the current state and the action executed by the agent, then we say the environment
tainty that arises purely from the actions of other agents in a multiagent environment; thus,
a game can be deterministic even though each agent may be unable to predict the actions of
the others.) If the environment is partially observable, however, then it could appear to be
stochastic. Most real situations are so complex that it is impossible to keep track of all the
unobserved aspects; for practical purposes, they must be treated as stochastic. Taxi driving is
clearly stochastic in this sense, because one can never predict the behavior of trafﬁc exactly;
moreover, one’s tires blow out and one’s engine seizes up without warning. The vacuum
world as we described it is deterministic, but variations can include stochastic elements such
environment is uncertain if it is not fully observable or not deterministic. One ﬁnal note:
UNCERTAIN
tiﬁed in terms of probabilities; a nondeterministic environment is one in which actions are
NONDETERMINISTIC
terministic environment descriptions are usually associated with performance measures that
require the agent to succeed for all possible outcomes of its actions.
Episodic vs. sequential: In an episodic task environment, the agent’s experience is
EPISODIC
SEQUENTIAL
divided into atomic episodes. In each episode the agent receives a percept and then performs
a single action. Crucially, the next episode does not depend on the actions taken in previous
episodes. Many classiﬁcation tasks are episodic. For example, an agent that has to spot
defective parts on an assembly line bases each decision on the current part, regardless of
previous decisions; moreover, the current decision doesn’t affect whether the next part isChapter
Intelligent Agents
defective. In sequential environments, on the other hand, the current decision could affect
environments because the agent does not need to think ahead.
Static vs. dynamic: If the environment can change while an agent is deliberating, then
STATIC
DYNAMIC
we say the environment is dynamic for that agent; otherwise, it is static. Static environments
are easy to deal with because the agent need not keep looking at the world while it is deciding
on an action, nor need it worry about the passage of time. Dynamic environments, on the
other hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet,
that counts as deciding to do nothing. If the environment itself does not change with the
passage of time but the agent’s performance score does, then we say the environment is
semidynamic. Taxi driving is clearly dynamic: the other cars and the taxi itself keep moving
SEMIDYNAMIC
while the driving algorithm dithers about what to do next. Chess, when played with a clock,
is semidynamic. Crossword puzzles are static.
DISCRETE
CONTINUOUS
environment, to the way time is handled, and to the percepts and actions of the agent. For
example, the chess environment has a ﬁnite number of distinct states (excluding the clock).
ing, but is typically treated as representing continuously varying intensities and locations.
Known vs. unknown: Strictly speaking, this distinction refers not to the environment
KNOWN
UNKNOWN
itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of
the environment. In a known environment, the outcomes (or outcome probabilities if the
environment is stochastic) for all actions are given. Obviously, if the environment is unknown,
the agent will have to learn how it works in order to make good decisions. Note that the
distinction between known and unknown environments is not the same as the one between
fully and partially observable environments. It is quite possible for a known environment
to be partially observable—for example, in solitaire card games, I know the rules but am
still unable to see the cards that have not yet been turned over. Conversely, an unknown
environment can be fully observable—in a new video game, the screen may show the entire
game state but I still don’t know what the buttons do until I try them.
As one might expect, the hardest case is partially observable, multiagent, stochastic,
sequential, dynamic, continuous, and unknown. Taxi driving is hard in all these senses, except
that for the most part the driver’s environment is known. Driving a rented car in a new country
with unfamiliar geography and trafﬁc laws is a lot more exciting.
episodic, because it normally considers each part in isolation. But if one day there is a large
largely unrelated.The Nature of Environments
Task Environment
Observable Agents Deterministic
Episodic
Static
Discrete
Crossword puzzle
Fully
Single Deterministic Sequential
Static
Discrete
Chess with a clock
Fully
Multi
Deterministic Sequential
Semi
Discrete
Poker
Partially
Multi
Stochastic
Sequential
Static
Discrete
Backgammon
Fully
Multi
Stochastic
Sequential
Static
Discrete
Taxi driving
Partially
Multi
Stochastic
Sequential Dynamic Continuous
Medical diagnosis
Partially
Single
Stochastic
Sequential Dynamic Continuous
Image analysis
Fully
Single Deterministic
Episodic
Semi
Continuous
Partially
Single
Stochastic
Episodic
Dynamic Continuous
Reﬁnery controller
Partially
Single
Stochastic
Sequential Dynamic Continuous
Interactive English tutor
Partially
Multi
Stochastic
Sequential Dynamic
Discrete
Examples of task environments and their characteristics.
batch of defective parts, the robot should learn from several observations that the distribution
of defects has changed, and should modify its behavior for subsequent parts. We have not
erty of the environment. For some environments, such as chess and poker, it is quite easy to
supply the agent with full knowledge of the rules, but it is nonetheless interesting to consider
how an agent might learn to play these games without such knowledge.
Several of the answers in the table depend on how the task environment is deﬁned. We
deal with recalcitrant patients and skeptical staff, so the environment could have a multiagent
aspect. Furthermore, medical diagnosis is episodic if one conceives of the task as selecting a
diagnosis given a list of symptoms; the problem is sequential if the task can include proposing
a series of tests, evaluating progress over the course of treatment, and so on. Also, many
environments are episodic at higher levels than the agent’s individual actions. For example,
a chess tournament consists of a sequence of games; each game is an episode because (by
and large) the contribution of the moves in one game to the agent’s overall performance is
not affected by the moves in its previous game. On the other hand, decision making within a
single game is certainly sequential.
lator that places one or more agents in a simulated environment, observes their behavior over
time, and evaluates them according to a given performance measure. Such experiments are
vironment class. For example, to evaluate a taxi driver in simulated trafﬁc, we would want to
ENVIRONMENT
CLASS
run many simulations with different trafﬁc, lighting, and weather conditions. If we designed
the agent for a single scenario, we might be able to take advantage of speciﬁc properties
of the particular case but might not identify a good design for driving in general. For thisChapter
Intelligent Agents
reason, the code repository also includes an environment generator for each environment
ENVIRONMENT
GENERATOR
class that selects particular environments (with certain likelihoods) in which to run the agent.
For example, the vacuum environment generator initializes the dirt pattern and agent location
randomly. We are then interested in the agent’s average performance over the environment
class. A rational agent for a given environment class maximizes this average performance.
evaluating various agents therein.
THE STRUCTURE OF AGENTS
So far we have talked about agents by describing behavior—the action that is performed after
any given sequence of percepts. Now we must bite the bullet and talk about how the insides
work. The job of AI is to design an agent program that implements the agent function—
AGENT PROGRAM
the mapping from percepts to actions. We assume this program will run on some sort of
computing device with physical sensors and actuators—we call this the architecture:
ARCHITECTURE
agent = architecture + program .
Obviously, the program we choose has to be one that is appropriate for the architecture. If the
program is going to recommend actions like Walk, the architecture had better have legs. The
architecture might be just an ordinary PC, or it might be a robotic car with several onboard
computers, cameras, and other sensors. In general, the architecture makes the percepts from
the sensors available to the program, runs the program, and feeds the program’s action choices
to the actuators as they are generated. Most of this book is about designing agent programs,
Agent programs
The agent programs that we design in this book all have the same skeleton: they take the
difference between the agent program, which takes the current percept as input, and the agent
function, which takes the entire percept history. The agent program takes just the current
percept as input because nothing more is available from the environment; if the agent’s actions
need to depend on the entire percept sequence, the agent will have to remember the percepts.
We describe the agent programs in the simple pseudocode language that is deﬁned in
Appendix B. (The online code repository contains implementations in real programming
the percept sequence and then uses it to index into a table of actions to decide what to do.
explicitly the agent function that the agent program embodies. To build a rational agent in
coroutines that run asynchronously with the environment. Each such coroutine has an input and output port and
consists of a loop that reads the input port for percepts and writes actions to the output port.The Structure of Agents
persistent: percepts, a sequence, initially empty
table, a table of actions, indexed by percept sequences, initially fully speciﬁed
append percept to the end of percepts
action ←LOOKUP(percepts,table)
return action
returns an action each time. It retains the complete percept sequence in memory.
this way, we as designers must construct a table that contains the appropriate action for every
possible percept sequence.
doomed to failure. Let P be the set of possible percepts and let T be the lifetime of the
agent (the total number of percepts it will receive). The lookup table will contain T
entries. Consider the automated taxi: the visual input from a single camera comes in at the
universe will have the space to store the table, (b) the designer would not have time to create
the table, (c) no agent could ever learn all the right table entries from its experience, and (d)
even if the environment is simple enough to yield a feasible table size, the designer still has
no guidance about how to ﬁll in the table entries.
desired agent function. The key challenge for AI is to ﬁnd out how to write programs that,
to the extent possible, produce rational behavior from a smallish program rather than from
a vast table. We have many examples showing that this can be done successfully in other
areas: for example, the huge tables of square roots used by engineers and schoolchildren prior
on electronic calculators. The question is, can AI do for general intelligent behavior what
Newton did for square roots? We believe the answer is yes.
In the remainder of this section, we outline four basic kinds of agent programs that
embody the principles underlying almost all intelligent systems:
• Simple reﬂex agents;
Each kind of agent program combines particular components in particular ways to generateChapter
Intelligent Agents
if status = Dirty then return Suck
else if location = A then return Right
else if location = B then return Left
agents that can improve the performance of their components so as to generate better actions.
be represented within the agent. This variety provides a major organizing principle for the
ﬁeld and for the book itself.
Simple reﬂex agents
The simplest kind of agent is the simple reﬂex agent. These agents select actions on the basis
SIMPLE REFLEX
AGENT
of the current percept, ignoring the rest of the percept history. For example, the vacuum agent
is based only on the current location and on whether that location contains dirt. An agent
ing table. The most obvious reduction comes from ignoring the percept history, which cuts
the fact that when the current square is dirty, the action does not depend on the location.
Simple reﬂex behaviors occur even in more complex environments. Imagine yourself
as the driver of the automated taxi. If the car in front brakes and its brake lights come on, then
you should notice this and initiate braking. In other words, some processing is done on the
visual input to establish the condition we call “The car in front is braking.” Then, this triggers
some established connection in the agent program to the action “initiate braking.” We call
CONDITION–ACTION
RULE
ing) and some of which are innate reﬂexes (such as blinking when something approaches the
eye). In the course of the book, we show several different ways in which such connections
can be learned and implemented.
structure of this general program in schematic form, showing how the condition–action rules
allow the agent to make the connection from percept to action. (Do not worry if this seemsThe Structure of Agents
Agent
Environment
Sensors
What action I
should do now
Actuators
What the world
is like now
Schematic diagram of a simple reﬂex agent.
persistent: rules, a set of condition–action rules
action ←rule.ACTION
return action
A simple reﬂex agent. It acts according to a rule whose condition matches
the current state, as deﬁned by the percept.
trivial; it gets more interesting shortly.) We use rectangles to denote the current internal state
of the agent’s decision process, and ovals to represent the background information used in
the given state description. Note that the description in terms of “rules” and “matching” is
purely conceptual; actual implementations can be as simple as a collection of logic gates
implementing a Boolean circuit.
Simple reﬂex agents have the admirable property of being simple, but they turn out to be
able. Even a little bit of unobservability can cause serious trouble. For example, the braking
the current percept—a single frame of video. This works if the car in front has a centrally
mounted brake light. Unfortunately, older models have different conﬁgurations of taillights,Chapter
Intelligent Agents
whether the car is braking. A simple reﬂex agent driving behind such a car would either brake
continuously and unnecessarily, or, worse, never brake at all.
We can see a similar problem arising in the vacuum world. Suppose that a simple reﬂex
vacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent
has just two possible percepts: [Dirty] and [Clean]. It can Suck in response to [Dirty]; what
should it do in response to [Clean]? Moving Left fails (forever) if it happens to start in square
A, and moving Right fails (forever) if it happens to start in square B. Inﬁnite loops are often
unavoidable for simple reﬂex agents operating in partially observable environments.
RANDOMIZATION
ample, if the vacuum agent perceives [Clean], it might ﬂip a coin to choose between Left and
Right. It is easy to show that the agent will reach the other square in an average of two steps.
Then, if that square is dirty, the agent will clean it and the task will be complete. Hence, a
randomized simple reﬂex agent might outperform a deterministic simple reﬂex agent.
rational. It is a useful trick that helps a simple reﬂex agent in some situations, but in most
cases we can do much better with more sophisticated deterministic agents.
The most effective way to handle partial observability is for the agent to keep track of the
part of the world it can’t see now. That is, the agent should maintain some sort of internal
state that depends on the percept history and thereby reﬂects at least some of the unobserved
INTERNAL STATE
aspects of the current state. For the braking problem, the internal state is not too extensive—
just the previous frame from the camera, allowing the agent to detect when two red lights at
the edge of the vehicle go on or off simultaneously. For other driving tasks such as changing
lanes, the agent needs to keep track of where the other cars are if it can’t see them all at once.
And for any driving to be possible at all, the agent needs to keep track of where its keys are.
edge to be encoded in the agent program. First, we need some information about how the
world evolves independently of the agent—for example, that an overtaking car generally will
be closer behind than it was a moment ago. Second, we need some information about how
the agent’s own actions affect the world—for example, that when the agent turns the steering
wheel clockwise, the car turns to the right, or that after driving for ﬁve minutes northbound
on the freeway, one is usually about ﬁve miles north of where one was ﬁve minutes ago. This
knowledge about “how the world works”—whether implemented in simple Boolean circuits
or in complete scientiﬁc theories—is called a model of the world. An agent that uses such a
AGENT
ing how the current percept is combined with the old internal state to generate the updated
description of the current state, based on the agent’s model of how the world works. The agentThe Structure of Agents
Agent
Environment
Sensors
State
How the world evolves
What my actions do
Actuators
What the world
is like now
What action I
should do now
persistent: state, the agent’s current conception of the world state
model, a description of how the next state depends on current state and action
rules, a set of condition–action rules
action, the most recent action, initially none
action ←rule.ACTION
return action
using an internal model. It then chooses an action in the same way as the reﬂex agent.
is responsible for creating the new internal state description. The details of how models and
states are represented vary widely depending on the type of environment and the particular
technology used in the agent design. Detailed examples of models and updating algorithms
Regardless of the kind of representation used, it is seldom possible for the agent to
determine the current state of a partially observable environment exactly. Instead, the box
sometimes best guesses). For example, an automated taxi may not be able to see around the
large truck that has stopped in front of it and can only guess about what may be causing the
to make a decision.
agent is that it does not have to describe “what the world is like now” in a literal sense. ForChapter
Intelligent Agents
Agent
Environment
Sensors
What action I
should do now
State
How the world evolves
What my actions do
Actuators
What the world
is like now
What it will be like
  if I do action A
Goals
a set of goals it is trying to achieve, and chooses an action that will (eventually) lead to the
achievement of its goals.
example, the taxi may be driving back home, and it may have a rule telling it to ﬁll up with
gas on the way home unless it has at least half a tank. Although “driving back home” may
seem to an aspect of the world state, the fact of the taxi’s destination is actually an aspect of
the agent’s internal state. If you ﬁnd this puzzling, consider that the taxi could be in exactly
the same place at the same time, but intending to reach a different destination.
Knowing something about the current state of the environment is not always enough to decide
what to do. For example, at a road junction, the taxi can turn left, turn right, or go straight
on. The correct decision depends on where the taxi is trying to get to. In other words, as well
as a current state description, the agent needs some sort of goal information that describes
GOAL
situations that are desirable—for example, being at the passenger’s destination. The agent
agent’s structure.
isfaction results immediately from a single action. Sometimes it will be more tricky—for
example, when the agent has to consider long sequences of twists and turns in order to ﬁnd a
subﬁelds of AI devoted to ﬁnding action sequences that achieve the agent’s goals.
Notice that decision making of this kind is fundamentally different from the condition–
action rules described earlier, in that it involves consideration of the future—both “What willThe Structure of Agents
principle, could reason that if the car in front has its brake lights on, it will slow down. Given
the way the world usually evolves, the only action that will achieve the goal of not hitting
other cars is to brake.
knowledge that supports its decisions is represented explicitly and can be modiﬁed. If it starts
to rain, the agent can update its knowledge of how effectively its brakes will operate; this will
automatically cause all of the relevant behaviors to be altered to suit the new conditions.
For the reﬂex agent, on the other hand, we would have to rewrite many condition–action
simply by specifying that destination as the goal. The reﬂex agent’s rules for when to turn
and when to go straight will work only for a single destination; they must all be replaced to
go somewhere new.
example, many action sequences will get the taxi to its destination (thereby achieving the
goal) but some are quicker, safer, more reliable, or cheaper than others. Goals just provide a
crude binary distinction between “happy” and “unhappy” states. A more general performance
measure should allow a comparison of different world states according to exactly how happy
they would make the agent. Because “happy” does not sound very scientiﬁc, economists and
UTILITY
We have already seen that a performance measure assigns a score to any given sequence
of environment states, so it can easily distinguish between more and less desirable ways of
getting to the taxi’s destination. An agent’s utility function is essentially an internalization
UTILITY FUNCTION
of the performance measure. If the internal utility function and the external performance
measure are in agreement, then an agent that chooses actions to maximize its utility will be
rational according to the external performance measure.
Let us emphasize again that this is not the only way to be rational—we have already
terms of ﬂexibility and learning. Furthermore, in two kinds of cases, goals are inadequate but
only some of which can be achieved (for example, speed and safety), the utility function
speciﬁes the appropriate tradeoff. Second, when there are several goals that the agent can
aim for, none of which can be achieved with certainty, utility provides a way in which the
likelihood of success can be weighed against the importance of the goals.
Partial observability and stochasticity are ubiquitous in the real world, and so, therefore,
chooses the action that maximizes the expected utility of the action outcomes—that is, the
EXPECTED UTILITY
utility the agent expects to derive, on average, given the probabilities and utilities of eachChapter
Intelligent Agents
Agent
Environment
Sensors
How happy I will be
in such a state
State
How the world evolves
What my actions do
Utility
Actuators
What action I
should do now
What it will be like
if I do action A
What the world
is like now
a utility function that measures its preferences among states of the world. Then it chooses the
action that leads to the best expected utility, where expected utility is computed by averaging
over all possible outcome states, weighted by the probability of the outcome.
rational agent must behave as if it possesses a utility function whose expected value it tries
to maximize. An agent that possesses an explicit utility function can make rational decisions
maximized. In this way, the “global” deﬁnition of rationality—designating as rational those
agent functions that have the highest performance—is turned into a “local” constraint on
inherent in stochastic or partially observable environments.
At this point, the reader may be wondering, “Is it that simple? We just build agents that
maximize expected utility, and we’re done?” It’s true that such agents would be intelligent,
tasks that have involved a great deal of research on perception, representation, reasoning,
and learning. The results of this research ﬁll many of the chapters of this book. Choosing
that ﬁll several more chapters. Even with these algorithms, perfect rationality is usually
Learning agents
We have described agent programs with various methods for selecting actions. We have
not, so far, explained how the agent programs come into being. In his famous early paper,The Structure of Agents
Performance standard
Agent
Environment
Sensors
Performance
element
changes
knowledge
learning
  goals
Problem
generator
feedback
  Learning
element
Critic
Actuators
A general learning agent.
He estimates how much work this might take and concludes “Some more expeditious method
seems desirable.” The method he proposes is to build learning machines and then to teach
systems. Learning has another advantage, as we noted earlier: it allows the agent to operate
in initially unknown environments and to become more competent than its initial knowledge
alone might allow. In this section, we brieﬂy introduce the main ideas of learning agents.
Throughout the book, we comment on opportunities and methods for learning in particular
kinds of agents. Part V goes into much more depth on the learning algorithms themselves.
LEARNING ELEMENT
sponsible for making improvements, and the performance element, which is responsible for
PERFORMANCE
ELEMENT
selecting external actions. The performance element is what we have previously considered
to be the entire agent: it takes in percepts and decides on actions. The learning element uses
feedback from the critic on how the agent is doing and determines how the performance
CRITIC
element should be modiﬁed to do better in the future.
The design of the learning element depends very much on the design of the performance
element. When trying to design an agent that learns a certain capability, the ﬁrst question is
not “How am I going to get it to learn this?” but “What kind of performance element will my
agent need to do this once it has learned how?” Given an agent design, learning mechanisms
can be constructed to improve every part of the agent.
The critic tells the learning element how well the agent is doing with respect to a ﬁxed
performance standard. The critic is necessary because the percepts themselves provide no
indication of the agent’s success. For example, a chess program could receive a percept
indicating that it has checkmated its opponent, but it needs a performance standard to know
that this is a good thing; the percept itself does not say so. It is important that the performanceChapter
Intelligent Agents
standard be ﬁxed. Conceptually, one should think of it as being outside the agent altogether
because the agent must not modify it to ﬁt its own behavior.
The last component of the learning agent is the problem generator. It is responsible
PROBLEM
GENERATOR
for suggesting actions that will lead to new and informative experiences. The point is that
if the performance element had its way, it would keep doing the actions that are best, given
what it knows. But if the agent is willing to explore a little and do some perhaps suboptimal
actions in the short run, it might discover much better actions for the long run. The problem
generator’s job is to suggest these exploratory actions. This is what scientists do when they
carry out experiments. Galileo did not think that dropping rocks from the top of a tower in
Pisa was valuable in itself. He was not trying to break the rocks or to modify the brains of
of the motion of objects.
To make the overall design more concrete, let us return to the automated taxi example.
The performance element consists of whatever collection of knowledge and procedures the
taxi has for selecting its driving actions. The taxi goes out on the road and drives, using
this performance element. The critic observes the world and passes information along to the
ﬁc, the critic observes the shocking language used by other drivers. From this experience, the
learning element is able to formulate a rule saying this was a bad action, and the performance
element is modiﬁed by installation of the new rule. The problem generator might identify
certain areas of behavior in need of improvement and suggest experiments, such as trying out
the brakes on different road surfaces under different conditions.
The learning element can make changes to any of the “knowledge” components shown
ment can allow the agent to learn “How the world evolves,” and observation of the results of
its actions can allow the agent to learn “What my actions do.” For example, if the taxi exerts
a certain braking pressure when driving on a wet road, then it will soon ﬁnd out how much
deceleration is actually achieved. Clearly, these two learning tasks are more difﬁcult if the
environment is only partially observable.
The forms of learning in the preceding paragraph do not need to access the external
performance standard—in a sense, the standard is the universal one of making predictions
no tips from passengers who have been thoroughly shaken up during the trip. The external
performance standard must inform the agent that the loss of tips is a negative contribution to
its overall performance; then the agent might be able to learn that violent maneuvers do not
contribute to its own utility. In a sense, the performance standard distinguishes part of the
incoming percept as a reward (or penalty) that provides direct feedback on the quality of the
sented in many ways within the agent program, so there appears to be great variety amongThe Structure of Agents
learning methods. There is, however, a single unifying theme. Learning in intelligent agents
can be summarized as a process of modiﬁcation of each component of the agent to bring the
ing the overall performance of the agent.
How the components of agent programs work
nents, whose function it is to answer questions such as: “What is the world like now?” “What
action should I do now?” “What do my actions do?” The next question for a student of AI
is, “How on earth do these components work?” It takes about a thousand pages to begin to
answer that question properly, but here we want to draw the reader’s attention to some basic
distinctions among the various ways that the components can represent the environment that
the agent inhabits.
plexity and expressive power—atomic, factored, and structured. To illustrate these ideas,
it helps to consider a particular agent component, such as the one that deals with “What my
actions do.” This component describes the changes that might occur in the environment as
transitions might be represented.
B
C
(a) Atomic
(b) Factored
(b) Structured
B
C
Three ways to represent states and the transitions between them. (a) Atomic
representation: a state (such as B or C) is a black box with no internal structure; (b) Factored
valued, or one of a ﬁxed set of symbols. (c) Structured representation: a state includes
objects, each of which may have attributes of its own as well as relationships to other objects.
In an atomic representation each state of the world is indivisible—it has no internal
ATOMIC
REPRESENTATION
structure. Consider the problem of ﬁnding a driving route from one end of a country to the
purposes of solving this problem, it may sufﬁce to reduce the state of world to just the name
of the city we are in—a single atom of knowledge; a “black box” whose only discernible
property is that of being identical to or different from another black box. The algorithmsChapter
Intelligent Agents
least, they treat representations as if they were atomic.
concerned with more than just atomic location in one city or another; we might need to pay
attention to how much gas is in the tank, our current GPS coordinates, whether or not the oil
warning light is working, how much spare change we have for toll crossings, what station is
on the radio, and so on. A factored representation splits up each state into a ﬁxed set of
FACTORED
REPRESENTATION
variables or attributes, each of which can have a value. While two different atomic states
VARIABLE
ATTRIBUTE
VALUE
have nothing in common—they are just different black boxes—two different factored states
can share some attributes (such as being at some particular GPS location) and not others (such
as having lots of gas or having no gas); this makes it much easier to work out how to turn
one state into another. With factored representations, we can also represent uncertainty—for
example, ignorance about the amount of gas in the tank can be represented by leaving that
attribute blank. Many important areas of AI are based on factored representations, including
For many purposes, we need to understand the world as having things in it that are
related to each other, not just variables with values. For example, we might notice that a
large truck ahead of us is reversing into the driveway of a dairy farm but a cow has got loose
with the attribute TruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow with
STRUCTURED
REPRESENTATION
jects such as cows and trucks and their various and varying relationships can be described
concerns objects and their relationships.
sentations lie is the axis of increasing expressiveness. Roughly speaking, a more expressive
EXPRESSIVENESS
representation can capture, at least as concisely, everything a less expressive one can capture,
plus some more. Often, the more expressive language is much more concise; for example, the
language such as propositional logic. On the other hand, reasoning and learning become
more complex as the expressive power of the representation increases. To gain the beneﬁts
of expressive representations while avoiding their drawbacks, intelligent systems for the real
world may need to operate at all points along the axis simultaneously.Summary
SUMMARY
This chapter has been something of a whirlwind tour of AI, which we have conceived of as
the science of agent design. The major points to recall are as follows:
• An agent is something that perceives and acts in an environment. The agent function
for an agent speciﬁes the action taken by the agent in response to any percept sequence.
• The performance measure evaluates the behavior of the agent in an environment. A
rational agent acts so as to maximize the expected value of the performance measure,
given the percept sequence it has seen so far.
vironment, the actuators, and the sensors. In designing an agent, the ﬁrst step must
always be to specify the task environment as fully as possible.
• Task environments vary along several signiﬁcant dimensions. They can be fully or
sequential, static or dynamic, discrete or continuous, and known or unknown.
• The agent program implements the agent function. There exists a variety of basic
decision process. The designs vary in efﬁciency, compactness, and ﬂexibility. The
appropriate design of the agent program depends on the nature of the environment.
maintain internal state to track aspects of the world that are not evident in the current
maximize their own expected “happiness.”
• All agents can improve their performance through learning.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The central role of action in intelligence—the notion of practical reasoning—goes back at
least as far as Aristotle’s Nicomachean Ethics. Practical reasoning was also the subject of
and control theory are, by their very nature, concerned principally with physical agents. The
CONTROLLER
prisingly, AI has concentrated for most of its history on isolated components of agents—Chapter
Intelligent Agents
predicted that rational agent design would come to be seen as the core mission of AI, while
other popular topics would spin off to form new disciplines.
tional agent design is most apparent in the control theory tradition—for example, classical
vironments is also central in the dynamic programming literature developed in the ﬁeld of
Reﬂex agents were the primary model for psychological behaviorists such as Skinner
work in AI views the idea of pure reﬂex agents with state as too simple to provide much
soning to McCarthy’s early papers on logical AI. Shakey the Robot (Fikes and Nilsson,
AUTONOMIC
COMPUTING
trol themselves with a perceive–act loop and machine learning methods (Kephart and Chess,
multiagent environment necessarily exhibits modularity—the programs share no internal state
and communicate with each other only through the environment—it is common within the
ﬁeld of multiagent systems to design the agent program of a single agent as a collection of
MULTIAGENT
SYSTEMS
the same optimal solutions as a monolithic design.
Goals, further analyzed as desires (general) and intentions (currently pursued), are central toChapter
Intelligent Agents
b. There exist task environments in which no pure reﬂex agent can behave rationally.
c. There exists a task environment in which every agent is rational.
d. The input to an agent program is the same as the input to the agent function.
f. Suppose an agent selects its action uniformly at random from the set of possible actions.
There exists a deterministic task environment in which this agent is rational.
g. It is possible for a given agent to be perfectly rational in two distinct task environments.
h. Every agent is rational in an unobservable environment.
For each of the following activities, give a PEAS description of the task environment
• Playing soccer.
• Exploring the subsurface oceans of Titan.
• Shopping for used AI books on the Internet.
• Playing a tennis match.
• Practicing tennis against a wall.
• Performing a high jump.
• Knitting a sweater.
• Bidding on an item at an auction.
Deﬁne in your own words the following terms: agent, agent function, agent program,
learning agent.
This exercise explores the differences between agent functions and agent programs.
a. Can there be more than one agent program that implements a given agent function?
Give an example, or show why one is not possible.
b. Are there agent functions that cannot be implemented by any agent program?
c. Given a ﬁxed machine architecture, does each agent program implement exactly one
agent function?
grams are there?
e. Suppose we keep the agent program ﬁxed but speed up the machine by a factor of two.
Does that change the agent function?
The following exercises all concern the implementation of environments and agents for theSOLVING PROBLEMS BY
SEARCHING
In which we see how an agent can ﬁnd a sequence of actions that achieves its
goals when no single action will do.
a direct mapping from states to actions. Such agents cannot operate well in environments for
agents, on the other hand, consider future actions and the desirability of their outcomes.
AGENT
Our discussion of problem solving begins with precise deﬁnitions of problems and their
solutions and give several examples to illustrate these deﬁnitions. We then describe several
several uninformed search algorithms—algorithms that are given no information about the
problem other than its deﬁnition. Although some of these algorithms can solve any solvable
problem, none of them can do so efﬁciently. Informed search algorithms, on the other hand,
can do quite well given some guidance on where to look for solutions.
In this chapter, we limit ourselves to the simplest kind of task environment, for which
the solution to a problem is always a ﬁxed sequence of actions. The more general case—where
This chapter uses the concepts of asymptotic complexity (that is, O() notation) and
Intelligent agents are supposed to maximize their performance measure. As we mentioned
satisfying it. Let us ﬁrst look at why and how an agent might do this.Imagine an agent in the city of Arad, Romania, enjoying a touring holiday. The agent’s
manian, take in the sights, enjoy the nightlife (such as it is), avoid hangovers, and so on. The
ing day. In that case, it makes sense for the agent to adopt the goal of getting to Bucharest.
eration and the agent’s decision problem is greatly simpliﬁed. Goals help organize behavior
by limiting the objectives that the agent is trying to achieve and hence the actions it needs
to consider. Goal formulation, based on the current situation and the agent’s performance
GOAL FORMULATION
measure, is the ﬁrst step in problem solving.
We will consider a goal to be a set of world states—exactly those states in which the
goal is satisﬁed. The agent’s task is to ﬁnd out how to act, now and in the future, so that it
reaches a goal state. Before it can do this, it needs to decide (or we need to decide on its
behalf) what sorts of actions and states it should consider. If it were to consider actions at
the level of “move the left foot forward an inch” or “turn the steering wheel one degree left,”
the agent would probably never ﬁnd its way out of the parking lot, let alone to Bucharest,
because at that level of detail there is too much uncertainty in the world and there would be
too many steps in a solution. Problem formulation is the process of deciding what actions
PROBLEM
FORMULATION
and states to consider, given a goal. We discuss this process in more detail later. For now, let
us assume that the agent will consider actions at the level of driving from one major town to
another. Each state therefore corresponds to being in a particular town.
Our agent has now adopted the goal of driving to Bucharest and is considering where
to go from Arad. Three roads lead out of Arad, one toward Sibiu, one to Timisoara, and one
to Zerind. None of these achieves the goal, so unless the agent is familiar with the geography
which of its possible actions is best, because it does not yet know enough about the state
that results from taking each action. If the agent has no additional information—i.e., if the
But suppose the agent has a map of Romania. The point of a map is to provide the
agent with information about the states it might get itself into and the actions it can take. The
agent can use this information to consider subsequent stages of a hypothetical journey via
each of the three towns, trying to ﬁnd a journey that eventually gets to Bucharest. Once it has
found a path on the map from Arad to Bucharest, it can achieve its goal by carrying out the
driving actions that correspond to the legs of the journey. In general, an agent with several
immediate options of unknown value can decide what to do by ﬁrst examining future actions
that eventually lead to states of known value.
To be more speciﬁc about what we mean by “examining future actions,” we have to
as our agent. We apologize to Romanian readers who are unable to take advantage of this pedagogical device.Chapter
Solving Problems by Searching
we assume that the environment is observable, so the agent always knows the current state.
For the agent driving in Romania, it’s reasonable to suppose that each city on the map has a
sign indicating its presence to arriving drivers. We also assume the environment is discrete,
so at any given state there are only ﬁnitely many actions to choose from. This is true for
navigating in Romania because each city is connected to a small number of other cities. We
will assume the environment is known, so the agent knows which states are reached by each
action. (Having an accurate map sufﬁces to meet this condition for navigation problems.)
Finally, we assume that the environment is deterministic, so each action has exactly one
outcome. Under ideal conditions, this is true for the agent in Romania—it means that if it
chooses to drive from Arad to Sibiu, it does end up in Sibiu. Of course, conditions are not
Under these assumptions, the solution to any problem is a ﬁxed sequence of actions.
“Of course!” one might say, “What else could it be?” Well, in general it could be a branching
strategy that recommends different actions in the future depending on what percepts arrive.
For example, under less than ideal conditions, the agent might plan to drive from Arad to
Sibiu and then to Rimnicu Vilcea but may also need to have a contingency plan in case it
arrives by accident in Zerind instead of Sibiu. Fortunately, if the agent knows the initial state
and the environment is known and deterministic, it knows exactly where it will be after the
ﬁrst action and what it will perceive. Since only one percept is possible after the ﬁrst action,
the solution can specify only one possible second action, and so on.
The process of looking for a sequence of actions that reaches the goal is called search.
SEARCH
A search algorithm takes a problem as input and returns a solution in the form of an action
SOLUTION
sequence. Once a solution is found, the actions it recommends can be carried out. This
is called the execution phase. Thus, we have a simple “formulate, search, execute” design
EXECUTION
the agent calls a search procedure to solve it. It then uses the solution to guide its actions,
doing whatever the solution recommends as the next thing to do—typically, the ﬁrst action of
the sequence—and then removing that step from the sequence. Once the solution has been
executed, the agent will formulate a new goal.
Notice that while the agent is executing the solution sequence it ignores its percepts
when choosing an action because it knows in advance what they will be. An agent that
carries out its plans with its eyes closed, so to speak, must be quite certain of what is going
loop between agent and environment.
We ﬁrst describe the process of problem formulation, and then devote the bulk of the
chapter to various algorithms for the SEARCH function. We do not discuss the workings of
A problem can be deﬁned formally by ﬁve components:
PROBLEM
• The initial state that the agent starts in. For example, the initial state for our agent in
INITIAL STATE
Romania might be described as In(Arad).persistent: seq, an action sequence, initially empty
state, some description of the current world state
goal, a goal, initially null
problem, a problem formulation
if seq is empty then
seq ←SEARCH(problem)
if seq = failure then return a null action
action ←FIRST(seq)
seq ←REST(seq)
return action
searches for a sequence of actions that would solve the problem, and then executes the actions
one at a time. When this is complete, it formulates another goal and starts over.
• A description of the possible actions available to the agent. Given a particular state s,
ACTIONS
ACTIONS(s) returns the set of actions that can be executed in s. We say that each of
these actions is applicable in s. For example, from the state In(Arad), the applicable
APPLICABLE
actions are {Go(Sibiu), Go(Timisoara), Go(Zerind)}.
• A description of what each action does; the formal name for this is the transition
model, speciﬁed by a function RESULT(s, a) that returns the state that results from
TRANSITION MODEL
doing action a in state s. We also use the term successor to refer to any state reachable
SUCCESSOR
RESULT(In(Arad), Go(Zerind)) = In(Zerind) .
Together, the initial state, actions, and transition model implicitly deﬁne the state space
STATE SPACE
of the problem—the set of all states reachable from the initial state by any sequence
of actions. The state space forms a directed network or graph in which the nodes
GRAPH
are states and the links between nodes are actions. (The map of Romania shown in
for two driving actions, one in each direction.) A path in the state space is a sequence
PATH
of states connected by a sequence of actions.
• The goal test, which determines whether a given state is a goal state. Sometimes there
GOAL TEST
is an explicit set of possible goal states, and the test simply checks whether the given
state is one of them. The agent’s goal in Romania is the singleton set {In(Bucharest)}.
returns the set of all successors, instead of separate ACTIONS and RESULT functions. The successor function
makes it difﬁcult to describe an agent that knows what actions it can try but not what they achieve. Also, note
some author use RESULT(a, s) instead of RESULT(s, a), and some use DO instead of RESULT.Chapter
Solving Problems by Searching
Giurgiu
Urziceni
Hirsova
Eforie
Neamt
Oradea
Zerind
Arad
Timisoara
Lugoj
Mehadia
Drobeta
Craiova
Sibiu
Fagaras
Pitesti
Vaslui
Iasi
Rimnicu Vilcea
Bucharest
A simpliﬁed road map of part of Romania.
ated set of states. For example, in chess, the goal is to reach a state called “checkmate,”
where the opponent’s king is under attack and can’t escape.
PATH COST
agent chooses a cost function that reﬂects its own performance measure. For the agent
trying to get to Bucharest, time is of the essence, so the cost of a path might be its length
in kilometers. In this chapter, we assume that the cost of a path can be described as the
STEP COST
a in state s to reach state s′ is denoted by c(s, a, s′). The step costs for Romania are
The preceding elements deﬁne a problem and can be gathered into a single data structure
sequence that leads from the initial state to a goal state. Solution quality is measured by the
path cost function, and an optimal solution has the lowest path cost among all solutions.
OPTIMAL SOLUTION
Formulating problems
In the preceding section we proposed a formulation of the problem of getting to Bucharest in
terms of the initial state, actions, transition model, goal test, and path cost. This formulation
seems reasonable, but it is still a model—an abstract mathematical description—and not theExample Problems
country trip, where the state of the world includes so many things: the traveling companions,
the current radio program, the scenery out of the window, the proximity of law enforcement
ofﬁcers, the distance to the next rest stop, the condition of the road, the weather, and so on.
All these considerations are left out of our state descriptions because they are irrelevant to the
problem of ﬁnding a route to Bucharest. The process of removing detail from a representation
is called abstraction.
ABSTRACTION
In addition to abstracting the state description, we must abstract the actions themselves.
cupants, it takes up time, consumes fuel, generates pollution, and changes the agent (as they
say, travel is broadening). Our formulation takes into account only the change in location.
Also, there are many actions that we omit altogether: turning on the radio, looking out of
the window, slowing down for law enforcement ofﬁcers, and so on. And of course, we don’t
specify actions at the level of “turn steering wheel to the left by one degree.”
Can we be more precise about deﬁning the appropriate level of abstraction? Think of the
abstract states and actions we have chosen as corresponding to large sets of detailed world
states and detailed action sequences. Now consider a solution to the abstract problem: for
example, the path from Arad to Sibiu to Rimnicu Vilcea to Pitesti to Bucharest. This abstract
solution corresponds to a large number of more detailed paths. For example, we could drive
with the radio on between Sibiu and Rimnicu Vilcea, and then switch it off for the rest of
the trip. The abstraction is valid if we can expand any abstract solution into a solution in the
more detailed world; a sufﬁcient condition is that for every detailed state that is “in Arad,”
if carrying out each of the actions in the solution is easier than the original problem; in this
case they are easy enough that they can be carried out without further search or planning by
an average driving agent. The choice of a good abstraction thus involves removing as much
detail as possible while retaining validity and ensuring that the abstract actions are easy to
carry out. Were it not for the ability to construct useful abstractions, intelligent agents would
be completely swamped by the real world.
EXAMPLE PROBLEMS
TOY PROBLEM
given a concise, exact description and hence is usable by different researchers to compare the
PROBLEM
the general ﬂavor of their formulations.Chapter
Solving Problems by Searching
R
L
S
S
S
S
R
L
R
L
R
L
S
S
S
S
L
L
L
L
R
R
R
R
The state space for the vacuum world. Links denote actions: L = Left, R =
Right, S = Suck.
Toy problems
• States: The state is determined by both the agent location and the dirt locations. The
agent is in one of two locations, each of which might or might not contain dirt. Thus,
n · 2n states.
• Initial state: Any state can be designated as the initial state.
• Actions: In this simple environment, each state has just three actions: Left, Right, and
Suck. Larger environments might also include Up and Down.
• Transition model: The actions have their expected effects, except that moving Left in
the leftmost square, moving Right in the rightmost square, and Sucking in a clean square
• Goal test: This checks whether all the squares are clean.
Compared with the real world, this toy problem has discrete locations, discrete dirt, reliable
eight numbered tiles and a blank space. A tile adjacent to the blank space can slide into the
space. The object is to reach a speciﬁed goal state, such as the one shown on the right of the
ﬁgure. The standard formulation is as follows:Example Problems
Start State
Goal State
• States: A state description speciﬁes the location of each of the eight tiles and the blank
in one of the nine squares.
• Initial state: Any state can be designated as the initial state. Note that any given goal
• Actions: The simplest formulation deﬁnes the actions as movements of the blank space
Left, Right, Up, or Down. Different subsets of these are possible depending on where
the blank is.
• Transition model: Given a state and action, this returns the resulting state; for example,
switched.
What abstractions have we included here? The actions are abstracted to their beginning and
ﬁnal states, ignoring the intermediate locations where the block is sliding. We have abstracted
away actions such as shaking the board when pieces get stuck and ruled out extracting the
pieces with a knife and putting them back again. We are left with a description of the rules of
the puzzle, avoiding all the details of physical manipulations.
PUZZLES
so one does not expect to ﬁnd methods signiﬁcantly better in the worst case than the search
random instances can be solved optimally in a few milliseconds by the best search algorithms.
hours to solve optimally.
attacked by the queen at the top left.Chapter
Solving Problems by Searching
kinds of formulation. An incremental formulation involves operators that augment the state
INCREMENTAL
FORMULATION
FORMULATION
the board and moves them around. In either case, the path cost is of no interest because only
the ﬁnal state counts. The ﬁrst incremental formulation one might try is the following:
• Initial state: No queens on the board.
• Actions: Add a queen to any empty square.
• Transition model: Returns the board with a queen added to the speciﬁed square.
better formulation would prohibit placing a queen in any square that is already attacked:
leftmost n columns, with no queen attacking another.
• Actions: Add a queen to any square in the leftmost empty column such that it is not
attacked by any other queen.Example Problems
torial, square root, and ﬂoor operations will reach any desired positive integer. For example,





	


The problem deﬁnition is very simple:
• States: Positive numbers.
• Actions: Apply factorial, square root, or ﬂoor operation (factorial for integers only).
• Transition model: As given by the mathematical deﬁnitions of the operations.
• Goal test: State is the desired positive integer.
state spaces arise frequently in tasks involving the generation of mathematical expressions,
circuits, proofs, programs, and other recursively deﬁned objects.
PROBLEM
are relatively straightforward extensions of the Romania example. Others, such as routing
systems, involve much more complex speciﬁcations. Consider the airline travel problems that
• States: Each state obviously includes a location (e.g., an airport) and the current time.
Furthermore, because the cost of an action (a ﬂight segment) may depend on previous
segments, their fare bases, and their status as domestic or international, the state must
record extra information about these “historical” aspects.
• Initial state: This is speciﬁed by the user’s query.
• Actions: Take any ﬂight from the current location, in any seat class, leaving after the
nation as the current location and the ﬂight’s arrival time as the current time.
• Goal test: Are we at the ﬁnal destination speciﬁed by the user?
awards, and so on.Chapter
Solving Problems by Searching
soned traveler knows, however, that not all air travel goes according to plan. A really good
system should include contingency plans—such as backup reservations on alternate ﬂights—
to the extent that these are justiﬁed by the cost and likelihood of failure of the original plan.
TOURING PROBLEM
once, starting and ending in Bucharest.”
As with route ﬁnding, the actions correspond
to trips between adjacent cities. The state space, however, is quite different. Each state
must include not just the current location but also the set of cities the agent has visited.
ate state would be In(Vaslui), Visited({Bucharest, Urziceni, Vaslui}), and the goal test
The traveling salesperson problem (TSP) is a touring problem in which each city
TRAVELING
SALESPERSON
PROBLEM
must be visited exactly once. The aim is to ﬁnd the shortest tour. The problem is known to
of TSP algorithms. In addition to planning trips for traveling salespersons, these algorithms
stocking machines on shop ﬂoors.
A VLSI layout problem requires positioning millions of components and connections
VLSI LAYOUT
imize manufacturing yield. The layout problem comes after the logical design phase and is
usually split into two parts: cell layout and channel routing. In cell layout, the primitive
components of the circuit are grouped into cells, each of which performs some recognized
function. Each cell has a ﬁxed footprint (size and shape) and requires a certain number of
connections to each of the other cells. The aim is to place the cells on the chip so that they do
not overlap and so that there is room for the connecting wires to be placed between the cells.
Channel routing ﬁnds a speciﬁc route for each wire through the gaps between the cells. These
search problems are extremely complex, but deﬁnitely worth solving. Later in this chapter,
we present some algorithms capable of solving them.
ROBOT NAVIGATION
Rather than following a discrete set of routes, a robot can move in a continuous space with
(in principle) an inﬁnite set of possible actions and states. For a circular robot moving on a
techniques are required just to make the search space ﬁnite. We examine some of these
deal with errors in their sensor readings and motor controls.
Automatic assembly sequencing of complex objects by a robot was ﬁrst demonstrated
AUTOMATIC
ASSEMBLY
SEQUENCING
the assembly of intricate objects such as electric motors is economically feasible. In assembly
problems, the aim is to ﬁnd an order in which to assemble the parts of some object. If the
wrong order is chosen, there will be no way to add some part later in the sequence withoutSearching for Solutions
undoing some of the work already done. Checking a step in the sequence for feasibility is a
difﬁcult geometrical search problem closely related to robot navigation. Thus, the generation
of legal actions is the expensive part of assembly sequencing. Any practical algorithm must
avoid exploring all but a tiny fraction of the state space. Another important assembly problem
is protein design, in which the goal is to ﬁnd a sequence of amino acids that will fold into a
PROTEIN DESIGN
SEARCHING FOR SOLUTIONS
Having formulated some problems, we now need to solve them. A solution is an action
sequence, so search algorithms work by considering various possible action sequences. The
possible action sequences starting at the initial state form a search tree with the initial state
SEARCH TREE
at the root; the branches are actions and the nodes correspond to states in the state space of
NODE
from Arad to Bucharest. The root node of the tree corresponds to the initial state, In(Arad).
The ﬁrst step is to test whether this is a goal state. (Clearly it is not, but it is important to
check so that we can solve trick problems like “starting in Arad, get to Arad.”) Then we
need to consider taking various actions. We do this by expanding the current state; that is,
EXPANDING
applying each legal action to the current state, thereby generating a new set of states. In
GENERATING
this case, we add three branches from the parent node In(Arad) leading to three new child
PARENT NODE
nodes: In(Sibiu), In(Timisoara), and In(Zerind). Now we must choose which of these three
CHILD NODE
possibilities to consider further.
This is the essence of search—following up one option now and putting the others aside
for later, in case the ﬁrst choice does not lead to a solution. Suppose we choose Sibiu ﬁrst.
We check to see whether it is a goal state (it is not) and then expand it to get In(Arad),
In(Fagaras), In(Oradea), and In(RimnicuVilcea). We can then choose any of these four or go
back and choose Timisoara or Zerind. Each of these six nodes is a leaf node, that is, a node
LEAF NODE
with no children in the tree. The set of all leaf nodes available for expansion at any given
point is called the frontier. (Many authors call it the open list, which is both geographically
FRONTIER
OPEN LIST
less evocative and less accurate, because other data structures are better suited than a list.) In
The process of expanding nodes on the frontier continues until either a solution is found
SEARCH STRATEGY
is a repeated state in the search tree, generated in this case by a loopy path. Considering
REPEATED STATE
LOOPY PATH
such loopy paths means that the complete search tree for Romania is inﬁnite because there
is no limit to how often one can traverse a loop. On the other hand, the state space—theChapter
Solving Problems by Searching
certain algorithms to fail, making otherwise solvable problems unsolvable. Fortunately, there
is no need to consider loopy paths. We can rely on more than intuition for this: because path
costs are additive and step costs are nonnegative, a loopy path to any given state is never
better than the same path with the loop removed.
Loopy paths are a special case of the more general concept of redundant paths, which
REDUNDANT PATH
exist whenever there is more than one way to get from one state to another. Consider the paths
second path is redundant—it’s just a worse way to get to the same state. If you are concerned
about reaching the goal, there’s never any reason to keep more than one path to any given
state, because any goal state that is reachable by extending one path is also reachable by
extending the other.
In some cases, it is possible to deﬁne the problem itself so as to eliminate redundant
placed in any column, then each state with n queens can be reached by n! different paths; but
if we reformulate the problem so that each new queen is placed in the leftmost empty column,
then each state can be reached only through one path.
(a) The initial state
(b) After expanding Arad
(c) After expanding Sibiu
Rimnicu Vilcea
Lugoj
Arad
Fagaras
Oradea
Arad
Arad
Oradea
Rimnicu Vilcea
Lugoj
Zerind
Sibiu
Arad
Fagaras
Oradea
Timisoara
Arad
Arad
Oradea
Lugoj
Arad
Arad
Oradea
Zerind
Arad
Sibiu
Timisoara
Arad
Rimnicu Vilcea
Zerind
Arad
Sibiu
Arad
Fagaras
Oradea
Timisoara
Partial search trees for ﬁnding a route from Arad to Bucharest. Nodes that
have been expanded are shaded; nodes that have been generated but not yet expanded are
outlined in bold; nodes that have not yet been generated are shown in faint dashed lines.Searching for Solutions
initialize the frontier using the initial state of problem
loop do
if the frontier is empty then return failure
choose a leaf node and remove it from the frontier
if the node contains a goal state then return the corresponding solution
expand the chosen node, adding the resulting nodes to the frontier
initialize the frontier using the initial state of problem
initialize the explored set to be empty
loop do
if the frontier is empty then return failure
choose a leaf node and remove it from the frontier
if the node contains a goal state then return the corresponding solution
add the node to the explored set
expand the chosen node, adding the resulting nodes to the frontier
only if not in the frontier or explored set
handle repeated states.
In other cases, redundant paths are unavoidable.
This includes all problems where
RECTANGULAR GRID
tant example in computer games. In such a grid, each state has four successors, so a search
tree of depth d that includes repeated states has 4d leaves; but there are only about 2d2 distinct
become intractable. This is true even for algorithms that know how to avoid inﬁnite loops.
As the saying goes, algorithms that forget their history are doomed to repeat it.
The
way to avoid exploring redundant paths is to remember where one has been. To do this, we
EXPLORED SET
known as the closed list), which remembers every expanded node. Newly generated nodes
CLOSED LIST
design.
SEPARATORChapter
Solving Problems by Searching
third stage, the northernmost city (Oradea) has become a dead end: both of its successors are
already explored via other paths.
(c)
(b)
(a)
problem. The frontier (white nodes) always separates the explored region of the state space
panded. In (b), one leaf node has been expanded. In (c), the remaining successors of the root
have been expanded in clockwise order.
the initial state to an unexplored state has to pass through a state in the frontier. (If this
As every step moves a state from the frontier into the explored region while moving some
states from the unexplored region into the frontier, we see that the algorithm is systematically
examining the states in the state space, one by one, until it ﬁnds a solution.
Infrastructure for search algorithms
structed. For each node n of the tree, we have a structure that contains four components:
• n.STATE: the state in the state space to which the node corresponds;
• n.PARENT: the node in the search tree that generated this node;
• n.ACTION: the action that was applied to the parent to generate the node;
to the node, as indicated by the parent pointers.Searching for Solutions
Node
STATE
PARENT
ACTION = Right
Nodes are the data structures from which the search tree is constructed. Each
has a parent, a state, and various bookkeeping ﬁelds. Arrows point from child to parent.
Given the components for a parent node, it is easy to see how to compute the necessary
and returns the resulting child node:
return a node with
STATE = problem.RESULT(parent.STATE,action),
PARENT = parent, ACTION = action,
string the nodes together into a tree structure. These pointers also allow the solution path to be
extracted when a goal node is found; we use the SOLUTION function to return the sequence
of actions obtained by following parent pointers back to the root.
Up to now, we have not been very careful to distinguish between nodes and states, but in
writing detailed algorithms it’s important to make that distinction. A node is a bookkeeping
data structure used to represent the search tree. A state corresponds to a conﬁguration of the
world. Thus, nodes are on particular paths, as deﬁned by PARENT pointers, whereas states
are not. Furthermore, two different nodes can contain the same world state if that state is
generated via two different search paths.
Now that we have nodes, we need somewhere to put them. The frontier needs to be
stored in such a way that the search algorithm can easily choose the next node to expand
according to its preferred strategy. The appropriate data structure for this is a queue. The
QUEUE
operations on a queue are as follows:
• EMPTY?(queue) returns true only if there are no more elements in the queue.
• POP(queue) removes the ﬁrst element of the queue and returns it.
• INSERT(element, queue) inserts an element and returns the resulting queue.Chapter
Solving Problems by Searching
Queues are characterized by the order in which they store the inserted nodes. Three common
FIFO QUEUE
LIFO QUEUE
of the queue; and the priority queue, which pops the element of the queue with the highest
PRIORITY QUEUE
priority according to some ordering function.
The explored set can be implemented with a hash table to allow efﬁcient checking for
repeated states. With a good implementation, insertion and lookup can be done in roughly
constant time no matter how many states are stored. One must take care to implement the
hash table with the right notion of equality between states. For example, in the traveling
{Bucharest,Urziceni,Vaslui} is the same as {Urziceni,Vaslui,Bucharest}. Sometimes this can
be achieved most easily by insisting that the data structures for states be in some canonical
form; that is, logically equivalent states should map to the same data structure. In the case
CANONICAL FORM
repetition would be canonical, whereas an unsorted list would not.
Before we get into the design of speciﬁc search algorithms, we need to consider the criteria
that might be used to choose among them. We can evaluate an algorithm’s performance in
four ways:
• Completeness: Is the algorithm guaranteed to ﬁnd a solution when there is one?
COMPLETENESS
OPTIMALITY
• Time complexity: How long does it take to ﬁnd a solution?
TIME COMPLEXITY
• Space complexity: How much memory is needed to perform the search?
SPACE COMPLEXITY
lem difﬁculty. In theoretical computer science, the typical measure is the size of the state
space graph, |V | + |E|, where V is the set of vertices (nodes) of the graph and E is the set
of edges (links). This is appropriate when the graph is an explicit data structure that is input
to the search program. (The map of Romania is an example of this.) In AI, the graph is often
represented implicitly by the initial state, actions, and transition model and is frequently inﬁ-
nite. For these reasons, complexity is expressed in terms of three quantities: b, the branching
factor or maximum number of successors of any node; d, the depth of the shallowest goal
BRANCHING FACTOR
DEPTH
node (i.e., the number of steps along the path from the root); and m, the maximum length of
any path in the state space. Time is often measured in terms of the number of nodes generated
during the search, and space in terms of the maximum number of nodes stored in memory.
For the most part, we describe time and space complexity for search on a tree; for a graph,
the answer depends on how “redundant” the paths in the state space are.
To assess the effectiveness of a search algorithm, we can consider just the search cost—
SEARCH COST
which typically depends on the time complexity but can also include a term for memory
usage—or we can use the total cost, which combines the search cost and the path cost of the
TOTAL COST
solution found. For the problem of ﬁnding a route from Arad to Bucharest, the search cost
is the amount of time taken by the search and the solution cost is the total length of the pathUninformed Search Strategies
in kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers.
There is no “ofﬁcial exchange rate” between the two, but it might be reasonable in this case to
convert kilometers into milliseconds by using an estimate of the car’s average speed (because
time is what the agent cares about). This enables the agent to ﬁnd an optimal tradeoff point
at which further computation to ﬁnd a shorter path becomes counterproductive. The more
UNINFORMED SEARCH STRATEGIES
This section covers several search strategies that come under the heading of uninformed
search (also called blind search). The term means that the strategies have no additional
UNINFORMED
SEARCH
BLIND SEARCH
information about states beyond that provided in the problem deﬁnition. All they can do is
are distinguished by the order in which nodes are expanded. Strategies that know whether
INFORMED SEARCH
HEURISTIC SEARCH
SEARCH
successors of the root node are expanded next, then their successors, and so on. In general,
all the nodes are expanded at a given depth in the search tree before any nodes at the next
level are expanded.
which the shallowest unexpanded node is chosen for expansion. This is achieved very simply
by using a FIFO queue for the frontier. Thus, new nodes (which are always deeper than their
parents) go to the back of the queue, and old nodes, which are shallower than the new nodes,
that the goal test is applied to each node when it is generated rather than when it is selected for
expansion. This decision is explained below, where we discuss time complexity. Note also
that the algorithm, following the general template for graph search, discards any new path to
a state already in the frontier or explored set; it is easy to see that any such path must be at
path to every node on the frontier.
simple binary tree.
tion? We can easily see that it is complete—if the shallowest goal node is at some ﬁnite depth
the branching factor b is ﬁnite). Note that as soon as a goal node is generated, we know it
is the shallowest goal node because all shallower nodes must have been generated already
and failed the goal test. Now, the shallowest goal node is not necessarily the optimal one;Chapter
Solving Problems by Searching
frontier ←a FIFO queue with node as the only element
explored ←an empty set
loop do
if EMPTY?(frontier) then return failure
node ←POP(frontier) /* chooses the shallowest node in frontier */
add node.STATE to explored
for each action in problem.ACTIONS(node.STATE) do
if child.STATE is not in explored or frontier then
frontier ←INSERT(child,frontier)
depth of the node. The most common such scenario is that all actions have the same cost.
space is not so good. Imagine searching a uniform tree where every state has b successors.
The root of the search tree generates b nodes at the ﬁrst level, each of which generates b more
nodes, for a total of b2 at the second level. Each of these generates b more nodes, yielding b3
nodes at the third level, and so on. Now suppose that the solution is at depth d. In the worst
case, it is the last node generated at that level. Then the total number of nodes generated is
b + b2 + b3 + · · · + bd = O(bd) .
(If the algorithm were to apply the goal test to nodes when selected for expansion, rather than
when generated, the whole layer of nodes at depth d would be expanded before the goal was
As for space complexity: for any kind of graph search, which stores every expanded
node in the explored set, the space complexity is always within a factor of b of the time
A
B
C
E
F
G
D
A
B
D
E
F
G
C
A
C
D
E
F
G
B
B
C
D
E
F
G
A
expanded next is indicated by a marker.Uninformed Search Strategies
so the space complexity is O(bd), i.e., it is dominated by the size of the frontier. Switching
to a tree search would not save much space, and in a state space with many redundant paths,
switching could cost a great deal of time.
personal computer.
Depth
Nodes
Time
Memory
the petabyte of memory it would take. Fortunately, other strategies require less memory.
The second lesson is that time is still a major factor. If your problem has a solution at
cannot be solved by uninformed methods for any but the smallest instances.
shallowest unexpanded node. By a simple extension, we can ﬁnd an algorithm that is optimal
SEARCH
expands the node n with the lowest path cost g(n). This is done by storing the frontier as a
In addition to the ordering of the queue by path cost, there are two other signiﬁcant
rather than when it is ﬁrst generated. The reason is that the ﬁrst goal node that is generatedChapter
Solving Problems by Searching
explored ←an empty set
loop do
if EMPTY?(frontier) then return failure
add node.STATE to explored
for each action in problem.ACTIONS(node.STATE) do
if child.STATE is not in explored or frontier then
frontier ←INSERT(child,frontier)
replace that frontier node with child
of an extra check in case a shorter path to a frontier state is discovered. The data structure for
frontier needs to support efﬁcient membership testing, so it should combine the capabilities
of a priority queue and a hash table.
Sibiu
Fagaras
Pitesti
Rimnicu Vilcea
Bucharest
may be on a suboptimal path. The second difference is that a test is added in case a better
path is found to a node currently on the frontier.
the problem is to get from Sibiu to Bucharest. The successors of Sibiu are Rimnicu Vilcea andUninformed Search Strategies
is selected for expansion and the solution is returned.
has been found. (Were this not the case, there would have to be another frontier node n′ on
because step costs are nonnegative, paths never get shorter as nodes are added. These two
cost. Hence, the ﬁrst goal node selected for expansion must be the optimal solution.
their total cost. Therefore, it will get stuck in an inﬁnite loop if there is a path with an inﬁnite
cost search can explore large trees of small steps before exploring paths involving large and
work by expanding nodes at depth d unnecessarily.
SEARCH
to the deepest level of the search tree, where the nodes have no successors. As those nodes
are expanded, they are dropped from the frontier, so then the search “backs up” to the next
deepest node that still has unexplored successors.
A LIFO queue means that the most recently generated node is chosen for expansion. This
must be the deepest unexpanded node because it is one deeper than its parent—which, in turn,
was the deepest unexpanded node when it was selected.Chapter
Solving Problems by Searching
A
C
F
G
M
N
O
A
C
F
G
L
M
N
O
A
C
F
G
L
M
N
O
C
F
G
L
M
N
O
A
B
C
E
F
G
K
L
M
N
O
A
C
E
F
G
J
K
L
M
N
O
A
C
E
F
G
J
K
L
M
N
O
A
B
C
D
E
F
G
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
gray. Explored nodes with no descendants in the frontier are removed from memory. Nodes
dundant paths, is complete in ﬁnite state spaces because it will eventually expand every node.
modiﬁed at no extra memory cost so that it checks new states against those on the path from
the root to the current node; this avoids inﬁnite loops in ﬁnite state spaces but does not avoid
the proliferation of redundant paths. In inﬁnite state spaces, both versions fail if an inﬁnite
keep applying the factorial operator forever.
ﬁrst search will explore the entire left subtree even if node C is a goal node. If node J wereUninformed Search Strategies
all of the O(bm) nodes in the search tree, where m is the maximum depth of any node; this
can be much greater than the size of the state space. Note that m itself can be much larger
than d (the depth of the shallowest solution) and is inﬁnite if the tree is unbounded.
so why do we include it? The reason is the space complexity. For a graph search, there is
to a leaf node, along with the remaining unexpanded sibling nodes for each node on the
path. Once a node has been expanded, it can be removed from memory as soon as all its
BACKTRACKING
SEARCH
than all successors; each partially expanded node remembers which successor to generate
next. In this way, only O(m) memory is needed rather than O(bm). Backtracking search
cessor by modifying the current state description directly rather than copying it ﬁrst. This
reduces the memory requirements to just one state description and O(m) actions. For this to
cessor. For problems with large state descriptions, such as robotic assembly, these techniques
are critical to success.
SEARCH
source of incompleteness if we choose ℓ< d, that is, the shallowest goal is beyond the depth
Sometimes, depth limits can be based on knowledge of the problem. For example, onChapter
Solving Problems by Searching
else
cutoﬀoccurred? ←false
for each action in problem.ACTIONS(node.STATE) do
if result = cutoﬀthen cutoﬀoccurred? ←true
else if result ̸= failure then return result
if cutoﬀoccurred? then return cutoﬀelse return failure
map carefully, we would discover that any city can be reached from any other city in at most
DIAMETER
not know a good depth limit until we have solved the problem.
kinds of failure: the standard failure value indicates no solution; the cutoﬀvalue indicates
no solution within the depth limit.
ITERATIVE
DEEPENING SEARCH
This will occur when the depth limit reaches d, the depth of the shallowest goal node. The
solution is found on the fourth iteration.
Iterative deepening search may seem wasteful because states are generated multiple
times. It turns out this is not too costly. The reason is that in a search tree with the same (or
nearly the same) branching factor at each level, most of the nodes are in the bottom level,
so it does not matter much that the upper levels are generated multiple times. In an iterative
deepening search, the nodes on the bottom level (depth d) are generated once, those on theUninformed Search Strategies
if result ̸= cutoff then return result
limited search returns failure, meaning that no solution exists.
A
A
A
B
C
A
B
C
A
B
C
A
B
C
A
B
C
D
E
F
G
A
B
C
D
E
F
G
A
B
C
D
E
F
G
A
B
C
D
E
F
G
A
B
C
D
E
F
G
A
B
C
D
E
F
G
A
B
C
D
E
F
G
A
B
C
D
E
F
G
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
A
B
C
D
E
F
G
H
J
K
L
M
N
O
I
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
Four iterations of iterative deepening search on a binary tree.Chapter
Solving Problems by Searching
generated d times. So the total number of nodes generated in the worst case is
There is some extra cost for generating the upper levels multiple times, but it is not large. For
If you are really concerned about repeating the repetition, you can use a hybrid approach
runs iterative deepening from all the nodes in the frontier. In general, iterative deepening is
the preferred uninformed search method when the search space is large and the depth of the
solution is not known.
plete layer of new nodes at each iteration before going on to the next layer. It would seem
rithm’s optimality guarantees while avoiding its memory requirements. The idea is to use
ITERATIVE
LENGTHENING
SEARCH
Bidirectional search
The idea behind bidirectional search is to run two simultaneous searches—one forward from
the initial state and the other backward from the goal—hoping that the two searches meet in
ﬁgure, the area of the two small circles is less than the area of one big circle centered on the
start and reaching to the goal.
Bidirectional search is implemented by replacing the goal test with a check to see
whether the frontiers of the two searches intersect; if they do, a solution has been found.
(It is important to realize that the ﬁrst such solution found may not be optimal, even if the
or selected for expansion and, with a hash table, will take constant time. For example, if a
time, then in the worst case the two searches meet when they have generated all of the nodes
We can reduce this by roughly half if one of the two searches is done by iterative deepening,
but at least one of the frontiers must be kept in memory so that the intersection check can be
done. This space requirement is the most signiﬁcant weakness of bidirectional search.Uninformed Search Strategies
Goal
Start
A schematic view of a bidirectional search that is about to succeed when a
branch from the start node meets a branch from the goal node.
The reduction in time complexity makes bidirectional search attractive, but how do we
search backward? This is not as easy as it sounds. Let the predecessors of a state x be all
PREDECESSOR
those states that have x as a successor. Bidirectional search requires a method for computing
predecessors. When all the actions in the state space are reversible, the predecessors of x are
just its successors. Other cases may require substantial ingenuity.
Consider the question of what we mean by “the goal” in searching “backward from the
backward search is very much like the forward search. If there are several explicitly listed
new dummy goal state whose immediate predecessors are all the actual goal states. But if the
goal is an abstract description, such as the goal that “no queen attacks another queen” in the
Comparing uninformed search strategies
time complexities are bounded by the size of the state space.
Criterion
Iterative
Bidirectional
First
Cost
First
Limited
Deepening
(if applicable)
Complete?
Yesa
Yesa,b
No
No
Yesa
Yesa,d
Time
O(bd)
O(bm)
O(bℓ)
O(bd)
Space
O(bd)
O(bm)
O(bℓ)
O(bd)
Optimal?
Yesc
Yes
No
No
Yesc
Yesc,d
of the shallowest solution; m is the maximum depth of the search tree; l is the depth limit.Chapter
Solving Problems by Searching
INFORMED (HEURISTIC) SEARCH STRATEGIES
INFORMED SEARCH
edge beyond the deﬁnition of the problem itself—can ﬁnd solutions more efﬁciently than can
an uninformed strategy.
selected for expansion based on an evaluation function, f(n). The evaluation function is
EVALUATION
FUNCTION
construed as a cost estimate, so the node with the lowest evaluation is expanded ﬁrst. The
include as a component of f a heuristic function, denoted h(n):
HEURISTIC
FUNCTION
h(n) = estimated cost of the cheapest path from the state at node n to a goal state.
(Notice that h(n) takes a node as input, but, unlike g(n), it depends only on the state at that
node.) For example, in Romania, one might estimate the cost of the cheapest path from Arad
Heuristic functions are the most common form in which additional knowledge of the
to use heuristic information to guide search.
SEARCH
that this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the
heuristic function; that is, f(n) = h(n).
line distance heuristic, which we will call hSLD.
If the goal is Bucharest, we need to
DISTANCE
problem description itself. Moreover, it takes a certain amount of experience to know that
hSLD is correlated with actual road distances and is, therefore, a useful heuristic.
from Arad to Bucharest. The ﬁrst node to be expanded from Arad will be Sibiu because it
is closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will
be Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. ForInformed (Heuristic) Search Strategies
Urziceni
Neamt
Oradea
Zerind
Timisoara
Mehadia
Sibiu
Pitesti
Rimnicu Vilcea
Vaslui
Bucharest
Giurgiu
Hirsova
Eforie
Arad
Lugoj
Drobeta
Craiova
Fagaras
Iasi
expanding a node that is not on the solution path; hence, its search cost is minimal. It is
than the path through Rimnicu Vilcea and Pitesti. This shows why the algorithm is called
“greedy”—at each step it tries to get as close to the goal as it can.
gests that Neamt be expanded ﬁrst because it is closest to Fagaras, but it is a dead end. The
solution is to go ﬁrst to Vaslui—a step that is actually farther from the goal according to
the heuristic—and then to continue to Urziceni, Bucharest, and Fagaras. The algorithm will
never ﬁnd this solution, however, because expanding Neamt puts Iasi back into the frontier,
Iasi is closer to Fagaras than Vaslui is, and so Iasi will be expanded again, leading to an inﬁ-
nite loop. (The graph search version is complete in ﬁnite spaces, but not in inﬁnite ones.) The
depth of the search space. With a good heuristic function, however, the complexity can be
reduced substantially. The amount of the reduction depends on the particular problem and on
the quality of the heuristic.
A∗SEARCH
search”). It evaluates nodes by combining g(n), the cost to reach the node, and h(n), the cost
to get from the node to the goal:
f(n) = g(n) + h(n) .
Since g(n) gives the path cost from the start node to node n, and h(n) is the estimated cost
of the cheapest path from n to the goal, we have
f(n) = estimated cost of the cheapest solution through n .
Thus, if we are trying to ﬁnd the cheapest solution, a reasonable thing to try ﬁrst is the
node with the lowest value of g(n) + h(n). It turns out that this strategy is more than just
reasonable: provided that the heuristic function h(n) satisﬁes certain conditions, A∗search is
that A∗uses g + h instead of g.Chapter
Solving Problems by Searching
Rimnicu Vilcea
Zerind
Arad
Sibiu
Arad
Fagaras
Oradea
Timisoara
Sibiu
Bucharest
Rimnicu Vilcea
Arad
Sibiu
Arad
Fagaras
Oradea
Timisoara
Zerind
Zerind
Arad
Sibiu
Timisoara
Arad
(a) The initial state
(b) After expanding Arad
(c) After expanding Sibiu
(d) After expanding Fagaras
Conditions for optimality: Admissibility and consistency
The ﬁrst condition we require for optimality is that h(n) be an admissible heuristic. An
ADMISSIBLE
HEURISTIC
admissible heuristic is one that never overestimates the cost to reach the goal. Because g(n)
is the actual cost to reach n along the current path, and f(n) = g(n) + h(n), we have as an
immediate consequence that f(n) never overestimates the true cost of a solution along the
current path through n.
Admissible heuristics are by nature optimistic because they think the cost of solving
the problem is less than it actually is. An obvious example of an admissible heuristic is the
admissible because the shortest path between any two points is a straight line, so the straightInformed (Heuristic) Search Strategies
A second, slightly stronger condition called consistency (or sometimes monotonicity)
CONSISTENCY
MONOTONICITY
every node n and every successor n′ of n generated by any action a, the estimated cost of
reaching the goal from n is no greater than the step cost of getting to n′ plus the estimated
cost of reaching the goal from n′:
h(n) ≤c(n, a, n′) + h(n′) .
This is a form of the general triangle inequality, which stipulates that each side of a triangle
TRIANGLE
INEQUALITY
cannot be longer than the sum of the other two sides. Here, the triangle is formed by n, n′,
and the goal Gn closest to n. For an admissible heuristic, the inequality makes perfect sense:
if there were a route from n to Gn via n′ that was cheaper than h(n), that would violate the
property that h(n) is a lower bound on the cost to reach Gn.
Consistency is therefore a stricter requirement than admissibility, but one has to work quite
hard to concoct heuristics that are admissible but not consistent. All the admissible heuristics
we discuss in this chapter are also consistent. Consider, for example, hSLD. We know that
Hence, hSLD is a consistent heuristic.
f—just as in the A∗algorithm itself.
The ﬁrst step is to establish the following: if h(n) is consistent, then the values of
f(n) along any path are nondecreasing. The proof follows directly from the deﬁnition of
consistency. Suppose n′ is a successor of n; then g(n′) = g(n) + c(n, a, n′) for some action
a, and we have
f(n′) = g(n′) + h(n′) = g(n) + c(n, a, n′) + h(n′) ≥g(n) + h(n) = f(n) .
The next step is to prove that whenever A∗selects a node n for expansion, the optimal path
to that node has been found. Were this not the case, there would have to be another frontier
node n′ on the optimal path from the start node to n, by the graph separation property ofChapter
Solving Problems by Searching
(a) The initial state
(b) After expanding Arad
(c) After expanding Sibiu
Arad
Sibiu
Timisoara
Zerind
Arad
(d) After expanding Rimnicu Vilcea
(e) After expanding Fagaras
(f) After expanding Pitesti
Zerind
Arad
Sibiu
Arad
Timisoara
Rimnicu Vilcea
Fagaras
Oradea
Zerind
Arad
Sibiu
Timisoara
Rimnicu Vilcea
Craiova
Pitesti
Sibiu
Zerind
Arad
Sibiu
Arad
Timisoara
Sibiu
Bucharest
Fagaras
Oradea
Craiova
Pitesti
Sibiu
Zerind
Arad
Sibiu
Arad
Timisoara
Sibiu
Bucharest
Oradea
Craiova
Pitesti
Sibiu
Bucharest
Craiova
Rimnicu Vilcea
Rimnicu Vilcea
Fagaras
Rimnicu Vilcea
Arad
Fagaras
Oradea
Stages in an A∗search for Bucharest. Nodes are labeled with f = g + h. TheInformed (Heuristic) Search Strategies
O
Z
A
T
L
M
D
C
R
F
P
G
B
U
H
E
V
I
N
S
contour value.
and would have been selected ﬁrst.
From the two preceding observations, it follows that the sequence of nodes expanded
selected for expansion must be an optimal solution because f is the true cost for goal nodes
CONTOUR
around the start state. With more accurate heuristics, the bands will stretch toward the goal
state and become more narrowly focused around the optimal path. If C∗is the cost of the
optimal solution path, then we can say the following:
• A∗expands all nodes with f(n) < C∗.
• A∗might then expand some of the nodes right on the “goal contour” (where f(n) = C∗)
before selecting a goal node.
Completeness requires that there be only ﬁnitely many nodes with cost less than or equal to
Notice that A∗expands no nodes with f(n) > C∗—for example, Timisoara is notChapter
Solving Problems by Searching
Timisoara is pruned; because hSLD is admissible, the algorithm can safely ignore this subtree
PRUNING
while still guaranteeing optimality. The concept of pruning—eliminating possibilities from
consideration without having to examine them—is important for many areas of AI.
One ﬁnal observation is that among optimal algorithms of this type—algorithms that
extend search paths from the root and use the same heuristic information—A∗is optimally
OPTIMALLY
EFFICIENT
f(n) = C∗). This is because any algorithm that does not expand all nodes with f(n) < C∗
runs the risk of missing the optimal solution.
That A∗search is complete, optimal, and optimally efﬁcient among all such algorithms
is rather satisfying. Unfortunately, it does not mean that A∗is the answer to all our searching
needs. The catch is that, for most problems, the number of states within the goal contour
search space is still exponential in the length of the solution. The details of the analysis are
beyond the scope of this book, but the basic results are as follows. For problems with constant
step costs, the growth in run time as a function of the optimal solution depth d is analyzed in
terms of the the absolute error or the relative error of the heuristic. The absolute error is
ABSOLUTE ERROR
RELATIVE ERROR
deﬁned as Δ ≡h∗−h, where h∗is the actual cost of getting from the root to the goal, and
The complexity results depend very strongly on the assumptions made about the state
space. The simplest model studied is a state space that has a single goal and is essentially a
In this case, the time complexity of A∗is exponential in the maximum absolute error, that is,
For almost all heuristics in practical use, the absolute error is at least proportional to the path
search process can be led astray from the optimal path and there is an extra cost proportional
general case of a graph, the situation is even worse. There can be exponentially many states
with f(n) < C∗even if the absolute error is bounded by a constant. For example, consider
a version of the vacuum world where the agent can clean up any square for unit cost without
even having to visit it: in that case, squares can be cleaned in any order. With N initially dirty
squares, there are 2N states where some subset has been cleaned and all of them are on an
The complexity of A∗often makes it impractical to insist on ﬁnding an optimal solution.
One can use variants of A∗that ﬁnd suboptimal solutions quickly, or one can sometimes
design heuristics that are more accurate but not strictly admissible. In any case, the use of a
good heuristic still provides enormous savings compared to the use of an uninformed search.
Computation time is not, however, A∗’s main drawback. Because it keeps all generatedInformed (Heuristic) Search Strategies
successors ←[ ]
for each action in problem.ACTIONS(node.STATE) do
if successors is empty then return failure, ∞
for each s in successors do /* update f with value from previous search, if any */
s.f ←max(s.g + s.h, node.f ))
loop do
if best.f > f limit then return failure, best.f
result,best.f ←RBFS(problem,best,min(f limit, alternative))
if result ̸= failure then return result
lems. There are, however, algorithms that overcome the space problem without sacriﬁcing
optimality or completeness, at a small cost in execution time. We discuss these next.
The simplest way to reduce memory requirements for A∗is to adapt the idea of iterative
DEEPENING
A∗
gorithm. The main difference between IDA∗and standard iterative deepening is that the cutoff
for many problems with unit step costs and avoids the substantial overhead associated with
RECURSIVE
rather than continuing indeﬁnitely down the current path, it uses the f limit variable to keep
node. If the current node exceeds this limit, the recursion unwinds back to the alternativeChapter
Solving Problems by Searching
Zerind
Arad
Sibiu
Arad
Fagaras
Oradea
Craiova
Sibiu
Bucharest
Craiova
Rimnicu Vilcea
Zerind
Arad
Sibiu
Arad
Sibiu
Bucharest
Rimnicu Vilcea
Oradea
Zerind
Arad
Sibiu
Arad
Timisoara
Timisoara
Timisoara
Fagaras
Oradea
Rimnicu Vilcea
Craiova
Pitesti
Sibiu
Rimnicu Vilcea
Fagaras
(a) After expanding Arad, Sibiu, 
      and Rimnicu Vilcea
(c) After switching back to Rimnicu Vilcea
      and expanding Pitesti
(b) After unwinding back to Sibiu 
      and expanding Fagaras
∞
∞
∞
Pitesti
value for each recursive call is shown on top of each current node, and every node is labeled
has a value that is worse than the best alternative path (Fagaras). (b) The recursion unwinds
the expansion continues to Bucharest.Informed (Heuristic) Search Strategies
“changes its mind” and tries Fagaras, and then changes its mind back again. These mind
increase—h is usually less optimistic for nodes closer to the goal. When this happens, the
Each mind change corresponds to an iteration of IDA∗and could require many reexpansions
of forgotten nodes to recreate the best path and extend it one more node.
Like A∗tree search, RBFS is an optimal algorithm if the heuristic function h(n) is
admissible. Its space complexity is linear in the depth of the deepest optimal solution, but
its time complexity is rather difﬁcult to characterize: it depends both on the accuracy of the
heuristic function and on how often the best path changes as nodes are expanded.
IDA∗and RBFS suffer from using too little memory. Between iterations, IDA∗retains
but it uses only linear space: even if more memory were available, RBFS has no way to make
panding the same states many times over. Furthermore, they suffer the potentially exponential
It seems sensible, therefore, to use all available memory. Two algorithms that do this
we will describe it. SMA∗proceeds just like A∗, expanding the best leaf until memory is full.
At this point, it cannot add a new node to the search tree without dropping an old one. SMA∗
then backs up the value of the forgotten node to its parent. In this way, the ancestor of a
forgotten subtree knows the quality of the best path in that subtree. With this information,
SMA∗regenerates the subtree only when all other paths have been shown to look worse than
the path it has forgotten. Another way of saying this is that, if all the descendants of a node n
are forgotten, then we will not know which way to go from n, but we will still have an idea
of how worthwhile it is to go anywhere from n.
worth mentioning. We said that SMA∗expands the best leaf and deletes the worst leaf. What
and expansion, SMA∗expands the newest best leaf and deletes the oldest worst leaf. These
coincide when there is only one leaf, but in that case, the current search tree must be a single
path from root to leaf that ﬁlls all of memory. If the leaf is not a goal node, then even if it is on
an optimal solution path, that solution is not reachable with the available memory. Therefore,
the node can be discarded exactly as if it had no successors.
SMA∗is complete if there is any reachable solution—that is, if d, the depth of the
shallowest goal node, is less than the memory size (expressed in nodes). It is optimal if any
optimal solution is reachable; otherwise, it returns the best reachable solution. In practical
terms, SMA∗is a fairly robust choice for ﬁnding optimal solutions, particularly when the state
space is a graph, step costs are not uniform, and node generation is expensive compared to
the overhead of maintaining the frontier and the explored set.Chapter
Solving Problems by Searching
On very hard problems, however, it will often be the case that SMA∗is forced to switch
back and forth continually among many candidate solution paths, only a small subset of which
can ﬁt in memory. (This resembles the problem of thrashing in disk paging systems.) Then
THRASHING
the extra time required for repeated regeneration of the same nodes means that problems
that would be practically solvable by A∗, given unlimited memory, become intractable for
SMA∗. That is to say, memory limitations can make a problem intractable from the point
of view of computation time. Although no current theory explains the tradeoff between time
and memory, it seems that this is an inescapable problem. The only way out is to drop the
optimality requirement.
Learning to search better
have been designed by computer scientists. Could an agent learn how to search better? The
answer is yes, and the method rests on an important concept called the metalevel state space.
METALEVEL STATE
SPACE
Each state in a metalevel state space captures the internal (computational) state of a program
SPACE
state of the A∗algorithm consists of the current search tree. Each action in the metalevel state
space is a computation step that alters the internal state; for example, each computation step
a sequence of larger and larger search trees, can be seen as depicting a path in the metalevel
that is not especially helpful. For harder problems, there will be many such missteps, and a
METALEVEL
LEARNING
goal of learning is to minimize the total cost of problem solving, trading off computational
expense and path cost.
HEURISTIC FUNCTIONS
heuristics in general.
possible; when it is in a corner, two; and when it is along an edge, three.) This meansHeuristic Functions
Start State
Goal State
to ﬁnd a good heuristic function. If we want to ﬁnd the shortest solutions by using A∗, we
need a heuristic function that never overestimates the number of steps to the goal. There is a
is clear that any tile that is out of place must be moved at least once.
• h2 = the sum of the distances of the tiles from their goal positions. Because tiles
cannot move along diagonals, the distance we will count is the sum of the horizontal
and vertical distances. This is sometimes called the city block distance or Manhattan
distance. h2 is also admissible because all any move can do is move one tile one step
MANHATTAN
DISTANCE
The effect of heuristic accuracy on performance
One way to characterize the quality of a heuristic is the effective branching factor b∗. If the
EFFECTIVE
BRANCHING FACTOR
total number of nodes generated by A∗for a particular problem is N and the solution depth is
d, then b∗is the branching factor that a uniform tree of depth d would have to have in order
it is fairly constant for sufﬁciently hard problems. (The existence of an effective branching
factor follows from the result, mentioned earlier, that the number of nodes expanded by A∗
grows exponentially with solution depth.) Therefore, experimental measurements of b∗on a
solved at reasonable computational cost.Chapter
Solving Problems by Searching
number of nodes generated by each strategy and the effective branching factor. The results
suggest that h2 is better than h1, and is far better than using iterative deepening search. Even
iterative deepening search.
Search Cost (nodes generated)
Effective Branching Factor
d
IDS
A∗(h1)
A∗(h2)
IDS
A∗(h1)
A∗(h2)
–
–
–
–
–
–
–
–
–
–
–
–
Comparison of the search costs and effective branching factors for the
One might ask whether h2 is always better than h1. The answer is “Essentially, yes.” It
is easy to see from the deﬁnitions of the two heuristics that, for any node n, h2(n) ≥h1(n).
We thus say that h2 dominates h1. Domination translates directly into efﬁciency: A∗using
DOMINATION
h2 will never expand more nodes than A∗using h1 (except possibly for some nodes with
with f(n) < C∗will surely be expanded. This is the same as saying that every node with
h(n) < C∗−g(n) will surely be expanded. But because h2 is at least as big as h1 for all
nodes, every node that is surely expanded by A∗search with h2 will also surely be expanded
with h1, and h1 might cause other nodes to be expanded as well. Hence, it is generally
better to use a heuristic function with higher values, provided it is consistent and that the
computation time for the heuristic is not too long.
Generating admissible heuristics from relaxed problems
We have seen that both h1 (misplaced tiles) and h2 (Manhattan distance) are fairly good
possible for a computer to invent such a heuristic mechanically?
perfectly accurate path lengths for simpliﬁed versions of the puzzle. If the rules of the puzzleHeuristic Functions
were changed so that a tile could move anywhere instead of just to the adjacent empty square,
then h1 would give the exact number of steps in the shortest solution. Similarly, if a tile could
move one square in any direction, even onto an occupied square, then h2 would give the exact
number of steps in the shortest solution. A problem with fewer restrictions on the actions is
RELAXED PROBLEM
the original state space because the removal of restrictions creates added edges in the graph.
Because the relaxed problem adds edges to the state space, any optimal solution in the
original problem is, by deﬁnition, also a solution in the relaxed problem; but the relaxed
problem may have better solutions if the added edges provide short cuts. Hence, the cost of
an optimal solution to a relaxed problem is an admissible heuristic for the original problem.
Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it must
If a problem deﬁnition is written down in a formal language, it is possible to construct
A tile can move from square A to square B if
A is horizontally or vertically adjacent to B and B is blank,
we can generate three relaxed problems by removing one or both of the conditions:
(a) A tile can move from square A to square B if A is adjacent to B.
(b) A tile can move from square A to square B if B is blank.
(c) A tile can move from square A to square B.
From (a), we can derive h2 (Manhattan distance). The reasoning is that h2 would be the
proper score if we moved each tile in turn to its destination. The heuristic derived from (b) is
the proper score if tiles could move to their intended destination in one step. Notice that it is
crucial that the relaxed problems generated by this technique can be solved essentially without
search, because the relaxed rules allow the problem to be decomposed into eight independent
subproblems. If the relaxed problem is hard to solve, then the values of the corresponding
heuristic and found the ﬁrst useful heuristic for the famous Rubik’s Cube puzzle.
One problem with generating new heuristic functions is that one often fails to get a
single “clearly best” heuristic. If a collection of admissible heuristics h1 . . . hm is available
for a problem and none of them dominates any of the others, which should we choose? As it
turns out, we need not make a choice. We can have the best of all worlds, by deﬁning
h(n) = max{h1(n), . . . , hm(n)} .
manipulated, the construction of relaxed problems can be automated. For now, we use English.
sly.” Thus, there is a tradeoff between accuracy and computation time for heuristic functions.Chapter
Solving Problems by Searching
Start State
Goal State
the other tiles.
This composite heuristic uses whichever function is most accurate on the node in question.
Because the component heuristics are admissible, h is admissible; it is also easy to prove that
h is consistent. Furthermore, h dominates all of its component heuristics.
Generating admissible heuristics from subproblems: Pattern databases
Admissible heuristics can also be derived from the solution cost of a subproblem of a given
SUBPROBLEM
plete problem. It turns out to be more accurate than Manhattan distance in some cases.
PATTERN DATABASE
sible subproblem instance—in our example, every possible conﬁguration of the four tiles
ing the subproblem, but moves of those tiles do count toward the cost.) Then we compute
an admissible heuristic hDB for each complete state encountered during a search simply by
looking up the corresponding subproblem conﬁguration in the database. The database itself is
countered; the expense of this search is amortized over many subsequent problem instances.
be combined, as explained earlier, by taking the maximum value. A combined heuristic of
this kind is much more accurate than the Manhattan distance; the number of nodes generatedHeuristic Functions
the sum of the two costs is still a lower bound on the cost of solving the entire problem. This
is the idea behind disjoint pattern databases. With such databases, it is possible to solve
DISJOINT PATTERN
DATABASES
roughly a factor of a million can be obtained.
divided up in such a way that each move affects only one subproblem—because only one tile
is moved at a time. For a problem such as Rubik’s Cube, this kind of subdivision is difﬁcult
but they have not yielded a heuristic better than the best nonadditive heuristic for the problem.
Learning heuristics from experience
A heuristic function h(n) is supposed to estimate the cost of a solution beginning from the
state at node n. How could an agent construct such a function? One solution was given in
the preceding sections—namely, to devise relaxed problems for which an optimal solution
can be found easily. Another solution is to learn from experience. “Experience” here means
tion path and the actual cost of the solution from that point. From these examples, a learning
algorithm can be used to construct a function h(n) that can (with luck) predict solution costs
Inductive learning methods work best when supplied with features of a state that are
FEATURE
relevant to predicting the state’s value, rather than with just the raw state description. For
example, the feature “number of misplaced tiles” might be helpful in predicting the actual
these data, the value of x1 can be used to predict h(n). Of course, we can use several features.
A second feature x2(n) might be “number of pairs of adjacent tiles that are not adjacent in the
goal state.” How should x1(n) and x2(n) be combined to predict h(n)? A common approach
is to use a linear combination:
h(n) = c1x1(n) + c2x2(n) .
The constants c1 and c2 are adjusted to give the best ﬁt to the actual data on solution costs.
One expects both c1 and c2 to be positive because misplaced tiles and incorrect adjacent pairs
make the problem harder to solve. Notice that this heuristic does satisfy the condition thatChapter
Solving Problems by Searching
SUMMARY
This chapter has introduced methods that an agent can use to select actions in environments
that are deterministic, observable, static, and completely known. In such cases, the agent can
construct sequences of actions that achieve its goals; this process is called search.
deﬁned problem must be formulated.
• A problem consists of ﬁve parts: the initial state, a set of actions, a transition model
describing the results of those actions, a goal test function, and a path cost function.
The environment of the problem is represented by a state space. A path through the
state space from the initial state to a goal state is a solution.
• Search algorithms treat states and actions as atomic: they do not consider any internal
structure they might possess.
ity, and space complexity. Complexity depends on b, the branching factor in the state
space, and d, the depth of the shallowest solution.
• Uninformed search methods have access only to the problem deﬁnition. The basic
algorithms are as follows:
for unit step costs, but has exponential space complexity.
for general step costs.
depth bound.
until a goal is found. It is complete, optimal for unit step costs, has time complexity
– Bidirectional search can enormously reduce time complexity, but it is not always
applicable and may require too much space.
• Informed search methods may have access to a heuristic function h(n) that estimates
the cost of a solution from n.
an evaluation function.
is often efﬁcient.Bibliographical and Historical Notes
– A∗search expands nodes with minimal f(n) = g(n) + h(n). A∗is complete and
are robust, optimal search algorithms that use limited amounts of memory; given
enough time, they can solve problems that A∗cannot solve because it runs out of
memory.
• The performance of heuristic search algorithms depends on the quality of the heuristic
function. One can sometimes construct good heuristics by relaxing the problem deﬁ-
nition, by storing precomputed solution costs for subproblems in a pattern database, or
by learning from experience with the problem class.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
lishment of search algorithms as the primary weapons in the armory of 1960s AI researchers
and to the establishment of problem solving as the canonical AI task. Work in operations
plifying optimization algorithms. The text on Automated Problem Solving by Nils Nilsson
in the literature and are less trivial than they might seem. The missionaries and cannibals
patent covering sliding blocks with letters, numbers, or pictures was granted to Ernest Kinsey
safely be said to have engaged the attention of nine out of ten persons of both sexes and all
and at that time drew the attention of the eminent mathematician Carl Friedrich Gauss, whoChapter
Solving Problems by Searching
O(n) algorithm.
good deal of research effort. Methods for selecting optimal airline ﬂights remain proprietary
for the most part, but Carl de Marcken (personal communication) has shown that airline ticket
pricing and restrictions have become so convoluted that the problem of selecting an optimal
many layout optimization papers appear in VLSI journals. Robotic navigation and assembly
(closed and open lists).
A version of iterative deepening designed to make efﬁcient use of the chess clock was
performance with admissible but inconsistent heuristics. The iterative deepening technique
came to the fore in work by Korf (1985a). Bidirectional search, which was introduced by
The use of heuristic information in problem solving appears in an early paper by Simon
ber of nodes examined so far), they appear to have ignored the information provided by the
path cost g(n). The A∗algorithm, incorporating the current path cost into heuristic search,
The original A∗paper introduced the consistency condition on heuristic functions. The
showed that the two were equivalent.
tions and the time complexity of A∗. Basic results were obtained for tree search with unit stepBibliographical and Historical Notes
cost is better modeled as O(bd−k), where k depends on the heuristic accuracy; this analysis
paths, implying exponential time complexity for A∗even with constant absolute error in h.
weighting, which uses a weighted sum fw(n) = wgg(n) + whh(n) of the current path length
and the heuristic function as an evaluation function, rather than the simple sum f(n) = g(n)+
h(n) used in A∗. The weights wg and wh are adjusted dynamically as the search progresses.
same property is exhibited by the A∗
selection can be done so as to minimize search cost.
Bidirectional versions of A∗have been investigated; a combination of bidirectional A∗
and known landmarks was used to efﬁciently ﬁnd driving routes for Microsoft’s online map
techniques that are widely used in operations research
The
of CDP—the “composite decision process.”
Because computers in the late 1950s and early 1960s had at most a few thousand words
of variants have been developed. An analysis of the efﬁciency of IDA∗and of its difﬁculties
ITERATIVE
EXPANSIONChapter
Solving Problems by Searching
order even with an inadmissible heuristic. The idea of keeping track of the best alternative
and metalevel learning.
The idea that admissible heuristics can be derived by problem relaxation appears in the
show how to compress pattern databases to save space. The probabilistic interpretation of
By far the most comprehensive source on heuristics and heuristic search algorithms
variety of offshoots and variations of A∗, including rigorous proofs of their formal properties.
search algorithms—which, remarkably, continue to be discovered—appear in journals such
as Artiﬁcial Intelligence and Journal of the ACM.
The topic of parallel search algorithms was not covered in the chapter, partly because
PARALLEL SEARCH
it requires a lengthy discussion of parallel computer architectures. Parallel search became a
popular topic in the 1990s in both AI and theoretical computer science (Mahanti and Daniels,
Also of increasing importance are search algorithms for very large graphs that require disk
EXERCISES
Explain why problem formulation must follow goal formulation.
Your goal is to navigate a robot out of a maze. The robot starts in the center of the mazeChapter
Solving Problems by Searching
S
G
A scene with polygonal obstacles. S and G are the start and goal states.
c. You have a program that outputs the message “illegal input record” when fed a certain
ﬁle of input records. You know that processing of each record is independent of the
other records. You want to discover what record is illegal.
You can ﬁll the jugs up or empty them out from one to another or onto the ground. You
need to measure out exactly one gallon.
Consider the problem of ﬁnding the shortest path between two points on a plane that has
that a robot has to solve to navigate in a crowded environment.
a. Suppose the state space consists of all positions (x, y) in the plane. How many states
are there? How many paths are there to the goal?
b. Explain brieﬂy why the shortest path from one polygon vertex to any other in the scene
Deﬁne a good state space now. How large is this state space?
c. Deﬁne the necessary functions to implement the search problem, including an ACTIONS
function that takes a vertex as input and returns a set of vectors, each of which maps the
current vertex to one of the vertices that can be reached in a straight line. (Do not forget
function.
d. Apply one or more of the algorithms in this chapter to solve a range of problems in the
domain, and comment on their performance.
this exercise, we explore this decision in more depth.
bility would force any optimal algorithm to explore the entire state space.Chapter
Solving Problems by Searching
The track pieces in a wooden railway set; each is labeled with the number of
copies in the set. Note that curved pieces and “fork” pieces (“switches” or “points”) can be
Which of the following are true and which are false? Explain your answers.
ble heuristic.
c. A∗is of no use in robotics because percepts, states, and actions are continuous.
e. Assume that a rook can move on a chessboard any number of squares in a straight line,
vertically or horizontally, but cannot jump over other pieces. Manhattan distance is an
admissible heuristic for the problem of moving the rook from square A to square B in
the smallest number of moves.
c. How well would bidirectional search work on this problem? What is the branching
factor in each direction of the bidirectional search?
d. Does the answer to (c) suggest a reformulation of the problem that would allow you to
ﬁnd an algorithm that outputs the solution to this problem without any search at all?
connect these pieces into a railway that has no overlapping tracks and no loose ends where a
train could run off onto the ﬂoor.
a. Suppose that the pieces ﬁt together exactly with no slack. Give a precise formulation of
the task as a search problem.
b. Identify a suitable uninformed search algorithm for this task and explain your choice.
c. Explain why removing any one of the “fork” pieces makes the problem unsolvable.Chapter
Solving Problems by Searching
Compare the performance of A∗and RBFS on a set of randomly generated problems
Trace the operation of A∗search applied to the problem of getting to Bucharest from
algorithm will consider and the f, g, and h score for each node.
with an h(n) function that is admissible but inconsistent.
HEURISTIC PATH
ALGORITHM
For what values is it optimal, assuming that h is admissible? What kind of search does this
a. What is the branching factor b in this state space?
e. Is h = |u −x| + |v −y| an admissible heuristic for a state at (u, v)? Explain.
f. How many nodes are expanded by A∗graph search using h?
g. Does h remain admissible if some links are removed?
h. Does h remain admissible if some links are added between nonadjacent states?
The vehicles must be moved to the top row but in reverse order; so the vehicle i that starts in
one square up, down, left, or right, or stay put; but if a vehicle stays put, one other adjacent
vehicle (but not more than one) can hop over it. Two vehicles cannot occupy the same square.
a. Calculate the size of the state space as a function of n.
b. Calculate the branching factor as a function of n.
c. Suppose that vehicle i is at (xi, yi); write a nontrivial admissible heuristic hi for the
other vehicles are on the grid.
cles to their destinations? Explain.
(i) n
(ii) max{h1, . . . , hn}.
(iii) min{h1, . . . , hn}.BEYOND CLASSICAL
SEARCH
In which we relax the simplifying assumptions of the previous chapter, thereby
getting closer to the real world.
ronments where the solution is a sequence of actions. In this chapter, we look at what happens
ing one or more current states rather than systematically exploring paths from an initial state.
These algorithms are suitable for problems in which all that matters is the solution state, not
the path cost to reach it. The family of local search algorithms includes methods inspired by
statistical physics (simulated annealing) and evolutionary biology (genetic algorithms).
of determinism and observability. The key idea is that if an agent cannot predict exactly what
percept it will receive, then it will need to consider what to do under each contingency that
its percepts may reveal. With partial observability, the agent will also need to keep track of
the states it might be in.
space that is initially unknown and must be explored.
LOCAL SEARCH ALGORITHMS AND OPTIMIZATION PROBLEMS
tematically. This systematicity is achieved by keeping one or more paths in memory and by
recording which alternatives have been explored at each point along the path. When a goal is
what matters is the ﬁnal conﬁguration of queens, not the order in which they are added. The
network optimization, vehicle routing, and portfolio management.Local Search Algorithms and Optimization Problems
rithms, ones that do not worry about paths at all. Local search algorithms operate using
LOCAL SEARCH
a single current node (rather than multiple paths) and generally move only to neighbors
CURRENT NODE
of that node. Typically, the paths followed by the search are not retained. Although local
or inﬁnite (continuous) state spaces for which systematic algorithms are unsuitable.
timization problems, in which the aim is to ﬁnd the best state according to an objective
OPTIMIZATION
PROBLEM
function. Many optimization problems do not ﬁt the “standard” search model introduced in
OBJECTIVE
FUNCTION
Darwinian evolution could be seen as attempting to optimize, but there is no “goal test” and
no “path cost” for this problem.
LANDSCAPE
by the value of the heuristic cost function or objective function). If elevation corresponds to
cost, then the aim is to ﬁnd the lowest valley—a global minimum; if elevation corresponds
GLOBAL MINIMUM
to an objective function, then the aim is to ﬁnd the highest peak—a global maximum. (You
GLOBAL MAXIMUM
can convert from one to the other just by inserting a minus sign.) Local search algorithms
explore this landscape. A complete local search algorithm always ﬁnds a goal if one exists;
current
state
objective function
state space
global maximum
local maximum
“flat” local maximum
shoulder
the current state to try to improve it, as shown by the arrow. The various topographic features
are deﬁned in the text.Chapter
Beyond Classical Search
loop do
current ←neighbor
nique. At each step the current node is replaced by the best neighbor; in this version, that
means the neighbor with the highest VALUE, but if a heuristic cost estimate h is used, we
would ﬁnd the neighbor with the lowest h.
HILL CLIMBING
STEEPEST ASCENT
simply a loop that continually moves in the direction of increasing value—that is, uphill. It
terminates when it reaches a “peak” where no neighbor has a higher value. The algorithm
does not maintain a search tree, so the data structure for the current node need only record
the state and the value of the objective function. Hill climbing does not look ahead beyond
the immediate neighbors of the current state. This resembles trying to ﬁnd the top of Mount
Everest in a thick fog while suffering from amnesia.
generated by moving a single queen to another square in the same column (so each state has
are attacking each other, either directly or indirectly. The global minimum of this function
is more than one.
Hill climbing is sometimes called greedy local search because it grabs a good neighbor
GREEDY LOCAL
SEARCH
state without thinking ahead about where to go next. Although greed is considered one of the
seven deadly sins, it turns out that greedy algorithms often perform quite well. Hill climbing
often makes rapid progress toward a solution because it is usually quite easy to improve a bad
often gets stuck for the following reasons:
• Local maxima: a local maximum is a peak that is higher than each of its neighboring
LOCAL MAXIMUM
vicinity of a local maximum will be drawn upward toward the peak but will then beLocal Search Algorithms and Optimization Problems
(a)
(b)
h for each possible successor obtained by moving a queen within its column. The best moves
successor has a higher cost.
cost h); every move of a single queen makes the situation worse.
RIDGE
that is very difﬁcult for greedy algorithms to navigate.
PLATEAU
maximum, from which no uphill exit exists, or a shoulder, from which progress is
SHOULDER
In each case, the algorithm reaches a point at which no progress is being made. Starting from
the same value as the current state. Might it not be a good idea to keep going—to allow a
SIDEWAYS MOVE
answer is usually yes, but we must take care. If we always allow sideways moves when there
are no uphill moves, an inﬁnite loop will occur whenever the algorithm reaches a ﬂat localChapter
Beyond Classical Search
Illustration of why ridges cause difﬁculties for hill climbing. The grid of states
(dark circles) is superimposed on a ridge rising from left to right, creating a sequence of local
maxima that are not directly connected to each other. From each local maximum, all the
available actions point downhill.
Many variants of hill climbing have been invented. Stochastic hill climbing chooses at
STOCHASTIC HILL
CLIMBING
random from among the uphill moves; the probability of selection can vary with the steepness
of the uphill move. This usually converges more slowly than steepest ascent, but in some
CLIMBING
hill climbing by generating successors randomly until one is generated that is better than the
current state. This is a good strategy when a state has many (e.g., thousands) of successors.
HILL CLIMBING
ﬁxed amount of time and that this can be much more efﬁcient than letting each search continue indeﬁnitely.
Disallowing or limiting the number of sideways moves is an example of this idea.Local Search Algorithms and Optimization Problems
good solution very quickly. On the other hand, many real problems have a landscape that
looks more like a widely scattered family of balding porcupines on a ﬂat ﬂoor, with miniature
cally have an exponential number of local maxima to get stuck on. Despite this, a reasonably
good local maximum can often be found after a small number of restarts.
Simulated annealing
mum. In contrast, a purely random walk—that is, moving to a successor chosen uniformly
at random from the set of successors—is complete but extremely inefﬁcient. Therefore, it
seems reasonable to try to combine hill climbing with a random walk in some way that yields
both efﬁciency and completeness. Simulated annealing is such an algorithm. In metallurgy,
SIMULATED
ANNEALING
annealing is the process used to temper or harden metals and glass by heating them to a
energy crystalline state. To explain simulated annealing, we switch our point of view from
hill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a
GRADIENT DESCENT
come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the
solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the
intensity of the shaking (i.e., lower the temperature).
hill climbing. Instead of picking the best move, however, it picks a random move. If the move
improves the situation, it is always accepted. Otherwise, the algorithm accepts the move with
creases as the “temperature” T goes down: “bad” moves are more likely to be allowed at the
start when T is high, and they become more unlikely as T decreases. If the schedule lowers
Simulated annealing was ﬁrst used extensively to solve VLSI layout problems in the
Local beam search
Keeping just one node in memory might seem to be an extreme reaction to the problem of
memory limitations. The local beam search algorithm3 keeps track of k states rather than
LOCAL BEAM
SEARCHChapter
Beyond Classical Search
inputs: problem, a problem
schedule, a mapping from time to “temperature”
T ←schedule(t)
next ←a randomly selected successor of current
ΔE ←next.VALUE – current.VALUE
The simulated annealing algorithm, a version of stochastic hill climbing where
ing schedule and then less often as time goes on. The schedule input determines the value of
the temperature T as a function of time.
just one. It begins with k randomly generated states. At each step, all the successors of all k
states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best
successors from the complete list and repeats.
At ﬁrst sight, a local beam search with k states might seem to be nothing more than
running k random restarts in parallel instead of in sequence. In fact, the two algorithms
the others. In a local beam search, useful information is passed among the parallel search
threads. In effect, the states that generate the best successors say to the others, “Come over
here, the grass is greener!” The algorithm quickly abandons unfruitful searches and moves
its resources to where the most progress is being made.
In its simplest form, local beam search can suffer from a lack of diversity among the
k states—they can quickly become concentrated in a small region of the state space, making
the search little more than an expensive version of hill climbing. A variant called stochastic
beam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead
STOCHASTIC BEAM
SEARCH
of choosing the best k from the the pool of candidate successors, stochastic beam search
chooses k successors at random, with the probability of choosing a given successor being
an increasing function of its value. Stochastic beam search bears some resemblance to the
process of natural selection, whereby the “successors” (offspring) of a “state” (organism)
populate the next generation according to its “value” (ﬁtness).
Genetic algorithms
A genetic algorithm (or GA) is a variant of stochastic beam search in which successor states
GENETIC
ALGORITHM
are generated by combining two parent states rather than by modifying a single state. The
analogy to natural selection is the same as in stochastic beam search, except that now we are
dealing with sexual rather than asexual reproduction.Local Search Algorithms and Optimization Problems
(a)
Initial Population
(b)
Fitness Function
(c)
Selection
(d)
Crossover
(e)
Mutation
The initial population in (a) is ranked by the ﬁtness function in (b), resulting in pairs for
mating in (c). They produce offspring in (d), which are subject to mutation in (e).
+
=
unshaded columns are retained.
Like beam searches, GAs begin with a set of k randomly generated states, called the
population. Each state, or individual, is represented as a string over a ﬁnite alphabet—most
POPULATION
INDIVIDUAL
each state is rated by the objective function, or (in GA terminology) the ﬁtness function. A
FITNESS FUNCTION
algorithm, the probability of being chosen for reproducing is directly proportional to the
ﬁtness score, and the percentages are shown next to the raw scores.Chapter
Beyond Classical Search
pair to be mated, a crossover point is chosen randomly from the positions in the string. In
CROSSOVER
In (d), the offspring themselves are created by crossing over the parent strings at the
crossover point. For example, the ﬁrst child of the ﬁrst pair gets the ﬁrst three digits from the
ﬁrst parent and the remaining digits from the second parent, whereas the second child gets
when two parent states are quite different, the crossover operation can produce a state that is
a long way from either parent state. It is often the case that the population is quite diverse
early on in the process, so crossover (like simulated annealing) frequently takes large steps in
the state space early in the search process and smaller steps later on when most individuals
are quite similar.
Finally, in (e), each location is subject to random mutation with a small independent
MUTATION
problem, this corresponds to choosing a queen at random and moving it to a random square
dom exploration and exchange of information among parallel search threads. The primary
advantage, if any, of genetic algorithms comes from the crossover operation. Yet it can be
shown mathematically that, if the positions of the genetic code are permuted initially in a
random order, crossover conveys no advantage. Intuitively, the advantage comes from the
form useful functions, thus raising the level of granularity at which the search operates. For
not attack each other) constitutes a useful block that can be combined with other blocks to
construct a solution.
The theory of genetic algorithms explains how this works using the idea of a schema,
SCHEMA
which is a substring in which some of the positions can be left unspeciﬁed. For example,
called instances of the schema. It can be shown that if the average ﬁtness of the instances of
INSTANCE
a schema is above the mean, then the number of instances of the schema within the population
will grow over time. Clearly, this effect is unlikely to be signiﬁcant if adjacent bits are totally
unrelated to each other, because then there will be few contiguous blocks that provide a
consistent beneﬁt. Genetic algorithms work best when schemata correspond to meaningful
components of a solution. For example, if the string is a representation of an antenna, then the
schemata may represent components of the antenna, such as reﬂectors and deﬂectors. A goodLocal Search in Continuous Spaces
inputs: population, a set of individuals
repeat
new population ←empty set
child ←REPRODUCE(x,y)
if (small random probability) then child ←MUTATE(child)
add child to new population
population ←new population
until some individual is ﬁt enough, or enough time has elapsed
function REPRODUCE(x,y) returns an individual
inputs: x,y, parent individuals
A genetic algorithm. The algorithm is the same as the one diagrammed in
produces only one offspring, not two.
component is likely to be good in a variety of different designs. This suggests that successful
use of genetic algorithms requires careful engineering of the representation.
In practice, genetic algorithms have had a widespread impact on optimization problems,
of genetic algorithms arises from their performance or from their æsthetically pleasing origins
in the theory of evolution. Much work remains to be done to identify the conditions under
which genetic algorithms perform well.
LOCAL SEARCH IN CONTINUOUS SPACES
continuous state and action spaces, because they have inﬁnite branching factors. This section
tions in continuous spaces. The literature on this topic is vast; many of the basic techniquesChapter
Beyond Classical Search
EVOLUTION AND SEARCH
The theory of evolution was developed in Charles Darwin’s On the Origin of
will be preserved in successive generations approximately in proportion to their
effect on reproductive ﬁtness.
DNA molecule and its alphabet, AGTC (adenine, guanine, thymine, cytosine). In
the standard model, variation occurs both by point mutations in the letter sequence
and by “crossover” (in which the DNA of an offspring is generated by combining
long sections of DNA from each parent).
production, wherein successors are generated from multiple organisms rather than
just one. The actual mechanisms of evolution are, however, far richer than most
tions, and movement of large chunks of DNA; some viruses borrow DNA from one
organism and insert it in another; and there are transposable genes that do nothing
but copy themselves many thousands of times within the genome. There are even
creasing their own chances of replication. Most important is the fact that the genes
lated into an organism. In genetic algorithms, those mechanisms are a separate
program that is not represented within the strings being manipulated.
Darwinian evolution may appear inefﬁcient, having generated blindly some
Fifty
years before Darwin, however, the otherwise great French naturalist Jean Lamarck
ing an organism’s lifetime would be passed on to its offspring. Such a process
lection pressures operating on individuals that have found local optima among the
set of possible behaviors allowed by their genetic makeup. Computer simulations
conﬁrm that the “Baldwin effect” is real, once “ordinary” evolution has created
organisms whose internal performance measure correlates with actual ﬁtness.Local Search in Continuous Spaces
ﬁnd uses for these techniques at several places in the book, including the chapters on learning,
vision, and robotics.
We begin with an example. Suppose we want to place three new airports anywhere
to its nearest airport is minimized. The state space is then deﬁned by the coordinates of
VARIABLE
vector of variables, x.) Moving around in this space corresponds to moving one or more of
the airports on the map. The objective function f(x1, y1, x2, y2, x3, y3) is relatively easy to
compute for any particular state once we compute the closest cities. Let Ci be the set of
cities whose closest airport (in the current state) is airport i. Then, in the neighborhood of the
current state, where the Cis remain constant, we have
f(x1, y1, x2, y2, x3, y3) =




This expression is correct locally, but not globally because the sets Ci are (discontinuous)
functions of the state.
One way to avoid continuous problems is simply to discretize the neighborhood of each
DISCRETIZATION
state. For example, we can move only one airport at a time in either the x or y direction by
ply stochastic hill climbing and simulated annealing directly, without discretizing the space.
Many methods attempt to use the gradient of the landscape to ﬁnd a maximum. The
GRADIENT
gradient of the objective function is a vector ∇f that gives the magnitude and direction of the
steepest slope. For our problem, we have
∇f =

 ∂f
∂x1
, ∂f
∂y1
, ∂f
∂x2
, ∂f
∂y2
, ∂f
∂x3
, ∂f
∂y3

.
for example, if we were placing just one airport; the solution is the arithmetic mean of all the
cities’ coordinates.) In many cases, however, this equation cannot be solved in closed form.
For example, with three airports, the expression for the gradient depends on what cities are
closest to each airport in the current state. This means we can compute the gradient locally
(but not globally); for example,
∂f
∂x1


(xi −xc) .Chapter
Beyond Classical Search
ing by updating the current state according to the formula
x ←x + α∇f(x) ,
where α is a small constant often called the step size. In other cases, the objective function
STEP SIZE
might not be available in a differentiable form at all—for example, the value of a particular set
EMPIRICAL
GRADIENT
response to small increments and decrements in each coordinate. Empirical gradient search
Hidden beneath the phrase “α is a small constant” lies a huge variety of methods for
adjusting α. The basic problem is that, if α is too small, too many steps are needed; if α
is too large, the search could overshoot the maximum. The technique of line search tries to
LINE SEARCH
overcome this dilemma by extending the current gradient direction—usually by repeatedly
doubling α—until f starts to decrease again. The point at which this occurs becomes the new
current state. There are several schools of thought about how the new direction should be
chosen at this point.
For many problems, the most effective algorithm is the venerable Newton–Raphson
NEWTON–RAPHSON
method. This is a general technique for ﬁnding roots of functions—that is, solving equations
Newton’s formula
x ←x −g(x)/g′(x) .
To ﬁnd a maximum or minimum of f, we need to ﬁnd x such that the gradient is zero (i.e.,
be written in matrix–vector form as
f (x)∇f(x) ,
where Hf(x) is the Hessian matrix of second derivatives, whose elements Hij are given
HESSIAN
i are just twice the number of cities in Ci. A moment’s calculation shows that one step of
the update moves airport i directly to the centroid of Ci, which is the minimum of the local
the n2 entries of the Hessian and inverting it may be expensive, so many approximate versions
of the Newton–Raphson method have been developed.
Local search methods suffer from local maxima, ridges, and plateaux in continuous
state spaces just as much as in discrete spaces. Random restarts and simulated annealing can
in which it is easy to get lost.
A ﬁnal topic with which a passing acquaintance is useful is constrained optimization.
CONSTRAINED
OPTIMIZATION
An optimization problem is constrained if solutions must satisfy some hard constraints on the
directly to the minimum of that surface—which is also the minimum of f if f is quadratic.Searching with Nondeterministic Actions
to be inside Romania and on dry land (rather than in the middle of lakes). The difﬁculty of
constrained optimization problems depends on the nature of the constraints and the objective
LINEAR
PROGRAMMING
CONVEX SET
linear. The time complexity of linear programming is polynomial in the number of variables.
Linear programming is probably the most widely studied and broadly useful class of
mization, which allows the constraint region to be any convex region and the objective to
CONVEX
OPTIMIZATION
be any function that is convex within the constraint region. Under certain conditions, convex
optimization problems are also polynomially solvable and may be feasible in practice with
thousands of variables. Several important problems in machine learning and control theory
SEARCHING WITH NONDETERMINISTIC ACTIONS
the agent knows what the effects of each action are. Therefore, the agent can calculate exactly
which state results from any sequence of actions and always knows which state it is in. Its
percepts provide no new information after each action, although of course they tell the agent
the initial state.
cepts become useful. In a partially observable environment, every percept helps narrow down
the set of possible states the agent might be in, thus making it easier for the agent to achieve
sible outcomes of its actions has actually occurred. In both cases, the future percepts cannot
be determined in advance and the agent’s future actions will depend on those future percepts.
CONTINGENCY PLAN
egy) that speciﬁes what to do depending on what percepts are received. In this section, we
STRATEGY
The erratic vacuum world
one for which the space “above” it forms a convex set; by deﬁnition, convex functions have no local (as opposed
to global) minima.Chapter
Beyond Classical Search
Now suppose that we introduce nondeterminism in the form of a powerful but erratic
vacuum cleaner. In the erratic vacuum world, the Suck action works as follows:
ERRATIC VACUUM
WORLD
• When applied to a dirty square the action cleans the square and sometimes cleans up
dirt in an adjacent square, too.
that returns a single state, we use a RESULTS function that returns a set of possible outcome
We also need to generalize the notion of a solution to the problem. For example, if we
need a contingency plan such as the following:
Thus, solutions for nondeterministic problems can contain nested if–then–else statements;
this means that they are trees rather than sequences. This allows the selection of actions
based on contingencies arising during execution. Many problems in the real, physical world
are contingency problems because exact prediction is impossible. For this reason, many
people keep their eyes open while walking around or driving.
owners of modern, efﬁcient home appliances who cannot take advantage of this pedagogical device.Searching with Nondeterministic Actions
AND–OR search trees
The next question is how to ﬁnd contingent solutions to nondeterministic problems. As in
In a deterministic environment, the only branching is introduced by the agent’s own choices
in each state. We call these nodes OR nodes. In the vacuum world, for example, at an OR
OR NODE
node the agent chooses Left or Right or Suck. In a nondeterministic environment, branching
is also introduced by the environment’s choice of outcome for each action. We call these
AND NODE
AND–OR TREE
at each of its AND nodes. The solution is shown in bold lines in the ﬁgure; it corresponds
branches, but when there are more than two branches at a node, it might be better to use a case
Left
Suck
Right
Suck
Right
Suck
GOAL
GOAL
LOOP
LOOP
LOOP
Left
Suck
LOOP
GOAL
The ﬁrst two levels of the search tree for the erratic vacuum world. State
nodes are OR nodes where some action must be chosen. At the AND nodes, shown as circles,
every outcome must be handled, as indicated by the arc linking the outgoing branches. The
solution found is shown in bold lines.Chapter
Beyond Classical Search
if state is on path then return failure
for each action in problem.ACTIONS(state) do
if plan ̸= failure then return [action | plan]
return failure
for each si in states do
if plani = failure then return failure
An algorithm for searching AND–OR graphs generated by nondeterministic
environments. It returns a conditional plan that reaches a goal state in all circumstances. (The
notation [x | l] refers to the list formed by adding object x to the front of list l.)
tingent solutions of this kind is straightforward. One may also consider a somewhat different
agent design, in which the agent can act before it has found a guaranteed plan and deals with
some contingencies only as they arise during execution. This type of interleaving of search
INTERLEAVING
key aspect of the algorithm is the way in which it deals with cycles, which often arise in
nondeterministic problems (e.g., if an action sometimes has no effect or if an unintended
effect can be corrected). If the current state is identical to a state on the path from the root,
then it returns with failure. This doesn’t mean that there is no solution from the current state;
it simply means that if there is a noncyclic solution, it must be reachable from the earlier
incarnation of the current state, so the new incarnation can be discarded. With this check, we
ensure that the algorithm terminates in every ﬁnite state space, because every path must reach
a goal, a dead end, or a repeated state. Notice that the algorithm does not check whether the
current state is a repetition of a state on some other path from the root, which is important for
of a heuristic function must be modiﬁed to estimate the cost of a contingent solution rather
than a sequence, but the notion of admissibility carries over and there is an analog of the A∗
algorithm for ﬁnding optimal solutions. Pointers are given in the bibliographical notes at the
end of the chapter.Searching with Nondeterministic Actions
Suck
Right
Right
Part of the search graph for the slippery vacuum world, where we have shown
(some) cycles explicitly. All solutions for this problem are cyclic plans because there is no
way to move reliably.
Try, try again
CYCLIC SOLUTION
which is to keep trying Right until it works. We can express this solution by adding a label to
LABEL
denote some portion of the plan and using that label later instead of repeating the plan itself.
Thus, our cyclic solution is
In general a cyclic plan may be considered a solution provided that every leaf is a goal
state and that a leaf is reachable from every point in the plan. The modiﬁcations needed
in the state space back to a state L translates to a loop in the plan back to the point where the
subplan for state L is executed.
ally reach the goal provided that each outcome of a nondeterministic action eventually occurs.
Is this condition reasonable? It depends on the reason for the nondeterminism. If the action
rolls a die, then it’s reasonable to suppose that eventually a six will be rolled. If the action is
to insert a hotel card key into the door lock, but it doesn’t work the ﬁrst time, then perhaps it
will eventually work, or perhaps one has the wrong key (or the wrong room!). After seven orChapter
Beyond Classical Search
eight tries, most people will assume the problem is with the key and will go back to the front
desk to get a new one. One way to understand this decision is to say that the initial problem
formulation (observable, nondeterministic) is abandoned in favor of a different formulation
SEARCHING WITH PARTIAL OBSERVATIONS
ﬁce to pin down the exact state. As noted at the beginning of the previous section, if the
agent is in one of several possible states, then an action may lead to one of several possible
outcomes—even if the environment is deterministic. The key concept required for solving
partially observable problems is the belief state, representing the agent’s current belief about
BELIEF STATE
the possible physical states it might be in, given the sequence of actions and percepts up to
that point. We begin with the simplest scenario for studying belief states, which is when the
agent has no sensors at all; then we add in partial sensing as well as nondeterministic actions.
Searching with no observation
less problem or sometimes a conformant problem. At ﬁrst, one might think the sensorless
SENSORLESS
CONFORMANT
agent has no hope of solving a problem if it has no idea what state it’s in; in fact, sensorless
problems are quite often solvable. Moreover, sensorless agents can be surprisingly useful,
primarily because they don’t rely on sensors working properly. In manufacturing systems,
for example, many ingenious methods have been developed for orienting parts correctly from
an unknown initial position by using a sequence of actions with no sensing at all. The high
spectrum antibiotic rather than using the contingent plan of doing an expensive blood test,
then waiting for the results to come back, and then prescribing a more speciﬁc antibiotic and
perhaps hospitalization because the infection has progressed too far.
We can make a sensorless version of the vacuum world. Assume that the agent knows
the geography of its world, but doesn’t know its location or the distribution of dirt. In that
agent now has more information! Furthermore, the action sequence [Right,Suck] will always
COERCION
To solve sensorless problems, we search in the space of belief states rather than physicalSearching with Partial Observations
always knows its own belief state. Furthermore, the solution (if any) is always a sequence of
each action are completely predictable—they’re always empty! So there are no contingencies
to plan for. This is true even if the environment is nondeterminstic.
If P has N states, then the sensorless problem has up to 2N states, although many may
be unreachable from the initial state.
• Initial state: Typically the set of all states in P, although in some cases the agent will
have more knowledge than this.
• Actions: This is slightly tricky. Suppose the agent is in belief state b = {s1, s2}, but
ACTIONSP (s1) ̸= ACTIONSP(s2); then the agent is unsure of which actions are legal.
If we assume that illegal actions have no effect on the environment, then it is safe to
take the union of all the actions in any of the physical states in the current belief state b:
ACTIONS(b) =

ACTIONSP(s) .
On the other hand, if an illegal action might be the end of the world, it is safer to allow
only the intersection, that is, the set of actions legal in all the states. For the vacuum
world, every state has the same legal actions, so both methods give the same result.
• Transition model: The agent doesn’t know which state in the belief state is the right
one; so as far as it knows, it might get to any of the states resulting from applying the
action to one of the physical states in the belief state. For deterministic actions, the set
of states that might be reached is
With deterministic actions, b′ is never larger than b. With nondeterminism, we have
=

RESULTSP(s, a) ,
The process of generating
the new belief state after the action is called the prediction step; the notation b′ =
PREDICTION
PREDICTP (b, a) will come in handy.
• Goal test: The agent wants a plan that is sure to work, which means that a belief state
may accidentally achieve the goal earlier, but it won’t know that it has done so.
• Path cost: This is also tricky. If the same action can have different costs in different
states, then the cost of taking an action in a given belief state could be one of several
For now we assume that the cost of an action is the same in all states and so can be
transferred directly from the underlying physical problem.Chapter
Beyond Classical Search
(b)
(a)
(a) Predicting the next belief state for the sensorless vacuum world with a
deterministic action, Right. (b) Prediction for the same belief state and action in the slippery
version of the sensorless vacuum world.
formulation from the deﬁnition of the underlying physical problem. Once this is done, we
than that. In “ordinary” graph search, newly generated states are tested to see if they are
action sequence [Suck,Left,Suck] starting at the initial state reaches the same belief state as
dramatically improve the efﬁciency of sensorless problem solving.
space—even though it is exponentially larger than the underlying physical state space; in
state space are not so different. The real difﬁculty lies with the size of each belief state. For
list of states.
One solution is to represent the belief state by some more compact description. In
English, we could say the agent knows “Nothing” in the initial state; after moving Left, we
formal representation scheme. Another approach is to avoid the standard search algorithms,
which treat belief states as black boxes just like any other problem state. Instead, we can lookSearching with Partial Observations
L
R
S
L
R
S
L
R
S
L
R
S
L
R
S
L
R
S
L
R
S
less vacuum world. Each shaded box corresponds to a single belief state. At any given point,
the agent is in a particular belief state but does not know which physical state it is in. The
initial belief state (complete ignorance) is the top center box. Actions are represented by
INCREMENTAL
SEARCH
the solution one physical state at a time. For example, in the sensorless vacuum world, the
as an AND–OR search has to ﬁnd a solution for every branch at an AND node, this algorithm
has to ﬁnd a solution for every state in the belief state; the difference is that AND–OR search
to ﬁnd one solution that works for all the states.
The main advantage of the incremental approach is that it is typically able to detect
failure quickly—when a belief state is unsolvable, it is usually the case that a small subset of
the belief state, consisting of the ﬁrst few states examined, is also unsolvable. In some cases,Chapter
Beyond Classical Search
this leads to a speedup proportional to the size of the belief states, which may themselves be
as large as the physical state space itself.
Even the most efﬁcient solution algorithm is not of much use when no solutions exist.
impossible. On the other hand, a little bit of sensing can go a long way. For example, every
tile in turn into the visible square and then keeping track of its location.
Searching with observations
For a general partially observable problem, we have to specify how the environment generates
one in which the agent has a position sensor and a local dirt sensor but has no sensor capable
of detecting dirt in other squares. The formal problem speciﬁcation includes a PERCEPT(s)
function that returns the percept received in a given state. (If sensing is nondeterministic,
then we use a PERCEPTS function that returns a set of possible percepts.) For example, in the
are a special case in which PERCEPT(s) = s for every state s, while sensorless problems are
a special case in which PERCEPT(s) = null.
When observations are partial, it will usually be the case that several states could have
are constructed from the underlying physical problem just as for sensorless problems, but the
transition model is a bit more complicated. We can think of transitions from one belief state
• The prediction stage is the same as for sensorless problems: given the action a in belief
served in the predicted belief state:
• The update stage determines, for each possible percept, the belief state that would
result from the percept. The new belief state bo is just the set of states in ˆb that could
have produced the percept:
Notice that each updated belief state bo can be no larger than the predicted belief state ˆb;
over, for deterministic sensing, the belief states for the different possible percepts will
be disjoint, forming a partition of the original predicted belief state.Searching with Partial Observations
(b)
(a)
Right
[A,Dirty]
[B,Dirty]
[B,Clean]
Right
[B,Dirty]
[B,Clean]
terministic world, Right is applied in the initial belief state, resulting in a new belief state
with two possible physical states; for those states, the possible percepts are [B, Dirty] and
[B, Clean], leading to two belief states, each of which is a singleton. (b) In the slippery
cal states; for those states, the possible percepts are [A, Dirty], [B, Dirty], and [B, Clean],
leading to three belief states as shown.
Putting these three stages together, we obtain the possible belief states resulting from a given
action and the subsequent possible percepts:
RESULTS(b, a) = {bo : bo = UPDATE(PREDICT(b, a), o) and
Again, the nondeterminism in the partially observable problem comes from the inability
to predict exactly which percept will be received after acting; underlying nondeterminism in
the physical environment may contribute to this inability by enlarging the belief state at the
prediction stage, leading to more percepts at the observation stage.
Solving partially observable problems
The preceding section showed how to derive the RESULTS function for a nondeterministicChapter
Beyond Classical Search
Suck
[B,Dirty]
[B,Clean]
Right
[A,Clean]
vacuum world; Suck is the ﬁrst step of the solution.
world, assuming an initial percept [A, Dirty]. The solution is the conditional plan
returned a conditional plan that tests the belief state rather than the actual state. This is as it
should be: in a partially observable environment the agent won’t be able to execute a solution
that requires testing the actual state.
As in the case of standard search algorithms applied to sensorless problems, the AND–
OR search algorithm treats belief states as black boxes, just like any other states. One can
improve on this by checking for previously generated belief states that are subsets or supersets
of the current state, just as for sensorless problems. One can also derive incremental search
algorithms, analogous to those described for sensorless problems, that provide substantial
An agent for partially observable environments
There are two main differences. First, the solution to a problem will be a conditional plan
rather than a sequence; if the ﬁrst step is an if–then–else expression, the agent will need to
the agent will need to maintain its belief state as it performs actions and receives percepts.
actually simpler because the percept is given by the environment rather than calculated by theSearching with Partial Observations
[B,Dirty]
Right
[A,Clean]
Suck
vacuum world with local sensing.
agent. Given an initial belief state b, an action a, and a percept o, the new belief state is:
b′ = UPDATE(PREDICT(b, a), o) .
local sensing, wherein any square may become dirty at any time unless the agent is actively
environments—maintaining one’s belief state is a core function of any intelligent system.
MONITORING
FILTERING
STATE ESTIMATION
RECURSIVE
state from the previous one rather than by examining the entire percept sequence. If the agent
is not to “fall behind,” the computation has to happen as fast as percepts are coming in. As
the environment becomes more complex, the exact update computation becomes infeasible
plications of the percept for the aspects of the environment that are of current interest. Most
discrete environment with detrministic sensors and nondeterministic actions.
The example concerns a robot with the task of localization: working out where it is,
LOCALIZATION
given a map of the world and a sequence of percepts and actions. Our robot is placed in the
tell whether there is an obstacle—the outer wall or a black square in the ﬁgure—in each of
the four compass directions. We assume that the sensors give perfectly correct data, and that
the robot has a correct map of the enviornment. But unfortunately the robot’s navigational
system is broken, so when it executes a Move action, it moves randomly to one of the adjacent
squares. The robot’s task is to determine its current location.
Suppose the robot has just been switched on, so it does not know where it is. Thus its
initial belief state b consists of the set of all locations. The the robot receives the perceptChapter
Beyond Classical Search
(a) Possible locations of robot after E1 = NSW
(b) Possible locations of robot After E1 = NSW, E2 = NS
Possible positions of the robot, ⊙, (a) after one observation E1 = NSW and
(b) after a second observation E2 = NS. When sensors are noiseless and the transition model
is accurate, there are no other possible locations for the robot consistent with this sequence
of two observations.
NSW, meaning there are obstacles to the north, west, and south, and does an update using the
the maze to see that those are the only four locations that yield the percept NWS.
lief state, ba = PREDICT(bo, Move), contains all the locations that are one step away from the
locations in bo. When the second percept, NS, arrives, the robot does UPDATE(ba, NS) and
That’s the only location that could be the result of
UPDATE(PREDICT(UPDATE(b, NSW ), Move), NS) .
With nondetermnistic actions the PREDICT step grows the belief state, but the UPDATE step
shrinks it back down—as long as the percepts provide some useful identifying information.
Sometimes the percepts don’t help much for localization: If there were one or more long
know where in the corridor(s) it was.Online Search Agents and Unknown Environments
ONLINE SEARCH AGENTS AND UNKNOWN ENVIRONMENTS
So far we have concentrated on agents that use ofﬂine search algorithms. They compute
OFFLINE SEARCH
a complete solution before setting foot in the real world and then execute the solution. In
contrast, an online search13 agent interleaves computation and action: ﬁrst it takes an action,
ONLINE SEARCH
then it observes the environment and computes the next action. Online search is a good idea
in dynamic or semidynamic domains—domains where there is a penalty for sitting around
and computing too long. Online search is also helpful in nondeterministic domains because
it allows the agent to focus its computational efforts on the contingencies that actually arise
rather than those that might happen but probably won’t. Of course, there is a tradeoff: the
more an agent plans ahead, the less often it will ﬁnd itself up the creek without a paddle.
Online search is a necessary idea for unknown environments, where the agent does not
know what states exist or what its actions do. In this state of ignorance, the agent faces an
exploration problem and must use its actions as experiments in order to learn enough to
EXPLORATION
PROBLEM
make deliberation worthwhile.
The canonical example of online search is a robot that is placed in a new building and
must explore it to build a map that it can use for getting from A to B. Methods for escaping
from labyrinths—required knowledge for aspiring heroes of antiquity—are also examples of
online search algorithms. Spatial exploration is not the only form of exploration, however.
Consider a newborn baby: it has many possible actions but knows the outcomes of none of
them, and it has experienced only a few of the possible states that it can reach. The baby’s
gradual discovery of how the world works is, in part, an online search process.
Online search problems
An online search problem must be solved by an agent executing actions, rather than by pure
laxes these assumptions), but we stipulate that the agent knows only the following:
• ACTIONS(s), which returns a list of actions allowed in state s;
that s′ is the outcome; and
Note in particular that the agent cannot determine RESULT(s, a) except by actually being
applications—for example, a robot explorer might know how its movement actions work and
be ignorant only of the locations of obstacles.
as they are received rather than waiting for the entire input data set to become available.Chapter
Beyond Classical Search
G
S
A simple maze problem. The agent starts at S and must reach G but knows
nothing of the environment.
S
G
S
G
A
A
S
G
(a)
(b)
(a) Two state spaces that might lead an online search agent into a dead end.
that can cause an online search agent to follow an arbitrarily inefﬁcient route to the goal.
Whichever choice the agent makes, the adversary blocks that route with another long, thin
wall, so that the path followed is much longer than the best possible path.
Typically, the agent’s objective is to reach a goal state while minimizing cost. (Another
possible objective is simply to explore the entire environment.) The cost is the total path cost
of the path that the agent actually travels. It is common to compare this cost with the path
cost of the path the agent would follow if it knew the search space in advance—that is, the
actual shortest path (or shortest complete exploration). In the language of online algorithms,
this is called the competitive ratio; we would like it to be as small as possible.
COMPETITIVE RATIOOnline Search Agents and Unknown Environments
Although this sounds like a reasonable request, it is easy to see that the best achievable
competitive ratio is inﬁnite in some cases. For example, if some actions are irreversible—
IRREVERSIBLE
i.e., they lead to a state from which no action leads back to the previous state—the online
DEAD END
haps the term “accidentally” is unconvincing—after all, there might be an algorithm that
spaces look identical, so it must make the same decision in both. Therefore, it will fail in
one of them. This is an example of an adversary argument—we can imagine an adversary
ADVERSARY
ARGUMENT
constructing the state space while the agent explores it and putting the goals and dead ends
wherever it chooses.
streets, and all kinds of natural terrain present opportunities for irreversible actions. To make
progress, we simply assume that the state space is safely explorable—that is, some goal state
SAFELY EXPLORABLE
is reachable from every reachable state. State spaces with reversible actions, such as mazes
shows. For this reason, it is common to describe the performance of online search algorithms
in terms of the size of the entire state space rather than just the depth of the shallowest goal.
Online search agents
After each action, an online agent receives a percept telling it what state it has reached; from
this information, it can augment its map of the environment. The current map is used to
decide where to go next. This interleaving of planning and action means that online search
algorithms are quite different from the ofﬂine search algorithms we have seen previously. For
example, ofﬂine algorithms such as A∗can expand a node in one part of the space and then
immediately expand a node in another part of the space, because node expansion involves
simulated rather than real actions. An online algorithm, on the other hand, can discover
successors only for a node that it physically occupies. To avoid traveling all the way across
search has exactly this property because (except when backtracking) the next node expanded
is a child of the previous node expanded.
in a table, RESULT[s, a], that records the state resulting from executing action a in state s.
Whenever an action from the current state has not been explored, the agent tries that action.
search, the state is simply dropped from the queue; in an online search, the agent has to
agent most recently entered the current state. To achieve that, the algorithm keeps a table thatChapter
Beyond Classical Search
inputs: s′, a percept that identiﬁes the current state
persistent: result, a table indexed by state and action, initially empty
untried, a table that lists, for each state, the actions not yet tried
unbacktracked, a table that lists, for each state, the backtracks not yet tried
s, a, the previous state and action, initially null
if s′ is a new state (not in untried) then untried[s′] ←ACTIONS(s′)
if s is not null then
result[s,a] ←s′
add s to the front of unbacktracked[s′]
if untried[s′] is empty then
if unbacktracked[s′] is empty then return stop
else a ←an action b such that result[s′,b] = POP(unbacktracked[s′])
else a ←POP(untried[s′])
s ←s′
return a
cable only in state spaces in which every action can be “undone” by some other action.
lists, for each state, the predecessor states to which the agent has not yet backtracked. If the
agent has run out of states to which it can backtrack, then its search is complete.
the worst case, end up traversing every link in the state space exactly twice. For exploration,
this is optimal; for ﬁnding a goal, on the other hand, the agent’s competitive ratio could be
arbitrarily bad if it goes off on a long excursion when there is a goal right next to the initial
state. An online variant of iterative deepening solves this problem; for an environment that is
a uniform tree, the competitive ratio of such an agent is a small constant.
spaces where the actions are reversible. There are slightly more complex algorithms that
work in general state spaces, but no such algorithm has a bounded competitive ratio.
Online local search
already an online search algorithm! Unfortunately, it is not very useful in its simplest form
because it leaves the agent sitting at local maxima with nowhere to go. Moreover, random
restarts cannot be used, because the agent cannot transport itself to a new state.
Instead of random restarts, one might consider using a random walk to explore the
RANDOM WALK
environment. A random walk simply selects at random one of the available actions from theOnline Search Agents and Unknown Environments
S
G
An environment in which a random walk will take exponentially many steps
to ﬁnd the goal.
current state; preference can be given to actions that have not yet been tried. It is easy to
prove that a random walk will eventually ﬁnd a goal or complete its exploration, provided
an environment in which a random walk will take exponentially many steps to ﬁnd the goal
because, at each step, backward progress is twice as likely as forward progress. The example
these kinds of “traps” for random walks.
Augmenting hill climbing with memory rather than randomness turns out to be a more
effective approach. The basic idea is to store a “current best estimate” H(s) of the cost to
reach the goal from each state that has been visited. H(s) starts out being just the heuristic
stuck in a ﬂat local minimum at the shaded state. Rather than staying where it is, the agent
should follow what seems to be the best path to the goal given the current cost estimates for
its neighbors. The estimated cost to reach the goal through a neighbor s′ is the cost to get
to s′ plus the estimated cost to get to a goal from there—that is, c(s, a, s′) + H(s′). In the
updating H each time and “ﬂattening out” the local minimum until it escapes to the right.
the result table. It updates the cost estimate for the state it has just left and then chooses the
“apparently best” move according to its current cost estimates. One important detail is that
actions that have not yet been tried in a state s are always assumed to lead immediately to the
goal with the least possible cost, namely h(s). This optimism under uncertainty encourages
OPTIMISM UNDER
UNCERTAINTY
the agent to explore new, possibly promising paths.
An LRTA∗agent is guaranteed to ﬁnd a goal in any ﬁnite, safely explorable environment.
Unlike A∗, however, it is not complete for inﬁnite state spaces—there are cases where it can be
led inﬁnitely astray. It can explore an environment of n states in O(n2) steps in the worst case,Chapter
Beyond Classical Search
(a)
(b)
(c)
(d)
(e)
labeled with H(s), the current cost estimate to reach a goal, and each link is labeled with its
step cost. The shaded state marks the location of the agent, and the updated cost estimates at
each iteration are circled.
inputs: s′, a percept that identiﬁes the current state
persistent: result, a table, indexed by state and action, initially empty
H , a table of cost estimates indexed by state, initially empty
s, a, the previous state and action, initially null
if s′ is a new state (not in H ) then H [s′] ←h(s′)
if s is not null
result[s,a] ←s′
H [s] ←
min
s ←s′
return a
if s′ is undeﬁned then return h(s)
else return c(s, a, s′) + H[s′]
states, which are updated as the agent moves about the state space.Summary
but often does much better. The LRTA∗agent is just one of a large family of online agents that
one can deﬁne by specifying the action selection rule and the update rule in different ways.
Learning in online search
The initial ignorance of online search agents provides several opportunities for learning. First,
the agents learn a “map” of the environment—more precisely, the outcome of each action in
each state—simply by recording each of their experiences. (Notice that the assumption of
deterministic environments means that one experience is enough for each action.) Second,
the local search agents acquire more accurate estimates of the cost of each state by using local
to exact values for every state, provided that the agent explores the state space in the right
way. Once exact values are known, optimal decisions can be taken simply by moving to the
For this to happen, we need two things. First, we need a formal and explicitly manipulable
representation for these kinds of general rules; so far, we have hidden the information inside
the black box called the RESULT function. Part III is devoted to this issue. Second, we need
algorithms that can construct suitable general rules from the speciﬁc observations made by
SUMMARY
This chapter has examined search algorithms for problems beyond the “classical” case of
ﬁnding the shortest path to a goal in an observable, deterministic, discrete environment.
keeping only a small number of nodes in memory. Several stochastic algorithms have
been developed, including simulated annealing, which returns optimal solutions when
given an appropriate cooling schedule.
gramming and convex optimization problems obey certain restrictions on the shape
algorithms that are often extremely efﬁcient in practice.
states is maintained. New states are generated by mutation and by crossover, which
combines pairs of states from the population.Chapter
Beyond Classical Search
tingent plans that reach the goal regardless of which outcomes occur during execution.
• When the environment is partially observable, the belief state represents the set of
possible states that the agent might be in.
state are often more efﬁcient.
• Exploration problems arise when the agent has no idea about the states and actions of
its environment. For safely explorable environments, online search agents can build a
map and ﬁnd a goal if one exists. Updating heuristic estimates from experience provides
an effective method to escape from local minima.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Local search techniques have a long history in mathematics and computer science. Indeed,
cient local search method for continuous spaces in which gradient information is available.
formation. Beam search, which we have presented as a local search algorithm, originated
itriou has called “New Age” algorithms also sparked increased interest among theoretical
TABU SEARCH
states that cannot be revisited; as well as improving efﬁciency when searching graphs, this list
can allow the algorithm to escape from some local minima. Another useful improvement on
scape. The algorithm ﬁts a smooth surface to the set of local maxima and then calculates the
DISTRIBUTION
which means that the probability of a very long run time is more than would be predicted if
random restarts ﬁnd a solution faster, on average, than a single run to completion.Bibliographical and Historical Notes
directly from the Metropolis algorithm (which is used to simulate complex systems in
Simulated annealing is now a ﬁeld in itself, with hundreds of papers published every year.
Finding optimal solutions in continuous spaces is the subject matter of several ﬁelds,
including optimization theory, optimal control theory, and the calculus of variations. The
of algorithms and provide working software.
mization algorithms from a wide variety of ﬁelds of study: metallurgy (simulated annealing),
taineering (hill climbing), and others.
tant precursor to the development of genetic algorithms. In the 1950s, several statisticians,
EVOLUTION
STRATEGY
timization problems for airfoils that the approach gained popularity. In the 1960s and 1970s,
ARTIFICIAL LIFE
of genetic algorithms as organisms rather than solutions to problems. Work in this ﬁeld by
plications of the Baldwin effect. For general background on evolution, we recommend Smith
Most comparisons of genetic algorithms to other approaches (especially stochastic hill
GENETIC
PROGRAMMING
pal difference is that the representations that are mutated and combined are programs ratherChapter
Beyond Classical Search
than bit strings. The programs are represented in the form of expression trees; the expressions
can be in a standard language such as Lisp or can be specially designed to represent circuits,
which would not be the case if programs were manipulated as strings.
of genetic programming to design circuit devices.
plex Systems, Adaptive Behavior, and Artiﬁcial Life. The main conference is the Genetic
The unpredictability and partial observability of real environments were recognized
early on in robotics projects that used planning techniques, including Shakey (Fikes et al.,
tion of McDermott’s (1978a) inﬂuential article, Planning and Acting.
The ﬁrst work to make explicit use of AND–OR trees seems to have been Slagle’s SAINT
optimal solutions given an admissible heuristic. AO∗was analyzed and improved by Martelli
ing actions. More practical methods, based on a series of precisely oriented diagonal barriersChapter
Beyond Classical Search
that pieces ﬁt exactly with no slack. Now consider the real problem, in which pieces don’t
Explain how to formulate the problem so it could be solved by simulated annealing.
In this exercise, we explore the use of local search methods to solve TSPs of the type
b. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to
restart, and simulated annealing. Measure the search cost and percentage of solved problems
and graph these against the optimal solution cost. Comment on your results.
only on the path from the root to the current state. Suppose that, in addition, the algorithm
algorithm should use that information when a repeated state is found. (Hint: You will need to
distinguish at least between states for which a successful subplan was constructed previously
and states for which no subplan could be found.) Explain how to use labels, as deﬁned in
a cyclic plan if no acyclic plan exists. You will need to deal with three issues: labeling the plan
so that it continues to look for acyclic plans after ﬁnding a cyclic plan, and augmenting the
plan representation to indicate whether a plan is cyclic. Show how your algorithm works on
(a) the slippery vacuum world, and (b) the slippery, erratic vacuum world. You might wish to
use a computer implementation to check your results.
sequence of actions solves a sensorless problem if it maps every physical state in the initial
belief state b to a goal state. Suppose the agent knows h∗(s), the true optimal cost of solving
the physical state s in the fully observable problem, for every state s in b. Find an admissible
heuristic h(b) for the sensorless problem in terms of these costs, and prove its admissibilty.
How well does A∗perform?
This exercise explores subset–superset relations between belief states in sensorless or
partially observable environments.
a. Prove that if an action sequence is a solution for a belief state b, it is also a solution for
any subset of b. Can anything be said about supersets of b?Chapter
Beyond Classical Search
performance of the three recovery schemes. Would the inclusion of search costs affect
the comparison?
e. Now suppose that there are locations from which the view is identical. (For example,
suppose the world is a grid with square obstacles.) What kind of problem does the agent
now face? What do solutions look like?
actions Up, Down, Left, Right have their usual effects unless blocked by a wall. The agent
does not know where the internal walls are. In any given state, the agent perceives the set of
legal actions; it can also tell whether the state is one it has visited before.
space, where the initial belief state includes all possible environment conﬁgurations.
How large is the initial belief state? How large is the space of belief states?
b. How many distinct percepts are possible in the initial state?
c. Describe the ﬁrst few branches of a contingency plan for this problem. How large
(roughly) is the complete plan?
Notice that this contingency plan is a solution for every possible environment ﬁtting the given
description. Therefore, interleaving of search and execution is not strictly necessary even in
unknown environments.
In this exercise, we examine hill climbing in the context of robot navigation, using the
minimum? Is it possible for it to get stuck with convex obstacles?
b. Construct a nonconvex polygonal environment in which the agent gets stuck.
one step along it, and then repeat the process.
d. Is there some k for which the new algorithm is guaranteed to escape from local minima?
e. Explain how LRTA∗enables the agent to escape from local minima in this case.
Like DFS, online DFS is incomplete for reversible state spaces with inﬁnite paths. For
state, all successor states and the actions that would lead to them. Write an algorithm that
is complete even for bidirected state spaces with inﬁnite paths. What states does it visit inADVERSARIAL SEARCH
In which we examine the problems that arise when we try to plan ahead in a world
where other agents are planning against us.
GAMES
actions of other agents and how they affect its own welfare. The unpredictability of these
goals are in conﬂict, giving rise to adversarial search problems—often known as games.
GAME
Mathematical game theory, a branch of economics, views any multiagent environment
as a game, provided that the impact of each agent on the others is “signiﬁcant,” regardless
PERFECT
INFORMATION
deterministic, fully observable environments in which two agents act alternately and in which
the utility values at the end of the game are always equal and opposite. For example, if one
player wins a game of chess, the other player necessarily loses. It is this opposition between
the agents’ utility functions that makes the situation adversarial.
Games have engaged the intellectual faculties of humans—sometimes to an alarming
degree—for as long as civilization has existed. For AI researchers, the abstract nature of
games makes them an appealing subject for study. The state of a game is easy to represent,
and agents are usually restricted to a small number of actions whose outcomes are deﬁned by
precise rules. Physical games, such as croquet and ice hockey, have much more complicated
descriptions, a much larger range of possible actions, and rather imprecise rules deﬁning
the legality of actions. With the exception of robot soccer, these physical games have not
attracted much interest in the AI community.Chapter
Adversarial Search
world, therefore require the ability to make some decision even when calculating the optimal
decision is infeasible. Games also penalize inefﬁciency severely. Whereas an implementation
of A∗search that is half as efﬁcient will simply take twice as long to run to completion, a chess
program that is half as efﬁcient in using its available time probably will be beaten into the
interesting ideas on how to make the best possible use of time.
We begin with a deﬁnition of the optimal move and an algorithm for ﬁnding it. We
then look at techniques for choosing a good move when time is limited. Pruning allows us
PRUNING
to ignore portions of the search tree that make no difference to the ﬁnal choice, and heuristic
chance; we also discuss bridge, which includes elements of imperfect information because
IMPERFECT
INFORMATION
programs fare against human opposition and at directions for future developments.
We ﬁrst consider games with two players, whom we call MAX and MIN for reasons that
will soon become obvious. MAX moves ﬁrst, and then they take turns moving until the game
is over. At the end of the game, points are awarded to the winning player and penalties are
given to the loser. A game can be formally deﬁned as a kind of search problem with the
following elements:
• S0: The initial state, which speciﬁes how the game is set up at the start.
• PLAYER(s): Deﬁnes which player has the move in a state.
• ACTIONS(s): Returns the set of legal moves in a state.
• RESULT(s, a): The transition model, which deﬁnes the result of a move.
TERMINAL TEST
otherwise. States where the game has ended are called terminal states.
TERMINAL STATES
• UTILITY(s, p): A utility function (also called an objective function or payoff function),
deﬁnes the ﬁnal numeric value for a game that ends in terminal state s for a player p. In
The initial state, ACTIONS function, and RESULT function deﬁne the game tree for the
GAME TREE
nine possible moves. Play alternates between MAX’s placing an X and MIN’s placing an OOptimal Decisions in Games
until we reach leaf nodes corresponding to terminal states such that one player has three in
a row or all the squares are ﬁlled. The number on each leaf node indicates the utility value
of the terminal state from the point of view of MAX; high values are assumed to be good for
MAX and bad for MIN (which is how the players get their names).
theoretical construct that we cannot realize in the physical world. But regardless of the size
of the game tree, it is MAX’s job to search for a good move. We use the term search tree for a
SEARCH TREE
tree that is superimposed on the full game tree, and examines enough nodes to allow a player
to determine what move to make.
X
X
X
X
X
X
X
X
X
X
X
O
O
X O
O
X O
X O
X
. . .
. . .
. . .
. . .
. . .
. . .
. . .
X
X
X
X
X
X
O
X
X
O
X
X
O
O
O
X
X
X
O
O
O
O O X
X
MAX (X)
MIN (O)
MAX (X)
MIN (O)
TERMINAL
Utility
state, and MAX moves ﬁrst, placing an X in an empty square. We show part of the tree, giving
alternating moves by MIN (O) and MAX (X), until we eventually reach terminal states, which
can be assigned utilities according to the rules of the game.
OPTIMAL DECISIONS IN GAMES
In a normal search problem, the optimal solution would be a sequence of actions leading to
a goal state—a terminal state that is a win. In adversarial search, MIN has something to say
about it.
MAX therefore must ﬁnd a contingent strategy, which speciﬁes MAX’s move in
STRATEGY
the initial state, then MAX’s moves in the states resulting from every possible response byChapter
Adversarial Search
MAX
A
B
C
D
a1
a2
a3
b1
b2
b3
c1
c2
c3
d1
d2
d3
MIN
turn to move, and the ▽nodes are “MIN nodes.” The terminal nodes show the utility values
for MAX; the other nodes are labeled with their minimax values. MAX’s best move at the root
is a1, because it leads to the state with the highest minimax value, and MIN’s best reply is b1,
because it leads to the state with the lowest minimax value.
MIN, then MAX’s moves in the states resulting from every possible response by MIN to those
with MAX playing the role of OR and MIN equivalent to AND. Roughly speaking, an optimal
strategy leads to outcomes at least as good as any other strategy when one is playing an
infallible opponent. We begin by showing how to ﬁnd this optimal strategy.
at the root node are labeled a1, a2, and a3. The possible replies to a1 for MIN are b1, b2,
b3, and so on. This particular game ends after one move each by MAX and MIN. (In game
PLY
Given a game tree, the optimal strategy can be determined from the minimax value
MINIMAX VALUE
of each node, which we write as MINIMAX(n). The minimax value of a node is the utility
(for MAX) of being in the corresponding state, assuming that both players play optimally
from there to the end of the game. Obviously, the minimax value of a terminal state is just
its utility. Furthermore, given a choice, MAX prefers to move to a state of maximum value,
whereas MIN prefers a state of minimum value. So we have the following:
MINIMAX(s) =
⎧
⎨
⎩
UTILITY(s)
if PLAYER(s) = MIN
level get their utility values from the game’s UTILITY function. The ﬁrst MIN node, labeledOptimal Decisions in Games
the minimax decision at the root: action a1 is the optimal choice for MAX because it leads to
MINIMAX DECISION
the state with the highest minimax value.
This deﬁnition of optimal play for MAX assumes that MIN also plays optimally—it
opponents may do better than the minimax strategy, but these strategies necessarily do worse
against optimal opponents.
The minimax algorithm
MINIMAX ALGORITHM
It uses a simple recursive computation of the minimax values of each successor state, directly
implementing the deﬁning equations. The recursion proceeds all the way down to the leaves
of the tree, and then the minimax values are backed up through the tree as the recursion
If the maximum depth of the tree is m and there are b legal moves at each point, then the
time complexity of the minimax algorithm is O(b m). The space complexity is O(bm) for an
algorithm that generates all actions at once, or O(m) for an algorithm that generates actions
but this algorithm serves as the basis for the mathematical analysis of games and for more
practical algorithms.
Optimal decisions in multiplayer games
Many popular games allow more than two players. Let us examine how to extend the minimax
idea to multiplayer games. This is straightforward from the technical viewpoint, but raises
some interesting new conceptual issues.
First, we need to replace the single value for each node with a vector of values. For
with each node. For terminal states, this vector gives the utility of the state from each player’s
value because the values are always opposite.) The simplest way to implement this is to have
the UTILITY function return a vector of utilities.
Now we have to consider nonterminal states. Consider the node marked X in the gameChapter
Adversarial Search
v ←−∞
for each a in ACTIONS(state) do
return v
v ←∞
for each a in ACTIONS(state) do
return v
sponding to the best possible move, that is, the move that leads to the outcome with the
best utility, under the assumption that the opponent plays to minimize utility. The functions
element a of set S that has the maximum value of f(a).
to move
A
B
C
A
X
The ﬁrst three plies of a game tree with three players (A, B, C). Each node is
labeled with values from the viewpoint of each player. The best move is marked at the root.
vector of the successor state with the highest value for the player choosing at n. Anyone
who plays multiplayer games, such as Diplomacy, quickly becomes aware that much more
ALLIANCE
formal or informal, among the players. Alliances are made and broken as the game proceeds.
How are we to understand such behavior? Are alliances a natural consequence of optimal
strategies for each player in a multiplayer game? It turns out that they can be. For example,Alpha–Beta Pruning
suppose A and B are in weak positions and C is in a stronger position. Then it is often
optimal for both A and B to attack C rather than each other, lest C destroy each of them
individually. In this way, collaboration emerges from purely selﬁsh behavior. Of course,
as soon as C weakens under the joint onslaught, the alliance loses its value, and either A
or B could violate the agreement. In some cases, explicit alliances merely make concrete
what would have happened anyway. In other cases, a social stigma attaches to breaking an
alliance, so players must balance the immediate advantage of breaking an alliance against the
these complications.
players to do everything possible to reach this state—that is, the players will automatically
cooperate to achieve a mutually desirable goal.
ALPHA–BETA PRUNING
The problem with minimax search is that the number of game states it has to examine is
exponential in the depth of the tree. Unfortunately, we can’t eliminate the exponent, but it
turns out we can effectively cut it in half. The trick is that it is possible to compute the correct
minimax decision without looking at every node in the game tree. That is, we can borrow the
particular technique we examine is called alpha–beta pruning. When applied to a standard
ALPHA–BETA
PRUNING
minimax tree, it returns the same move as minimax would, but prunes away branches that
cannot possibly inﬂuence the ﬁnal decision.
of the optimal decision once more, this time paying careful attention to what we know at
identify the minimax decision without ever evaluating two of the leaf nodes.
Another way to look at this is as a simpliﬁcation of the formula for MINIMAX. Let the
the root node is given by
In other words, the value of the root and hence the minimax decision are independent of the
values of the pruned leaves x and y.
Alpha–beta pruning can be applied to trees of any depth, and it is often possible to
prune entire subtrees rather than just leaves. The general principle is this: consider a node nChapter
Adversarial Search
(a)
(b)
(c)
(d)
(e)
(f)
A
B
A
B
A
B
C
D
A
B
C
D
A
B
A
B
C
[−∞, +∞]
[−∞, +∞]
At each point, we show the range of possible values for each node. (a) The ﬁrst leaf below B
would never choose C. Therefore, there is no point in looking at the other successor states
to keep exploring D’s successor states. Notice also that we now have bounds on all of the
If Player has a better choice m either at the parent node of n or at any choice point further up,
then n will never be reached in actual play. So once we have found out enough about n (by
examining some of its descendants) to reach this conclusion, we can prune it.
sider the nodes along a single path in the tree. Alpha–beta pruning gets its name from the
along the path:Alpha–Beta Pruning
Player
Opponent
Player
Opponent
m
n
•
•
•
The general case for alpha–beta pruning. If m is better than n for Player, we
will never get to n in play.
along the path for MAX.
along the path for MIN.
Alpha–beta search updates the values of α and β as it goes along and prunes the remaining
branches at a node (i.e., terminates the recursive call) as soon as the value of the current
node is known to be worse than the current α or β value for MAX or MIN, respectively. The
Move ordering
The effectiveness of alpha–beta pruning is highly dependent on the order in which the states
at all because the worst successors (from the point of view of MIN) were generated ﬁrst. If
the third successor of D had been generated ﬁrst, we would have been able to prune the other
two. This suggests that it might be worthwhile to try to examine ﬁrst the successors that are
likely to be best.
nodes to pick the best move, instead of O(bm) for minimax. This means that the effective
branching factor becomes
√
way, alpha–beta can solve a tree roughly twice as deep as minimax in the same amount of
function (such as trying captures ﬁrst, then threats, then forward moves, and then backwardChapter
Adversarial Search
return the action in ACTIONS(state) with value v
v ←−∞
for each a in ACTIONS(state) do
α ←MAX(α, v)
return v
v ←+∞
for each a in ACTIONS(state) do
β ←MIN(β, v)
return v
The alpha–beta search algorithm. Notice that these routines are the same as
to be best in the past, brings us quite close to the theoretical limit. The past could be the
previous move—often the same threats remain—or it could come from previous exploration
of the current move. One way to gain information from the current move is with iterative
iterative deepening on an exponential game tree adds only a constant fraction to the total
search time, which can be more than made up from better move ordering. The best moves are
often called killer moves and to try them ﬁrst is called the killer move heuristic.
KILLER MOVES
sitions—different permutations of the move sequence that end up in the same position. For
TRANSPOSITION
lated move a2 on the other side of the board that can be answered by b2, then the sequences
[a1, b1, a2, b2] and [a2, b2, a1, b1] both end up in the same position. It is worthwhile to store
the evaluation of the resulting position in a hash table the ﬁrst time it is encountered so that
we don’t have to recompute it on subsequent occurrences. The hash table of previously seen
positions is traditionally called a transposition table; it is essentially identical to the explored
TRANSPOSITION
TABLEsometimes as much as doubling the reachable search depth in chess. On the other hand, if we
are evaluating a million nodes per second, at some point it is not practical to keep all of them
in the transposition table. Various strategies have been used to choose which nodes to keep
and which to discard.
rithm allows us to prune large parts of it. However, alpha–beta still has to search all the way
to terminal states for at least a portion of the search space. This depth is usually not practical,
because moves must be made in a reasonable amount of time—typically a few minutes at
tion to states in the search, effectively turning nonterminal nodes into terminal leaves. In
EVALUATION
FUNCTION
other words, the suggestion is to alter minimax or alpha–beta in two ways: replace the utility
function by a heuristic evaluation function EVAL, which estimates the position’s utility, and
replace the terminal test by a cutoff test that decides when to apply EVAL. That gives us the
CUTOFF TEST
following for heuristic minimax for state s and maximum depth d:
⎧
⎨
⎩
EVAL(s)
if PLAYER(s) = MAX
if PLAYER(s) = MIN.
Evaluation functions
An evaluation function returns an estimate of the expected utility of the game from a given
the goal. The idea of an estimator was not new when Shannon proposed it. For centuries,
chess players (and aﬁcionados of other games) have developed ways of judging the value of
a position because humans are even more limited in the amount of search they can do than
depends strongly on the quality of its evaluation function. An inaccurate evaluation function
will guide an agent toward positions that turn out to be lost. How exactly do we design good
evaluation functions?
First, the evaluation function should order the terminal states in the same way as the
true utility function: states that are wins must evaluate better than draws, which in turn must
be better than losses. Otherwise, an agent using the evaluation function might err even if it
can see ahead all the way to the end of the game. Second, the computation must not take
too long! (The whole point is to search faster.) Third, for nonterminal states, the evaluation
function should be strongly correlated with the actual chances of winning.Chapter
Adversarial Search
One might well wonder about the phrase “chances of winning.” After all, chess is not a
game of chance: we know the current state with certainty, and no dice are involved. But if the
search must be cut off at nonterminal states, then the algorithm will necessarily be uncertain
about the ﬁnal outcomes of those states. This type of uncertainty is induced by computational,
rather than informational, limitations. Given the limited amount of computation that the
evaluation function is allowed to do for a given state, the best it can do is make a guess about
the ﬁnal outcome.
Let us make this idea more concrete. Most evaluation functions work by calculating
various features of the state—for example, in chess, we would have features for the number
of white pawns, black pawns, white queens, black queens, and so on. The features, taken
together, deﬁne various categories or equivalence classes of states: the states in each category
that lead to wins, some that lead to draws, and some that lead to losses. The evaluation
function cannot know which states are which, but it can return a single value that reﬂects the
proportion of states with each outcome. For example, suppose our experience suggests that
EXPECTED VALUE
principle, the expected value can be determined for each category, resulting in an evaluation
function that works for any state. As with terminal states, the evaluation function need not
return actual expected values as long as the ordering of the states is the same.
In practice, this kind of analysis requires too many categories and hence too much
experience to estimate all the probabilities of winning. Instead, most evaluation functions
compute separate numerical contributions from each feature and then combine them to ﬁnd
the total value. For example, introductory chess books give an approximate material value
MATERIAL VALUE
Other features such as “good pawn structure” and “king safety” might be worth half a pawn,
say. These feature values are then simply added up to obtain the evaluation of the position.
A secure advantage equivalent to a pawn gives a substantial likelihood of winning, and
a secure advantage equivalent to three pawns should give almost certain victory, as illustrated
function because it can be expressed as
WEIGHTED LINEAR
FUNCTION
EVAL(s) = w1f1(s) + w2f2(s) + · · · + wnfn(s) =
n


wifi(s) ,
where each wi is a weight and each fi is a feature of the position. For chess, the fi could be
the numbers of each kind of piece on the board, and the wi could be the values of the pieces
Adding up the values of features seems like a reasonable thing to do, but in fact it
involves a strong assumption: that the contribution of each feature is independent of the
that bishops are more powerful in the endgame, when they have a lot of space to maneuver.(b) White to move
(a) White to move
Two chess positions that differ only in the position of the rook at lower right.
In (a), Black has an advantage of a knight and two pawns, which should be enough to win
the game. In (b), White will capture the queen, giving it an advantage that should be strong
enough to win.
For this reason, current programs for chess and other games also use nonlinear combinations
of features. For example, a pair of bishops might be worth slightly more than twice the value
of a single bishop, and a bishop is worth more in the endgame (that is, when the move number
feature is high or the number of remaining pieces feature is low).
The astute reader will have noticed that the features and weights are not part of the rules
kind of experience is not available, the weights of the evaluation function can be estimated
to chess has conﬁrmed that a bishop is indeed worth about three pawns.
Cutting off search
We also must arrange for some bookkeeping so that the current depth is incremented on each
recursive call. The most straightforward approach to controlling the amount of search is to set
did.) The depth d is chosen so that a move is selected within the allocated time. A more
program returns the move selected by the deepest completed search. As a bonus, iterative
deepening also helps with move ordering.Chapter
Adversarial Search
uation function. Consider again the simple evaluation function for chess based on material
heuristic value of the state, thereby declaring that the state is a probable win by Black. But
White’s next move captures Black’s queen with no compensation. Hence, the position is
really won for White, but this can be seen only by looking ahead one more ply.
Obviously, a more sophisticated cutoff test is needed. The evaluation function should be
applied only to positions that are quiescent—that is, unlikely to exhibit wild swings in value
QUIESCENCE
in the near future. In chess, for example, positions in which favorable captures can be made
are not quiescent for an evaluation function that just counts material. Nonquiescent positions
can be expanded further until quiescent positions are reached. This extra search is called a
quiescence search; sometimes it is restricted to consider only certain types of moves, such
QUIESCENCE
SEARCH
as capture moves, that will quickly resolve the uncertainties in the position.
The horizon effect is more difﬁcult to eliminate. It arises when the program is facing
HORIZON EFFECT
an opponent’s move that causes serious damage and is ultimately unavoidable, but can be
that there is no way for the black bishop to escape. For example, the white rook can capture
of moves that pushes the capture of the bishop “over the horizon.” Suppose Black searches
will be marked as “bad” moves. But Black will consider checking the white king with the
pawn at e4. This will lead to the king capturing the pawn. Now Black will consider checking
play has saved the bishop at the price of two pawns, when actually all it has done is push the
inevitable capture of the bishop beyond the horizon that Black can see.
One strategy to mitigate the horizon effect is the singular extension, a move that is
SINGULAR
EXTENSION
“clearly better” than all other moves in a given position. Once discovered anywhere in the
tree in the course of a search, this singular move is remembered. When the search reaches the
normal depth limit, the algorithm checks to see if the singular extension is a legal move; if it
is, the algorithm allows the move to be considered. This makes the tree deeper, but because
there will be few singular extensions, it does not add many total nodes to the tree.
Forward pruning
So far, we have talked about cutting off search at a certain level and about doing alpha–
beta pruning that provably has no effect on the result (at least with respect to the heuristic
evaluation values). It is also possible to do forward pruning, meaning that some moves at
FORWARD PRUNING
a given node are pruned immediately without further consideration. Clearly, most humans
playing chess consider only a few moves from each position (at least consciously). One
approach to forward pruning is beam search: on each ply, consider only a “beam” of the n
BEAM SEARCH
best moves (according to the evaluation function) rather than considering all possible moves.a     b    c    d    e     f     g    h
The horizon effect. With Black to move, the black bishop is surely doomed.
But Black can forestall that event by checking the white king with its pawns, forcing the king
to capture the pawns. This pushes the inevitable loss of the bishop over the horizon, and thus
the pawn sacriﬁces are seen by the search algorithm as good moves rather than bad ones.
Unfortunately, this approach is rather dangerous because there is no guarantee that the best
move will not be pruned away.
sion of alpha–beta search that uses statistics gained from prior experience to lessen the chance
side the current (α, β) window. PROBCUT also prunes nodes that are probably outside the
value v of a node and then using past experience to estimate how likely it is that a score of v
gram, LOGISTELLO, and found that a version of his program with PROBCUT beat the regular
itable chess (or other games). Let us assume we have implemented an evaluation function for
chess, a reasonable cutoff test with a quiescence search, and a large transposition table. Let
per move under standard time controls (three minutes per move). The branching factor for
we could look ahead only about ﬁve plies. Though not incompetent, such a program can be
fooled easily by an average human chess player, who can occasionally plan six or eight plies
evaluation function and a large database of optimal opening and endgame moves.Chapter
Adversarial Search
Search versus lookup
Somehow it seems like overkill for a chess program to start a game by considering a tree of a
billion game states, only to conclude that it will move its pawn to e4. Books describing good
play in the opening and endgame in chess have been available for about a century (Tattersall,
rather than search for the opening and ending of games.
For the openings, the computer is mostly relying on the expertise of humans. The best
advice of human experts on how to play each opening is copied from books and entered into
tables for the computer’s use. However, computers can also gather statistics from a database
of previously played games to see which opening sequences most often lead to a win. In
the early moves there are few choices, and thus much expert commentary and past games on
which to draw. Usually after ten moves we end up in a rarely seen position, and the program
must switch from table lookup to search.
Near the end of the game there are again fewer possible positions, and thus more chance
to do lookup.
But here it is the computer that has the expertise: computer analysis of
king’s mobility by squeezing it toward one edge of the board, using your king to prevent the
opponent from escaping the squeeze. Other endings, such as king, bishop, and knight versus
king (KBNK), are difﬁcult to master and have no succinct strategy description. A computer,
POLICY
ping from every possible state to the best move in that state. Then we can just look up the best
move rather than recompute it anew. How big will the KBNK lookup table be? It turns out
positions. Some of these are checkmates; mark them as such in a table. Then do a retrograde
RETROGRADE
minimax search: reverse the rules of chess to do unmoves rather than moves. Any move by
White that, no matter what move Black responds with, ends up in a position marked as a win,
loss, or draw, and you have an infallible lookup table for all KBNK endgames.
some with six pieces, making them available on the Internet. Stiller discovered one case
know on the opening move whether it would be a win, loss, or draw. This has not happened
so far for chess, but it has happened for checkers, as explained in the historical notes section.Stochastic Games
STOCHASTIC GAMES
In real life, many unpredictable external events can put us into unforeseen situations. Many
games mirror this unpredictability by including a random element, such as the throwing of
dice. We call these stochastic games. Backgammon is a typical game that combines luck
STOCHASTIC GAMES
and skill. Dice are rolled at the beginning of a player’s turn to determine the legal moves. In
possible moves.
A typical backgammon position. The goal of the game is to move all one’s
is one opponent, it is captured and must start over. In the position shown, White has rolled
Although White knows what his or her own legal moves are, White does not know what
Black is going to roll and thus does not know what Black’s legal moves will be. That means
game tree in backgammon must include chance nodes in addition to MAX and MIN nodes.
CHANCE NODES
node denote the possible dice rolls; each branch is labeled with the roll and its probability.Chapter
Adversarial Search
CHANCE
MIN
MAX
CHANCE
MAX
. . .
. . .
B
. . .
TERMINAL
...
...
...
...
...
...
...
...
...
...
...
...
...
C
. . .
Schematic game tree for a backgammon position.
The next step is to understand how to make correct decisions. Obviously, we still want
to pick the move that leads to the best position. However, positions do not have deﬁnite
minimax values. Instead, we can only calculate the expected value of a position: the average
EXPECTED VALUE
over all possible outcomes of the chance nodes.
minimax value for games with chance nodes. Terminal nodes and MAX and MIN nodes (for
EXPECTIMINIMAX
VALUE
which the dice roll is known) work exactly the same way as before. For chance nodes we
compute the expected value, which is the sum of the value over all outcomes, weighted by
the probability of each chance action:
EXPECTIMINIMAX(s) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
UTILITY(s)
maxa EXPECTIMINIMAX(RESULT(s, a))
if PLAYER(s) = MAX
mina EXPECTIMINIMAX(RESULT(s, a))
if PLAYER(s) = MIN

r P(r)EXPECTIMINIMAX(RESULT(s, r)) if PLAYER(s) = CHANCE
where r represents a possible dice roll (or other chance event) and RESULT(s, r) is the same
state as s, with the additional fact that the result of the dice roll is r.
Evaluation functions for games of chance
As with minimax, the obvious approximation to make with expectiminimax is to cut the
search off at some point and apply an evaluation function to each leaf. One might think that
evaluation functions for games such as backgammon should be just like evaluation functionsStochastic Games
for chess—they just need to give higher scores to better positions. But in fact, the presence of
chance nodes means that one has to be more careful about what the evaluation values mean.
the program behaves totally differently if we make a change in the scale of some evaluation
values! It turns out that to avoid this sensitivity, the evaluation function must be a positive
linear transformation of the probability of winning from a position (or, more generally, of the
expected utility of the position). This is an important and general property of situations in
CHANCE
MIN
MAX
a1
a2
a1
a2
If the program knew in advance all the dice rolls that would occur for the rest of the
max does in O(bm) time, where b is the branching factor and m is the maximum depth of the
it will take O(bmnm), where n is the number of distinct rolls.
Even if the search depth is limited to some small depth d, the extra cost compared with
that of minimax makes it unrealistic to consider looking ahead very far in most games of
Another way to think about the problem is this: the advantage of alpha–beta is that
it ignores future developments that just are not going to happen, given best play. Thus, it
concentrates on likely occurrences. In games with dice, there are no likely sequences of
moves, because for those moves to take place, the dice would ﬁrst have to come out the right
way to make them legal. This is a general problem whenever uncertainty enters the picture:
the possibilities are multiplied enormously, and forming detailed plans of action becomes
pointless because the world probably will not play along.
It may have occurred to you that something like alpha–beta pruning could be appliedChapter
Adversarial Search
to game trees with chance nodes. It turns out that it can. The analysis for MIN and MAX
nodes is unchanged, but we can also prune chance nodes, using a bit of ingenuity. Consider
its children. Is it possible to ﬁnd an upper bound on the value of C before we have looked
at all its children? (Recall that this is what alpha–beta needs in order to prune a node and its
subtree.) At ﬁrst sight, it might seem impossible because the value of C is the average of its
children’s values, and in order to compute the average of a set of numbers, we must look at
all the numbers. But if we put bounds on the possible values of the utility function, then we
can arrive at bounds for the average without looking at every number. For example, say that
we can place an upper bound on the value of a chance node without looking at all its children.
An alternative is to do Monte Carlo simulation to evaluate a position. Start with
MONTE CARLO
SIMULATION
an alpha–beta (or other) search algorithm. From a start position, have the algorithm play
thousands of games against itself, using random dice rolls. In the case of backgammon, the
resulting win percentage has been shown to be a good approximation of the value of the
position, even if the algorithm has an imperfect heuristic and is searching only a few plies
ROLLOUT
PARTIALLY OBSERVABLE GAMES
teristic of real wars, namely, partial observability. In the “fog of war,” the existence and
disposition of enemy units is often unknown until revealed by direct contact. As a result,
warfare includes the use of scouts and spies to gather information and the use of concealment
and bluff to confuse the enemy. Partially observable games share these characteristics and
are thus qualitatively different from the games described in the preceding sections.
Kriegspiel: Partially observable chess
tirely from lack of access to the choices made by the opponent. This class includes children’s
games such as Battleships (where each player’s ships are placed in locations hidden from the
opponent but do not move) and Stratego (where piece locations are known but piece types are
hidden). We will examine the game of Kriegspiel, a partially observable variant of chess in
KRIEGSPIEL
which pieces can move but are completely invisible to the opponent.
The rules of Kriegspiel are as follows: White and Black each see a board containing
ically makes announcements that are heard by both players. On his turn, White proposes to
the referee any move that would be legal if there were no black pieces. If the move is in fact
not legal (because of the black pieces), the referee announces “illegal.” In this case, White
may keep proposing moves until a legal one is found—and learns more about the location of
Black’s pieces in the process. Once a legal move is proposed, the referee announces one orPartially Observable Games
more of the following: “Capture on square X” if there is a capture, and “Check by D” if the
black king is in check, where D is the direction of the check, and can be one of “Knight,”
eree may make two “Check” announcements.) If Black is checkmated or stalemated, the
referee says so; otherwise, it is Black’s turn to move.
puter programs are beginning to catch up. It helps to recall the notion of a belief state as
move. Keeping track of the belief state as the game progresses is exactly the problem of state
of White’s move are composed from the (predictable) outcome of White’s own move and the
Given a current belief state, White may ask, “Can I win the game?” For a partially
observable game, the notion of a strategy is altered; instead of specifying a move to make
for each possible move the opponent might make, we need a move for every possible percept
mate, is one that, for each possible percept sequence, leads to an actual checkmate for every
GUARANTEED
CHECKMATE
possible board state in the current belief state, regardless of how the opponent moves. With
this deﬁnition, the opponent’s belief state is irrelevant—the strategy has to work even if the
part of a guaranteed checkmate for the KRK (king and rook against king) endgame. In this
case, Black has just one piece (the king), so a belief state for White can be shown in a single
board by marking each possible position of the Black king.
the abilities of human players.
In addition to guaranteed checkmates, Kriegspiel admits an entirely new concept that
makes no sense in fully observable games: probabilistic checkmate. Such checkmates are
PROBABILISTIC
CHECKMATE
still required to work in every board state in the belief state; they are probabilistic with respect
to randomization of the winning player’s moves. To get the basic idea, consider the problem
of ﬁnding a lone black king using just the white king. Simply by moving randomly, the
white king will eventually bump into the black king even if the latter tries to avoid this fate,
since Black cannot keep guessing the right evasive moves indeﬁnitely. In the terminology ofChapter
Adversarial Search
a
d
b
c
Kc3 ?
“Illegal”
“OK”
Rc3 ?
“OK”
“Check”
Part of a guaranteed checkmate in the KRK endgame, shown on a reduced
board. In the initial belief state, Black’s king is in one of three possible locations. By a
combination of probing moves, the strategy narrows this down to one. Completion of the
checkmate is left as an exercise.
and knight against king—is won in this sense; White presents Black with an inﬁnite random
sequence of choices, for one of which Black will guess incorrectly and reveal his position,
White can force a win only by leaving one of his bishops unprotected for one move. If
Black happens to be in the right place and captures the bishop (a move that would lose if the
bishops are protected), the game is drawn. White can choose to make the risky move at some
It is quite rare that a guaranteed or probabilistic checkmate can be found within any
reasonable depth, except in the endgame. Sometimes a checkmate strategy works for some of
the board states in the current belief state but not others. Trying such a strategy may succeed,
leading to an accidental checkmate—accidental in the sense that White could not know that
ACCIDENTAL
CHECKMATE
it would be checkmate—if Black’s pieces happen to be in the right places. (Most checkmates
in games between humans are of this accidental nature.) This idea leads naturally to the
question of how likely it is that a given strategy will win, which leads in turn to the question
of how likely it is that each board state in the current belief state is the true board state.Partially Observable Games
One’s ﬁrst inclination might be to propose that all board states in the current belief state
are equally likely—but this can’t be right. Consider, for example, White’s belief state after
Black’s ﬁrst move of the game. By deﬁnition (assuming that Black plays optimally), Black
must have played an optimal move, so all board states resulting from suboptimal moves ought
to be assigned zero probability. This argument is not quite right either, because each player’s
goal is not just to move pieces to the right squares but also to minimize the information that
the opponent has about their location. Playing any predictable “optimal” strategy provides
the opponent with information. Hence, optimal play in partially observable games requires
a willingness to play somewhat randomly. (This is why restaurant hygiene inspectors do
cally” weak—but they gain strength from their very unpredictability, because the opponent is
unlikely to have prepared any defense against them.
From these considerations, it seems that the probabilities associated with the board
ria is prohibitively expensive, however, even for small games, and is out of the question for
Kriegspiel. At present, the design of effective algorithms for general Kriegspiel play is an
state space, ignoring the opponent’s belief state. Evaluation functions resemble those for the
observable game but include a component for the size of the belief state—smaller is better!
Card games
Card games provide many examples of stochastic partial observability, where the missing
information is generated randomly. For example, in many games, cards are dealt randomly at
the beginning of the game, with each player receiving a hand that is not visible to the other
players. Such games include bridge, whist, hearts, and some forms of poker.
At ﬁrst sight, it might seem that these card games are just like dice games: the cards are
dealt randomly and determine the moves available to each player, but all the “dice” are rolled
at the beginning! Even though this analogy turns out to be incorrect, it suggests an effective
algorithm: consider all possible deals of the invisible cards; solve each one as if it were a
fully observable game; and then choose the move that has the best outcome averaged over all
the deals. Suppose that each deal s occurs with probability P(s); then the move we want is
argmax
a


s
P(s) MINIMAX(RESULT(s,a)) .
Now, in most card games, the number of possible deals is rather large. For example,
cards each, so the number of deals is

difﬁcult, so solving ten million is out of the question. Instead, we resort to a Monte CarloChapter
Adversarial Search
approximation: instead of adding up all the deals, we take a random sample of N deals,
where the probability of deal s appearing in the sample is proportional to P(s):
argmax
a
N
N


MINIMAX(RESULT(si, a)) .
ready drawn according to P(s).) As N grows large, the sum over the random sample tends
approximation. It can also be applied to deterministic games such as Kriegspiel, given some
reasonable estimate of P(s).
For games like whist and hearts, where there is no bidding or betting phase before play
commences, each deal will be equally likely and so the values of P(s) are all equal. For
bridge, play is preceded by a bidding phase in which each team indicates how many tricks it
expects to win. Since players bid based on the cards they hold, the other players learn more
about the probability of each deal. Taking this into account in deciding how to play the hand
is tricky, for the reasons mentioned in our description of Kriegspiel: players may bid in such
a way as to minimize the information conveyed to their opponents. Even so, the approach is
mediately after the ﬁrst move. Despite its intuitive appeal, the strategy can lead one astray.
Consider the following story:
you’ll ﬁnd a bigger heap of gold, but take the right fork and you’ll be run over by a bus.
you’ll ﬁnd a bigger heap of gold, but take the left fork and you’ll be run over by a bus.
fork leads to a bigger heap of gold, but take the wrong fork and you’ll be hit by a bus.
Unfortunately you don’t know which fork is which.
so B must still be the right choice.
Now we can see how averaging over clairvoyance fails: it does not consider the belief
pecially when one possibility is certain death. Because it assumes that every future state will
from the opponent or provide information to a partner because it assumes that they already
BLUFF
virtue of solving the true partially observable decision problem.tiﬁcial intelligence.” John McCarthy disagrees: whereas geneticists use fruit ﬂies to make
discoveries that apply to biology more broadly, AI has used chess to do the equivalent of
breeding very fast fruit ﬂies. Perhaps a better analogy is that chess is to AI as Grand Prix
optimized machines that incorporate the latest engineering advances, but they aren’t much
erate excitement and a steady stream of innovations that have been adopted by the wider
community. In this section we look at what it takes to come out on top in various games.
Chess: IBM’s DEEP BLUE chess program, now retired, is well known for defeating world
CHESS
and move ordering for the last few levels of the tree, and evaluated the leaf nodes. Deep Blue
success seems to have been its ability to generate singular extensions beyond the depth limit
taining all positions with ﬁve pieces and many with six pieces. This database had the effect
of substantially extending the effective search depth, allowing Deep Blue to play perfectly in
some cases even when it was many moves away from checkmate.
The success of DEEP BLUE reinforced the widely held belief that progress in computer
by IBM. But algorithmic improvements have allowed programs running on standard PCs
to win World Computer Chess Championships. A variety of pruning heuristics are used to
NULL MOVE
lower bound on the value of a position, using a shallow search in which the opponent gets
to move twice at the beginning. This lower bound often allows alpha–beta pruning without
FUTILITY PRUNING
advance which moves will cause a beta cutoff in the successor nodes.
aggressive use of the null move heuristic and forward pruning.Chapter
Adversarial Search
vantage appears to be its evaluation function, which has been tuned by its main developer,
International Master Vasik Rajlich, and at least three other grandmasters.
The most recent matches suggest that the top computer chess programs have pulled
ahead of all human contenders. (See the historical notes for details.)
Checkers: Jonathan Schaeffer and colleagues developed CHINOOK, which runs on regular
CHECKERS
Othello, also called Reversi, is probably more popular as a computer game than as a board
OTHELLO
defeated the human world champion, Takeshi Murakami, by six games to none. It is generally
acknowledged that humans are no match for computers at Othello.
BACKGAMMON
deep search an expensive luxury. Most work on backgammon has gone into improving the
evaluation function.
ing moves of the game have in some cases radically altered the received wisdom.
GO
uation function because control of territory is often very unpredictable until the endgame.
Therefore the top programs, such as MOGO, avoid alpha–beta search and instead use Monte
Carlo rollouts. The trick is to decide what moves to make in the course of the rollout. There is
no aggressive pruning; all moves are possible. The UCT (upper conﬁdence bounds on trees)
method works by making random moves in the ﬁrst few iterations, and over time guiding
the sampling process to prefer moves that have led to wins in previous samples. Some tricks
pattern is detected and limited local search to decide tactical questions. Some programs also
include special techniques from combinatorial game theory to analyze endgames. These
COMBINATORIAL
GAME THEORY
this way have surprised many professional Go players, who thought they had been playing
but are still at advanced amateur level on a full board.
Bridge is a card game of imperfect information: a player’s cards are hidden from the other
BRIDGE
players. Bridge is also a multiplayer game with four players instead of two, although theAlternative Approaches
games like bridge can include elements of information gathering, communication, and careful
weighing of probabilities. Many of these techniques are used in the Bridge Baron program
squeezing, that are familiar to bridge players.
decisively using the Monte Carlo method. Since then, other winning programs have followed
BASED
GENERALIZATION
and cache general rules for optimal play in various standard classes of situations rather than
evaluating each situation individually. For example, in a situation where one player has the
ways that the ﬁrst player can lead from that suit and the second player can follow. But GIB
treats these situations as just two: the ﬁrst player can lead either a high card or a low card;
the exact cards played don’t matter. With this optimization (and a few others), GIB can solve
exceeding the expectations of many human experts.
There are several reasons why GIB plays at expert level with Monte Carlo simulation,
whereas Kriegspiel programs do not. First, GIB’s evaluation of the fully observable version
of the game is exact, searching the full game tree, while Kriegspiel programs rely on inexact
heuristics. But far more important is the fact that in bridge, most of the uncertainty in the
sarial play of the opponent. Monte Carlo simulation handles randomness well, but does not
always handle strategy well, especially when the strategy involves the value of information.
Scrabble: Most people think the hard part about Scrabble is coming up with good words, but
SCRABBLE
given the ofﬁcial dictionary, it turns out to be rather easy to program a move generator to ﬁnd
problem is that Scrabble is both partially observable and stochastic: you don’t know what
letters the other player has or what letters you will draw next. So playing Scrabble well
ALTERNATIVE APPROACHES
Because calculating optimal decisions in games is intractable in most cases, all algorithms
max, evaluation functions, and alpha–beta, is just one way to do this. Probably because it hasChapter
Adversarial Search
MAX
MIN
been worked on for so long, the standard approach dominates other methods in tournament
stream of AI research: the standard approach no longer provides much room for new insight
into general questions of decision making. In this section, we look at the alternatives.
First, let us consider heuristic minimax. It selects an optimal move in a given search
tree provided that the leaf node evaluations are exactly correct. In reality, evaluations are
usually crude estimates of the value of a position and can be considered to have large errors
are all correct. But of course the evaluation function is only approximate. Suppose that
the evaluation of each node has an error that is independent of other nodes and is randomly
In reality, circumstances are actually worse than this because the error in the evaluation
function is not independent. If we get one node wrong, the chances are high that nearby nodes
suggests that in fact it might have a higher true value. We can use an evaluation function
that returns a probability distribution over possible values, but it is difﬁcult to combine these
distributions properly, because we won’t have a good model of the very strong dependencies
that exist between the values of sibling nodes
Next, we consider the search algorithm that generates the tree. The aim of an algorithm
designer is to specify a computation that runs quickly and yields a good move. The alpha–beta
algorithm is designed not just to select a good move but also to calculate bounds on the values
of all the legal moves. To see why this extra information is unnecessary, consider a position
in which there is only one legal move. Alpha–beta search still will generate and evaluate a
large search tree, telling us that the only move is the best move and assigning it a value. But
since we have to make the move anyway, knowing the move’s value is useless. Similarly, if
there is one obviously good move and several moves that are legal but lead to a quick loss, weSummary
would not want alpha–beta to waste time determining a precise value for the lone good move.
Better to just make the move quickly and save the time for later. This leads to the idea of the
utility of a node expansion. A good search algorithm should select node expansions of high
utility—that is, ones that are likely to lead to the discovery of a signiﬁcantly better move. If
there are no node expansions whose utility is higher than their cost (in terms of time), then
the algorithm should stop searching and make a move. Notice that this works not only for
search will show that one move is better than another.
METAREASONING
soning about reasoning). It applies not just to game playing but to any kind of reasoning
at all. All computations are done in the service of trying to reach better decisions, all have
costs, and all have some likelihood of resulting in a certain improvement in decision quality.
Alpha–beta incorporates the simplest kind of metareasoning, namely, a theorem to the effect
that certain branches of the tree can be ignored without loss. It is possible to do much better.
Finally, let us reexamine the nature of search itself. Algorithms for heuristic search
and for game playing generate sequences of concrete states, starting from the initial state
and then applying an evaluation function. Clearly, this is not how humans play games. In
chess, one often has a particular goal in mind—for example, trapping the opponent’s queen—
and can use this goal to selectively generate plausible plans for achieving it. This kind of
combination. As yet there is no good understanding of how to combine the two kinds of
algorithms into a robust and efﬁcient system, although Bridge Baron might be a step in the
right direction. A fully integrated system would be a signiﬁcant achievement not just for
for a general intelligent agent.
SUMMARY
stand how to play well in practice. The most important ideas are as follows:
• A game can be deﬁned by the initial state (how the board is set up), the legal actions
in each state, the result of each action, a terminal test (which says when the game is
over), and a utility function that applies to terminal states.
• The alpha–beta search algorithm computes the same optimal move as minimax, but
achieves much greater efﬁciency by eliminating subtrees that are provably irrelevant.
• Usually, it is not feasible to consider the whole game tree (even with alpha–beta), so weChapter
Adversarial Search
need to cut the search off at some point and apply a heuristic evaluation function that
estimates the utility of a state.
• Many game programs precompute tables of best moves in the opening and endgame so
that they can look up a move rather than search.
uates a chance node by taking the average utility of all its children, weighted by the
probability of each child.
quires reasoning about the current and future belief states of each player. A simple
approximation can be obtained by averaging the value of an action over each possible
conﬁguration of missing information.
• Programs have bested even champion human players at games such as chess, checkers,
and Othello. Humans retain the edge in several games of imperfect information, such
as poker, bridge, and Kriegspiel, and in games with very large branching factors and
little good heuristic knowledge, such as Go.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The early history of mechanical game playing was marred by numerous frauds. The most
the ﬁrst serious discussion of the feasibility of computer chess and checkers (Morrison and
“the combinations involved in the Analytical Engine enormously surpassed any required,
by the Spanish engineer Leonardo Torres y Quevedo. It specialized in the “KRK” (king and
rook vs. king) chess endgame, guaranteeing a win with king and rook from any position.
imax correctly. On the other hand, it did lay out the ideas of retrograde analysis and proposed
(but did not prove) what became known as Zermelo’s theorem: that chess is determined—
White can force a win or Black can or it is a draw; we just don’t know which. Zermelo says
that should we eventually know, “Chess would of course lose the character of a game at all.”
A solid foundation for game theory was developed in the seminal work Theory of Games
showing that some games require strategies that are randomized (or otherwise unpredictable).Bibliographical and Historical Notes
beta; it was the ﬁrst chess program to do so. Alpha–beta pruning was described by Hart and
proved the correctness of alpha–beta and analysed its time complexity. Pearl (1982b) shows
tain interval bounds on the possible value of a node in the game tree rather than giving it
conspiracy number search expands leaf nodes that, by changing their values, could cause
terms of the expected improvement in decision quality at the root. It outplayed an alpha–
beta algorithm at Othello despite searching an order of magnitude fewer nodes. The MGSS∗
approach is, in principle, applicable to the control of any form of deliberation.
reach the same decision. The memory requirements and computational overhead of the queue
come the drawbacks of the original algorithm and developing a new variant called MTD(f)
that has been adopted by a number of top programs.
tribution of leaf values in the tree, minimaxing can yield values at the root that are actually less
sults in better choices in certain games. The expectiminimax algorithm was proposed by
able games. The system is quite general, handling games whose optimal strategy requires
randomized moves and games that are more complex than those handled by any previous
system. Still, it can’t handle games as complex as poker, bridge, and Kriegspiel. FrankChapter
Adversarial Search
complete information but MAX does not. Among deterministic, partially observable games,
against a king. The ﬁrst Kriegspiel programs concentrated on ﬁnding endgame checkmates
of ideas, describing a representation for board positions, an evaluation function, quiescence
commentators on his article also explored the possibilities for computer chess play.
not play a full game. Stan Ulam and a group at the Los Alamos National Lab produced a
something that went well beyond our wildest expectations of how well a computer would
Probably the most complete description of a modern chess program is provided by ErnstBibliographical and Historical Notes
(a)
(b)
Pioneers in computer chess: (a) Herbert Simon and Allen Newell, developers
In recent years, chess programs are pulling ahead of even the world’s best humans.
learned to exploit some of the weaknesses of the computer programs.
Checkers was the ﬁrst of the classic games fully played by a computer. Christopher
Samuel of IBM, working in his spare time, developed a checkers program that learned its
defeated Robert Nealy, a champion at “blind checkers,” through an error on his part. When
a great accomplishment.
The challenge started by Samuel was taken up by Jonathan Schaeffer of the University
right to challenge for the world championship. It then ran up against a problem, in the form
three games in all that time. In the ﬁrst match against CHINOOK, Tinsley suffered his fourthChapter
Adversarial Search
ended prematurely when Tinsley had to withdraw for health reasons. CHINOOK became the
ofﬁcial world champion. Schaeffer kept on building on his database of endgames, and in
to retrograde analysis, he wrote, “In checkers, the number of possible moves in any given
situation is so small that we can conﬁdently expect a complete digital computer solution to
the problem of optimal play in this game.” Bellman did not, however, fully appreciate the
From there, they were able to do forward alpha–beta search to derive a policy that proves
that checkers is in fact a draw with best play by both sides. Note that this is an application
be impractical: it would require a billion gigabytes of storage. Searching without any table
of years to search with today’s technology. Only a combination of clever search, endgame
data, and a drop in the price of processors and memory could solve checkers. Thus, checkers
Backgammon, a game of chance, was analyzed mathematically by Gerolamo Cardano
gram (Berliner, 1980b); it used a complex, manually constructed evaluation function and
jor classic game (Berliner, 1980a). Berliner readily acknowledged that BKG was very lucky
Go is a deterministic game, but the large branching factor makes it challeging. The key
play most of their moves at the master level; the only problem is that over the course of a
game they usually make at least one serious blunder that allows a strong opponent to win.
Whereas alpha–beta search reigns in most games, many recent Go programs have adopted
Monte Carlo methods based on the UCT (upper conﬁdence bounds on trees) scheme (Kocsis
win against top professional Myungwan Kim, albeit with MOGO receiving a handicap of
nine stones (about the equivalent of a queen handicap in chess). Kim estimated MOGO’s
as experimentation continues with new forms of Monte Carlo search. The Computer GoChapter
Adversarial Search
(b)
(a)
a
f
e
d
c
b
bd
cd
ad
ce
cf
cc
ae
af
ac
de
df
dd
dd
?
?
?
?
?
P
E
b and the evader E is at node d. (b) A partial game tree for this map. Each node is labeled
with the P, E positions. P moves ﬁrst. Branches marked “?” have yet to be explored.
PURSUIT–EVASION
turns moving. The game ends only when the players are on the same node; the terminal
payoff to the pursuer is minus the total time taken. (The evader “wins” by never losing.) An
a. Copy the game tree and mark the values of the terminal nodes.
b. Next to each internal node, write the strongest fact you can infer about its value (a
c. Beneath each question mark, write the name of the node reached by that branch.
d. Explain how a bound on the value of the nodes in (c) can be derived from consideration
the cost to get to each leaf as well as the cost to solve it.
e. Now suppose that the tree as given, with the leaf bounds from (d), is evaluated from left
to right. Circle those “?” nodes that would not need to be expanded further, given the
bounds from part (d), and cross out those that need not be considered at all.
f. Can you prove anything in general about who wins the game on a map that is a tree?Chapter
Adversarial Search
with exactly n X’s and no O’s. Similarly, On is the number of rows, columns, or diagonals
we use a linear evaluation function deﬁned as Eval(s) = 3X2(s)+X1(s)−(3O2(s)+O1(s)).
and one O on the board), taking symmetry into account.
applied, assuming the nodes are generated in the optimal order for alpha–beta pruning.
ular game is speciﬁed by a set S of squares and a collection W of winning positions. Each
board, players alternate placing their marks on an empty square. A player who marks every
square in a winning position wins the game. It is a tie if all squares are marked and neither
player has won.
a. Let N = |S|, the number of squares. Give an upper bound on the number of nodes in
b. Give a lower bound on the size of the game tree for the worst case, where W = { }.
c. Propose a plausible evaluation function that can be used for any instance of generalized
d. Assume that it is possible to generate a new board and check whether it is a winning
memory limitations. Using your estimate in (a), roughly how large a game tree can be
completely solved by alpha–beta in a second of CPU time? a minute? an hour?
a. Implement move generators and evaluation functions for one or more of the following
games: Kalah, Othello, checkers, and chess.
ing the evaluation function. How close does your effective branching factor come to the
ideal case of perfect move ordering?Chapter
Adversarial Search
The complete game tree for a trivial game with chance nodes.
three minutes of search allocated for one move? How many table lookups can you do in the
time it would take to do one evaluation? Now suppose the transposition table is stored on
disk. About how many evaluations could you do in the time it takes to do one disk seek with
standard disk hardware?
range of possible values is −∞to ∞.
a. Copy the ﬁgure, mark the value of all the internal nodes, and indicate the best move at
the root with an arrow.
b. Given the values of the ﬁrst six leaves, do we need to evaluate the seventh and eighth
leaves? Given the values of the ﬁrst seven leaves, do we need to evaluate the eighth
leaf? Explain your answers.
d. Circle all the leaves that need not be evaluated under the assumption in (c).
Implement the expectiminimax algorithm and the *-alpha–beta algorithm, which is
such as backgammon and measure the pruning effectiveness of *-alpha–beta.
Prove that with a positive linear transformation of leaf values (i.e., transforming a
when there are chance nodes.
Consider the following procedure for choosing moves in games with chance nodes:
quence, solve the resulting deterministic game tree using alpha–beta.CONSTRAINT
SATISFACTION PROBLEMS
In which we see how treating states as more than just little black boxes leads to the
invention of a range of powerful new search methods and a deeper understanding
of problem structure and complexity.
they are goal states. From the point of view of the search algorithm, however, each state is
atomic, or indivisible—a black box with no internal structure.
This chapter describes a way to solve a wide variety of problems more efﬁciently. We
use a factored representation for each state: a set of variables, each of which has a value.
A problem is solved when each variable has a value that satisﬁes all the constraints on the
variable. A problem described this way is called a constraint satisfaction problem, or CSP.
CONSTRAINT
SATISFACTION
PROBLEM
combinations that violate the constraints.
DEFINING CONSTRAINT SATISFACTION PROBLEMS
A constraint satisfaction problem consists of three components, X, D, and C:
X is a set of variables, {X1, . . . , Xn}.
D is a set of domains, {D1, . . . , Dn}, one for each variable.
C is a set of constraints that specify allowable combinations of values.
Each domain Di consists of a set of allowable values, {v1, . . . , vk} for variable Xi. Each
constraint Ci consists of a pair ⟨scope, rel⟩, where scope is a tuple of variables that participate
in the constraint and rel is a relation that deﬁnes the values that those variables can take on. A
relation can be represented as an explicit list of all tuples of values that satisfy the constraint,
or as an abstract relation that supports two operations: testing if a tuple is a member of the
relation and enumerating the members of the relation. For example, if X1 and X2 both haveDeﬁning Constraint Satisfaction Problems
the domain {A,B}, then the constraint saying the two variables must have different values
can be written as ⟨(X1, X2), [(A, B), (B, A)]⟩or as ⟨(X1, X2), X1 ̸= X2⟩.
To solve a CSP, we need to deﬁne a state space and the notion of a solution. Each
state in a CSP is deﬁned by an assignment of values to some or all of the variables, {Xi =
ASSIGNMENT
vi, Xj = vj, . . .}. An assignment that does not violate any constraints is called a consistent
CONSISTENT
or legal assignment. A complete assignment is one in which every variable is assigned, and
COMPLETE
ASSIGNMENT
a solution to a CSP is a consistent, complete assignment. A partial assignment is one that
SOLUTION
PARTIAL
ASSIGNMENT
assigns values to only some of the variables.
Example problem: Map coloring
Suppose that, having tired of Romania, we are looking at a map of Australia showing each
either red, green, or blue in such a way that no neighboring regions have the same color. To
formulate this as a CSP, we deﬁne the variables to be the regions
X = {WA, NT, Q, NSW , V, SA, T} .
The domain of each variable is the set Di = {red, green, blue}. The constraints require
neighboring regions to have distinct colors. Since there are nine places where regions border,
there are nine constraints:
C = {SA ̸= WA, SA ̸= NT, SA ̸= Q, SA ̸= NSW , SA ̸= V,
WA ̸= NT, NT ̸= Q, Q ̸= NSW , NSW ̸= V } .
Here we are using abbreviations; SA ̸= WA is a shortcut for ⟨(SA, WA), SA ̸= WA⟩, where
SA ̸= WA can be fully enumerated in turn as
{(red, green), (red, blue), (green, red), (green, blue), (blue, red), (blue, green)} .
There are many possible solutions to this problem, such as
{WA = red, NT = green, Q = red, NSW = green, V = red, SA = blue, T = red }.
CONSTRAINT GRAPH
ables that participate in a constraint.
often easier to solve a problem using it than to design a custom solution using another search
solver can quickly eliminate large swatches of the search space. For example, once we have
ing variables can take on the value blue. Without taking advantage of constraint propagation,
variables; with constraint propagation we never have to consider blue as a value, so we have
about this one? With CSPs, once we ﬁnd out that a partial assignment is not a solution, we canChapter
Constraint Satisfaction Problems
Western
Australia
Northern
Territory
South
Australia
Queensland
New
South
Wales
Victoria
Tasmania
WA
NT
SA
Q
NSW
V
T
(a)
(b)
(a) The principal states and territories of Australia. Coloring this map can
be viewed as a constraint satisfaction problem (CSP). The goal is to assign colors to each
represented as a constraint graph.
immediately discard further reﬁnements of the partial assignment. Furthermore, we can see
why the assignment is not a solution—we see which variables violate a constraint—so we can
focus attention on the variables that matter. As a result, many problems that are intractable
Factories have the problem of scheduling a day’s worth of jobs, subject to various constraints.
In practice, many of these problems are solved with CSP techniques. Consider the problem of
scheduling the assembly of a car. The whole job is composed of tasks, and we can model each
task as a variable, where the value of each variable is the time that the task starts, expressed
as an integer number of minutes. Constraints can assert that one task must occur before
another—for example, a wheel must be installed before the hubcap is put on—and that only
so many tasks can go on at once. Constraints can also specify that a task takes a certain
amount of time to complete.
and back), afﬁx all four wheels (right and left, front and back), tighten nuts for each wheel,
X = {AxleF , AxleB, Wheel RF , WheelLF , WheelRB, WheelLB, NutsRF ,
NutsLF , NutsRB, NutsLB, CapRF , CapLF , CapRB, CapLB, Inspect} .
The value of each variable is the time that the task starts. Next we represent precedence
constraints between individual tasks. Whenever a task T1 must occur before task T2, and
PRECEDENCE
CONSTRAINTS
task T1 takes duration d1 to complete, we add an arithmetic constraint of the formDeﬁning Constraint Satisfaction Problems
minutes to install an axle, so we write
Suppose we have four workers to install wheels, but they have to share one tool that helps put
the axle in place. We need a disjunctive constraint to say that AxleF and AxleB must not
DISJUNCTIVE
CONSTRAINT
overlap in time; either one comes ﬁrst or the other does:
or
This looks like a more complicated constraint, combining arithmetic and logic. But it still
reduces to a set of pairs of values that AxleF and AxleF can take on.
limiting the domain of all variables:
ing problems like this with thousands of variables. In some cases, there are complicated
constraints that are difﬁcult to specify in the CSP formalism, and more advanced planning
Variations on the CSP formalism
The simplest kind of CSP involves variables that have discrete, ﬁnite domains.
DISCRETE DOMAIN
FINITE DOMAIN
A discrete domain can be inﬁnite, such as the set of integers or strings. (If we didn’t put
INFINITE
for each variable.) With inﬁnite domains, it is no longer possible to describe constraints by
enumerating all allowed combinations of values. Instead, a constraint language must be
CONSTRAINT
LANGUAGE
set of pairs of allowable values for (T1, T2). Special solution algorithms (which we do not
discuss here) exist for linear constraints on integer variables—that is, constraints, such as
LINEAR
CONSTRAINTS
the one just given, in which each variable appears only in linear form. It can be shown that
no algorithm exists for solving general nonlinear constraints on integer variables.
NONLINEAR
CONSTRAINTSChapter
Constraint Satisfaction Problems
Constraint satisfaction problems with continuous domains are common in the real
CONTINUOUS
DOMAINS
world and are widely studied in the ﬁeld of operations research. For example, the scheduling
of experiments on the Hubble Space Telescope requires very precise timing of observations;
straints must be linear equalities or inequalities. Linear programming problems can be solved
in time polynomial in the number of variables. Problems with different types of constraints
programming, and so on.
In addition to examining the types of variables that can appear in CSPs, it is useful to
look at the types of constraints. The simplest type is the unary constraint, which restricts
UNARY CONSTRAINT
that South Australians won’t tolerate the color green; we can express that with the unary
constraint ⟨(SA), SA ̸= green⟩
A binary constraint relates two variables. For example, SA ̸= NSW is a binary
BINARY CONSTRAINT
constraint. A binary CSP is one with only binary constraints; it can be represented as a
between X and Z, with the ternary constraint Between(X, Y, Z).
A constraint involving an arbitrary number of variables is called a global constraint.
GLOBAL
CONSTRAINT
lem). One of the most common global constraints is Alldiﬀ, which says that all of the
variables involved in the constraint must have different values. In Sudoku problems (see
CRYPTARITHMETIC
be represented as the global constraint Alldiﬀ(F, T, U, W, R, O). The addition constraints
C1000 = F ,
where C10, C100, and C1000 are auxiliary variables representing the digit carried over into the
tens, hundreds, or thousands column. These constraints can be represented in a constraint
CONSTRAINT
HYPERGRAPH
reduced to a set of binary constraints if enough auxiliary variables are introduced, so we could
transform any CSP into one with only binary constraints; this makes the algorithms simpler.
DUAL GRAPH
a new graph in which there will be one variable for each constraint in the original graph, andDeﬁning Constraint Satisfaction Problems
(a)
O
W
T
F
U
R
(b)
+
F
T
T
O
W
W
U
O
O
R
C3
C1
C2
(a) A cryptarithmetic problem. Each letter stands for a distinct digit; the aim is
to ﬁnd a substitution of digits for letters such that the resulting sum is arithmetically correct,
with the added restriction that no leading zeroes are allowed. (b) The constraint hypergraph
for the cryptarithmetic problem, showing the Alldiﬀconstraint (square box at the top) as
well as the column addition constraints (four square boxes in the middle). The variables C1,
C2, and C3 represent the carry digits for the three columns.
one binary constraint for each pair of constraints in the original graph that share variables. For
example, if the original graph has variables {X, Y, Z} and constraints ⟨(X, Y, Z), C1⟩and
⟨(X, Y ), C2⟩then the dual graph would have variables {C1, C2} with the binary constraint
⟨(X, Y ), R1⟩, where (X, Y ) are the shared variables and R1 is a new relation that deﬁnes the
constraint between the shared variables, as speciﬁed by the original C1 and C2.
There are however two reasons why we might prefer a global constraint such as Alldiﬀ
algorithms for global constraints that are not available for a set of more primitive constraints.
The constraints we have described so far have all been absolute constraints, violation of
PREFERENCE
CONSTRAINTS
lem there are absolute constraints that no professor can teach two classes at the same time.
But we also may allow preference constraints: Prof. R might prefer teaching in the morning,
whereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at
dividual variable assignments—for example, assigning an afternoon slot for Prof. R costs
formulation, CSPs with preferences can be solved with optimization search methods, either
CONSTRAINT
OPTIMIZATION
PROBLEM
Linear programming problems do this kind of optimization.Chapter
Constraint Satisfaction Problems
CONSTRAINT PROPAGATION: INFERENCE IN CSPS
choice: an algorithm can search (choose a new variable assignment from several possibilities)
or do a speciﬁc type of inference called constraint propagation: using the constraints to
INFERENCE
CONSTRAINT
PROPAGATION
reduce the number of legal values for a variable, which in turn can reduce the legal values
for another variable, and so on. Constraint propagation may be intertwined with search, or it
may be done as a preprocessing step, before search starts. Sometimes this preprocessing can
solve the whole problem, so no search is required at all.
The key idea is local consistency. If we treat each variable as a node in a graph (see
LOCAL
CONSISTENCY
sistency in each part of the graph causes inconsistent values to be eliminated throughout the
graph. There are different types of local consistency, which we now cover in turn.
Node consistency
NODE CONSISTENCY
the values in the variable’s domain satisfy the variable’s unary constraints. For example,
dislike green, the variable SA starts with domain {red, green, blue}, and we can make it
node consistent by eliminating green, leaving SA with the reduced domain {red, blue}. We
It is always possible to eliminate all the unary constraints in a CSP by running node
constraints; we make that assumption for the rest of this chapter, except where noted.
Arc consistency
ARC CONSISTENCY
for every value in the current domain Di there is some value in the domain Dj that satisﬁes
consistent with every other variable. For example, consider the constraint Y = X2 where the
domain of both X and Y is the set of digits. We can write this constraint explicitly as
lem. Consider the following inequality constraint on (SA, WA):
{(red, green), (red, blue), (green, red), (green, blue), (blue, red), (blue, green)} .Constraint Propagation: Inference in CSPs
inputs: csp, a binary CSP with components (X, D, C)
local variables: queue, a queue of arcs, initially all the arcs in csp
while queue is not empty do
if REVISE(csp, Xi, Xj) then
for each Xk in Xi.NEIGHBORS - {Xj} do
add (Xk, Xi) to queue
return true
function REVISE(csp, Xi, Xj) returns true iff we revise the domain of Xi
revised ←false
for each x in Di do
if no value y in Dj allows (x,y) to satisfy the constraint between Xi and Xj then
delete x from Di
revised ←true
return revised
it’s the third version developed in the paper.
No matter what value you choose for SA (or for WA), there is a valid value for the other
variable. So applying arc consistency has no effect on the domains of either variable.
(Actually, the order of consideration is not important, so the data structure is really a set, but
If this leaves Di unchanged, the algorithm just moves on to the next arc. But if this revises
Di (makes the domain smaller), then we add to the queue all arcs (Xk, Xi) where Xk is a
neighbor of Xi. We need to do that because the change in Di might enable further reductions
in the domains of Dk, even if we have previously considered Xk. If Di is revised down to
return failure. Otherwise, we keep checking, trying to remove values from the domains of
variables until no more arcs are in the queue. At that point, we are left with a CSP that is
CSP will in most cases be faster to search because its variables have smaller domains.
each with domain size at most d, and with c binary constraints (arcs). Each arc (Xk, Xi) can
be inserted in the queue only d times because Xi has at most d values to delete. CheckingChapter
Constraint Satisfaction Problems
tency, depending on the author. A variable Xi is generalized arc consistent with respect to
GENERALIZED ARC
CONSISTENT
is a member of the constraint, has all its values taken from the domains of the corresponding
Path consistency
Arc consistency can go a long way toward reducing the domains of variables, sometimes
only two colors allowed, red and blue. Arc consistency can do nothing because every variable
is already arc consistent: each can be red with blue at the other end of the arc (or vice versa).
But clearly there is no solution to the problem: because Western Australia, Northern Territory
and South Australia all touch each other, we need at least three colors for them alone.
Arc consistency tightens down the domains (unary constraints) using the arcs (binary
constraints). To make progress on problems like map coloring, we need a stronger notion of
consistency. Path consistency tightens the binary constraints by using implicit constraints
PATH CONSISTENCY
that are inferred by looking at triples of variables.
for every assignment {Xi = a, Xj = b} consistent with the constraints on {Xi, Xj}, there is
an assignment to Xm that satisﬁes the constraints on {Xi, Xm} and {Xm, Xj}. This is called
path consistency because one can think of it as looking at a path from Xi to Xj with Xm in
the middle.
Let’s see how path consistency fares in coloring the Australia map with two colors. We
will make the set {WA, SA} path consistent with respect to NT. We start by enumerating the
consistent assignments to the set. In this case, there are only two: {WA = red, SA = blue}
and {WA = blue, SA = red}. We can see that with both of these assignments NT can be
neither red nor blue (because it would conﬂict with either WA or SA). Because there is no
valid choice for NT, we eliminate both assignments, and we end up with no valid assignments
achieves arc consistency. Because it is so similar, we do not show it here.Constraint Propagation: Inference in CSPs
that, given the empty set, we can make any set of one variable consistent: this is what we
STRONGLY
then solve the problem as follows: First, we choose a consistent value for X1. We are then
a solution in time O(n2d). Of course, there is no free lunch: any algorithm for establishing
requires space that is exponential in n. The memory issue is even more severe than the time.
In practice, determining the appropriate level of consistency checking is mostly an empirical
Global constraints
Remember that a global constraint is one involving an arbitrary number of variables (but not
necessarily all variables). Global constraints occur frequently in real problems and can be
ods described so far. For example, the Alldiﬀconstraint says that all the variables involved
low). One simple form of inconsistency detection for Alldiﬀconstraints works as follows:
gether, and m > n, then the constraint cannot be satisﬁed.
straint that has a singleton domain, and delete that variable’s value from the domains of the
remaining variables. Repeat as long as there are singleton variables. If at any point an empty
domain is produced or there are more variables than domain values left, then an inconsistency
has been detected.
This method can detect the inconsistency in the assignment {WA = red, NSW = red}
with the partial assignment, the domain of each variable is reduced to {green, blue}. That
is, we have three variables and only two colors, so the Alldiﬀconstraint is violated. Thus,
than applying arc consistency to an equivalent set of binary constraints. There are moreChapter
Constraint Satisfaction Problems
more constraints but are more computationally expensive to run.
RESOURCE
CONSTRAINT
the atmost constraint. For example, in a scheduling problem, let P1, . . . , P4 denote the
inconsistency simply by checking the sum of the minimum values of the current domains;
satisﬁed. We can also enforce consistency by deleting the maximum value of any domain if it
is not consistent with the minimum values of the other domains. Thus, if each variable in our
involving moving thousands of people in hundreds of vehicles—it is usually not possible to
represent the domain of each variable as a large set of integers and gradually reduce that set by
BOUNDS
PROPAGATION
and
and
BOUNDS
CONSISTENT
between X and Y for every variable Y . This kind of bounds propagation is widely used in
practical constraint problems.
Sudoku example
SUDOKU
row, column, or box is called a unit.
The Sudoku puzzles that are printed in newspapers and puzzle books have the property
that there is exactly one solution. Although some can be tricky to solve by hand, taking tens
use the variable names A1 through A9 for the top row (left to right), down to I1 through I9Constraint Propagation: Inference in CSPs
A
B
C
D
E
F
G
H
 I
A
B
C
D
E
F
G
H
 I
(a)
(b)
(a) A Sudoku puzzle and (b) its solution.
Alldiﬀ(A1, A2, A3, A4, A5, A6, A7, A8, A9)
Alldiﬀ(B1, B2, B3, B4, B5, B6, B7, B8, B9)
· · ·
Alldiﬀ(A1, B1, C1, D1, E1, F1, G1, H1, I1)
Alldiﬀ(A2, B2, C2, D2, E2, F2, G2, H2, I2)
· · ·
Alldiﬀ(A1, A2, A3, B1, B2, B3, C1, C2, C3)
Alldiﬀ(A4, A5, A6, B4, B5, B6, C4, C5, C6)
· · ·
Let us see how far arc consistency can take us. Assume that the Alldiﬀconstraints have been
solve the entire puzzle—all the variables have their domains reduced to a single value, as
Of course, Sudoku would soon lose its appeal if every puzzle could be solved by aChapter
Constraint Satisfaction Problems
and to make efﬁcient progress, we will have to be more clever.
Indeed, the appeal of Sudoku puzzles for the human solver is the need to be resourceful
in applying more complex inference strategies. Aﬁcionados give them colorful names, such
as “naked triples.” That strategy works as follows: in any unit (row, column or box), ﬁnd
three squares that each have a domain that contains the same three numbers or a subset of
domains of every other square in the unit.
sistency, path consistency, etc.—apply generally to all CSPs, not just to Sudoku problems.
Even naked triples is really a strategy for enforcing consistency of Alldiﬀconstraints and
has nothing to do with Sudoku per se. This is the power of the CSP formalism: for each new
problem area, we only need to deﬁne the problem in terms of constraints; then the general
BACKTRACKING SEARCH FOR CSPS
Sudoku problems are designed to be solved by inference over constraints. But many other
CSPs cannot be solved by inference alone; there comes a time when we must search for a
signments; in the next section we look at local search algorithms over complete assignments.
partial assignment, and an action would be adding var = value to the assignment. But for a
CSP with n variables of domain size d, we quickly notice something terrible: the branching
factor at the top level is nd because any of d values can be assigned to any of n variables. At
with n! · dn leaves, even though there are only dn possible complete assignments!
Our seemingly reasonable but naive formulation ignores crucial property common to
all CSPs: commutativity. A problem is commutative if the order of application of any given
COMMUTATIVITY
set of actions has no effect on the outcome. CSPs are commutative because when assigning
values to variables, we reach the same partial assignment regardless of order. Therefore, we
need only consider a single variable at each node in the search tree. For example, at the root
node of a search tree for coloring the map of Australia, we might make a choice between
SA = red, SA = green, and SA = blue, but we would never choose between SA = red and
WA = blue. With this restriction, the number of leaves is dn, as we would hope.Backtracking Search for CSPs
return BACKTRACK({ },csp)
function BACKTRACK(assignment,csp) returns a solution, or failure
if assignment is complete then return assignment
if value is consistent with assignment then
add {var = value} to assignment
inferences ←INFERENCE(csp,var,value)
if inferences ̸= failure then
add inferences to assignment
result ←BACKTRACK(assignment,csp)
if result ̸= failure then
return result
remove {var = value} and inferences from assignment
return failure
(noticed either by INFERENCE or by BACKTRACK), then value assignments (including those
made by INFERENCE) are removed from the current assignment and a new value is tried.
BACKTRACKING
SEARCH
one variable at a time and backtracks when a variable has no legal values left to assign. The
all values in the domain of that variable in turn, trying to ﬁnd a solution. If an inconsistency is
detected, then BACKTRACK returns failure, causing the previous call to try another value. Part
variables in the order WA, NT, Q, . . .. Because the representation of CSPs is standardized,
action function, transition model, or goal test.
using them to address the following questions:Chapter
Constraint Satisfaction Problems
repeating this failure?
The subsections that follow answer each of these questions in turn.
Variable and value ordering
The backtracking algorithm contains the line
variable in order, {X1, X2, . . .}. This static variable ordering seldom results in the most efﬁ-
there is only one possible value for SA, so it makes sense to assign SA = blue next rather than
assigning Q. In fact, after SA is assigned, the choices for Q, NSW , and V are all forced. This
soon, thereby pruning the search tree. If some variable X has no legal values left, the MRV
heuristic will select X and failure will be detected immediately—avoiding pointless searches
through other variables. The MRV heuristic usually performs better than a random or static
on the problem.
The MRV heuristic doesn’t help at all in choosing the ﬁrst region to color in Australia,
because initially every region has three legal colors. In this case, the degree heuristic comes
DEGREE HEURISTIC
able that is involved in the largest number of constraints on other unassigned variables. In
tic solves the problem without any false steps—you can choose any consistent color at eachBacktracking Search for CSPs
values heuristic is usually a more powerful guide, but the degree heuristic can be useful as a
Once a variable has been selected, the algorithm must decide on the order in which to
VALUE
cases. It prefers the value that rules out the fewest choices for the neighboring variables in
assignment with WA = red and NT = green and that our next choice is for Q. Blue would
be a bad choice because it eliminates the last legal value left for Q’s neighbor, SA. The
trying to leave the maximum ﬂexibility for subsequent variable assignments. Of course, if we
are trying to ﬁnd all the solutions to a problem, not just the ﬁrst one, then the ordering does
not matter because we have to consider every value anyway. The same holds if there are no
solutions to the problem.
that, for a wide variety of problems, a variable ordering that chooses a variable with the
minimum number of remaining values helps minimize the number of nodes in the search tree
by pruning larger parts of the tree earlier. For value ordering, the trick is that we only need
one solution; therefore it makes sense to look for the most likely values ﬁrst. If we wanted to
enumerate all solutions rather than just ﬁnd one, then value ordering would be irrelevant.
Interleaving search and inference
variables before we begin the search. But inference can be even more powerful in the course
opportunity to infer new domain reductions on the neighboring variables.
FORWARD
CHECKING
unassigned variable Y that is connected to X by a constraint, delete from Y ’s domain any
value that is inconsistent with the value chosen for X. Because forward checking only does
arc consistency inferences, there is no reason to do forward checking if we have already done
arc consistency as a preprocessing step.
ward checking. There are two important points to notice about this example. First, notice
that after WA = red and Q = green are assigned, the domains of NT and SA are reduced
main of SA is empty. Hence, forward checking has detected that the partial assignment
{WA = red, Q = green, V = blue} is inconsistent with the constraints of the problem, and
the algorithm will therefore backtrack immediately.
seems that that assignment constrains its neighbors, NT and SA, so we should handle thoseChapter
Constraint Satisfaction Problems
Initial domains
R G B
R
R
B
R G B
R G B
B
R G B
R G B
R G B
R
R
R
R G B
B
B
G B
R G B
G
G
R G B
R G B
B
G B
R G B
R G B
R G B
R G B
WA
T
SA
V
NSW
Q
NT
is assigned ﬁrst; then forward checking deletes red from the domains of the neighboring
variables NT and SA. After Q = green is assigned, green is deleted from the domains of
NT, SA, and NSW . After V = blue is assigned, blue is deleted from the domains of NSW
and SA, leaving SA with no legal values.
pens with MRV: NT and SA have two values, so one of them is chosen ﬁrst, then the other,
then Q, NSW , and V in order. Finally T still has three values, and any one of them works.
We can view forward checking as an efﬁcient way to incrementally compute the information
that the MRV heuristic needs to do its job.
Although forward checking detects many inconsistencies, it does not detect all of them.
It shows that when WA is red and Q is green, both NT and SA are forced to be blue. Forward
checking does not look far enough ahead to notice that this is an inconsistency: NT and SA
are adjacent and so cannot have the same value.
The algorithm called MAC (for Maintaining Arc Consistency (MAC)) detects this
MAINTAINING ARC
CONSISTENCY (MAC)
but instead of a queue of all arcs in the CSP, we start with only the arcs (Xj, Xi) for all
propagation in the usual way, and if any variable has its domain reduced to the empty set, the
more powerful than forward checking because forward checking does the same thing as MAC
on the initial arcs in MAC’s queue; but unlike MAC, forward checking does not recursively
propagate constraints when changes are made to the domains of variables.
Intelligent backtracking: Looking backward
do when a branch of the search fails: back up to the preceding variable and try a different
value for it. This is called chronological backtracking because the most recent decision
CHRONOLOGICAL
BACKTRACKING
point is revisited. In this subsection, we consider better possibilities.
variable ordering Q, NSW , V , T, SA, WA, NT. Suppose we have generated the partial
assignment {Q = red, NSW = green, V = blue, T = red}. When we try the next variable,
SA, we see that every value violates a constraint. We back up to T and try a new color forBacktracking Search for CSPs
Tasmania! Obviously this is silly—recoloring Tasmania cannot possibly resolve the problem
with South Australia.
A more intelligent approach to backtracking is to backtrack to a variable that might ﬁx
the problem—a variable that was responsible for making one of the possible values of SA
impossible. To do this, we will keep track of a set of assignments that are in conﬂict with
some value for SA. The set (in this case {Q = red, NSW = green, V = blue, }), is called the
conﬂict set for SA. The backjumping method backtracks to the most recent assignment in
CONFLICT SET
BACKJUMPING
the conﬂict set; in this case, backjumping would jump over Tasmania and try a new value
for V . This method is easily implemented by a modiﬁcation to BACKTRACK such that it
accumulates the conﬂict set while checking for a legal value to assign. If no legal value is
found, the algorithm should return the most recent element of the conﬂict set along with the
failure indicator.
set with no extra work: whenever forward checking based on an assignment X = x deletes a
value from Y ’s domain, it should add X = x to Y ’s conﬂict set. If the last value is deleted
from Y ’s domain, then the assignments in the conﬂict set of Y are added to the conﬂict set
of X. Then, when we get to Y , we know immediately where to backtrack if needed.
every value in a domain is in conﬂict with the current assignment; but forward checking
detects this event and prevents the search from ever reaching such a node! In fact, it can be
shown that every branch pruned by backjumping is also pruned by forward checking. Hence,
uses stronger consistency checking, such as MAC.
mains a good one: to backtrack based on the reasons for failure. Backjumping notices failure
when a variable’s domain becomes empty, but in many cases a branch is doomed long before
this occurs. Consider again the partial assignment {WA = red, NSW = red} (which, from
our earlier discussion, is inconsistent). Suppose we try T = red next and then assign NT, Q,
V , SA. We know that no assignment can work for these last four variables, so eventually we
run out of values to try at NT. Now, the question is, where to backtrack? Backjumping cannot
work, because NT does have values consistent with the preceding assigned variables—NT
doesn’t have a complete conﬂict set of preceding variables that caused it to fail. We know,
however, that the four variables NT, Q, V , and SA, taken together, failed because of a set of
preceding variables, which must be those variables that directly conﬂict with the four. This
ing variables that caused NT, together with any subsequent variables, to have no consistent
solution. In this case, the set is WA and NSW , so the algorithm should backtrack to NSW
and skip over Tasmania. A backjumping algorithm that uses conﬂict sets deﬁned in this way
BACKJUMPING
We must now explain how these new conﬂict sets are computed. The method is in
fact quite simple. The “terminal” failure of a branch of the search always occurs because a
variable’s domain becomes empty; that variable has a standard conﬂict set. In our example,
SA fails, and its conﬂict set is (say) {WA, NT, Q}. We backjump to Q, and Q absorbsChapter
Constraint Satisfaction Problems
the conﬂict set from SA (minus Q itself, of course) into its own direct conﬂict set, which is
{NT, NSW }; the new conﬂict set is {WA, NT, NSW }. That is, there is no solution from
Q onward, given the preceding assignment to {WA, NT, NSW }. Therefore, we backtrack
to NT, the most recent of these. NT absorbs {WA, NT, NSW } −{NT} into its own
direct conﬂict set {WA}, giving {WA, NSW } (as stated in the previous paragraph). Now
the algorithm backjumps to NSW , as we would hope. To summarize: let Xj be the current
variable, and let conf (Xj) be its conﬂict set. If every possible value for Xj fails, backjump
to the most recent variable Xi in conf (Xj), and set
conf (Xi) ←conf (Xi) ∪conf (Xj) −{Xi} .
When we reach a contradiction, backjumping can tell us how far to back up, so we don’t
waste time changing variables that won’t ﬁx the problem. But we would also like to avoid
running into the same problem again. When the search arrives at a contradiction, we know
that some subset of the conﬂict set is responsible for the problem. Constraint learning is the
CONSTRAINT
LEARNING
idea of ﬁnding a minimum set of variables from the conﬂict set that causes the problem. This
For example, consider the state {WA = red, NT = green, Q = blue} in the bottom
once we prune this branch from the search tree, we will never encounter this combination
that started by ﬁrst assigning values for V and T. Then it would be worthwhile to record
same problem again for each possible set of assignments to V and T.
learning is one of the most important techniques used by modern CSP solvers to achieve
efﬁciency on complex problems.
LOCAL SEARCH FOR CSPS
each step moves a single queen to a new position in its column. Typically, the initial guess
In choosing a new value for a variable, the most obvious heuristic is to select the value
for hill climbing and simulated annealing can be applied to optimize the objective function.Local Search for CSPs
inputs: csp, a constraint satisfaction problem
max steps, the number of steps allowed before giving up
current ←an initial complete assignment for csp
if current is a solution for csp then return current
var ←a randomly chosen conﬂicted variable from csp.VARIABLES
value ←the value v for var that minimizes CONFLICTS(var,v,current,csp)
set var = value in current
return failure
conﬂict value for each variable in turn. The CONFLICTS function counts the number of
constraints violated by a particular value, given the rest of the current assignment.
stage, a queen is chosen for reassignment in its column. The number of conﬂicts (in this
case, the number of attacking queens) is shown in each square. The algorithm moves the
leading to a great deal of research in the 1990s on local search and the distinction between
easy for local search because solutions are densely distributed throughout the state space.
observations for the Hubble Space Telescope, reducing the time taken to schedule a week ofChapter
Constraint Satisfaction Problems
ways moves to another state with the same score—can help local search ﬁnd its way off this
plateau. This wandering on the plateau can be directed with tabu search: keeping a small
list of recently visited states and forbidding the algorithm to return to those states. Simulated
annealing can also be used to escape from plateaux.
Another technique, called constraint weighting, can help concentrate the search on the
CONSTRAINT
WEIGHTING
lowest total weight of all violated constraints. The weights are then adjusted by incrementing
the weight of each constraint that is violated by the current assignment. This has two beneﬁts:
it adds topography to plateaux, making sure that it is possible to improve from the current
state, and it also, over time, adds weight to the constraints that are proving difﬁcult to solve.
Another advantage of local search is that it can be used in an online setting when the
problem changes. This is particularly important in scheduling problems. A week’s airline
schedule may involve thousands of ﬂights and tens of thousands of personnel assignments,
but bad weather at one airport can render the schedule infeasible. We would like to repair the
schedule with a minimum number of changes. This can be easily done with a local search
algorithm starting from the current schedule. A backtracking search with the new set of
constraints usually requires much more time and might ﬁnd a solution with many changes
from the current schedule.
THE STRUCTURE OF PROBLEMS
In this section, we examine ways in which the structure of the problem, as represented by
the constraint graph, can be used to ﬁnd solutions quickly. Most of the approaches here also
apply to other problems besides CSPs, such as probabilistic reasoning. After all, the only way
we can possibly hope to deal with the real world is to decompose it into many subproblems.
coloring Tasmania and coloring the mainland are independent subproblems—any solution
INDEPENDENT
SUBPROBLEMS
for the mainland combined with any solution for Tasmania yields a solution for the whole
map. Independence can be ascertained simply by ﬁnding connected components of the
CONNECTED
COMPONENT
constraint graph. Each component corresponds to a subproblem CSPi. If assignment Si is
a solution of CSPi, then 
i Si is a solution of 
i CSPi. Why is this important? Consider
the following: suppose each CSPi has c variables from the total of n variables, where c is
its nearest mainland neighbor, to avoid the impression that it might be part of that state.The Structure of Problems
without the decomposition, the total work is O(dn), which is exponential in n. Let’s make
Completely independent subproblems are delicious, then, but rare. Fortunately, some
other graph structures are also easy to solve. For example, a constraint graph is a tree when
DIRECTED ARC
CONSISTENCY
Xj for j > i.
choose an ordering of the variables such that each variable appears after its parent in the tree.
TOPOLOGICAL SORT
graph, we can just march down the list of variables and choose any remaining value. Since
each link from a parent to its child is arc consistent, we know that for any value we choose for
the parent, there will be a valid value left to choose for the child. That means we won’t have
to backtrack; we can move linearly through the variables. The complete algorithm is shown
A
C
B
D
E
F
(a)
A
C
B
D
E
F
(b)
variables consistent with the tree with A as the root. This is known as a topological sort of
the variables.
Now that we have an efﬁcient algorithm for trees, we can consider whether more general
constraint graphs can be reduced to trees somehow. There are two primary ways to do this,
one based on removing nodes and one based on collapsing nodes together.
The ﬁrst approach involves assigning values to some variables so that the remaining
variables form a tree.
Fortunately, we can do this (in the graph, not the continent) by ﬁxing a value for SA andChapter
Constraint Satisfaction Problems
inputs: csp, a CSP with components X, D, C
n ←number of variables in X
assignment ←an empty assignment
root ←any variable in X
X ←TOPOLOGICALSORT(X ,root)
if it cannot be made consistent then return failure
assignment[Xi] ←any consistent value from Di
if there is no consistent value then return failure
return assignment
CSP has a solution, we will ﬁnd it in linear time; if not, we will detect a contradiction.
WA
NT
SA
Q
NSW
V
T
WA
NT
Q
NSW
V
T
(a)
(b)
after the removal of SA.
deleting from the domains of the other variables any values that are inconsistent with the
value chosen for SA.
sistent with the value chosen for SA. (This works for binary CSPs; the situation is more
the algorithm given above and thus solve the whole problem. Of course, in the general case
(as opposed to map coloring), the value chosen for SA could be the wrong one, so we would
need to try each possible value. The general algorithm is as follows:The Structure of Problems
after removal of S. S is called a cycle cutset.
CYCLE CUTSET
tent with the assignment for S, and
(b) If the remaining CSP has a solution, return it together with the assignment for S.
If the cycle cutset has size c, then the total run time is O(dc · (n −c)d2): we have to try each
of the dc combinations of values for the variables in S, and for each combination we must
solve a tree problem of size n −c. If the graph is “nearly a tree,” then c will be small and the
savings over straight backtracking will be huge. In the worst case, however, c can be as large
algorithms are known. The overall algorithmic approach is called cutset conditioning; it
CUTSET
CONDITIONING
The second approach is based on constructing a tree decomposition of the constraint
TREE
DECOMPOSITION
graph into a set of connected subproblems. Each subproblem is solved independently, and the
coloring problem into ﬁve subproblems. A tree decomposition must satisfy the following
three requirements:
• Every variable in the original problem appears in at least one of the subproblems.
• If two variables are connected by a constraint in the original problem, they must appear
together (along with the constraint) in at least one of the subproblems.
• If a variable appears in two subproblems in the tree, it must appear in every subproblem
along the path connecting those subproblems.
The ﬁrst two conditions ensure that all the variables and constraints are represented in the
decomposition. The third condition seems rather technical, but simply reﬂects the constraint
that any given variable must have the same value in every subproblem in which it appears;
the links joining subproblems in the tree enforce this constraint. For example, SA appears in
this decomposition makes sense.
tire problem has no solution. If we can solve all the subproblems, then we attempt to construct
main is the set of all solutions for the subproblem. For example, the leftmost subproblem in
is {WA = red, SA = blue, NT = green}. Then, we solve the constraints connecting the
subproblems, using the efﬁcient algorithm for trees given earlier. The constraints between
subproblems simply insist that the subproblem solutions agree on their shared variables. For
example, given the solution {WA = red, SA = blue, NT = green} for the ﬁrst subproblem,
the only consistent solution for the next subproblem is {SA = blue, NT = green, Q = red}.
sition, the aim is to make the subproblems as small as possible. The tree width of a tree
TREE WIDTHChapter
Constraint Satisfaction Problems
T
WA
NT
SA
NT
SA
Q
SA
Q
NSW
SA
NSW
V
decomposition of a graph is one less than the size of the largest subproblem; the tree width
of the graph itself is deﬁned to be the minimum tree width among all its tree decompositions.
If a graph has tree width w and we are given the corresponding tree decomposition, then the
tree width are solvable in polynomial time. Unfortunately, ﬁnding the decomposition with
So far, we have looked at the structure of the constraint graph. There can be important
For every consistent solution, there is actually a set of n! solutions formed by permuting the
color names. For example, on the Australia map we know that WA, NT, and SA must all have
This is called value symmetry. We would like to reduce the search space by a factor of
VALUE SYMMETRY
BREAKING
CONSTRAINT
For our example, we might impose an arbitrary ordering constraint, NT < SA < WA, that
requires the three values to be in alphabetical order. This constraint ensures that only one of
the n! solutions is possible: {NT = blue, SA = green, WA = red}.
For map coloring, it was easy to ﬁnd a constraint that eliminates the symmetry, and
in general it is possible to ﬁnd constraints that eliminate all but one symmetric solution in
values during search. In practice, breaking value symmetry has proved to be important and
effective on a wide range of problems.Summary
SUMMARY
pairs and represent the conditions for a solution by a set of constraints on the variables.
Inference can be interwoven with search.
variable. Backtracking occurs when no legal assignment can be found for a variable.
faction problems with great success.
• The complexity of solving a CSP is strongly related to the structure of its constraint
be found. Tree decomposition techniques transform the CSP into a tree of subproblems
and are efﬁcient if the tree width of the constraint graph is small.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The earliest work related to constraint satisfaction dealt largely with numerical constraints.
magupta in the seventh century; they are often called Diophantine equations, after the Greek
DIOPHANTINE
EQUATIONS
tionals. Systematic methods for solving linear equations by variable elimination were studied
graph coloring (of which map coloring is a special case) is an old problem in mathematics.
GRAPH COLORING
despite several published claims to the contrary—until a proof was devised by Appel and
theorem prover, derived a formal proof that Appel and Haken’s proof was correct.
Speciﬁc classes of constraint satisfaction problems occur throughout the history ofChapter
Constraint Satisfaction Problems
runner of modern drawing programs and CAD tools. The identiﬁcation of CSPs as a general
Charles Sanders Peirce. It was introduced into the CSP literature by Dechter (1990b) and
the notion of constraint networks and propagation by path consistency. Alan Mackworth
ter Mackworth’s paper appeared, researchers began experimenting with the tradeoff between
the cost of consistency enforcement and the beneﬁts in terms of search reduction. Haralick
framework within which consistency propagation algorithms can be analyzed, and Bessi`ere
excellent coverage of research in this area. The Alldiﬀconstraint was studied by Regin
Sudoku has become the most widely known CSP and was described as such by Simonis
interactive solver based on CSP techniques.
the basic algorithm back to the 19th century. Bitner and Reingold also introduced the MRV
degree heuristic as a tiebreaker after applying the MRV heuristic. The resulting algorithm,Bibliographical and Historical Notes
erful form of intelligent backtracking was actually developed very early on by Stallman and
DIRECTED
BACKTRACKING
The work of Stallman and Sussman also introduced the idea of constraint learning,
in which partial results obtained by search can be saved and reused later in the search. The
BACKMARKING
ple method in which consistent and inconsistent pairwise assignments are saved and used
DYNAMIC
BACKTRACKING
tains successful partial assignments from later subsets of variables when backtracking over
an earlier choice that does not invalidate the later success.
Empirical studies of several randomized backtracking methods were done by Gomes
difﬁculty of randomly generated CSPs and discovered that almost all such problems either
are trivially easy or have no solutions. Only if the parameters of the problem generator are
set in a certain narrow range, within which roughly half of the problems are solvable, do we
degree of local structure; this led to work that combined local search and inference, such as
showed that search on arc consistent trees works without any backtracking. A similar result,
with extensions to acyclic hypergraphs, was developed in the database community (Beeri
that runs in linear time without any preprocessing.
Since those papers were published, there has been a great deal of progress in developing
more general results relating the complexity of solving a CSP to the structure of its constraint
graph. The notion of tree width was introduced by the graph theorists Robertson and Seymour
notion (which they called induced width) to constraint satisfaction problems and developedChapter
Constraint Satisfaction Problems
from database theory, Gottlob et al. (1999a, 1999b) developed a notion, hypertree width, that
is based on the characterization of the CSP as a hypergraph. In addition to showing that any
hypertree width subsumes all previously deﬁned measures of “width” in the sense that there
are cases where the hypertree width is bounded and the other measures are unbounded.
The ﬁeld of distributed constraint satisfaction looks at solving CSPs when there is a
DISTRIBUTED
CONSTRAINT
SATISFACTION
collection of agents, each of which controls a subset of the constraint variables. There have
Comparing CSP algorithms is mostly an empirical science: few theoretical results show
that one algorithm dominates another on all problems; instead, we need to run experiments
points out, we need to be careful to distinguish between competitive testing—as occurs in
competitions among algorithms based on run time—and scientiﬁc testing, whose goal is to
identify the properties of an algorithm that determine its efﬁcacy on a class of problems.
and methods that rely on properties of the domains or constraints themselves. Kondrak and
satisfaction appear regularly in Artiﬁcial Intelligence and in the specialist journal Constraints.
The primary conference venue is the International Conference on Principles and Practice of
Constraint Programming, often called CP.
EXERCISES
solutions if four colors are allowed? Two colors?
Consider the problem of placing k knights on an n × n chessboard such that no twoChapter
Constraint Satisfaction Problems
The Englishman lives in the red house.
The Spaniard owns the dog.
The Norwegian lives in the ﬁrst house on the left.
The green house is immediately to the right of the ivory house.
The man who eats Hershey bars lives in the house next to the man with the fox.
Kit Kats are eaten in the yellow house.
The Norwegian lives next to the blue house.
The Smarties eater owns snails.
The Snickers eater drinks orange juice.
The Ukrainian drinks tea.
The Japanese eats Milky Ways.
Kit Kats are eaten in a house next to the house where the horse is kept.
Coffee is drunk in the green house.
Milk is drunk in the middle house.
sentation over another?
value order R, G, B.
Explain why it is a good heuristic to choose the variable that is most constrained but the
value that is least constraining in a CSP search.
the unit square; select a point X at random, connect X by a straight line to the nearest point
Y such that X is not already connected to Y and the line crosses no other line; repeat the
previous step until no more connections are possible. The points represent regions on the
backtracking with MAC. Construct a table of average run times for each algorithm for values
of n up to the largest you can manage. Comment on your results.
the domain of Xi, even if each value of Xk is consistent with several remaining values of Xi.
Suppose that, for every arc (Xk, Xi), we keep track of the number of remaining values of Xi
that are consistent with each value of Xk. Explain how to update these numbers efﬁciently
and hence show that arc consistency can be enforced in total time O(n2d2).LOGICAL AGENTS
In which we design agents that can form representations of a complex world, use a
process of inference to derive new representations about the world, and use these
new representations to deduce what to do.
Humans, it seems, know things; and what they know helps them do things.
These are
not empty statements. They make strong claims about how the intelligence of humans is
achieved—not by purely reﬂex mechanisms but by processes of reasoning that operate on
REASONING
internal representations of knowledge. In AI, this approach to intelligence is embodied in
REPRESENTATION
AGENTS
used to predict the outcome of actions but not to deduce that two tiles cannot occupy the
same space or that states with odd parity cannot be reached from states with even parity. The
observable environment, an agent’s only choice for representing what it knows about the
current state is to list all possible concrete states—a hopeless prospect in large environments.
ables; this is a step in the right direction, enabling some parts of the agent to work in a
those that follow, we take this step to its logical conclusion, so to speak—we develop logic
LOGIC
combine and recombine information to suit myriad purposes. Often, this process can be quite
far removed from the needs of the moment—as when a mathematician proves a theorem or
tasks in the form of explicitly described goals; they can achieve competence quickly by being
told or learning new knowledge about the environment; and they can adapt to changes in the
environment by updating the relevant knowledge.
agent without going into any technical detail. Then we explain the general principles of logicthe technology of propositional logic to build some simple agents for the wumpus world.
KNOWLEDGE BASE
edge base is a set of sentences. (Here “sentence” is used as a technical term. It is related
SENTENCE
but not identical to the sentences of English and other natural languages.) Each sentence is
expressed in a language called a knowledge representation language and represents some
KNOWLEDGE
REPRESENTATION
LANGUAGE
assertion about the world. Sometimes we dignify a sentence with the name axiom, when the
AXIOM
sentence is taken as given without being derived from other sentences.
There must be a way to add new sentences to the knowledge base and a way to query
what is known. The standard names for these operations are TELL and ASK, respectively.
Both operations may involve inference—that is, deriving new sentences from old. Inference
INFERENCE
must obey the requirement that when one ASKs a question of the knowledge base, the answer
should follow from what has been told (or TELLed) to the knowledge base previously. Later
in this chapter, we will be more precise about the crucial word “follow.” For now, take it to
mean that the inference process should not make things up as it goes along.
it takes a percept as input and returns an action. The agent maintains a knowledge base, KB,
which may initially contain some background knowledge.
BACKGROUND
KNOWLEDGE
edge base what it perceives. Second, it ASKs the knowledge base what action it should
perform. In the process of answering this query, extensive reasoning may be done about
the current state of the world, about the outcomes of possible action sequences, and so on.
Third, the agent program TELLs the knowledge base which action was chosen, and the agent
executes the action.
ment the interface between the sensors and actuators on one side and the core representation
constructs a sentence that asks what action should be done at the current time. Finally,
ecuted. The details of the inference mechanisms are hidden inside TELL and ASK. Later
sections will reveal these details.
agent is not an arbitrary program for calculating actions. It is amenable to a description atChapter
Logical Agents
persistent: KB, a knowledge base
return action
to its knowledge base, asks the knowledge base for the best action, and tells the knowledge
base that it has in fact taken that action.
the knowledge level, where we need specify only what the agent knows and what its goals
KNOWLEDGE LEVEL
are, in order to ﬁx its behavior. For example, an automated taxi might have the goal of
taking a passenger from San Francisco to Marin County and might know that the Golden
Gate Bridge is the only link between the two locations. Then we can expect it to cross the
Golden Gate Bridge because it knows that that will achieve its goal. Notice that this analysis
is independent of how the taxi works at the implementation level. It doesn’t matter whether
IMPLEMENTATION
LEVEL
its geographical knowledge is implemented as linked lists or pixel maps, or whether it reasons
by manipulating strings of symbols stored in registers or by propagating noisy signals in a
network of neurons.
Starting with an empty knowledge base, the agent designer can TELL sentences one by one
DECLARATIVE
proach to system building. In contrast, the procedural approach encodes desired behaviors
directly as program code. In the 1970s and 1980s, advocates of the two approaches engaged
in heated debates. We now understand that a successful agent often combines both declarative
and procedural elements in its design, and that declarative knowledge can often be compiled
into more efﬁcient procedural code.
about the environment from a series of percepts. A learning agent can be fully autonomous.
THE WUMPUS WORLD
worth. The wumpus world is a cave consisting of rooms connected by passageways. Lurking
WUMPUS WORLD
somewhere in the cave is the terrible wumpus, a beast that eats anyone who enters its room.
The wumpus can be shot by an agent, but the agent has only one arrow. Some rooms containThe Wumpus World
bottomless pits that will trap anyone who wanders into these rooms (except for the wumpus,
which is too big to fall in). The only mitigating feature of this bleak environment is the
possibility of ﬁnding a heap of gold. Although the wumpus world is rather tame by modern
computer game standards, it illustrates some important points about intelligence.
using up the arrow. The game ends either when the agent dies or when the agent climbs
out of the cave.
domly, with a uniform distribution, from the squares other than the start square. In
agent dies a miserable death if it enters a square containing a pit or a live wumpus. (It
is safe, albeit smelly, to enter a square with a dead wumpus.) If an agent tries to move
forward and bumps into a wall, then the agent does not move. The action Grab can be
used to pick up the gold if it is in the same square as the agent. The action Shoot can
be used to ﬁre an arrow in a straight line in the direction the agent is facing. The arrow
continues until it either hits (and hence kills) the wumpus or hits a wall. The agent has
only one arrow, so only the ﬁrst Shoot action has any effect. Finally, the action Climb
• Sensors: The agent has ﬁve sensors, each of which gives a single bit of information:
– In the square containing the wumpus and in the directly (not diagonally) adjacent
squares, the agent will perceive a Stench.
– In the squares directly adjacent to a pit, the agent will perceive a Breeze.
– In the square where the gold is, the agent will perceive a Glitter.
– When an agent walks into a wall, it will perceive a Bump.
where in the cave.
The percepts will be given to the agent program in the form of a list of ﬁve symbols;
for example, if there is a stench and a breeze, but no glitter, bump, or scream, the agent
program will get [Stench, Breeze, None, None, None].
It is sequential, because rewards may come only after many actions are taken. It is partially
cation, the wumpus’s state of health, and the availability of an arrow. As for the locations
pen to be immutable—in which case, the transition model for the environment is completelyChapter
Logical Agents
PIT
START
Stench
Stench
B
r
ee
z
e
Gold
PIT
PIT
B
r
ee
z
e
B
r
ee
z
e
B
r
ee
z
e
B
r
ee
z
e
B
r
ee
z
e
Stench
A typical wumpus world. The agent is in the bottom left corner, facing right.
known; or we could say that the transition model itself is unknown because the agent doesn’t
know which Forward actions are fatal—in which case, discovering the locations of pits and
wumpus completes the agent’s knowledge of the transition model.
ﬁguration of the environment; overcoming this ignorance seems to require logical reasoning.
In most instances of the wumpus world, it is possible for the agent to retrieve the gold safely.
or surrounded by pits.
The agent’s initial knowledge base contains the rules of the environment, as described
A cautious agent will move only into a square that it knows to be OK. Let us suppose
indicates a possible pit in those squares. At this point, there is only one known square that isThe Wumpus World
A
B
G
P
S
W
 = Agent
 = Breeze
 = Glitter, Gold
 = Pit
 = Stench
 = Wumpus
OK = Safe square
V
 = Visited
A
OK
OK
OK
B
P?
P?
A
OK
OK
OK
V
(a)
(b)
uation, after percept [None, None, None, None, None]. (b) After one move, with percept
[None, Breeze, None, None, None].
B
B
P!
A
OK
OK
OK
V
OK
W!
V
P!
A
OK
OK
OK
V
S
OK
W!
V
V
V
B
S G
P?
P?
(b)
(a)
S
A
B
G
P
S
W
 = Agent
 = Breeze
 = Glitter, Gold
 = Pit
 = Stench
 = Wumpus
OK = Safe square
V
 = Visited
Two later stages in the progress of the agent.
(a) After the third move,
with percept [Stench, None, None, None, None]. (b) After the ﬁfth move, with percept
[Stench, Breeze, Glitter, None, None].
inference, because it combines knowledge gained at different times in different places and
relies on the lack of a percept to make one crucial step.Chapter
Logical Agents
glitter, so it should grab the gold and then return home.
formation, that conclusion is guaranteed to be correct if the available information is correct.
This is a fundamental property of logical reasoning. In the rest of this chapter, we describe
how to build logical agents that can represent information and draw conclusions such as those
described in the preceding paragraphs.
LOGIC
This section summarizes the fundamental concepts of logical representation and reasoning.
pone the technical details of those forms until the next section, using instead the familiar
example of ordinary arithmetic.
are expressed according to the syntax of the representation language, which speciﬁes all the
SYNTAX
sentences that are well formed. The notion of syntax is clear enough in ordinary arithmetic:
A logic must also deﬁne the semantics or meaning of sentences. The semantics deﬁnes
SEMANTICS
the truth of each sentence with respect to each possible world. For example, the semantics
TRUTH
POSSIBLE WORLD
When we need to be precise, we use the term model in place of “possible world.”
MODEL
Whereas possible worlds might be thought of as (potentially) real environments that the agent
might or might not be in, models are mathematical abstractions, each of which simply ﬁxes
the truth or falsehood of every relevant sentence. Informally, we may think of a possible world
as, for example, having x men and y women sitting at a table playing bridge, and the sentence
all possible assignments of real numbers to the variables x and y. Each such assignment ﬁxes
the truth of any sentence of arithmetic whose variables are x and y. If a sentence α is true in
model m, we say that m satisﬁes α or sometimes m is a model of α. We use the notation
SATISFACTION
M(α) to mean the set of all models of α.
Now that we have a notion of truth, we are ready to talk about logical reasoning. This
ENTAILMENT
lows logically from another sentence. In mathematical notation, we write
α |= βLogic
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
KB
α1
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
PIT
KB
B
r
eez
e
α2
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
B
r
eez
e
(a)
(b)
to mean that the sentence α entails the sentence β. The formal deﬁnition of entailment is this:
α |= β if and only if, in every model in which α is true, β is also true. Using the notation just
introduced, we can write
α |= β if and only if M(α) ⊆M(β) .
more possible worlds.) The relation of entailment is familiar from arithmetic; we are happy
where x is zero, it is the case that xy is zero (regardless of the value of y).
of the rules of the wumpus world, constitute the KB. The agent is interested (among other
The KB can be thought of as a set of sentences or as a single sentence that asserts all
the individual sentences. The KB is false in models that contradict what the agent knows—
have ’orrible ’airy wumpuses in them.Chapter
Logical Agents
respectively. By inspection, we see the following:
in every model in which KB is true, α1 is also true.
in some models in which KB is true, α2 is false.
The preceding example not only illustrates entailment but also shows how the deﬁnition
of entailment can be applied to derive conclusions—that is, to carry out logical inference.
LOGICAL INFERENCE
MODEL CHECKING
merates all possible models to check that α is true in all models in which KB is true, that is,
that M(KB) ⊆M(α).
quences of KB as a haystack and of α as a needle. Entailment is like the needle being in the
haystack; inference is like ﬁnding it. This distinction is embodied in some formal notation: if
an inference algorithm i can derive α from KB, we write
KB ⊢i α ,
which is pronounced “α is derived from KB by i” or “i derives α from KB.”
SOUND
sentially makes things up as it goes along—it announces the discovery of nonexistent needles.
The property of completeness is also desirable: an inference algorithm is complete if
COMPLETENESS
it can derive any sentence that is entailed. For real haystacks, which are ﬁnite in extent,
it seems obvious that a systematic examination can always decide whether the needle is in
the haystack. For many knowledge bases, however, the haystack of consequences is inﬁnite,
procedures for logics that are sufﬁciently expressive to handle many knowledge bases.
We have described a reasoning process whose conclusions are guaranteed to be true
in any world in which the premises are true; in particular, if KB is true in the real world,
then any sentence α derived from KB by a sound inference procedure is also true in the real
world. So, while an inference process operates on “syntax”—internal physical conﬁgurations
such as bits in registers or patterns of electrical blips in brains—the process corresponds
arithmetic, on the other hand, the space of models is inﬁnite: even if we restrict ourselves to the integers, therePropositional Logic: A Very Simple Logic
Follows
Sentences
Sentence
Entails
Semantics
Semantics
Representation
World
Aspects of the
    real world
Aspect of the
   real world
Sentences are physical conﬁgurations of the agent, and reasoning is a process
sure that the new conﬁgurations represent aspects of the world that actually follow from the
aspects that the old conﬁgurations represent.
of other aspects of the real world being the case. This correspondence between world and
The ﬁnal issue to consider is grounding—the connection between logical reasoning
GROUNDING
processes and the real environment in which the agent exists. In particular, how do we know
that KB is true in the real world? (After all, KB is just “syntax” inside the agent’s head.)
This is a philosophical question about which many, many books have been written. (See
whenever there is a smell. Then, whenever that sentence is in the knowledge base, it is
true in the real world. Thus, the meaning and truth of percept sentences are deﬁned by the
processes of sensing and sentence construction that produce them. What about the rest of the
agent’s knowledge, such as its belief that wumpuses cause smells in adjacent squares? This
is not a direct representation of a single percept, but a general rule—derived, perhaps, from
perceptual experience but not identical to a statement of that experience. General rules like
this are produced by a sentence construction process called learning, which is the subject
of Part V. Learning is fallible. It could be the case that wumpuses cause smells except on
the real world, but with good learning procedures, there is reason for optimism.
PROPOSITIONAL LOGIC: A VERY SIMPLE LOGIC
We now present a simple but powerful logic called propositional logic. We cover the syntax
PROPOSITIONAL
LOGIC
mined. Then we look at entailment—the relation between a sentence and another sentence
erything takes place, of course, in the wumpus world.Chapter
Logical Agents
Syntax
The syntax of propositional logic deﬁnes the allowable sentences. The atomic sentences
ATOMIC SENTENCES
consist of a single proposition symbol. Each such symbol stands for a proposition that can
PROPOSITION
SYMBOL
be true or false. We use symbols that start with an uppercase letter and may contain other
Complex sentences are constructed from simpler sentences, using parentheses and logical
COMPLEX
SENTENCES
connectives. There are ﬁve connectives in common use:
LOGICAL
CONNECTIVES
NEGATION
LITERAL
atomic sentence (a positive literal) or a negated atomic sentence (a negative literal).
junction; its parts are the conjuncts. (The ∧looks like an “A” for “And.”)
CONJUNCTION
DISJUNCTION
IMPLICATION
PREMISE
CONCLUSION
RULES
BICONDITIONAL
write this as ≡.
Sentence
→
AtomicSentence | ComplexSentence
AtomicSentence
→
True | False | P | Q | R | . . .
ComplexSentence
→
( Sentence ) | [ Sentence ]
|
¬ Sentence
|
Sentence ∧Sentence
|
Sentence ∨Sentence
|
Sentence
⇒Sentence
|
Sentence
⇔
Sentence
OPERATOR PRECEDENCE
:
¬, ∧, ∨, ⇒, ⇔
A BNF (Backus–Naur Form) grammar of sentences in propositional logic,
along with operator precedences, from highest to lowest.Propositional Logic: A Very Simple Logic
familiar with the BNF notation. The BNF grammar by itself is ambiguous; a sentence with
several operators can be parsed by the grammar in multiple ways. To eliminate the ambiguity
we deﬁne a precedence for each operator. The “not” operator (¬) has the highest precedence,
which means that in the sentence ¬A ∧B the ¬ binds most tightly, giving us the equivalent
brackets mean the same thing as parentheses; the choice of square brackets or parentheses is
solely to make it easier for a human to read a sentence.
Semantics
mantics deﬁnes the rules for determining the truth of a sentence with respect to a particular
TRUTH VALUE
ery proposition symbol. For example, if the sentences in the knowledge base make use of the
The semantics for propositional logic must specify how to compute the truth value of
any sentence, given a model. This is done recursively. All sentences are constructed from
atomic sentences and the ﬁve connectives; therefore, we need to specify how to compute the
truth of atomic sentences and how to compute the truth of sentences formed with each of the
ﬁve connectives. Atomic sentences are easy:
• True is true in every model and False is false in every model.
• The truth value of every other proposition symbol must be speciﬁed directly in the
For complex sentences, we have ﬁve rules, which hold for any subsentences P and Q in any
model m (here “iff” means “if and only if”):
• ¬P is true iff P is false in m.
• P ∧Q is true iff both P and Q are true in m.
• P ∨Q is true iff either P or Q is true in m.
• P ⇒Q is true unless P is true and Q is false in m.
• P ⇔Q is true iff P and Q are both true or both false in m.
The rules can also be expressed with truth tables that specify the truth value of a complex
TRUTH TABLE
sentence for each possible assignment of truth values to its components. Truth tables for the
can be computed with respect to any model m by a simple recursive evaluation. For example,Chapter
Logical Agents
P
Q
¬P
P ∧Q
P ∨Q
P ⇒Q
P ⇔Q
false
false
true
false
false
true
true
false
true
true
false
true
true
false
true
false
false
false
true
false
false
true
true
false
true
true
true
true
Truth tables for the ﬁve logical connectives. To use the table to compute, for
example, the value of P ∨Q when P is true and Q is false, ﬁrst look on the left for the row
where P is true and Q is false (the third row). Then look in that row under the P ∨Q column
to see the result: true.
the truth value of a propositional logic sentence s in a model m.
The truth tables for “and,” “or,” and “not” are in close accord with our intuitions about
the English words. The main point of possible confusion is that P ∨Q is true when P is true
or Q is true or both. A different connective, called “exclusive or” (“xor” for short), yields
some choices are ˙∨or ̸= or ⊕.
The truth table for ⇒may not quite ﬁt one’s intuitive understanding of “P implies Q”
or “if P then Q.” For one thing, propositional logic does not require any relation of causation
is a true sentence of propositional logic (under the normal interpretation), even though it is
a decidedly odd sentence of English. Another point of confusion is that any implication is
regardless of whether Sam is smart. This seems bizarre, but it makes sense if you think of
“P ⇒Q” as saying, “If P is true, then I am claiming that Q is true. Otherwise I am making
no claim.” The only way for this sentence to be false is if P is true but Q is false.
The biconditional, P ⇔Q, is true whenever both P ⇒Q and Q ⇒P are true. In
English, this is often written as “P if and only if Q.” Many of the rules of the wumpus world
are best written using ⇔. For example, a square is breezy if a neighboring square has a pit,
and a square is breezy only if a neighboring square has a pit. So we need a biconditional,
A simple knowledge base
Now that we have deﬁned the semantics for propositional logic, we can construct a knowledge
base for the wumpus world. We focus ﬁrst on the immutable aspects of the wumpus world,
leaving the mutable aspects for a later section. For now, we need the following symbols for
each [x, y] location:Propositional Logic: A Very Simple Logic
Px,y is true if there is a pit in [x, y].
Wx,y is true if there is a wumpus in [x, y], dead or alive.
Bx,y is true if the agent perceives a breeze in [x, y].
Sx,y is true if the agent perceives a stench in [x, y].
R1 :
• A square is breezy if and only if there is a pit in a neighboring square. This has to be
stated for each square; for now, we include just the relevant squares:
R2 :
⇔
R3 :
⇔
• The preceding sentences are true in all wumpus worlds. Now we include the breeze
percepts for the ﬁrst two squares visited in the speciﬁc world the agent is in, leading up
R4 :
R5 :
A simple inference procedure
direct implementation of the deﬁnition of entailment: enumerate the models, and check that
α is true in every model in which KB is true. Models are assignments of true or false to
enumeration of a ﬁnite space of assignments to symbols. The algorithm is sound because it
implements directly the deﬁnition of entailment, and complete because it works for any KB
and α and always terminates—there are only ﬁnitely many models to examine.
Of course, “ﬁnitely many” is not always the same as “few.” If KB and α contain n
symbols in all, then there are 2n models. Thus, the time complexity of the algorithm is
this chapter we show algorithms that are much more efﬁcient in many cases. Unfortunately,
complexity that is exponential in the size of the input.Chapter
Logical Agents
R1
R2
R3
R4
R5
KB
false
false
false
false
false
false
false
true
true
true
true
false
false
false
false
false
false
false
false
true
true
true
false
true
false
false
...
...
...
...
...
...
...
...
...
...
...
...
...
false
true
false
false
false
false
false
true
true
false
true
true
false
false
true
false
false
false
false
true
true
true
true
true
true
true
false
true
false
false
false
true
false
true
true
true
true
true
true
false
true
false
false
false
true
true
true
true
true
true
true
true
false
true
false
false
true
false
false
true
false
false
true
true
false
...
...
...
...
...
...
...
...
...
...
...
...
...
true
true
true
true
true
true
true
false
true
true
false
true
false
A truth table constructed for the knowledge base given in the text. KB is true
inputs: KB, the knowledge base, a sentence in propositional logic
α, the query, a sentence in propositional logic
symbols ←a list of the proposition symbols in KB and α
if EMPTY?(symbols) then
else return true // when KB is false, always return true
else do
P ←FIRST(symbols)
rest ←REST(symbols)
and
word “and” is used here as a logical operation on its two arguments, returning true or false.Propositional Theorem Proving
(α ∧β) ≡(β ∧α)
commutativity of ∧
(α ∨β) ≡(β ∨α)
commutativity of ∨
((α ∧β) ∧γ) ≡(α ∧(β ∧γ))
associativity of ∧
((α ∨β) ∨γ) ≡(α ∨(β ∨γ))
associativity of ∨
¬(¬α) ≡α
(α ⇒β) ≡(¬β ⇒¬α)
contraposition
(α ⇒β) ≡(¬α ∨β)
implication elimination
(α ⇔β) ≡((α ⇒β) ∧(β ⇒α))
biconditional elimination
¬(α ∧β) ≡(¬α ∨¬β)
De Morgan
¬(α ∨β) ≡(¬α ∧¬β)
De Morgan
(α ∧(β ∨γ)) ≡((α ∧β) ∨(α ∧γ))
distributivity of ∧over ∨
(α ∨(β ∧γ)) ≡((α ∨β) ∧(α ∨γ))
distributivity of ∨over ∧
Standard logical equivalences. The symbols α, β, and γ stand for arbitrary
sentences of propositional logic.
PROPOSITIONAL THEOREM PROVING
So far, we have shown how to determine entailment by model checking: enumerating models
ment can be done by theorem proving—applying rules of inference directly to the sentences
THEOREM PROVING
in our knowledge base to construct a proof of the desired sentence without consulting models.
If the number of models is large but the length of the proof is short, then theorem proving can
be more efﬁcient than model checking.
LOGICAL
EQUIVALENCE
tences α and β are logically equivalent if they are true in the same set of models. We write
this as α ≡β. For example, we can easily show (using truth tables) that P ∧Q and Q ∧P
play much the same role in logic as arithmetic identities do in ordinary mathematics. An
alternative deﬁnition of equivalence is as follows: any two sentences α and β are equivalent
only if each of them entails the other:
α ≡β
if and only if
α |= β and β |= α .
The second concept we will need is validity. A sentence is valid if it is true in all models. For
VALIDITY
example, the sentence P ∨¬P is valid. Valid sentences are also known as tautologies—they
TAUTOLOGY
are necessarily true. Because the sentence True is true in all models, every valid sentence
is logically equivalent to True. What good are valid sentences? From our deﬁnition of
entailment, we can derive the deduction theorem, which was known to the ancient Greeks:
DEDUCTION
THEOREM
For any sentences α and β, α |= β if and only if the sentence (α ⇒β) is valid.Chapter
Logical Agents
or by proving that (α ⇒β) is equivalent to True. Conversely, the deduction theorem states
that every valid implication sentence describes a legitimate inference.
The ﬁnal concept we will need is satisﬁability. A sentence is satisﬁable if it is true
SATISFIABILITY
in, or satisﬁed by, some model. For example, the knowledge base given earlier, (R1 ∧R2 ∧
R3 ∧R4 ∧R5), is satisﬁable because there are three models in which it is true, as shown
found that satisﬁes the sentence. The problem of determining the satisﬁability of sentences
SAT
Many problems in computer science are really satisﬁability problems. For example, all the
some assignment.
Validity and satisﬁability are of course connected: α is valid iff ¬α is unsatisﬁable;
contrapositively, α is satisﬁable iff ¬α is not valid. We also have the following useful result:
α |= β if and only if the sentence (α ∧¬β) is unsatisﬁable.
Proving β from α by checking the unsatisﬁability of (α ∧¬β) corresponds exactly to the
standard mathematical proof technique of reductio ad absurdum (literally, “reduction to an
REDUCTIO AD
ABSURDUM
absurd thing”). It is also called proof by refutation or proof by contradiction. One assumes a
REFUTATION
CONTRADICTION
sentence β to be false and shows that this leads to a contradiction with known axioms α. This
contradiction is exactly what is meant by saying that the sentence (α ∧¬β) is unsatisﬁable.
Inference and proofs
INFERENCE RULES
PROOF
MODUS PONENS
mode that afﬁrms) and is written
α ⇒β,
α
β
.
The notation means that, whenever any sentences of the form α ⇒β and α are given, then
the sentence β can be inferred. For example, if (WumpusAhead ∧WumpusAlive) ⇒Shoot
and (WumpusAhead ∧WumpusAlive) are given, then Shoot can be inferred.
any of the conjuncts can be inferred:
α ∧β
α
.
For example, from (WumpusAhead ∧WumpusAlive), WumpusAlive can be inferred.
By considering the possible truth values of α and β, one can show easily that Modus
any particular instances where they apply, generating sound inferences without the need for
enumerating models.
ple, the equivalence for biconditional elimination yields the two inference rules
α ⇔β
(α ⇒β) ∧(β ⇒α)
and
(α ⇒β) ∧(β ⇒α)
α ⇔β
.Propositional Theorem Proving
Not all inference rules work in both directions like this. For example, we cannot run Modus
Ponens in the opposite direction to obtain α ⇒β and α from β.
Let us see how these inference rules and equivalences can be used in the wumpus world.
R6 :
R7 :
Logical equivalence for contrapositives gives
R8 :
R9 :
Finally, we apply De Morgan’s rule, giving the conclusion
R10 :
to ﬁnd a sequence of steps that constitutes a proof. We just need to deﬁne a proof problem as
follows:
• INITIAL STATE: the initial knowledge base.
tences that match the top half of the inference rule.
• RESULT: the result of an action is to add the sentence in the bottom half of the inference
rule.
• GOAL: the goal is a state that contains the sentence we are trying to prove.
Thus, searching for proofs is an alternative to enumerating models. In many practical cases
ﬁnding a proof can be more efﬁcient because the proof can ignore irrelevant propositions, no
appear only in R4 and R2; so R1, R3, and R5 have no bearing on the proof. The same would
algorithm, on the other hand, would be overwhelmed by the exponential explosion of models.
MONOTONICITY
sentences α and β,
if
KB |= α
then
KB ∧β |= α .Chapter
Logical Agents
For example, suppose the knowledge base contains the additional assertion β stating that there
clusions, but it cannot invalidate any conclusion α already inferred—such as the conclusion
suitable premises are found in the knowledge base—the conclusion of the rule must follow
regardless of what else is in the knowledge base.
Proof by resolution
We have argued that the inference rules covered so far are sound, but we have not discussed
the question of completeness for the inference algorithms that use them. Search algorithms
any reachable goal, but if the available inference rules are inadequate, then the goal is not
reachable—no proof exists that uses only those inference rules. For example, if we removed
the biconditional elimination rule, the proof in the preceding section would not go through.
The current section introduces a single inference rule, resolution, that yields a complete
inference algorithm when coupled with any complete search algorithm.
We begin by using a simple version of the resolution rule in the wumpus world. Let us
knowledge base:
R11 :
R12 :
R13 :
R14 :
We can also apply biconditional elimination to R3, followed by Modus Ponens with R5, to
R15 :
RESOLVENT
R16 :
R17 :
two inference steps are examples of the unit resolution inference rule,
UNIT RESOLUTION
ℓ1 ∨· · · ∨ℓk,
m
,
where each ℓis a literal and ℓi and m are complementary literals (i.e., one is the negation
COMPLEMENTARY
LITERALSPropositional Theorem Proving
of the other). Thus, the unit resolution rule takes a clause—a disjunction of literals—and a
CLAUSE
literal and produces a new clause. Note that a single literal can be viewed as a disjunction of
one literal, also known as a unit clause.
UNIT CLAUSE
The unit resolution rule can be generalized to the full resolution rule,
RESOLUTION
ℓ1 ∨· · · ∨ℓk,
m1 ∨· · · ∨mn
,
where ℓi and mj are complementary literals. This says that resolution takes two clauses and
produces a new clause containing all the literals of the two original clauses except the two
complementary literals. For example, we have
.
There is one more technical aspect of the resolution rule: the resulting clause should contain
FACTORING
For example, if we resolve (A ∨B) with (A ∨¬B), we obtain (A ∨A), which is reduced to
just A.
The soundness of the resolution rule can be seen easily by considering the literal ℓi that
is complementary to literal mj in the other clause. If ℓi is true, then mj is false, and hence
ℓi is either true or false, so one or other of these conclusions holds—exactly as the resolution
rule states.
What is more surprising about the resolution rule is that it forms the basis for a family
α and β in propositional logic, decide whether α |= β. The next two subsections explain
how resolution accomplishes this.
Conjunctive normal form
The resolution rule applies only to clauses (that is, disjunctions of literals), so it would seem
to be relevant only to knowledge bases and queries consisting of clauses. How, then, can
it lead to a complete inference procedure for all of propositional logic? The answer is that
every sentence of propositional logic is logically equivalent to a conjunction of clauses. A
sentence expressed as a conjunction of clauses is said to be in conjunctive normal form or
CONJUNCTIVE
NORMAL FORM
follows:
clauses makes the resolution rule much cleaner, at the cost of introducing additional notation.Chapter
Logical Agents
¬(α ∧β) ≡(¬α ∨¬β) (De Morgan)
¬(α ∨β) ≡(¬α ∧¬β) (De Morgan)
In the example, we require just one application of the last rule:
The original sentence is now in CNF, as a conjunction of three clauses. It is much harder to
read, but it can be used as input to a resolution procedure.
A resolution algorithm
unsatisﬁable. We do this by proving a contradiction.
CNF. Then, the resolution rule is applied to the resulting clauses. Each pair that contains
complementary literals is resolved to produce a new clause, which is added to the set if it is
not already present. The process continues until one of two things happens:
• there are no new clauses that can be added, in which case KB does not entail α; or,
• two clauses resolve to yield the empty clause, in which case KB entails α.
The empty clause—a disjunction of no disjuncts—is equivalent to False because a disjunction
is true only if at least one of its disjuncts is true. Another way to see that an empty clause
represents a contradiction is to observe that it arises only from resolving two complementary
unit clauses such as P and ¬P.
We can apply the resolution procedure to a very simple inference in the wumpus world.
The relevant knowledge base is
Therefore, any clause in which two complementary literals appear can be discarded.Propositional Theorem Proving
inputs: KB, the knowledge base, a sentence in propositional logic
α, the query, a sentence in propositional logic
clauses ←the set of clauses in the CNF representation of KB ∧¬α
new ←{ }
loop do
for each pair of clauses Ci, Cj in clauses do
if resolvents contains the empty clause then return true
A simple resolution algorithm for propositional logic.
The function
^
^
^
^
^
^
^
^
^
^
^
^
Completeness of resolution
To do this, we introduce the resolution closure RC (S) of a set of clauses S, which is the set
RESOLUTION
CLOSURE
of all clauses derivable by repeated application of the resolution rule to clauses in S or their
the variable clauses. It is easy to see that RC (S) must be ﬁnite, because there are only ﬁnitely
many distinct clauses that can be constructed out of the symbols P1, . . . , Pk that appear in S.
(Notice that this would not be true without the factoring step that removes multiple copies of
The completeness theorem for resolution in propositional logic is called the ground
resolution theorem:
GROUND
RESOLUTION
THEOREM
If a set of clauses is unsatisﬁable, then the resolution closure of those clauses
contains the empty clause.
This theorem is proved by demonstrating its contrapositive: if the closure RC(S) does notChapter
Logical Agents
contain the empty clause, then S is satisﬁable. In fact, we can construct a model for S with
suitable truth values for P1, . . . , Pk. The construction procedure is as follows:
– If a clause in RC(S) contains the literal ¬Pi and all its other literals are false under
– Otherwise, assign true to Pi.
This assignment to P1, . . . , Pk is a model of S. To see this, assume the opposite—that, at
some stage i in the sequence, assigning symbol Pi causes some clause C to become false.
For this to happen, it must be the case that all the other literals in C must already have been
· · · false∨Pi) or like (false∨false∨· · · false∨¬Pi). If just one of these two is in RC(S), then
the algorithm will assign the appropriate truth value to Pi to make C true, so C can only be
falsiﬁed if both of these clauses are in RC(S). Now, since RC(S) is closed under resolution,
it will contain the resolvent of these two clauses, and that resolvent will have all of its literals
the ﬁrst falsiﬁed clause appears at stage i. Hence, we have proved that the construction never
falsiﬁes a clause in RC(S); that is, it produces a model of RC(S) and thus a model of S
itself (since S is contained in RC(S)).
Horn clauses and deﬁnite clauses
The completeness of resolution makes it a very important inference method. In many practical
bases satisfy certain restrictions on the form of sentences they contain, which enables them
to use a more restricted and efﬁcient inference algorithm.
One such restricted form is the deﬁnite clause, which is a disjunction of literals of
DEFINITE CLAUSE
Slightly more general is the Horn clause, which is a disjunction of literals of which at
HORN CLAUSE
most one is positive. So all deﬁnite clauses are Horn clauses, as are clauses with no positive
literals; these are called goal clauses. Horn clauses are closed under resolution: if you resolve
GOAL CLAUSES
two Horn clauses, you get back a Horn clause.
Knowledge bases containing only deﬁnite clauses are interesting for three reasons:
In Horn form, the premise is called the body and the conclusion is called the head. A
BODY
HEAD
FACTPropositional Theorem Proving
CNFSentence
→
Clause1 ∧· · · ∧Clausen
Clause
→
Literal
→
Symbol | ¬Symbol
Symbol
→
P | Q | R | . . .
HornClauseForm
→
DeﬁniteClauseForm | GoalClauseForm
DeﬁniteClauseForm
→
GoalClauseForm
→
A grammar for conjunctive normal form, Horn clauses, and deﬁnite clauses.
A clause such as A ∧B ⇒C is still a deﬁnite clause when it is written as ¬A ∨¬B ∨C,
but only the former is considered the canonical form for deﬁnite clauses. One more class is
chaining algorithms, which we explain next. Both of these algorithms are natural,
CHAINING
in that the inference steps are obvious and easy for humans to follow. This type of
the knowledge base—a pleasant surprise.
Forward and backward chaining
tion symbol q—the query—is entailed by a knowledge base of deﬁnite clauses. It begins
be added. This process continues until the query q is added or until no further inferences can
it runs in linear time.
link must be proved—while multiple links without an arc indicate a disjunction—any link
can be proved. It is easy to see how forward chaining works in the graph. The known leaves
ever a conjunction appears, the propagation waits until all the conjuncts are known before
proceeding. The reader is encouraged to work through the example in detail.Chapter
Logical Agents
inputs: KB, the knowledge base, a set of propositional deﬁnite clauses
q, the query, a proposition symbol
count ←a table, where count[c] is the number of symbols in c’s premise
inferred ←a table, where inferred[s] is initially false for all symbols
agenda ←a queue of symbols, initially symbols known to be true in KB
while agenda is not empty do
p ←POP(agenda)
if p = q then return true
if inferred[p] = false then
inferred[p] ←true
for each clause c in KB where p is in c.PREMISE do
decrement count[c]
return false
track of symbols known to be true but not yet “processed.” The count table keeps track of
how many premises of each implication are as yet unknown. Whenever a new symbol p from
the agenda is processed, the count is reduced by one for each implication in whose premise
p appears (easily identiﬁed in constant time with appropriate indexing.) If a count reaches
zero, all the premises of the implication are known, so its conclusion can be added to the
agenda. Finally, we need to keep track of which symbols have been processed; a symbol that
is already in the set of inferred symbols need not be added to the agenda again. This avoids
redundant work and prevents loops caused by implications such as P ⇒Q and Q ⇒P.
cation of Modus Ponens. Forward chaining is also complete: every entailed atomic sentence
will be derived. The easiest way to see this is to consider the ﬁnal state of the inferred table
(after the algorithm reaches a ﬁxed point where no new inferences are possible). The table
FIXED POINT
contains true for each symbol inferred during the process, and false for all other symbols.
We can view the table as a logical model; moreover, every deﬁnite clause in the original KB is
true in this model. To see this, assume the opposite, namely that some clause a1∧. . .∧ak ⇒b
is false in the model. Then a1 ∧. . . ∧ak must be true in the model and b must be false in
the model. But this contradicts our assumption that the algorithm has reached a ﬁxed point!
We can conclude, therefore, that the set of atomic sentences inferred at the ﬁxed point deﬁnes
a model of the original KB. Furthermore, any atomic sentence q that is entailed by the KB
must be true in all its models and in this model in particular. Hence, every entailed atomic
sentence q must be inferred by the algorithm.
is, reasoning in which the focus of attention starts with the known data. It can be used within
an agent to derive conclusions from incoming percepts, often without a speciﬁc query in
mind. For example, the wumpus agent might TELL its percepts to the knowledge base usingEffective Propositional Model Checking
P ⇒Q
L ∧M ⇒P
B ∧L ⇒M
A ∧P ⇒L
A ∧B ⇒L
A
B
Q
P
M
L
B
A
(a)
(b)
(a) A set of Horn clauses. (b) The corresponding AND–OR graph.
information arrives. For example, if I am indoors and hear rain starting to fall, it might occur
to me that the picnic will be canceled. Yet it will probably not occur to me that the seventeenth
petal on the largest rose in my neighbor’s garden will get wet; humans keep forward chaining
under careful control, lest they be swamped with irrelevant consequences.
query. If the query q is known to be true, then no work is needed. Otherwise, the algorithm
ﬁnds those implications in the knowledge base whose conclusion is q. If all the premises of
one of those implications can be proved true (by backward chaining), then q is true. When
known facts, A and B, that forms the basis for a proof. The algorithm is essentially identical
efﬁcient implementation runs in linear time.
REASONING
speciﬁc questions such as “What shall I do now?” and “Where are my keys?” Often, the cost
of backward chaining is much less than linear in the size of the knowledge base, because the
process touches only relevant facts.
EFFECTIVE PROPOSITIONAL MODEL CHECKING
In this section, we describe two families of efﬁcient algorithms for general propositional
inference based on model checking: One approach based on backtracking search, and one
logic. This section can be skimmed on a ﬁrst reading of the chapter.Chapter
Logical Agents
The algorithms we describe are for checking satisﬁability: the SAT problem. (As noted
earlier, testing entailment, α |= β, can be done by testing unsatisﬁability of α ∧¬β.) We
have already noted the connection between ﬁnding a satisfying model for a logical sentence
and ﬁnding a solution for a constraint satisfaction problem, so it is perhaps not surprising that
their own right because so many combinatorial problems in computer science can be reduced
to checking the satisﬁability of a propositional sentence. Any improvement in satisﬁability
algorithms has huge consequences for our ability to handle complexity in general.
A complete backtracking algorithm
DAVIS–PUTNAM
ALGORITHM
tials of all four authors. DPLL takes as input a sentence in conjunctive normal form—a set
• Early termination: The algorithm detects whether the sentence must be true or false,
even with a partially completed model. A clause is true if any literal is true, even if
the other literals do not yet have truth values; hence, the sentence as a whole could be
judged true even before the model is complete. For example, the sentence (A ∨B) ∧
(A ∨C) is true if A is true, regardless of the values of B and C. Similarly, a sentence
is false if any clause is false, which occurs when each of its literals is false. Again, this
can occur long before the model is complete. Early termination avoids examination of
entire subtrees in the search space.
• Pure symbol heuristic: A pure symbol is a symbol that always appears with the same
PURE SYMBOL
“sign” in all clauses. For example, in the three clauses (A ∨¬B), (¬B ∨¬C), and
(C ∨A), the symbol A is pure because only the positive literal appears, B is pure
because only the negative literal appears, and C is impure. It is easy to see that if
a sentence has a model, then it has a model with the pure symbols assigned so as to
make their literals true, because doing so can never make a clause false. Note that, in
determining the purity of a symbol, the algorithm can ignore clauses that are already
known to be true in the model constructed so far. For example, if the model contains
B = false, then the clause (¬B ∨¬C) is already true, and in the remaining clauses C
appears only as a positive literal; therefore C becomes pure.
eral. In the context of DPLL, it also means clauses in which all literals but one are
already assigned false by the model. For example, if the model contains B = true,
then (¬B ∨¬C) simpliﬁes to ¬C, which is a unit clause. Obviously, for this clause
to be true, C must be set to false. The unit clause heuristic assigns all such symbols
before branching on the remainder. One important consequence of the heuristic is thatEffective Propositional Model Checking
inputs: s, a sentence in propositional logic
clauses ←the set of clauses in the CNF representation of s
symbols ←a list of the proposition symbols in s
return DPLL(clauses,symbols,{ })
function DPLL(clauses,symbols,model) returns true or false
if every clause in clauses is true in model then return true
if some clause in clauses is false in model then return false
P ←FIRST(symbols); rest ←REST(symbols)
The DPLL algorithm for checking satisﬁability of a sentence in propositional
the text; each returns a symbol (or null) and the truth value to assign to that symbol. Like
any attempt to prove (by refutation) a literal that is already in the knowledge base will
create another unit clause—for example, when C is set to false, (C ∨A) becomes a
unit clause, causing true to be assigned to A. This “cascade” of forced assignments
is called unit propagation. It resembles the process of forward chaining with deﬁnite
UNIT PROPAGATION
clauses, and indeed, if the CNF expression contains only deﬁnite clauses then DPLL
search process.
large problems. It is interesting that most of these tricks are in fact rather general, and we
have seen them before in other guises:
ponents, that share no unassigned variables. Given an efﬁcient way to detect when this
occurs, a solver can gain considerable speed by working on each component separately.
mentation of DPLL uses an arbitrary variable ordering and always tries the value true
appears most frequently over all remaining clauses.Chapter
Logical Agents
not be solved in hours of run time with chronological backtracking can be solved in
seconds with intelligent backtracking that backs up all the way to the relevant point of
conﬂict. All SAT solvers that do intelligent backtracking use some form of conﬂict
clause learning to record conﬂicts so that they won’t be repeated later in the search.
to be making progress. In this case, we can start over from the top of the search tree,
rather than trying to continue. After restarting, different random choices (in variable
and value selection) are made. Clauses that are learned in the ﬁrst run are retained after
the restart and can help prune the search space. Restarting does not guarantee that a
solution will be found faster, but it does reduce the variance on the time to solution.
itself, as well as the tricks used in modern solvers, require fast indexing of such things
as “the set of clauses in which variable Xi appears as a positive literal.” This task is
complicated by the fact that the algorithms are interested only in the clauses that have
not yet been satisﬁed by previous assignments to variables, so the indexing structures
must be updated dynamically as the computation proceeds.
ables. They have revolutionized areas such as hardware veriﬁcation and security protocol
Local search algorithms
cause the goal is to ﬁnd an assignment that satisﬁes every clause, an evaluation function that
counts the number of unsatisﬁed clauses will do the job. In fact, this is exactly the measure
in the space of complete assignments, ﬂipping the truth value of one symbol at a time. The
ness are required. In recent years, there has been a great deal of experimentation to ﬁnd a
good balance between greediness and randomness.
One of the simplest and most effective algorithms to emerge from all this work is called
picks a symbol in the clause to ﬂip. It chooses randomly between two ways to pick which
When WALKSAT returns a model, the input sentence is indeed satisﬁable, but when
it returns failure, there are two possible causes: either the sentence is unsatisﬁable or weEffective Propositional Model Checking
function WALKSAT(clauses,p,max ﬂips) returns a satisfying model or failure
inputs: clauses, a set of clauses in propositional logic
max ﬂips, number of ﬂips allowed before giving up
if model satisﬁes clauses then return model
clause ←a randomly selected clause from clauses that is false in model
with probability p ﬂip the value in model of a randomly selected symbol from clause
else ﬂip whichever symbol in clause maximizes the number of satisﬁed clauses
return failure
The WALKSAT algorithm for checking satisﬁability by randomly ﬂipping
the values of variables. Many versions of the algorithm exist.
upon the solution. Alas, if max ﬂips is inﬁnity and the sentence is unsatisﬁable, then the
algorithm never terminates!
WALKSAT cannot always detect unsatisﬁability, which is required for deciding entailment.
For example, an agent cannot reliably use WALKSAT to prove that a square is safe in the
wumpus world. Instead, it can say, “I thought about it for an hour and couldn’t come up with
a possible world in which the square isn’t safe.” This may be a good empirical indicator that
the square is safe, but it’s certainly not a proof.
The landscape of random SAT problems
tracking search algorithms—turned out to be trivially easy for local search methods, such as
easy because it is underconstrained.
UNDERCONSTRAINED
strained problem is one with relatively few clauses constraining the variables. For example,
(¬D ∨¬B ∨C) ∧(B ∨¬A ∨¬C) ∧(¬C ∨¬B ∨E)
∧(E ∨¬D ∨B) ∧(B ∨E ∨¬C) .
take just two random guesses to ﬁnd a model. This is an easy satisﬁability problem, as areChapter
Logical Agents
most such underconstrained problems. On the other hand, an overconstrained problem has
many clauses relative to the number of variables and is likely to have no solutions.
To go beyond these basic intuitions, we must deﬁne exactly how random sentences
symbols, where the clauses are chosen uniformly, independently, and without replacement
from among all clauses with k different literals, which are positive or negative at random. (A
symbol may not appear twice in a clause, nor may a clause appear twice in a sentence.)
Given a source of random sentences, we can measure the probability of satisﬁability.
SATISFIABILITY
THRESHOLD
CONJECTURE
there is a threshold ratio rk such that, as n goes to inﬁnity, the probability that CNFk(n, rn)
The conjecture remains unproven.
P(satisfiable)
Runtime
DPLL
WalkSAT
(a)
(b)
Now that we have a good idea where the satisﬁable and unsatisﬁable problems are, the
next question is, where are the hard problems? It turns out that they are also often at the
problems are easiest to solve (because it is so easy to guess a solution); the overconstrained
problems are not as easy as the underconstrained, but still are much easier than the ones right
at the threshold.Agents Based on Propositional Logic
AGENTS BASED ON PROPOSITIONAL LOGIC
In this section, we bring together what we have learned so far in order to construct wumpus
world agents that use propositional logic. The ﬁrst step is to enable the agent to deduce, to the
extent possible, the state of the world given its percept history. This requires writing down a
complete logical model of the effects of actions. We also show how the agent can keep track of
the world efﬁciently without going back into the percept history for each inference. Finally,
we show how the agent can use logical inference to construct plans that are guaranteed to
achieve its goals.
The current state of the world
As stated at the beginning of the chapter, a logical agent operates by deducing what to do
from a knowledge base of sentences about the world. The knowledge base is composed of
axioms—general knowledge about how the world works—and percept sentences obtained
from the agent’s experience in a particular world. In this section, we focus on the problem of
deducing the current state of the wumpus world—where am I, is that square safe, and so on.
the square is breezy if and only if a neighboring square has a pit; and a square is smelly if and
only if a neighboring square has a wumpus. Thus, we include a large collection of sentences
of the following form:
· · ·
The agent also knows that there is exactly one wumpus. This is expressed in two parts. First,
we have to say that there is at least one wumpus:
Then, we have to say that there is at most one wumpus. For each pair of locations, we add a
· · ·
So far, so good. Now let’s consider the agent’s percepts. If there is currently a stench, one
might suppose that a proposition Stench should be added to the knowledge base. This is not
ready be asserted, and the new assertion would simply result in a contradiction. The problem
is solved when we realize that a percept asserts something only about the current time. Thus,Chapter
Logical Agents
Stench4 to the knowledge base, rather than Stench—neatly avoiding any contradiction with
¬Stench3. The same goes for the breeze, bump, glitter, and scream percepts.
The idea of associating propositions with time steps extends to any aspect of the world
that changes over time. For example, the initial knowledge base includes L0
the word ﬂuent (from the Latin ﬂuens, ﬂowing) to refer an aspect of the world that changes.
FLUENT
“Fluent” is a synonym for “state variable,” in the sense described in the discussion of factored
the world do not need a time superscript and are sometimes called atemporal variables.
ATEMPORAL
VARIABLE
We can connect stench and breeze percepts directly to the properties of the squares
any square [x, y], we assert
Lt
x,y ⇒(Breezet ⇔Bx,y)
Lt
x,y ⇒(Stencht ⇔Sx,y) .
Now, of course, we need axioms that allow the agent to keep track of ﬂuents such as Lt
x,y.
These ﬂuents change as the result of actions taken by the agent, so, in the terminology of
logical sentences.
First, we need proposition symbols for the occurrences of actions. As with percepts,
the action for that time step, followed by a transition to the next time step.
To describe how the world changes, we can try writing effect axioms that specify the
EFFECT AXIOM
L0
and each of the four orientations. We would also need similar sentences for the other actions:
Grab, Shoot, Climb, TurnLeft, and TurnRight.
is, ASK(KB, L1
has the arrow; nor can it prove it doesn’t have it! The information has been lost because the
effect axiom fails to state what remains unchanged as the result of an action. The need to do
FRAME PROBLEM
with respect to which motion is measured. It also has an analogy to the frames of a movie, in which normally
most of the background stays constant while changes occur in the foreground.Agents Based on Propositional Logic
be to add frame axioms explicitly asserting all the propositions that remain the same. For
FRAME AXIOM
example, for each time t we would have
· · ·
where we explicitly mention every proposition that stays unchanged from time t to time
after moving forward and that the wumpus hasn’t died or come back to life, the proliferation
of frame axioms seems remarkably inefﬁcient. In a world with m different actions and n
ﬂuents, the set of frame axioms will be of size O(mn). This speciﬁc manifestation of the
frame problem is sometimes called the representational frame problem. Historically, the
REPRESENTATIONAL
FRAME PROBLEM
problem was a signiﬁcant one for AI researchers; we explore it further in the notes at the end
of the chapter.
The representational frame problem is signiﬁcant because the real world has very many
ﬂuents, to put it mildly. Fortunately for us humans, each action typically changes no more
LOCALITY
sentational frame problem requires deﬁning the transition model with a set of axioms of size
O(mk) rather than size O(mn). There is also an inferential frame problem: the problem
INFERENTIAL FRAME
PROBLEM
of projecting forward the results of a t step plan of action in time O(kt) rather than O(nt).
The solution to the problem involves changing one’s focus from writing axioms about
actions to writing axioms about ﬂuents. Thus, for each ﬂuent F, we will have an axiom that
axiom and has this schema:
AXIOM
action for reloading, the ActionCausesF t part goes away and we are left with
when facing west; or (b) Lt
because the action was not Forward or because the action bumped into a wall). Written out
in propositional logic, this becomes
⇔
(Lt
∨(Lt
∨(LtChapter
Logical Agents
ning of this section, the agent will be able to ASK and answer any answerable question about
actions is
At this point, we have ASK(KB, L6
one of the pits. The most important question for the agent is whether a square is OK to move
into, that is, the square contains no pit nor live wumpus. It’s convenient to add axioms for
this, having the form
OK t
x,y ⇔¬Px,y ∧¬(Wx,y ∧WumpusAlivet) .
sound and complete inference algorithm such as DPLL, the agent can answer any answerable
medium wumpus worlds.
Solving the representational and inferential frame problems is a big step forward, but
a pernicious problem remains: we need to conﬁrm that all the necessary preconditions of an
action hold for it to have its intended effect. We said that the Forward action moves the agent
ahead unless there is a wall in the way, but there are many other unusual exceptions that could
cause the action to fail: the agent might trip and fall, be stricken with a heart attack, be carried
away by giant bats, etc. Specifying all these exceptions is called the qualiﬁcation problem.
QUALIFICATION
PROBLEM
There is no complete solution within logic; system designers have to use good judgment in
deciding how detailed they want to be in specifying their model, and what details they want
exceptions without explicitly naming them.
A hybrid agent
HYBRID AGENT
to do this. The agent program maintains and updates a knowledge base as well as a current
plan. The initial knowledge base contains the atemporal axioms—those that don’t depend
on t, such as the axiom relating the breeziness of squares to the presence of pits. At each
time step, the new percept sentence is added along with all the axioms that depend on t, suchAgents Based on Propositional Logic
for future time steps.) Then, the agent uses logical inference, by ASKing questions of the
knowledge base, to work out which squares are safe and which have yet to be visited.
The main body of the agent program constructs a plan based on a decreasing priority of
goals. First, if there is a glitter, the program constructs a plan to grab the gold, follow a route
back to the initial location, and climb out of the cave. Otherwise, if there is no current plan,
the program plans a route to the closest safe square that it has not visited yet, making sure
the route goes through only safe squares. Route planning is done with A∗search, not with
ASK. If there are no safe squares to explore, the next step—if the agent still has an arrow—is
to try to make a safe square by shooting at one of the possible wumpus locations. These are
determined by asking where ASK(KB, ¬Wx,y) is false—that is, where it is not known that
sequence of actions that will line up this shot. If this fails, the program looks for a square to
explore that is not provably unsafe—that is, a square for which ASK(KB, ¬OK t
x,y) returns
and climbs out of the cave.
Logical state estimation
goes by, the computational expense involved in the calls to ASK goes up and up. This happens
mainly because the required inferences have to go back further and further in time and involve
more and more proposition symbols. Obviously, this is unsustainable—we cannot have an
agent whose time to process each percept grows in proportion to the length of its life! What
we really need is a constant update time—that is, independent of t. The obvious answer is to
save, or cache, the results of inference, so that the inference process at the next time step can
CACHING
build on the results of earlier steps instead of having to start again from scratch.
be replaced by the belief state—that is, some representation of the set of all possible current
we can use a logical sentence involving the proposition symbols associated with the current
time step, as well as the atemporal symbols. For example, the logical sentence
WumpusAlive1 ∧L1
Maintaining an exact belief state as a logical formula turns out not to be easy. If there
are n ﬂuent symbols for time t, then there are 2n possible states—that is, assignments of truth
values to those symbols. Now, the set of belief states is the powerset (set of all subsets) of the
set of physical states. There are 2n physical states, hence 22n belief states. Even if we used
the most compact possible encoding of logical formulas, with each belief state represented
increasingly expensive as the history gets longer.Chapter
Logical Agents
inputs: percept, a list, [stench,breeze,glitter,bump,scream]
persistent: KB, a knowledge base, initially the atemporal “wumpus physics”
plan, an action sequence, initially empty
TELL the KB the temporal “physics” sentences for time t
safe ←{[x, y] : ASK(KB,OK t
x,y) = true}
if ASK(KB,Glitter t) = true then
if plan is empty then
unvisited ←{[x, y] : ASK(KB, Lt′
x,y) = false for all t′ ≤t}
if plan is empty and ASK(KB,HaveArrow t) = true then
possible wumpus ←{[x, y] : ASK(KB, ¬ Wx,y) = false}
if plan is empty then // no choice but to take a risk
not unsafe ←{[x, y] : ASK(KB, ¬ OK t
x,y) = false}
if plan is empty then
action ←POP(plan)
return action
inputs: current, the agent’s current position
goals, a set of squares; try to plan a route to one of them
allowed, a set of squares that can form part of the route
by a unique binary number, we would need numbers with log2(22n) = 2n bits to label the
current belief state. That is, exact state estimation may require logical formulas whose size is
exponential in the number of symbols.
One very common and natural scheme for approximate state estimation is to represent
simply tries to prove Xt and ¬Xt for each symbol Xt (as well as each atemporal symbolAgents Based on Propositional Logic
conservative approximation to the exact (wiggly) belief state (shaded region with dashed
outline). Each possible world is shown as a circle; the shaded ones are consistent with all the
percepts.
provable literals becomes the new belief state, and the previous belief state is discarded.
It is important to understand that this scheme may lose some information as time goes
CNF belief state acts as a simple outer envelope, or conservative approximation, around the
CONSERVATIVE
APPROXIMATION
exact belief state. We see this idea of conservative approximations to complicated sets as a
recurring theme in many areas of AI.
Making plans by propositional inference
A∗search to make plans. In this section, we show how to make plans by logical inference.
The basic idea is very simple:
(a) Init0, a collection of assertions about the initial state;
at each time up to some maximum time t;
(c) the assertion that the goal is achieved at time t: HaveGoldt ∧ClimbedOutt.Chapter
Logical Agents
the goal is achievable; if the sentence is unsatisﬁable, then the planning problem is
impossible.
tions and are assigned true. Together they represent a plan to achieve the goals.
basic idea just given, with one twist. Because the agent does not know how many steps it
will take to reach the goal, the algorithm tries each possible number of steps t, up to some
maximum conceivable plan length Tmax. In this way, it is guaranteed to ﬁnd the shortest plan
if one exists. Because of the way SATPLAN searches for a solution, this approach cannot
be used in a partially observable environment; SATPLAN would just set the unobservable
variables to the values it needs to create a solution.
function SATPLAN(init, transition, goal,T max) returns solution or failure
inputs: init, transition, goal, constitute a description of the problem
T max, an upper limit for plan length
if model is not null then
return failure
The SATPLAN algorithm. The planning problem is translated into a CNF
sentence in which the goal is asserted to hold at a ﬁxed time step t and axioms are included
for each time step up to t. If the satisﬁability algorithm ﬁnds a model, then a plan is extracted
by looking at those proposition symbols that refer to actions and are assigned true in the
model. If no model exists, then the process is repeated with the goal moved one step later.
The key step in using SATPLAN is the construction of the knowledge base. It might
entailment (as tested by ASK) and those for satisﬁability. Consider, for example, the agent’s
The initial knowledge base contains L0
if Forward0 is asserted, and, reassuringly, we cannot prove L1
instead. Now, SATPLAN will ﬁnd the plan [Forward0]; so far, so good. Unfortunately,
SATPLAN also ﬁnds the plan [Shoot0]. How could this be? To ﬁnd out, we inspect the model
that SATPLAN constructs: it includes the assignment L0
For entailment, L0Agents Based on Propositional Logic
on the other hand, L0
make the goal true. For this reason, SATPLAN is a good debugging tool for knowledge bases
because it reveals places where knowledge is missing. In this particular case, we can ﬁx the
knowledge base by asserting that, at each time step, the agent is in exactly one location, using
a collection of sentences similar to those used to assert the existence of exactly one wumpus.
Alternatively, we can assert ¬L0
for location takes care of subsequent time steps. The same ﬁxes also work to make sure the
agent has only one orientation.
SATPLAN has more surprises in store, however. The ﬁrst is that it ﬁnds models with
impossible actions, such as shooting with no arrow. To understand why, we need to look more
preconditions are not satisﬁed. The axioms do predict correctly that nothing will happen when
executed! To avoid generating plans with illegal actions, we must add precondition axioms
PRECONDITION
AXIOMS
need to say, for each time t, that
Shoot t ⇒HaveArrow t .
This ensures that if a plan selects the Shoot action at any time, it must be the case that the
agent has an arrow at that time.
SATPLAN’s second surprise is the creation of plans with multiple simultaneous actions.
which is not allowed. To eliminate this problem, we introduce action exclusion axioms: for
ACTION EXCLUSION
AXIOM
every pair of actions At
i and At
j we add the axiom
¬At
i ∨¬At
j .
It might be pointed out that walking forward and shooting at the same time is not so hard to
do, whereas, say, shooting and grabbing at the same time is rather impractical. By imposing
action exclusion axioms only on pairs of actions that really do interfere with each other, we
can allow for plans that include multiple simultaneous actions—and because SATPLAN ﬁnds
the shortest legal plan, we can be sure that it will take advantage of this capability.
To summarize, SATPLAN ﬁnds models for a sentence containing the initial state, the
It can be shown that this collection of axioms is sufﬁcient, in the sense that there are no
longer any spurious “solutions.” Any model satisfying the propositional sentence will be a
This section has described a declarative approach to agent construction: the agent works
ence. This approach has some weaknesses hidden in phrases such as “for each time t” andChapter
Logical Agents
“for each square [x, y].” For any practical agent, these phrases have to be implemented by
code that generates instances of the general sentence schema automatically for insertion into
the knowledge base. For a wumpus world of reasonable size—one comparable to a smallish
edge bases with tens or hundreds of millions of sentences. Not only does this become rather
pus world—namely, that the “physics” works the same way across all squares and all time
steps—that we cannot express directly in the language of propositional logic. To solve this
problem, we need a more expressive language, one in which phrases like “for each time t”
can be described in about ten sentences rather than ten million or ten trillion.
SUMMARY
which such agents can reason about the world. The main points are as follows:
• Intelligent agents need knowledge about the world in order to reach good decisions.
tation language that are stored in a knowledge base.
nism. It operates by storing sentences about the world in its knowledge base, using the
inference mechanism to infer new sentences, and using these sentences to decide what
action to take.
• A representation language is deﬁned by its syntax, which speciﬁes the structure of
sentences, and its semantics, which deﬁnes the truth of each sentence in each possible
world or model.
• The relationship of entailment between sentences is crucial to our understanding of
reasoning. A sentence α entails another sentence β if β is true in all worlds where
α is true. Equivalent deﬁnitions include the validity of the sentence α ⇒β and the
unsatisﬁability of the sentence α ∧¬β.
rithms derive only sentences that are entailed; complete algorithms derive all sentences
that are entailed.
• Propositional logic is a simple language consisting of proposition symbols and logical
connectives. It can handle propositions that are known true, known false, or completely
unknown.
algorithms for propositional logic include backtracking and local search methods and
can often solve large problems quickly.Bibliographical and Historical Notes
• Inference rules are patterns of sound inference that can be used to ﬁnd proofs. The
resolution rule yields a complete inference algorithm for knowledge bases that are
expressed in conjunctive normal form. Forward chaining and backward chaining
are very natural reasoning algorithms for knowledge bases in Horn form.
rithms are sound but not complete.
• Logical state estimation involves maintaining a logical sentence that describes the set
of possible states consistent with the observation history. Each update step requires
state axioms that specify how each ﬂuent changes.
• Decisions within a logical agent can be made by SAT solving: ﬁnding possible models
specifying future action sequences that reach the goal. This approach works only for
fully observable or sensorless environments.
• Propositional logic does not scale to environments of unbounded size because it lacks
tionships among objects.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
gated the notion of agents that use logical reasoning to mediate between percepts and actions.
It also raised the ﬂag of declarativism, pointing out that telling an agent what it needs to know
makes the case that rational agents can be described and analyzed at an abstract level deﬁned
ical principles—principles connecting the syntactic structure of sentences with their truth
and falsity, with their meaning, or with the validity of arguments in which they ﬁgure—are
scattered in the works of Plato. The ﬁrst known systematic study of logic was carried out
treatise called the Organon. Aristotle’s syllogisms were what we would now call inference
SYLLOGISM
the system as a whole lacked the compositional properties required to handle sentences of
arbitrary complexity.
The closely related Megarian and Stoic schools (originating in the ﬁfth century B.C.
and continuing for several centuries thereafter) began the systematic study of the basic logical
connectives. The use of truth tables for deﬁning connectives is due to Philo of Megara. TheChapter
Logical Agents
Stoics took ﬁve basic inference rules as valid without proof, including the rule we now call
Modus Ponens. They derived a number of other rules from these ﬁve, using, among other
proof than was Aristotle. A good account of the history of Megarian and Stoic logic is given
system of formal logic in his book The Mathematical Analysis of Logic. Boole’s logic was
closely modeled on the ordinary algebra of real numbers and used substitution of logically
equivalent expressions as its primary inference method. Although Boole’s system still fell
short of full propositional logic, it was close enough that other mathematicians could quickly
(“Concept Writing” or “Conceptual Notation”).
The ﬁrst mechanical device to carry out logical inferences was constructed by the third
certain inferences in the theory of probability. William Stanley Jevons, one of those who
form inferences in Boolean logic. An entertaining and instructive history of these and other
lished computer program for logical inference was the Logic Theorist of Newell, Shaw,
Logic Theorist’s results were published slightly earlier.
Truth tables as a method of testing validity or unsatisﬁability in propositional logic were
of efﬁcient propositional algorithms in the 1960s was motivated largely by the interest of
but was in most cases much less efﬁcient than the DPLL backtracking algorithm introduced
without resort to propositional techniques.
which the satisﬁability problem is polynomially solvable; Horn clauses are one such subset.Bibliographical and Historical Notes
nals in a circuit.
plexity for certain natural distributions of problems. This potentially exciting fact became
described in the chapter produces much harder problems. Motivated by the empirical success
On the practical side, efﬁciency gains in propositional solvers have been marked. Given
ables. Two crucial contributions were the watched literal indexing technique of Zhang and
ularly; most of the winning entries have either been descendants of CHAFF or have used the
implementation available at http://minisat.se that is designed to be easily modiﬁed
Local search algorithms for satisﬁability were tried by various authors throughout the
1980s; all of the algorithms were based on the idea of minimizing the number of unsatisﬁed
it was capable of solving a wide range of very hard problems very quickly. The WALKSAT
research—due, in part, to the obvious connection to phase transition phenomena in statisticalChapter
Logical Agents
summary of the early literature on the problem.
The satisﬁability threshold conjecture states that, for each k, there is a sharp satisﬁability
SATISFIABILITY
THRESHOLD
CONJECTURE
bility threshold, but instead to a phase transition in the solution distribution and structure of
SURVEY
PROPAGATION
advantage of special properties of random SAT instances near the satisﬁability threshold and
greatly outperform general SAT solvers on such instances.
The best sources for information on satisﬁability, both theoretical and practical, are the
Theory and Applications of Satisﬁability Testing, known as SAT.
The idea of building agents with propositional logic can be traced back to the seminal
trary to popular supposition, the paper was concerned with the implementation of a Boolean
computers, have received little attention in AI, however. The most notable exception is the
ronment. (Rosenschein’s approach is described at some length in the second edition of this
are cumbersome, expensive, and unnecessary. In our view, neither approach is sufﬁcient by
wumpus agent has been used to control NASA spacecraft, planning sequences of actions and
diagnosing and recovering from faults.
PROJECTION
mining what propositions hold true after an action sequence is executed, can be seen as a
special case of state estimation with empty percepts. Many authors have studied this problem
because of its importance in planning; some important hardness results were established byChapter
Logical Agents
each of the following sentences is true:
Hence show that KB |= α2 and KB |= α3.
that the unicorn is mythical? How about magical? Horned?
If the unicorn is mythical, then it is immortal, but if it is not mythical, then it is a
mortal mammal. If the unicorn is either immortal or a mammal, then it is horned.
The unicorn is magical if it is horned.
Consider the problem of deciding whether a propositional logic sentence is true in a
given model.
tence s is true in the model m (where m assigns a truth value for every symbol in s).
The algorithm should run in time linear in the size of the sentence. (Alternatively, use a
version of this function from the online code repository.)
b. Give three examples of sentences that can be determined to be true or false in a partial
model that does not specify a truth value for some of the symbols.
c. Show that the truth value (if any) of a sentence in a partial model cannot be determined
efﬁciently in general.
models, while retaining its recursive structure and linear run time. Give three examples
of sentences whose truth in a partial model is not detected by your algorithm.
Which of the following are correct?
a. False |= True.
b. True |= False.
c. (A ∧B) |= (A ⇔B).
d. A ⇔B |= A ∨B.
e. A ⇔B |= ¬A ∨B.
f. (A ∧B) ⇒C |= (A ⇒C) ∨(B ⇒C).
g. (C ∨(¬A ∧¬B)) ≡((A ⇒C) ∧(B ⇒C)).
h. (A ∨B) ∧(¬C ∨¬D ∨E) |= (A ∨B).
i. (A ∨B) ∧(¬C ∨¬D ∨E) |= (A ∨B) ∧(¬D ∨E).
j. (A ∨B) ∧¬(A ⇒B) is satisﬁable.
k. (A ⇔B) ∧(¬A ∨B) is satisﬁable.
l. (A ⇔B) ⇔C has the same number of models as (A ⇔B) for any ﬁxed set of
proposition symbols that includes A, B, C.Chapter
Logical Agents
a. Show that the clause (¬P1 ∨· · · ∨¬Pm ∨Q) is logically equivalent to the implication
sentence (P1 ∧· · · ∧Pm) ⇒Q.
b. Show that every clause (regardless of the number of positive literals) can be written in
the form (P1 ∧· · · ∧Pm) ⇒(Q1 ∨· · · ∨Qn), where the Ps and Qs are proposition
symbols. A knowledge base consisting of such sentences is in implicative normal
IMPLICATIVE
NORMAL FORM
c. Write down the full resolution rule for sentences in implicative normal form.
According to some political pundits, a person who is radical (R) is electable (E) if
a. Which of the following are correct representations of this assertion?
(i) (R ∧E) ⇐⇒C
(ii) R ⇒(E ⇐⇒C)
(iii) R ⇒((C ⇒E) ∨¬E)
b. Which of the sentences in (a) can be expressed in Horn form?
This question considers representing satisﬁability (SAT) problems as CSPs.
a. Draw the constraint graph corresponding to the SAT problem
b. How many solutions are there for this general SAT problem as a function of n?
CSP of the type given in (a). (To ﬁnd all solutions to a CSP, we simply modify the
basic algorithm so it continues searching after each solution is found.) Assume that
variables are ordered X1, . . . , Xn and false is ordered before true. How much time
will the algorithm take to terminate? (Write an O(·) expression as a function of n.)
d. We know that SAT problems in Horn form can be solved in linear time by forward
exactly three distinct variables. What is the smallest set of such clauses that is unsatisﬁable?
Construct such a set.
(A ∨B) ∧(¬A ∨C) ∧(¬B ∨D) ∧(¬C ∨G) ∧(¬D ∨G) .
a. Prove using resolution that the above sentence entails G.Chapter
Logical Agents
clauses? Explain.
A minesweeper world is a rectangular grid of N squares with M invisible mines scattered
among them. Any square may be probed by the agent; instant death follows if a mine is
probed. Minesweeper indicates the presence of mines by revealing, in each probed square,
the number of mines that are directly or diagonally adjacent. The goal is to probe every
unmined square.
a. Let Xi,j be true iff square [i, j] contains a mine. Write down the assertion that exactly
Xi,j propositions.
b. Generalize your assertion from (a) by explaining how to construct a CNF sentence
asserting that k of n neighbors contain mines.
c. Explain precisely how an agent can use DPLL to prove that a given square does (or
does not) contain a mine, ignoring the global constraint that there are exactly M mines
in all.
d. Suppose that the global constraint is constructed from your method from part (b). How
does the number of clauses depend on M and N? Suggest a way to modify DPLL so
that the global constraint does not need to be represented explicitly.
e. Are any conclusions derived by the method in part (c) invalidated when the global
constraint is taken into account?
such that the contents of a given unprobed square would give information about the
How long does it take to prove KB |= α using DPLL when α is a literal already
contained in KB? Explain.
suming the only actions available are Lock and Unlock.
world. Write down axioms for all remaining ﬂuent symbols.
gest a method for overcoming this problem by deﬁning additional proposition symbols, and
try it out in the wumpus world. Does it improve the performance of the agent?In which we notice that the world is blessed with many objects, some of which are
related to other objects, and in which we endeavor to reason about them.
operates and deduce what actions to take. We used propositional logic as our representation
agents. Unfortunately, propositional logic is too puny a language to represent knowledge
which is sufﬁciently expressive to represent a good deal of our commonsense knowledge.
It also either subsumes or forms the foundation of many other representation languages and
REPRESENTATION REVISITED
In this section, we discuss the nature of representation languages. Our discussion motivates
turies of thought, trial, and error into a few paragraphs.
formal languages in common use. Programs themselves represent, in a direct sense, only
computational processes. Data structures within programs can represent facts; for example,Chapter
retrieve facts.) What programming languages lack is any general mechanism for deriving
whose details are derived by the programmer from his or her own knowledge of the domain.
This procedural approach can be contrasted with the declarative nature of propositional logic,
in which knowledge and inference are separate, and inference is entirely domain independent.
A second drawback of data structures in programs (and of databases, for that matter)
and some systems allow the value to be “unknown,” but they lack the expressiveness required
to handle partial information.
Propositional logic is a declarative language because its semantics is based on a truth
relation between sentences and possible worlds. It also has sufﬁcient expressive power to
deal with partial information, using disjunction and negation. Propositional logic has a third
COMPOSITIONALITY
positional language, the meaning of a sentence is a function of the meaning of its parts. For
in last week’s ice hockey qualifying match. Clearly, noncompositionality makes life much
more difﬁcult for the reasoning system.
concisely describe an environment with many objects. For example, we were forced to write
a separate rule about breezes and pits for each square, such as
In English, on the other hand, it seems easy enough to say, once and for all, “Squares adjacent
to pits are breezy.” The syntax and semantics of English somehow make it possible to describe
the environment concisely.
The language of thought
Natural languages (such as English or Spanish) are very expressive indeed. We managed to
tion in linguistics and the philosophy of language that views natural language as a declarative
knowledge representation language. If we could uncover the rules for natural language, we
could use it in representation and reasoning systems and gain the beneﬁt of the billions of
pages that have been written in natural language.
The modern view of natural language is that it serves a as a medium for communication
rather than pure representation. When a speaker points and says, “Look!” the listener comes
to know that, say, Superman has ﬁnally appeared over the rooftops. Yet we would not want
to say that the sentence “Look!” represents that fact. Rather, the meaning of the sentence
depends both on the sentence itself and on the context in which the sentence was spoken.
Clearly, one could not store a sentence such as “Look!” in a knowledge base and expect toRepresentation Revisited
recover its meaning without also storing a representation of the context—which raises the
question of how the context itself can be represented. Natural languages also suffer from
AMBIGUITY
think about spring, surely they are not confused as to whether they are thinking about a season
or something that goes boing—and if one word can correspond to two thoughts, thoughts
can’t be words.”
The famous Sapir–Whorf hypothesis claims that our understanding of the world is
nize it into concepts, and ascribe signiﬁcances as we do, largely because we are parties to an
nity and is codiﬁed in the patterns of our language.” It is certainly true that different speech
teuil,” for a concept that English speakers cover with one: “chair.” But English speakers
does language really make a difference? Whorf relied mainly on intuition and speculation,
but in the intervening years we actually have real data from anthropological, psychological
and neurological studies.
For example, can you remember which of the following two phrases formed the opening
“In this section, we discuss the nature of representation languages . . .”
“This section covers the topic of knowledge representation languages . . .”
nonverbal representation.
More interesting is the case in which a concept is completely absent in a language.
Speakers of the Australian aboriginal language Guugu Yimithirr have no words for relative
directions, such as front, back, right, or left. Instead they use absolute directions, saying,
for example, the equivalent of “I have a pain in my north arm.” This difference in language
makes a difference in behavior: Guugu Yimithirr speakers are better at navigating in open
terrain, while English speakers are better at placing the fork to the right of the plate.
Language also seems to inﬂuence thought through seemingly arbitrary grammatical
features such as the gender of nouns. For example, “bridge” is masculine in Spanish and
scribe a photograph of a particular bridge. Spanish speakers chose big, dangerous, strong,
and towering, whereas German speakers chose beautiful, elegant, fragile, and slender. Words
showed experimental subjects a movie of an auto accident. Subjects who were asked “How
while subjects who were asked the question with the word “smashed” instead of “contacted”
reported 41mph for the same cars in the same movie.Chapter
“¬(A ∨B)” and “¬A ∧¬B” are the same because we can look inside the system and see
that the two sentences are stored as the same canonical CNF form. Can we do that with the
human brain? Until recently the answer was “no,” but now it is “maybe.” Mitchell et al.
them words such as “celery,” and imaged their brains. The researchers were then able to train
a computer program to predict, from a brain image, what word the subject had been presented
an fMRI image of before (by considering the images of related words) and for people it has
never seen before (proving that fMRI reveals some level of common representation across
people). This type of work is still in its infancy, but fMRI (and other imaging technology
concrete ideas of what human knowledge representations are like.
From the viewpoint of formal logic, representing the same knowledge in two different
sion, meaning that a reasoner with limited resources could get to the conclusion using one
representation but not the other. For nondeductive tasks such as learning from experience,
outcomes are necessarily dependent on the form of the representations used. We show in
of which are consistent with all the data, the most common way of breaking the tie is to choose
the most succinct theory—and that depends on the language used to represent theories. Thus,
the inﬂuence of language on thought is unavoidable for any agent that does learning.
Combining the best of formal and natural languages
We can adopt the foundation of propositional logic—a declarative, compositional semantics
backs. When we look at the syntax of natural language, the most obvious elements are nouns
and noun phrases that refer to objects (squares, pits, wumpuses) and verbs and verb phrases
OBJECT
RELATION
tions are functions—relations in which there is only one “value” for a given “input.” It is
FUNCTION
easy to start listing examples of objects, relations, and functions:
• Objects: people, houses, numbers, theories, Ronald McDonald, colors, baseball games,
wars, centuries . . .
• Relations: these can be unary relations or properties such as red, round, bogus, prime,
PROPERTY
part of, has color, occurred after, owns, comes between, . . .
• Functions: father of, best friend, third inning of, one more than, beginning of . . .
tions. Some examples follow:Representation Revisited
• “One plus two equals three.”
Objects: one, two, three, one plus two; Relation: equals; Function: plus. (“One plus
two” is a name for the object that is obtained by applying the function “plus” to the
objects “one” and “two.” “Three” is another name for this object.)
• “Squares neighboring the wumpus are smelly.”
Objects: wumpus, squares; Property: smelly; Relation: neighboring.
is built around objects and relations. It has been so important to mathematics, philosophy, and
artiﬁcial intelligence precisely because those ﬁelds—and indeed, much of everyday human
existence—can be usefully thought of as dealing with objects and the relations among them.
enables one to represent general laws or rules, such as the statement “Squares neighboring
the wumpus are smelly.”
cal commitment made by each language—that is, what it assumes about the nature of reality.
ONTOLOGICAL
COMMITMENT
Mathematically, this commitment is expressed through the nature of the formal models with
respect to which the truth of sentences is deﬁned. For example, propositional logic assumes
that there are facts that either hold or do not hold in the world. Each fact can be in one
objects with certain relations among them that do or do not hold. The formal models are
make still further ontological commitments; for example, temporal logic assumes that facts
TEMPORAL LOGIC
hold at particular times and that those times (which may be points or intervals) are ordered.
class” status within the logic, rather than simply deﬁning them within the knowledge base.
LOGIC
jects in themselves. This allows one to make assertions about all relations—for example, one
logic sentences.
A logic can also be characterized by its epistemological commitments—the possible
EPISTEMOLOGICAL
COMMITMENT
order logic, a sentence represents a fact and the agent either believes the sentence to be true,
believes it to be false, or has no opinion. These logics therefore have three possible states
of knowledge regarding any sentence. Systems using probability theory, on the other hand,Chapter
Language
Ontological Commitment
Epistemological Commitment
(What exists in the world)
(What an agent believes about facts)
Propositional logic
facts
facts, objects, relations
Temporal logic
facts, objects, relations, times
Probability theory
facts
Fuzzy logic
known interval value
Formal languages and their ontological and epistemological commitments.
physics requires some familiarity with mathematics, a student of AI must develop a talent for
working with logical notation. On the other hand, it is also important not to get too concerned
with the speciﬁcs of logical notation—after all, there are dozens of different versions. The
main things to keep hold of are how the language facilitates concise representations and how
its semantics leads to sound reasoning procedures.
We begin this section by specifying more precisely the way in which the possible worlds
introduce the various elements of the language, explaining their semantics as we go along.
constitute the possible worlds under consideration. Each model links the vocabulary of the
logical sentences to elements of the possible world, so that the truth of any sentence can
be determined. Thus, models for propositional logic link proposition symbols to predeﬁned
DOMAIN
DOMAIN ELEMENTS
main is required to be nonempty—every possible world must contain at least one object. (See
what these objects are—all that matters is how many there are in each particular model—but
Indeed, some fuzzy systems allow uncertainty (degree of belief) about degrees of truth.The objects in the model may be related in various ways. In the ﬁgure, Richard and
John are brothers. Formally speaking, a relation is just the set of tuples of objects that are
TUPLE
related. (A tuple is a collection of objects arranged in a ﬁxed order and is written with angle
brackets surrounding the objects.) Thus, the brotherhood relation in this model is the set
{ ⟨Richard the Lionheart, King John⟩, ⟨King John, Richard the Lionheart⟩} .
(Here we have named the objects in English, but you may, if you wish, mentally substitute the
pictures for the names.) The crown is on King John’s head, so the “on head” relation contains
just one tuple, ⟨the crown, King John⟩. The “brother” and “on head” relations are binary
relations—that is, they relate pairs of objects. The model also contains unary relations, or
properties: the “person” property is true of both Richard and John; the “king” property is true
only of John (presumably because Richard is dead at this point); and the “crown” property is
true only of the crown.
Certain kinds of relationships are best considered as functions, in that a given object
must be related to exactly one object in this way. For example, each person has one left leg,
so the model has a unary “left leg” function that includes the following mappings:
⟨Richard the Lionheart⟩→Richard’s left leg
⟨King John⟩→John’s left leg .
TOTAL FUNCTIONS
value for every input tuple. Thus, the crown must have a left leg and so must each of the left
legs. There is a technical solution to this awkward problem involving an additional “invisible”
R
J
$
left leg
on head
brother
brother
person
person
king
crown
left leg
A model containing ﬁve objects, two binary relations, three unary relationsChapter
object that is the left leg of everything that has no left leg, including itself. Fortunately, as
long as one makes no assertions about the left legs of things that have no left legs, these
technicalities are of no import.
other essential part of a model is the link between those elements and the vocabulary of the
logical sentences, which we explain next.
Symbols and interpretations
relations, and functions. The symbols, therefore, come in three kinds: constant symbols,
CONSTANT SYMBOL
PREDICATE SYMBOL
bols, which stand for functions. We adopt the convention that these symbols will begin with
FUNCTION SYMBOL
uppercase letters. For example, we might use the constant symbols Richard and John; the
predicate symbols Brother, OnHead, Person, King, and Crown; and the function symbol
LeftLeg. As with proposition symbols, the choice of names is entirely up to the user. Each
predicate and function symbol comes with an arity that ﬁxes the number of arguments.
ARITY
mine if any given sentence is true or false. Thus, in addition to its objects, relations, and
INTERPRETATION
tions and functions are referred to by the constant, predicate, and function symbols. One
tation—is as follows:
INTENDED
INTERPRETATION
• Richard refers to Richard the Lionheart and John refers to the evil King John.
• Brother refers to the brotherhood relation, that is, the set of tuples of objects given in
and King John; Person, King, and Crown refer to the sets of objects that are persons,
kings, and crowns.
There are many other possible interpretations, of course. For example, one interpretation
maps Richard to the crown and John to King John’s left leg. There are ﬁve objects in
and John. Notice that not all the objects need have a name—for example, the intended
interpretation does not name the crown or the legs. It is also possible for an object to have
several names; there is an interpretation under which both Richard and John refer to the
perfectly possible to have a model in which Cloudy and Sunny are both true; it is the job of
the knowledge base to rule out models that are inconsistent with our knowledge.Sentence
→
AtomicSentence | ComplexSentence
AtomicSentence
→
Predicate | Predicate(Term, . . .) | Term = Term
ComplexSentence
→
( Sentence ) | [ Sentence ]
|
¬ Sentence
|
Sentence ∧Sentence
|
Sentence ∨Sentence
|
Sentence
⇒Sentence
|
Sentence
⇔
Sentence
|
Quantiﬁer Variable, . . . Sentence
Term
→
Function(Term, . . .)
|
Constant
|
Variable
Quantiﬁer
→
∀| ∃
Constant
→
A | X1 | John | · · ·
Variable
→
a | x | s | · · ·
Predicate
→
True | False | After | Loves | Raining | · · ·
Function
→
Mother | LeftLeg | · · ·
OPERATOR PRECEDENCE
:
¬, =, ∧, ∨, ⇒, ⇔
from highest to lowest. The precedence of quantiﬁers is such that a quantiﬁer holds over
everything to the right of it.
R
J
R
J
R
J
R
J
R
J
R
J
. . .
. . .
. . .
bols, R and J, and one binary relation symbol. The interpretation of each constant symbol is
shown by a gray arrow. Within each model, the related objects are connected by arrows.Chapter
that maps constant symbols to objects, predicate symbols to relations on those objects, and
function symbols to functions on those objects. Just as with propositional logic, entailment,
validity, and so on are deﬁned in terms of all possible models. To get an idea of what the
objects they contain—from one up to inﬁnity—and in the way the constant symbols map
to objects. If there are two constant symbols and one object, then both symbols must refer
to the same object; but this can still happen even with more objects. When there are more
objects than constant symbols, some of the objects will have no names. Because the number
of possible models is unbounded, checking entailment by the enumeration of all possible
Terms
A term is a logical expression that refers to an object. Constant symbols are therefore terms,
TERM
but it is not always convenient to have a distinct symbol to name every object. For example,
in English we might use the expression “King John’s left leg” rather than giving a name
to his leg. This is what function symbols are for: instead of using a constant symbol, we
use LeftLeg(John). In the general case, a complex term is formed by a function symbol
followed by a parenthesized list of terms as arguments to the function symbol. It is important
to remember that a complex term is just a complicated kind of name. It is not a “subroutine
call” that “returns a value.” There is no LeftLeg subroutine that takes a person as input and
returns a leg. We can reason about left legs (e.g., stating the general rule that everyone has one
and then deducing that John must have one) without ever providing a deﬁnition of LeftLeg.
The formal semantics of terms is straightforward. Consider a term f(t1, . . . , tn). The
function symbol f refers to some function in the model (call it F); the argument terms refer
to objects in the domain (call them d1, . . . , dn); and the term as a whole refers to the object
that is the value of the function F applied to d1, . . . , dn. For example, suppose the LeftLeg
then LeftLeg(John) refers to King John’s left leg. In this way, the interpretation ﬁxes the
referent of every term.
Atomic sentences
Now that we have both terms for referring to objects and predicate symbols for referring to
relations, we can put them together to make atomic sentences that state facts. An atomic
example, the function that squares its argument can be written as (λx x × x) and can be applied to arguments
can be rewritten by “plugging in” its arguments to yield an equivalent sentence.sentence (or atom for short) is formed from a predicate symbol optionally followed by a
ATOMIC SENTENCE
ATOM
parenthesized list of terms, such as
Brother(Richard, John).
This states, under the intended interpretation given earlier, that Richard the Lionheart is the
Married(Father(Richard), Mother(John))
states that Richard the Lionheart’s father is married to King John’s mother (again, under a
suitable interpretation).
An atomic sentence is true in a given model if the relation referred to by the predicate
symbol holds among the objects referred to by the arguments.
Complex sentences
We can use logical connectives to construct more complex sentences, with the same syntax
and semantics as in propositional calculus. Here are four sentences that are true in the model
¬Brother(LeftLeg(Richard), John)
Brother(Richard, John) ∧Brother(John, Richard)
King(Richard) ∨King(John)
¬King(Richard) ⇒King(John) .
Quantiﬁers
Once we have a logic that allows objects, it is only natural to want to express properties of
entire collections of objects, instead of enumerating the objects by name. Quantiﬁers let us
QUANTIFIER
Universal quantiﬁcation (∀)
tional logic. Rules such as “Squares neighboring the wumpus are smelly” and “All kings
∀x King(x) ⇒Person(x) .
Thus, the sentence says, “For all x, if x is a king, then x is a person.” The symbol x is called
a variable. By convention, variables are lowercase letters. A variable is a term all by itself,
VARIABLE
and as such can also serve as the argument of a function—for example, LeftLeg(x). A term
with no variables is called a ground term.
GROUND TERM
for every object x. More precisely, ∀x P is true in a given model if P is true in all possible
extended interpretations constructed from the interpretation given in the model, where each
EXTENDED
INTERPRETATIONChapter
extended interpretation speciﬁes a domain element to which x refers.
interpretation that goes with it. We can extend the interpretation in ﬁve ways:
tions. That is, the universally quantiﬁed sentence is equivalent to asserting the following ﬁve
sentences:
Richard the Lionheart is a king ⇒Richard the Lionheart is a person.
King John is a king ⇒King John is a person.
Richard’s left leg is a king ⇒Richard’s left leg is a person.
John’s left leg is a king ⇒John’s left leg is a person.
The crown is a king ⇒the crown is a person.
Let us look carefully at this set of assertions. Since, in our model, King John is the only
king, the second sentence asserts that he is a person, as we would hope. But what about
the other four sentences, which appear to make claims about legs and crowns? Is that part
of the meaning of “All kings are persons”? In fact, the other four assertions are true in the
model, but make no claim whatsoever about the personhood qualiﬁcations of legs, crowns,
or indeed Richard. This is because none of these objects is a king. Looking at the truth table
false—regardless of the truth of the conclusion. Thus, by asserting the universally quantiﬁed
sentence, which is equivalent to asserting a whole list of individual implications, we end
up asserting the conclusion of the rule just for those objects for whom the premise is true
and saying nothing at all about those individuals for whom the premise is false. Thus, the
quantiﬁers.
graph several times, is to use conjunction instead of implication. The sentence
∀x King(x) ∧Person(x)
would be equivalent to asserting
Richard the Lionheart is a king ∧Richard the Lionheart is a person,
King John is a king ∧King John is a person,
Richard’s left leg is a king ∧Richard’s left leg is a person,
and so on. Obviously, this does not capture what we want.Existential quantiﬁcation (∃)
ment about some object in the universe without naming it, by using an existential quantiﬁer.
To say, for example, that King John has a crown on his head, we write
∃x Crown(x) ∧OnHead(x, John) .
∃x is pronounced “There exists an x such that . . .” or “For some x . . .”.
precisely, ∃x P is true in a given model if P is true in at least one extended interpretation
that assigns x to a domain element. That is, at least one of the following is true:
Richard the Lionheart is a crown ∧Richard the Lionheart is on John’s head;
King John is a crown ∧King John is on John’s head;
Richard’s left leg is a crown ∧Richard’s left leg is on John’s head;
John’s left leg is a crown ∧John’s left leg is on John’s head;
The crown is a crown ∧the crown is on John’s head.
The ﬁfth assertion is true in the model, so the original existentially quantiﬁed sentence is
true in the model. Notice that, by our deﬁnition, the sentence would also be true in a model
in which King John was wearing two crowns. This is entirely consistent with the original
indeed. Consider the following sentence:
∃x Crown(x) ⇒OnHead(x, John) .
On the surface, this might look like a reasonable rendition of our sentence. Applying the
semantics, we see that the sentence says that at least one of the following assertions is true:
Richard the Lionheart is a crown ⇒Richard the Lionheart is on John’s head;
King John is a crown ⇒King John is on John’s head;
Richard’s left leg is a crown ⇒Richard’s left leg is on John’s head;
and so on. Now an implication is true if both premise and conclusion are true, or if its premise
is false. So if Richard the Lionheart is not a crown, then the ﬁrst assertion is true and the
existential is satisﬁed. So, an existentially quantiﬁed implication sentence is true whenever
any object fails to satisfy the premise; hence such sentences really do not say much at all.
Nested quantiﬁers
plest case is where the quantiﬁers are of the same type. For example, “Brothers are siblings”
can be written as
The same meaning can be expressed using equality statements.Chapter
ables. For example, to say that siblinghood is a symmetric relationship, we can write
∀x, y Sibling(x, y) ⇔Sibling(y, x) .
In other cases we will have mixtures. “Everybody loves somebody” means that for every
person, there is someone that person loves:
On the other hand, to say “There is someone who is loved by everyone,” we write
erty that they love someone. On the other hand, ∃y (∀x Loves(x, y)) says that someone in
the world has a particular property, namely the property of being loved by everybody.
Some confusion can arise when two quantiﬁers are used with the same variable name.
Consider the sentence
∀x (Crown(x) ∨(∃x Brother(Richard, x))) .
Here the x in Brother(Richard, x) is existentially quantiﬁed. The rule is that the variable
belongs to the innermost quantiﬁer that mentions it; then it will not be subject to any other
quantiﬁcation. Another way to think of it is this: ∃x Brother(Richard, x) is a sentence
of confusion, we will always use different variable names with nested quantiﬁers.
serting that everyone dislikes parsnips is the same as asserting there does not exist someone
who likes them, and vice versa:
∀x ¬Likes(x, Parsnips)
is equivalent to
¬∃x Likes(x, Parsnips) .
We can go one step further: “Everyone likes ice cream” means that there is no one who does
not like ice cream:
∀x Likes(x, IceCream)
is equivalent to
¬∃x ¬Likes(x, IceCream) .
not be surprising that they obey De Morgan’s rules. The De Morgan rules for quantiﬁed and
unquantiﬁed sentences are as follows:
∀x ¬P
≡¬∃x P
¬(P ∨Q) ≡¬P ∧¬Q
¬∀x P
≡∃x ¬P
¬(P ∧Q) ≡¬P ∨¬Q
∀x P
≡¬∃x ¬P
P ∧Q
≡¬(¬P ∨¬Q)
∃x P
≡¬∀x ¬P
P ∨Q
≡¬(¬P ∧¬Q) .
readability is more important than parsimony, so we will keep both of the quantiﬁers.Equality
cate and terms as described earlier. We can use the equality symbol to signify that two terms
EQUALITY SYMBOL
refer to the same object. For example,
Father(John) = Henry
says that the object referred to by Father(John) and the object referred to by Henry are the
same. Because an interpretation ﬁxes the referent of any term, determining the truth of an
equality sentence is simply a matter of seeing that the referents of the two terms are the same
object.
The equality symbol can be used to state facts about a given function, as we just did for
the Father symbol. It can also be used with negation to insist that two terms are not the same
object. To say that Richard has at least two brothers, we would write
∃x, y Brother(x, Richard) ∧Brother(y, Richard) ∧¬(x = y) .
The sentence
∃x, y Brother(x, Richard) ∧Brother(y, Richard)
Richard has only one brother. To see this, consider the extended interpretation in which both
x and y are assigned to King John. The addition of ¬(x = y) rules out such models. The
notation x ̸= y is sometimes used as an abbreviation for ¬(x = y).
An alternative semantics?
Continuing the example from the previous section, suppose that we believe that Richard has
Brother(John, Richard) ∧Brother(Geoﬀrey, Richard) ?
Not quite. First, this assertion is true in a model where Richard has only one brother—
we need to add John ̸= Geoﬀrey. Second, the sentence doesn’t rule out models in which
Richard has many more brothers besides John and Geoffrey. Thus, the correct translation of
“Richard’s brothers are John and Geoffrey” is as follows:
Brother(John, Richard) ∧Brother(Geoﬀrey, Richard) ∧John ̸= Geoﬀrey
∧∀x Brother(x, Richard) ⇒(x = John ∨x = Geoﬀrey) .
language expression. As a consequence, humans may make mistakes in translating their
ward logical expression?
One proposal that is very popular in database systems works as follows. First, we insist
tion. Second, we assume that atomic sentences not known to be true are in fact false—the
ASSUMPTION
ASSUMPTION
DOMAIN CLOSUREChapter
. . .
R
J
R
J
R
J
R
J
R
J
R
J
R
J
R
J
R
J
R
J
bols, R and J, and one binary relation symbol, under database semantics. The interpretation
of the constant symbols is ﬁxed, and there is a distinct object for each constant symbol.
contains no more domain elements than those named by the constant symbols. Under the
resulting semantics, which we call database semantics to distinguish it from the standard
DATABASE
SEMANTICS
two brothers are John and Geoffrey. Database semantics is also used in logic programming
It is instructive to consider the set of all possible models under database semantics for
the model with no tuples satisfying the relation to the model with all tuples satisfying the
other hand, the database semantics requires deﬁnite knowledge of what the world contains.
This example brings up an important point: there is no one “correct” semantics for
logic. The usefulness of any proposed semantics depends on how concise and intuitive it
makes the expression of the kinds of knowledge we want to write down, and on how easy
and natural it is to develop the corresponding rules of inference. Database semantics is most
useful when we are certain about the identity of all the objects described in the knowledge
base and when we have all the facts at hand; in other cases, it is quite awkward. For the rest
of this chapter, we assume the standard semantics while noting instances in which this choice
leads to cumbersome expressions.
Now that we have deﬁned an expressive logical language, it is time to learn how to use it. The
best way to do this is through examples. We have seen some simple sentences illustrating the
various aspects of logical syntax; in this section, we provide more systematic representations
of some simple domains. In knowledge representation, a domain is just some part of the
DOMAIN
world about which we wish to express some knowledge.
bases. Then we look at the domains of family relationships, numbers, sets, and lists, and atthe wumpus world. The next section contains a more substantial example (electronic circuits)
Sentences are added to a knowledge base using TELL, exactly as in propositional logic. Such
sentences are called assertions. For example, we can assert that John is a king, Richard is a
ASSERTION
person, and all kings are persons:
TELL(KB, King(John)) .
TELL(KB, Person(Richard)) .
TELL(KB, ∀x King(x) ⇒Person(x)) .
We can ask questions of the knowledge base using ASK. For example,
ASK(KB, King(John))
returns true. Questions asked with ASK are called queries or goals. Generally speaking, any
QUERY
GOAL
query that is logically entailed by the knowledge base should be answered afﬁrmatively. For
example, given the two preceding assertions, the query
ASK(KB, Person(John))
should also return true. We can ask quantiﬁed queries, such as
ASK(KB, ∃x Person(x)) .
The answer is true, but this is perhaps not as helpful as we would like. It is rather like
answering “Can you tell me the time?” with “Yes.” If we want to know what value of x
makes the sentence true, we will need a different function, ASKVARS, which we call with
ASKVARS(KB, Person(x))
SUBSTITUTION
BINDING LIST
reserved for knowledge bases consisting solely of Horn clauses, because in such knowledge
bases every way of making the query true will bind the variables to speciﬁc values. That is
The kinship domain
The ﬁrst example we consider is the domain of family relationships, or kinship. This domain
includes facts such as “Elizabeth is the mother of Charles” and “Charles is the father of
William” and rules such as “One’s grandmother is the mother of one’s parent.”
Clearly, the objects in our domain are people. We have two unary predicates, Male and
Female. Kinship relations—parenthood, brotherhood, marriage, and so on—are represented
by binary predicates: Parent, Sibling, Brother, Sister, Child, Daughter, Son, Spouse,
Wife, Husband, Grandparent, Grandchild, Cousin, Aunt, and Uncle. We use functions
for Mother and Father, because every person has exactly one of each of these (at least
according to nature’s design).Chapter
We can go through each function and predicate, writing down what we know in terms
of the other symbols. For example, one’s mother is one’s female parent:
∀m, c Mother(c) = m ⇔Female(m) ∧Parent(m, c) .
One’s husband is one’s male spouse:
∀w, h Husband(h, w) ⇔Male(h) ∧Spouse(h, w) .
Male and female are disjoint categories:
∀x Male(x) ⇔¬Female(x) .
Parent and child are inverse relations:
∀p, c Parent(p, c) ⇔Child(c, p) .
A grandparent is a parent of one’s parent:
∀g, c Grandparent(g, c) ⇔∃p Parent(g, p) ∧Parent(p, c) .
A sibling is another child of one’s parents:
∀x, y Sibling(x, y) ⇔x ̸= y ∧∃p Parent(p, x) ∧Parent(p, y) .
Each of these sentences can be viewed as an axiom of the kinship domain, as explained
will see some axioms for numbers shortly—but they are needed in all domains. They provide
the basic factual information from which useful conclusions can be derived. Our kinship
. . .. The axioms deﬁne
DEFINITION
the Mother function and the Husband, Male, Parent, Grandparent, and Sibling predicates
in terms of other predicates. Our deﬁnitions “bottom out” at a basic set of predicates (Child,
Spouse, and Female) in terms of which the others are ultimately deﬁned. This is a natural
way in which to build up the representation of a domain, and it is analogous to the way in
which software packages are built up by successive deﬁnitions of subroutines from primitive
library functions. Notice that there is not necessarily a unique set of primitive predicates;
we could equally well have used Parent, Spouse, and Male. In some domains, as we show,
there is no clearly identiﬁable basic set.
Not all logical sentences about a domain are axioms. Some are theorems—that is, they
THEOREM
are entailed by the axioms. For example, consider the assertion that siblinghood is symmetric:
∀x, y Sibling(x, y) ⇔Sibling(y, x) .
Is this an axiom or a theorem? In fact, it is a theorem that follows logically from the axiom
that deﬁnes siblinghood. If we ASK the knowledge base this sentence, it should return true.
From a purely logical point of view, a knowledge base need contain only axioms and
no theorems, because the theorems do not increase the set of conclusions that follow from
the knowledge base. From a practical point of view, theorems are essential to reduce the
computational cost of deriving new sentences. Without them, a reasoning system has to start
from ﬁrst principles every time, rather like a physicist having to rederive the rules of calculus
for every new problem.Not all axioms are deﬁnitions. Some provide more general information about certain
predicates without constituting a deﬁnition. Indeed, some predicates have no complete deﬁ-
nition because we do not know enough to characterize them fully. For example, there is no
obvious deﬁnitive way to complete the sentence
∀x Person(x) ⇔. . .
pletely deﬁning it. Instead, we can write partial speciﬁcations of properties that every person
has and properties that make something a person:
∀x Person(x) ⇒. . .
∀x . . . ⇒Person(x) .
Axioms can also be “just plain facts,” such as Male(Jim) and Spouse(Jim, Laura).
Such facts form the descriptions of speciﬁc problem instances, enabling speciﬁc questions
to be answered. The answers to these questions will then be theorems that follow from
the axioms. Often, one ﬁnds that the expected answers are not forthcoming—for example,
from Spouse(Jim, Laura) one expects (under the laws of many countries) to be able to infer
¬Spouse(George, Laura); but this does not follow from the axioms given earlier—even after
Numbers, sets, and lists
Numbers are perhaps the most vivid example of how a large theory can be built up from
NATURAL NUMBERS
integers. We need a predicate NatNum that will be true of natural numbers; we need one
PEANO AXIOMS
∀n NatNum(n) ⇒NatNum(S(n)) .
∀m, n m ̸= n ⇒S(m) ̸= S(n) .
Now we can deﬁne addition in terms of the successor function:
∀m, n NatNum(m) ∧NatNum(n) ⇒+ (S(m), n) = S(+(m, n)) .
INFIXChapter
logic is called preﬁx.) To make our sentences about numbers easier to read, we allow the use
PREFIX
This axiom reduces addition to repeated application of the successor function.
The use of inﬁx notation is an example of syntactic sugar, that is, an extension to or
SYNTACTIC SUGAR
abbreviation of the standard syntax that does not change the semantics. Any sentence that
bers, and so on. Thus, the whole of number theory (including cryptography) can be built up
from one constant, one function, one predicate and four axioms.
The domain of sets is also fundamental to mathematics as well as to commonsense
SET
reasoning. (In fact, it is possible to deﬁne number theory in terms of set theory.) We want to
be able to represent individual sets, including the empty set. We need a way to build up sets
by adding an element to a set or taking the union or intersection of two sets. We will want
to know whether an element is a member of a set and we will want to distinguish sets from
objects that are not sets.
We will use the normal vocabulary of set theory as syntactic sugar. The empty set is a
constant written as { }. There is one unary predicate, Set, which is true of sets. The binary
(the union of two sets), and {x|s} (the set resulting from adjoining element x to set s). One
possible set of axioms is as follows:
∀s Set(s) ⇔(s = { }) ∨(∃x, s2 Set(s2) ∧s = {x|s2}) .
decompose { } into a smaller set and an element:
¬∃x, s {x|s} = { } .
this recursively, saying that x is a member of s if and only if s is equal to some set s2
adjoined with some element y, where either y is the same as x or x is a member of s2:
of the second set:Lists are similar to sets. The differences are that lists are ordered and the same element can
LIST
appear more than once in a list. We can use the vocabulary of Lisp for lists: Nil is the constant
icate that does for lists what Member does for sets. List? is a predicate that is true only of
lists. As with sets, it is common to use syntactic sugar in logical sentences involving lists. The
empty list is [ ]. The term Cons(x, y), where y is a nonempty list, is written [x|y]. The term
ments, such as [A, B, C], corresponds to the nested term Cons(A, Cons(B, Cons(C, Nil))).
The wumpus world
order axioms in this section are much more concise, capturing in a natural way exactly what
we want to say.
the time at which it occurred; otherwise, the agent will get confused about when it saw what.
We use integers for time steps. A typical percept sentence would be
Here, Percept is a binary predicate, and Stench and so on are constants placed in a list. The
actions in the wumpus world can be represented by logical terms:
Turn(Right), Turn(Left), Forward, Shoot, Grab, Climb .
To determine which is best, the agent program executes the query
the action to take. The raw percept data implies certain facts about the current state. For
example:
∀t, s, g, m, c Percept([s, Breeze, g, m, c], t) ⇒Breeze(t) ,
∀t, s, b, m, c Percept([s, b, Glitter, m, c], t) ⇒Glitter(t) ,
and so on. These rules exhibit a trivial form of the reasoning process called perception, which
we would need copies of each sentence for each time step.
Simple “reﬂex” behavior can also be implemented by quantiﬁed implication sentences.
For example, we have
∀t Glitter(t) ⇒BestAction(Grab, t) .Chapter
We have represented the agent’s inputs and outputs; now it is time to represent the
environment itself. Let us begin with objects. Obvious candidates are squares, pits, and the
fact for each pair of squares. It is better to use a complex term in which the row and column
squares can be deﬁned as
∀x, y, a, b Adjacent([x, y], [a, b]) ⇔
We could name each pit, but this would be inappropriate for a different reason: there is no
squares containing pits. Finally, since there is exactly one wumpus, a constant Wumpus is
just as good as a unary predicate (and perhaps more digniﬁed from the wumpus’s viewpoint).
The agent’s location changes over time, so we write At(Agent, s, t) to mean that the
We can then say that objects can only be at one location at a time:
∀x, s1, s2, t At(x, s1, t) ∧At(x, s2, t) ⇒s1 = s2 .
Given its current location, the agent can infer properties of the square from properties of its
current percept. For example, if the agent is at a square and perceives a breeze, then that
square is breezy:
∀s, t At(Agent, s, t) ∧Breeze(t) ⇒Breezy(s) .
It is useful to know that a square is breezy because we know that the pits cannot move about.
Notice that Breezy has no time argument.
Having discovered which places are breezy (or smelly) and, very important, not breezy
(or not smelly), the agent can deduce where the pits are (and where the wumpus is). Whereas
logic just needs one axiom:
∀s Breezy(s) ⇔∃r Adjacent(r, s) ∧Pit(r) .
axiom for each predicate, rather than a different copy for each time step. For example, the
ornithologist wishing to study migration patterns, survival rates, and so on does name each bird, by means of a
ring on its leg, because individual birds must be tracked.is invited to construct analogous axioms for the agent’s location and orientation; in these
cases, the axioms quantify over both space and time. As in the case of propositional state
estimation, an agent can use logical inference with axioms of this kind to keep track of aspects
a process called knowledge engineering. A knowledge engineer is someone who investigates
KNOWLEDGE
ENGINEERING
a particular domain, learns what concepts are important in that domain, and creates a formal
neering process in an electronic circuit domain that should already be fairly familiar, so that
we can concentrate on the representational issues involved. The approach we take is suitable
cover a broad range of human knowledge and are intended to support tasks such as natural
Knowledge engineering projects vary widely in content, scope, and difﬁculty, but all such
projects include the following steps:
the knowledge base will support and the kinds of facts that will be available for each
speciﬁc problem instance. For example, does the wumpus knowledge base need to be
able to choose actions or is it required to answer questions only about the contents
of the environment? Will the sensor facts include the current location? The task will
determine what knowledge must be represented in order to connect problem instances to
in the domain, or might need to work with real experts to extract what they know—a
process called knowledge acquisition. At this stage, the knowledge is not represented
KNOWLEDGE
ACQUISITION
formally. The idea is to understand the scope of the knowledge base, as determined by
the task, and to understand how the domain actually works.
For the wumpus world, which is deﬁned by an artiﬁcial set of rules, the relevant
knowledge is easy to identify. (Notice, however, that the deﬁnition of adjacency was
relevance can be quite difﬁcult—for example, a system for simulating VLSI designs
might or might not need to take into account stray capacitances and skin effects.Chapter
impact on the eventual success of the project. For example, should pits be represented
by objects or by a unary predicate on squares? Should the agent’s orientation be a
function or a predicate? Should the wumpus’s location depend on time? Once the
choices have been made, the result is a vocabulary that is known as the ontology of
ONTOLOGY
the domain. The word ontology means a particular theory of the nature of being or
existence. The ontology determines what kinds of things exist, but does not determine
their speciﬁc properties and interrelationships.
the axioms for all the vocabulary terms. This pins down (to the extent possible) the
meaning of the terms, enabling the expert to check the content. Often, this step reveals
iterating through the process.
stances of concepts that are already part of the ontology. For a logical agent, problem
plied with additional sentences in the same way that traditional programs are supplied
with input data.
derive the facts we are interested in knowing. Thus, we avoid the need for writing an
the ﬁrst try. More precisely, the answers will be correct for the knowledge base as
written, assuming that the inference procedure is sound, but they will not be the ones
that the user is expecting. For example, if an axiom is missing, some queries will not be
answerable from the knowledge base. A considerable debugging process could ensue.
Missing axioms or axioms that are too weak can be easily identiﬁed by noticing places
where the chain of reasoning stops unexpectedly. For example, if the knowledge base
∀s Smelly(s) ⇒Adjacent(Home(Wumpus), s) ,
instead of the biconditional, then the agent will never be able to prove the absence of
wumpuses. Incorrect axioms can be identiﬁed because they are false statements about
the world. For example, the sentence
is false for reptiles, amphibians, and, more importantly, tables. The falsehood of this
sentence can be determined independently of the rest of the knowledge base. In contrast,a typical error in a program looks like this:
It is impossible to tell whether this statement is correct without looking at the rest of the
program to see whether, for example, offset is used to refer to the current position,
or to one beyond the current position, or whether the value of position is changed
by another statement and so offset should also be changed again.
domain of electronic circuits.
The electronic circuits domain
We will develop an ontology and knowledge base that allow us to reason about digital circuits
Identify the task
There are many reasoning tasks associated with digital circuits. At the highest level, one
properly? If all the inputs are high, what is the output of gate A2? Questions about the
circuit’s structure are also interesting. For example, what are all the gates connected to the
ﬁrst input terminal? Does the circuit contain feedback loops? These will be our tasks in this
section. There are more detailed levels of analysis, including those related to timing delays,
circuit area, power consumption, production cost, and so on. Each of these levels would
require additional knowledge.
Assemble the relevant knowledge
What do we know about digital circuits? For our purposes, they are composed of wires and
gates. Signals ﬂow along wires to the input terminals of gates, and each gate produces a
X1
X2
A1
A2
O1
C1
are the two bits to be added, and the third input is a carry bit. The ﬁrst output is the sum, and
the second output is a carry bit for the next adder. The circuit contains two XOR gates, two
AND gates, and one OR gate.Chapter
signal on the output terminal that ﬂows along another wire. To determine what these signals
will be, we need to know how the gates transform their input signals. There are four types
of gates: AND, OR, and XOR gates have two input terminals, and NOT gates have one. All
gates have one output terminal. Circuits, like gates, have input and output terminals.
To reason about functionality and connectivity, we do not need to talk about the wires
themselves, the paths they take, or the junctions where they come together. All that matters
is the connections between terminals—we can say that one output terminal is connected to
another input terminal without having to say what actually connects them. Other factors such
as the size, shape, color, or cost of the various components are irrelevant to our analysis.
ogy would be different. For example, if we were interested in debugging faulty circuits, then
it would probably be a good idea to include the wires in the ontology, because a faulty wire
can corrupt the signal ﬂowing along it. For resolving timing faults, we would need to include
gate delays. If we were interested in designing a product that would be proﬁtable, then the
cost of the circuit and its speed relative to other products on the market would be important.
Decide on a vocabulary
We now know that we want to talk about circuits, terminals, signals, and gates. The next step
is to choose functions, predicates, and constants to represent them. First, we need to be able
to distinguish gates from each other and from other objects. Each gate is represented as an
object named by a constant, about which we assert that it is a gate with, say, Gate(X1). The
behavior of each gate is determined by its type: one of the constants AND, OR, XOR, or
NOT. Because a gate has exactly one type, a function is appropriate: Type(X1) = XOR.
Circuits, like gates, are identiﬁed by a predicate: Circuit(C1).
Next we consider terminals, which are identiﬁed by the predicate Terminal(x). A gate
or circuit can have one or more input terminals and one or more output terminals. We use the
put terminals. The connectivity between gates can be represented by a predicate, Connected,
Finally, we need to know whether a signal is on or off. One possibility is to use a unary
predicate, On(t), which is true when the signal at a terminal is on. This makes it a little
difﬁcult, however, to pose questions such as “What are all the possible values of the signals
Encode general knowledge of the domain
One sign that we have a good ontology is that we require only a few general rules, which can
be stated clearly and concisely. These are all the axioms we will need:
∀t1, t2 Terminal(t1) ∧Terminal(t2) ∧Connected(t1, t2) ⇒
Signal(t1) = Signal(t2) .∀t1, t2 Connected(t1, t2) ⇔Connected(t2, t1) .
∀g Gate(g) ∧k = Type(g) ⇒k = AND ∨k = OR ∨k = XOR ∨k = NOT .
∀g Gate(g) ∧Type(g) = AND ⇒
∀g Gate(g) ∧Type(g) = OR ⇒
∀g Gate(g) ∧Type(g) = XOR ⇒
∀g Gate(g) ∧(Type(g) = NOT) ⇒
∀g Gate(g) ∧k = Type(g) ∧(k = AND ∨k = OR ∨k = XOR) ⇒
∀c, i, j Circuit(c) ∧Arity(c, i, j) ⇒
∀g, t Gate(g) ∧Terminal(t) ⇒
∀g Gate(g) ⇒Circuit(g)
Encode the speciﬁc problem instance
we categorize the circuit and its component gates:
Gate(X1) ∧Type(X1) = XOR
Gate(X2) ∧Type(X2) = XOR
Gate(A1) ∧Type(A1) = AND
Gate(A2) ∧Type(A2) = AND
Gate(O1) ∧Type(O1) = OR .Chapter
Then, we show the connections between them:
Pose queries to the inference procedure
The answers are substitutions for the variables i1, i2, and i3 such that the resulting sentence
is entailed by the knowledge base. ASKVARS will give us three such substitutions:
What are the possible sets of values of all the terminals for the adder circuit?
This ﬁnal query will return a complete input–output table for the device, which can be used
to check that it does in fact add its inputs correctly. This is a simple example of circuit
veriﬁcation. We can also use the deﬁnition of the circuit to build larger digital systems, for
CIRCUIT
VERIFICATION
more complex concepts are deﬁned on top of simpler concepts.
Debug the knowledge base
We can perturb the knowledge base in various ways to see what kinds of erroneous behaviors
gate. For example, we can ask
at the axiom for XOR gates, as applied to X1:Summary
SUMMARY
erful than propositional logic. The important points are as follows:
• Knowledge representation languages should be declarative, compositional, expressive,
context independent, and unambiguous.
• Logics differ in their ontological commitments and epistemological commitments.
mits to the existence of objects and relations and thereby gains expressive power.
represent objects, and has universal and existential quantiﬁers to construct assertions
about all or some of the possible values of the quantiﬁed variables.
pretation that maps constant symbols to objects, predicate symbols to relations among
objects, and function symbols to functions on objects.
• An atomic sentence is true just when the relation named by the predicate holds between
ables to objects in the model, deﬁne the truth of quantiﬁed sentences.
the domain, choosing a vocabulary, and encoding the axioms required to support the
desired inferences.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
following example to show the sorts of inferences that Aristotle’s logic could not handle: “All
horses are animals; therefore, the head of a horse is the head of an animal.” This inference
is inaccessible to Aristotle because any valid rule that can support this inference must ﬁrst
tiﬁers was a big step forward, but he used an awkward notation. The present notation for
identical to Frege’s. Oddly enough, Peano’s axioms were due in large measure to GrassmannChapter
logic, including the ﬁrst proper treatment of the equality symbol. L¨owenheim’s results were
entrenched at Edinburgh through the LCF (Logic for Computable Functions) project (Gordon
world as analogous to the construction of scientiﬁc theories. Just as the predictions of a
machine learning algorithm depend strongly on the vocabulary supplied to it, so will the
child’s formulation of theories depend on the linguistic environment in which learning occurs.
automated reasoning. Logical Foundations of Artiﬁcial Intelligence (Genesereth and Nilsson,
with percepts and actions, and there are two good handbooks: van Bentham and ter Meulen
ematical logic is the Journal of Symbolic Logic, whereas the Journal of Applied Logic deals
with concerns closer to those of artiﬁcial intelligence.Chapter
standard semantics but not according to the new semantics. Discuss which outcome makes
more intuitive sense for your examples.
Does the fact ¬Spouse(George, Laura) follow from the facts Jim ̸= George and
Spouse(Jim, Laura)? If so, give a proof; if not, supply additional axioms as needed. What
happens if we use Spouse as a unary function symbol instead of a binary predicate?
This exercise uses the function MapColor and predicates In(x, y), Borders(x, y), and
Country(x), whose arguments are geographical regions, along with constant symbols for
is syntactically valid but does not express the meaning of the English sentence.
a. Paris and Marseilles are both in France.
(i) In(Paris ∧Marseilles, France).
(ii) In(Paris, France) ∧In(Marseilles, France).
(iii) In(Paris, France) ∨In(Marseilles, France).
b. There is a country that borders both Iraq and Pakistan.
(i) ∃c Country(c) ∧Border(c, Iraq) ∧Border(c, Pakistan).
(ii) ∃c Country(c) ⇒[Border(c, Iraq) ∧Border(c, Pakistan)].
(iii) [∃c
Country(c)] ⇒[Border(c, Iraq) ∧Border(c, Pakistan)].
(iv) ∃c Border(Country(c), Iraq ∧Pakistan).
c. All countries that border Ecuador are in South America.
(i) ∀c Country(c) ∧Border(c, Ecuador) ⇒In(c, SouthAmerica).
(ii) ∀c Country(c) ⇒[Border(c, Ecuador) ⇒In(c, SouthAmerica)].
(iii) ∀c [Country(c) ⇒Border(c, Ecuador)] ⇒In(c, SouthAmerica).
(iv) ∀c Country(c) ∧Border(c, Ecuador) ∧In(c, SouthAmerica).
d. No region in South America borders any region in Europe.
(i) ¬[∃c, d In(c, SouthAmerica) ∧In(d, Europe) ∧Borders(c, d)].
(ii) ∀c, d [In(c, SouthAmerica) ∧In(d, Europe)] ⇒¬Borders(c, d)].
(iii) ¬∀c In(c, SouthAmerica) ⇒∃d In(d, Europe) ∧¬Borders(c, d).
(iv) ∀c In(c, SouthAmerica) ⇒∀d In(d, Europe) ⇒¬Borders(c, d).
e. No two adjacent countries have the same map color.
(i) ∀x, y ¬Country(x) ∨¬Country(y) ∨¬Borders(x, y) ∨
¬(MapColor(x) = MapColor(y)).
(ii) ∀x, y (Country(x) ∧Country(y) ∧Borders(x, y) ∧¬(x = y)) ⇒
¬(MapColor(x) = MapColor(y)).
(iii) ∀x, y Country(x) ∧Country(y) ∧Borders(x, y) ∧
¬(MapColor(x) = MapColor(y)).
(iv) ∀x, y (Country(x) ∧Country(y) ∧Borders(x, y)) ⇒MapColor(x ̸= y).Chapter
Beatrice
Andrew
Eugenie
William Harry
Charles
Diana
Mum
George
Philip
Elizabeth
Margaret
Kydd
Spencer
Peter
Mark
Zara
Anne
Sarah
Edward
Sophie
Louise
James
A typical family tree. The symbol “▷◁” connects spouses and arrows point to
children.
Write axioms describing the predicates Grandchild, Greatgrandparent , Ancestor,
Brother, Sister, Daughter, Son, FirstCousin, BrotherInLaw, SisterInLaw, Aunt, and
Explain what is wrong with the following proposed deﬁnition of the set membership
Using the set axioms as examples, write axioms for the list domain, including all the
constants, functions, and predicates mentioned in the chapter.
Explain what is wrong with the following proposed deﬁnition of adjacent squares in
the wumpus world:
Write out the axioms required for reasoning about the wumpus’s location, using a
constant symbol Wumpus and a binary predicate At(Wumpus, Location). Remember that
there is only one wumpus.
Assuming predicates Parent(p, q) and Female(p) and constants Joan and Kevin,
a. Joan has a daughter (possibly more than one, and possibly sons as well).
b. Joan has exactly one daughter (but may have sons as well).
c. Joan has exactly one child, a daughter.
d. Joan and Kevin have exactly one child together.
e. Joan has at least one child with Kevin, and no children with anyone else.Chapter
Z0
Z1
Z2
Z3
Z4
X0
Y0
X1
Y1
X2
Y2
X3
Y3
Ad0
Ad1
Ad2
Ad3
X0
X1
X2
X3
Z0
Z1
Z2
Z3
Z4
Y0
Y1
Y2
Y3
+
h. There is a barber who shaves all men in town who do not shave themselves.
i. A person born in the UK, each of whose parents is a UK citizen or a UK resident, is a
UK citizen by birth.
j. A person born outside the UK, one of whose parents is a UK citizen by birth, is a UK
citizen by descent.
k. Politicians can fool some of the people all of the time, and they can fool all of the people
some of the time, but they can’t fool all of the people all of the time.
l. All Greeks speak the same language. (Use Speaks(x, l) to mean that person x speaks
language l.)
Write a general set of facts and axioms to represent the assertion “Wellington heard
about Napoleon’s death” and to correctly answer the question “Did Napoleon hear about
Wellington’s death?”
to verify that it is in fact correct.
songs, albums (e.g., “Meet the Beatles”) and disks (i.e., particular physical instances of CDs).
The vocabulary contains the following symbols:
CopyOf (d, a): Predicate. Disk d is a copy of album a.
Owns(p, d): Predicate. Person p owns disk d.
Sings(p, s, a): Album a includes a recording of song s sung by person p.
Wrote(p, s): Person p wrote song s.
McCartney, Gershwin, BHoliday, Joe, EleanorRigby, TheManILove, Revolver:
Constants with the obvious meanings.INFERENCE IN
order logic.
ward chaining can be very efﬁcient, but are applicable only to knowledge bases that can
This section and the next introduce the ideas underlying modern logical inference systems.
We begin with some simple inference rules that can be applied to sentences with quantiﬁers
inference can be done by converting the knowledge base to propositional logic and using
propositional inference, which we already know how to do. The next section points out an
Inference rules for quantiﬁers
Let us begin with universal quantiﬁers. Suppose our knowledge base contains the standard
folkloric axiom stating that all greedy kings are evil:
∀x King(x) ∧Greedy(x) ⇒Evil(x) .Then it seems quite permissible to infer any of the following sentences:
King(John) ∧Greedy(John) ⇒Evil(John)
King(Richard) ∧Greedy(Richard) ⇒Evil(Richard)
King(Father(John)) ∧Greedy(Father(John)) ⇒Evil(Father(John)) .
...
UNIVERSAL
INSTANTIATION
GROUND TERM
rule is written
∀v α
for any variable v and ground term g. For example, the three sentences given earlier are
EXISTENTIAL
INSTANTIATION
stant symbol. The formal statement is as follows: for any sentence α, variable v, and constant
symbol k that does not appear elsewhere in the knowledge base,
∃v α
For example, from the sentence
∃x Crown(x) ∧OnHead(x, John)
we can infer the sentence
Crown(C1) ∧OnHead(C1, John)
as long as C1 does not appear elsewhere in the knowledge base. Basically, the existential
tiation rule just gives a name to that object. Of course, that name must not already belong
to another object. Mathematics provides a nice example: suppose we discover that there is a
We can give this number a name, such as e, but it would be a mistake to give it the name of
SKOLEM CONSTANT
tial Instantiation is a special case of a more general process called skolemization, which we
Whereas Universal Instantiation can be applied many times to produce many different
we have added the sentence Kill(Murderer, Victim). Strictly speaking, the new knowledge
base is not logically equivalent to the old, but it can be shown to be inferentially equivalent
INFERENTIAL
EQUIVALENCE
in the sense that it is satisﬁable exactly when the original knowledge base is satisﬁable.
The substitution replaces a variable with a term (a piece of syntax) to produce a new sentence, whereas an
interpretation maps a variable to an object in the domain.Chapter
Reduction to propositional inference
The ﬁrst idea is that, just as an existentially quantiﬁed sentence can be replaced by
one instantiation, a universally quantiﬁed sentence can be replaced by the set of all possible
instantiations. For example, suppose our knowledge base contains just the sentences
∀x King(x) ∧Greedy(x) ⇒Evil(x)
King(John)
Greedy(John)
Brother(Richard, John) .
King(John) ∧Greedy(John) ⇒Evil(John)
King(Richard) ∧Greedy(Richard) ⇒Evil(Richard) ,
and we discard the universally quantiﬁed sentence. Now, the knowledge base is essentially
propositional if we view the ground atomic sentences—King(John), Greedy(John), and
so on—as proposition symbols. Therefore, we can apply any of the complete propositional
This technique of propositionalization can be made completely general, as we show
in such a way that entailment is preserved. Thus, we have a complete decision procedure
for entailment . . . or perhaps not. There is a problem: when the knowledge base includes
the knowledge base mentions the Father symbol, then inﬁnitely many nested terms such as
Father(Father(Father(John))) can be constructed. Our propositional algorithms will have
difﬁculty with an inﬁnitely large set of sentences.
involving just a ﬁnite subset of the propositionalized knowledge base. Since any such subset
has a maximum depth of nesting among its ground terms, we can ﬁnd the subset by ﬁrst
generating all the instantiations with constant symbols (Richard and John), then all terms of
are able to construct a propositional proof of the entailed sentence.
complete—that is, any entailed sentence can be proved. This is a major achievement, given
that the space of possible models is inﬁnite. On the other hand, we do not know until the
proof is done that the sentence is entailed! What happens when the sentence is not entailed?
go on and on, generating more and more deeply nested terms, but we will not know whether
it is stuck in a hopeless loop or whether the proof is just about to pop out. This is very muchUniﬁcation and Lifting
both proved, in rather different ways, the inevitability of this state of affairs. The question of
entailed sentence, but no algorithm exists that also says no to every nonentailed sentence.
UNIFICATION AND LIFTING
early 1960s) will have noticed that the propositionalization approach is rather inefﬁcient. For
verse to generate sentences such as King(Richard) ∧Greedy(Richard) ⇒Evil(Richard).
Indeed, the inference of Evil(John) from the sentences
∀x King(x) ∧Greedy(x) ⇒Evil(x)
King(John)
Greedy(John)
seems completely obvious to a human being. We now show how to make it completely
obvious to a computer.
this: to use the rule that greedy kings are evil, ﬁnd some x such that x is a king and x is
makes each of the conjuncts of the premise of the implication identical to sentences already
We can actually make the inference step do even more work. Suppose that instead of
knowing Greedy(John), we know that everyone is greedy:
∀y Greedy(y) .
Then we would still like to be able to conclude that Evil(John), because we know that
John is a king (given) and John is greedy (because everyone is greedy). What we need for
this to work is to ﬁnd a substitution both for the variables in the implication sentence and
for the variables in the sentences that are in the knowledge base. In this case, applying the
can infer the conclusion of the implication.
GENERALIZED
MODUS PONENSChapter
such that SUBST(θ, pi′) = SUBST(θ, pi), for all i,
p1′, p2′, . . . , pn′, (p1 ∧p2 ∧. . . ∧pn ⇒q)
SUBST(θ, q)
.
p1′ is King(John)
p1 is King(x)
p2′ is Greedy(y)
p2 is Greedy(x)
q is Evil(x)
SUBST(θ, q) is Evil(John) .
It is easy to show that Generalized Modus Ponens is a sound inference rule. First, we observe
that, for any sentence p (whose variables are assumed to be universally quantiﬁed) and for
p |= SUBST(θ, p)
the Generalized Modus Ponens rule. Thus, from p1′, . . . , pn′ we can infer
SUBST(θ, p1′) ∧. . . ∧SUBST(θ, pn′)
and from the implication p1 ∧. . . ∧pn ⇒q we can infer
SUBST(θ, p1) ∧. . . ∧SUBST(θ, pn) ⇒SUBST(θ, q) .
Now, θ in Generalized Modus Ponens is deﬁned so that SUBST(θ, pi′) = SUBST(θ, pi), for
all i; therefore the ﬁrst of these two sentences matches the premise of the second exactly.
Hence, SUBST(θ, q) follows by Modus Ponens.
LIFTING
rest of this chapter that we can develop lifted versions of the forward chaining, backward
inference rules over propositionalization is that they make only those substitutions that are
required to allow particular inferences to proceed.
Uniﬁcation
Lifted inference rules require ﬁnding substitutions that make different logical expressions
UNIFICATION
inference algorithms. The UNIFY algorithm takes two sentences and returns a uniﬁer for
UNIFIER
them if one exists:
UNIFY(p, q) = θ where SUBST(θ, p) = SUBST(θ, q) .
Let us look at some examples of how UNIFY should behave. Suppose we have a query
AskVars(Knows(John, x)): whom does John know? Answers to this query can be found
and the premise of the implication need match only up to a substitution, rather than exactly. On the other hand,
Modus Ponens allows any sentence α as the premise, rather than just a conjunction of atomic sentences.Uniﬁcation and Lifting
by ﬁnding all sentences in the knowledge base that unify with Knows(John, x). Here are the
results of uniﬁcation with four different sentences that might be in the knowledge base:
UNIFY(Knows(John, x), Knows(x, Elizabeth)) = fail .
The last uniﬁcation fails because x cannot take on the values John and Elizabeth at the
same time. Now, remember that Knows(x, Elizabeth) means “Everyone knows Elizabeth,”
so we should be able to infer that John knows Elizabeth. The problem arises only because
the two sentences happen to use the same variable name, x. The problem can be avoided
by standardizing apart one of the two sentences being uniﬁed, which means renaming its
STANDARDIZING
APART
variables to avoid name clashes. For example, we can rename x in Knows(x, Elizabeth) to
x17 (a new variable name) without changing its meaning. Now the uniﬁcation will work:
There is one more complication: we said that UNIFY should return a substitution
cation, whereas the second gives Knows(John, John). The second result could be obtained
general than the second, because it places fewer restrictions on the values of the variables. It
turns out that, for every uniﬁable pair of expressions, there is a single most general uniﬁer (or
MOST GENERAL
UNIFIER
is simple: recursively explore the two expressions simultaneously “side by side,” building up
a uniﬁer along the way, but failing if two corresponding points in the structures do not match.
There is one expensive step: when matching a variable against a complex term, one must
check whether the variable itself occurs inside the term; if it does, the match fails because no
called occur check makes the complexity of the entire algorithm quadratic in the size of the
OCCUR CHECK
expressions being uniﬁed. Some systems, including all logic programming systems, simply
omit the occur check and sometimes make unsound inferences as a result; other systems use
Storage and retrieval
Underlying the TELL and ASK functions used to inform and interrogate a knowledge base
are the more primitive STORE and FETCH functions. STORE(s) stores a sentence s into the
knowledge base and FETCH(q) returns all uniﬁers such that the query q uniﬁes with someChapter
function UNIFY(x,y,θ) returns a substitution to make x and y identical
inputs: x, a variable, constant, list, or compound expression
y, a variable, constant, list, or compound expression
θ, the substitution built up so far (optional, defaults to empty)
else if COMPOUND?(x) and COMPOUND?(y) then
return UNIFY(x.ARGS,y.ARGS, UNIFY(x.OP,y.OP,θ))
else if LIST?(x) and LIST?(y) then
return UNIFY(x.REST,y.REST, UNIFY(x.FIRST,y.FIRST,θ))
else return failure
The uniﬁcation algorithm. The algorithm works by comparing the structures
up along the way and is used to make sure that later comparisons are consistent with bindings
that were established earlier. In a compound expression such as F(A, B), the OP ﬁeld picks
out the function symbol F and the ARGS ﬁeld picks out the argument list (A, B).
sentence in the knowledge base. The problem we used to illustrate uniﬁcation—ﬁnding all
facts that unify with Knows(John, x)—is an instance of FETCHing.
The simplest way to implement STORE and FETCH is to keep all the facts in one long
list and unify each query against every element of the list. Such a process is inefﬁcient, but
it works, and it’s all you need to understand the rest of the chapter. The remainder of this
section outlines ways to make retrieval more efﬁcient; it can be skipped on ﬁrst reading.
We can make FETCH more efﬁcient by ensuring that uniﬁcations are attempted only
with sentences that have some chance of unifying. For example, there is no point in trying
to unify Knows(John, x) with Brother(Richard, John). We can avoid such uniﬁcations by
indexing the facts in the knowledge base. A simple scheme called predicate indexing puts
INDEXING
PREDICATE
INDEXING
all the Knows facts in one bucket and all the Brother facts in another. The buckets can be
stored in a hash table for efﬁcient access.
Predicate indexing is useful when there are many predicate symbols but only a few
clauses for each symbol. Sometimes, however, a predicate has many clauses. For example,
cate Employs(x, y). This would be a very large bucket with perhaps millions of employersUniﬁcation and Lifting
Employs(x,y)
Employs(x,Richard)
Employs(IBM,y)
Employs(IBM,Richard)
Employs(x,y)
Employs(John,John)
Employs(x,x)
Employs(x,John)
Employs(John,y)
(a)
(b)
(a) The subsumption lattice whose lowest node is Employs(IBM , Richard).
(b) The subsumption lattice for the sentence Employs(John, John).
and tens of millions of employees. Answering a query such as Employs(x, Richard) with
predicate indexing would require scanning the entire bucket.
For this particular query, it would help if facts were indexed both by predicate and by
second argument, perhaps using a combined hash table key. Then we could simply construct
the key from the query and retrieve exactly those facts that unify with the query. For other
queries, such as Employs(IBM , y), we would need to have indexed the facts by combining
the predicate with the ﬁrst argument. Therefore, facts can be stored under multiple index
keys, rendering them instantly accessible to various queries that they might unify with.
Given a sentence to be stored, it is possible to construct indices for all possible queries
that unify with it. For the fact Employs(IBM , Richard), the queries are
Employs(IBM , Richard)
Does IBM employ Richard?
Employs(x, Richard)
Who employs Richard?
Employs(IBM , y)
Whom does IBM employ?
Employs(x, y)
Who employs whom?
SUBSUMPTION
LATTICE
interesting properties. For example, the child of any node in the lattice is obtained from its
parent by a single substitution; and the “highest” common descendant of any two nodes is
the result of applying their most general uniﬁer. The portion of the lattice above any ground
sentences to be stored introduce still more interesting lattice structures.
The scheme we have described works very well whenever the lattice contains a small
number of nodes. For a predicate with n arguments, however, the lattice contains O(2n)
nodes. If function symbols are allowed, the number of nodes is also exponential in the size
of the terms in the sentence to be stored. This can lead to a huge number of indices. At some
point, the beneﬁts of indexing are outweighed by the costs of storing and maintaining all
the indices. We can respond by adopting a ﬁxed policy, such as maintaining indices only on
keys composed of a predicate plus each argument, or by using an adaptive policy that creates
indices to meet the demands of the kinds of queries being asked. For most AI systems, the
number of facts to be stored is small enough that efﬁcient indexing is considered a solved
problem. For commercial databases, where facts number in the billions, the problem has
been the subject of intensive study and technology development..Chapter
FORWARD CHAINING
The idea is simple: start with the atomic sentences in the knowledge base and apply Modus
Ponens in the forward direction, adding new atomic sentences, until no further inferences
Deﬁnite clauses such as Situation ⇒Response are especially useful for systems that make
inferences in response to newly arrived information. Many systems can be deﬁned this way,
and forward chaining can be implemented very efﬁciently.
are disjunctions of literals of which exactly one is positive. A deﬁnite clause either is atomic
King(x) ∧Greedy(x) ⇒Evil(x) .
King(John) .
Greedy(y) .
variables are assumed to be universally quantiﬁed. (Typically, we omit universal quantiﬁers
when writing deﬁnite clauses.) Not every knowledge base can be converted into a set of
following problem:
The law says that it is a crime for an American to sell weapons to hostile nations. The
country Nono, an enemy of America, has some missiles, and all of its missiles were sold
to it by Colonel West, who is American.
“. . . it is a crime for an American to sell weapons to hostile nations”:
American(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x) .
into two deﬁnite clauses by Existential Instantiation, introducing a new constant M1:
Owns(Nono, M1)
Missile(M1)
.
“All of its missiles were sold to it by Colonel West”:
Missile(x) ∧Owns(Nono, x) ⇒Sells(West, x, Nono) .
We will also need to know that missiles are weapons:
Missile(x) ⇒Weapon(x)Forward Chaining
and we must know that an enemy of America counts as “hostile”:
Enemy(x, America) ⇒Hostile(x) .
“West, who is American . . .”:
American(West) .
“The country Nono, an enemy of America . . .”:
Enemy(Nono, America) .
This knowledge base contains no function symbols and is therefore an instance of the class
DATALOG
clauses with no function symbols. Datalog gets its name because it can represent the type of
statements typically made in relational databases. We will see that the absence of function
symbols makes inference much easier.
ing from the known facts, it triggers all the rules whose premises are satisﬁed, adding their
conclusions to the known facts. The process repeats until the query is answered (assuming
that just one answer is required) or no new facts are added. Notice that a fact is not “new”
if it is just a renaming of a known fact. One sentence is a renaming of another if they
RENAMING
are identical except for the names of the variables. For example, Likes(x, IceCream) and
Likes(y, IceCream) are renamings of each other because they differ only in the choice of x
or y; their meanings are identical: everyone likes ice cream.
Criminal(West) is added.
at this point because every sentence that could be concluded by forward chaining is already
contained explicitly in the KB. Such a knowledge base is called a ﬁxed point of the inference
order ﬁxed point can include universally quantiﬁed atomic sentences.
application of Generalized Modus Ponens, which is sound. Second, it is complete for deﬁnite
clause knowledge bases; that is, it answers every query whose answers are entailed by any
knowledge base of deﬁnite clauses. For Datalog knowledge bases, which contain no function
symbols, the proof of completeness is fairly easy. We begin by counting the number ofChapter
α, the query, an atomic sentence
local variables: new, the new sentences inferred on each iteration
repeat until new is empty
new ←{ }
for each rule in KB do
n)
for some p′
n in KB
q′ ←SUBST(θ,q)
if q′ does not unify with some sentence already in KB or new then
add q′ to new
φ ←UNIFY(q′,α)
if φ is not fail then return φ
add new to KB
return false
rithm. On each iteration, it adds to KB all the atomic sentences that can be inferred in one
step from the implication sentences and the atomic sentences already in KB. The function
not been used before.
Hostile(Nono)
Enemy(Nono,America)
Owns(Nono,M1)
Missile(M1)
American(West)
Weapon(M1)
Criminal(West)
Sells(West,M1,Nono)
The proof tree generated by forward chaining on the crime example. The initial
facts appear at the bottom level, facts inferred on the ﬁrst iteration in the middle level, and
facts inferred on the second iteration at the top level.
possible facts that can be added, which determines the maximum number of iterations. Let k
be the maximum arity (number of arguments) of any predicate, p be the number of predicates,
and n be the number of constant symbols. Clearly, there can be no more than pnk distinct
ground facts, so after this many iterations the algorithm must have reached a ﬁxed point. Then
we can make an argument very similar to the proof of completeness for propositional forwardForward Chaining
ﬁnitely many new facts, so we need to be more careful. For the case in which an answer to
the query sentence q is entailed, we must appeal to Herbrand’s theorem to establish that the
answer, the algorithm could fail to terminate in some cases. For example, if the knowledge
base includes the Peano axioms
∀n NatNum(n) ⇒NatNum(S(n)) ,
ment with deﬁnite clauses is semidecidable.
Efﬁcient forward chaining
than for efﬁciency of operation. There are three possible sources of inefﬁciency. First, the
“inner loop” of the algorithm involves ﬁnding all possible uniﬁers such that the premise of
a rule uniﬁes with a suitable set of facts in the knowledge base. This is often called pattern
matching and can be very expensive. Second, the algorithm rechecks every rule on every
PATTERN MATCHING
iteration to see whether its premises are satisﬁed, even if very few additions are made to the
knowledge base on each iteration. Finally, the algorithm might generate many facts that are
irrelevant to the goal. We address each of these issues in turn.
Matching rules against known facts
The problem of matching the premise of a rule against the facts in the knowledge base might
seem simple enough. For example, suppose we want to apply the rule
Missile(x) ⇒Weapon(x) .
Then we need to ﬁnd all the facts that unify with Missile(x); in a suitably indexed knowledge
base, this can be done in constant time per fact. Now consider a rule such as
Missile(x) ∧Owns(Nono, x) ⇒Sells(West, x, Nono) .
Again, we can ﬁnd all the objects owned by Nono in constant time per object; then, for each
object, we could check whether it is a missile. If the knowledge base contains many objects
owned by Nono and very few missiles, however, it would be better to ﬁnd all the missiles ﬁrst
and then check whether they are owned by Nono. This is the conjunct ordering problem:
CONJUNCT
ORDERING
ﬁnd an ordering to solve the conjuncts of the rule premise so that the total cost is minimized.
would suggest ordering the conjuncts to look for missiles ﬁrst if fewer missiles than objects
are owned by Nono.Chapter
WA
NT
SA
Q
NSW
V
T
Diﬀ(wa, nt) ∧Diﬀ(wa, sa) ∧
Diﬀ(nt, q) ∧Diﬀ(nt, sa) ∧
Diﬀ(q, nsw) ∧Diﬀ(q, sa) ∧
Diﬀ(nsw, v) ∧Diﬀ(nsw, sa) ∧
Diﬀ(v, sa) ⇒Colorable()
Diﬀ(Red, Blue)
Diﬀ(Red, Green)
Diﬀ(Green, Red) Diﬀ(Green, Blue)
Diﬀ(Blue, Red)
Diﬀ(Blue, Green)
(a)
(b)
CSP expressed as a single deﬁnite clause. Each map region is represented as a variable whose
value can be one of the constants Red, Green or Blue.
The connection between pattern matching and constraint satisfaction is actually very
ample, Missile(x) is a unary constraint on x. Extending this idea, we can express every
Colorable() can be inferred only if the CSP has a solution. Because CSPs in general include
in its inner loop. There are three ways to cheer ourselves up:
simple (like the rules in our crime example) rather than large and complex (like the
both the sizes of rules and the arities of predicates are bounded by a constant and to
worry only about data complexity—that is, the complexity of inference as a function
DATA COMPLEXITY
of the number of ground facts in the knowledge base. It is easy to show that the data
complexity of forward chaining is polynomial.
• We can consider subclasses of rules for which matching is efﬁcient. Essentially every
Datalog clause can be viewed as deﬁning a CSP, so matching will be tractable just
of CSPs. For example, if the constraint graph (the graph whose nodes are variables
and whose links are constraints) forms a tree, then the CSP can be solved in linear
time. Exactly the same result holds for rule matching. For instance, if we remove SouthForward Chaining
Diﬀ(wa, nt) ∧Diﬀ(nt, q) ∧Diﬀ(q, nsw) ∧Diﬀ(nsw, v) ⇒Colorable()
algorithm, as described next.
Incremental forward chaining
example, on the second iteration, the rule
Missile(x) ⇒Weapon(x)
matches against Missile(M1) (again), and of course the conclusion Weapon(M1) is already
known so nothing happens. Such redundant rule matching can be avoided if we make the
following observation: Every new fact inferred on iteration t must be derived from at least
This is true because any inference that does not
at iteration t, we check a rule only if its premise includes a conjunct pi that uniﬁes with a fact
p′
i, but
allows the other conjuncts of the rule to match with facts from any previous iteration. This
is much more efﬁcient.
With suitable indexing, it is easy to identify all the rules that can be triggered by any
ing occurs in response to each new fact that is TELLed to the system. Inferences cascade
through the set of rules until the ﬁxed point is reached, and then the process begins again for
the next new fact.
Typically, only a small fraction of the rules in the knowledge base are actually triggered
by the addition of a given fact. This means that a great deal of redundant work is done in
ample is rather too small to show this effectively, but notice that a partial match is constructed
on the ﬁrst iteration between the rule
American(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x)
and the fact American(West). This partial match is then discarded and rebuilt on the second
iteration (when the rule succeeds). It would be better to retain and gradually complete the
partial matches as new facts arrive, rather than discarding them.
The rete algorithm3 was the ﬁrst to address this problem. The algorithm preprocesses
RETE
the set of rules in the knowledge base to construct a sort of dataﬂow network in which eachChapter
node is a literal from a rule premise. Variable bindings ﬂow through the network and are
ﬁltered out when they fail to match a literal. If two literals in a rule share a variable—for
example, Sells(x, y, z) ∧Hostile(z) in the crime example—then the bindings from each
ary literal such as Sells(x, y, z) might have to wait for bindings for the other variables to be
established before the process can continue. At any given point, the state of a rete network
captures all the partial matches of the rules, avoiding a great deal of recomputation.
PRODUCTION
SYSTEM
conﬁgurations of computer components for customers of the Digital Equipment Corporation.
It was one of the ﬁrst clear commercial successes in the emerging ﬁeld of expert systems.
Many other similar systems have been built with the same underlying technology, which has
COGNITIVE
ARCHITECTURES
against the working memory of facts. A production whose conditions are satisﬁed can add or
delete facts in working memory. In contrast to the typical situation in databases, production
systems often have many rules and relatively few facts. With suitably optimized matching
technology, some modern systems can operate in real time with tens of millions of rules.
Irrelevant facts
The ﬁnal source of inefﬁciency in forward chaining appears to be intrinsic to the approach
and also arises in the propositional context. Forward chaining makes all allowable inferences
based on the known facts, even if they are irrelevant to the goal at hand. In our crime example,
there were no rules capable of drawing irrelevant conclusions, so the lack of directedness was
not a problem. In other cases (e.g., if many rules describe the eating habits of Americans and
DEDUCTIVE
DATABASES
forward chaining as the standard inference tool rather than SQL queries. The idea is to rewrite
the rule set, using information from the goal, so that only relevant variable bindings—those
MAGIC SET
the goal is Criminal(West), the rule that concludes Criminal(x) will be rewritten to include
an extra conjunct that constrains the value of x:
Magic(x) ∧American(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x) .Backward Chaining
The fact Magic(West) is also added to the KB. In this way, even if the knowledge base
contains facts about millions of Americans, only Colonel West will be considered during the
forward inference process. The complete process for deﬁning magic sets and rewriting the
knowledge base is too complex to go into here, but the basic idea is to perform a sort of
“generic” backward inference from the goal in order to work out which variable bindings
need to be constrained. The magic sets approach can therefore be thought of as a kind of
hybrid between forward inference and backward preprocessing.
BACKWARD CHAINING
the goal, chaining through rules to ﬁnd known facts that support the proof. We describe
the basic algorithm, and then we describe how it is used in logic programming, which is the
most widely used form of automated reasoning. We also see that backward chaining has some
nally, we look at the close connection between logic programming and constraint satisfaction
problems.
goal) will be proved if the knowledge base contains a clause of the form lhs ⇒goal, where
as a clause whose lhs is the empty list. Now a query that contains variables might be proved
in multiple ways. For example, the query Person(x) could be proved with the substitution
GENERATOR
a function that returns multiple times, each time giving one possible result.
can be proved by any rule in the knowledge base, and the AND part because all the conjuncts
then, if the rhs of the clause does indeed unify with the goal, proving every conjunct in the
This means that its space requirements are linear in the size of the proof (neglecting, for
now, the space required to accumulate the solutions). It also means that backward chaining
(unlike forward chaining) suffers from problems with repeated states and incompleteness. We
will discuss these problems and some potential solutions, but ﬁrst we show how backward
chaining is used in logic programming systems.Chapter
else do
ﬁrst,rest ←FIRST(goals), REST(goals)
Hostile(Nono)
Enemy(Nono,America)
Owns(Nono,M1)
Missile(M1)
Criminal(West)
Missile(y)
Weapon(y)
Sells(West,M1,z)
American(West)
{ }
{ }
{ }
{ }
Proof tree constructed by backward chaining to prove that West is a criminal.
The tree should be read depth ﬁrst, left to right. To prove Criminal(West), we have to prove
the four conjuncts below it. Some of these are in the knowledge base, and others require
further backward chaining. Bindings for each successful uniﬁcation are shown next to the
corresponding subgoal. Note that once one subgoal in a conjunction succeeds, its substitution
originally Hostile(z), z is already bound to Nono.Backward Chaining
Logic programming
Logic programming is a technology that comes fairly close to embodying the declarative
a formal language and that problems should be solved by running inference processes on that
knowledge. The ideal is summed up in Robert Kowalski’s equation,
Algorithm = Logic + Control .
PROLOG
been written in Prolog for legal, medical, ﬁnancial, and other domains.
Prolog programs are sets of deﬁnite clauses written in a notation somewhat different
constants—the opposite of our convention for logic. Commas separate conjuncts in a clause,
and the clause is written “backwards” from what we are used to; instead of A ∧B ⇒C in
Prolog we have C :- A, B. Here is a typical example:
criminal(X) :- american(X), weapon(Y), sells(X,Y,Z), hostile(Z).
The notation [E|L] denotes a list whose ﬁrst element is E and whose rest is L. Here is a
Prolog program for append(X,Y,Z), which succeeds if list Z is the result of appending
lists X and Y:
append([],Y,Y).
append([A|X],Y,[A|Z]) :- append(X,Y,Z).
sive function that describes how to append two lists. The Prolog deﬁnition is actually much
more powerful, however, because it describes a relation that holds among three arguments,
rather than a function computed from two arguments. For example, we can ask the query
solutions
clauses are tried in the order in which they are written in the knowledge base. Some aspects
of Prolog fall outside standard logical inference:
are “proved” by executing code rather than doing further inference. For example, theChapter
base. Such predicates have no counterpart in logic and can produce confusing results—
for example, if facts are asserted in a branch of the proof tree that eventually fails.
• The occur check is omitted from Prolog’s uniﬁcation algorithm. This means that some
unsound inferences can be made; these are almost never a problem in practice.
This makes it very fast when given the right set of axioms, but incomplete when given
the wrong ones.
Prolog’s design represents a compromise between declarativeness and execution efﬁciency—
inasmuch as efﬁciency was understood at the time Prolog was designed.
Efﬁcient implementation of logic programs
The execution of a Prolog program can happen in two modes: interpreted and compiled.
with the program as the knowledge base. We say “essentially” because Prolog interpreters
contain a variety of improvements designed to maximize speed. Here we consider only two.
First, our implementation had to explicitly manage the iteration over possible results
generated by each of the subfunctions.
Prolog interpreters have a global data structure,
a stack of choice points, to keep track of the multiple possibilities that we considered in
CHOICE POINT
the debugger can move up and down the stack.
ating substitutions. Instead of explicitly constructing substitutions, Prolog has logic variables
ther is unbound or is bound to some value. Together, these variables and values implicitly
deﬁne the substitution for the current branch of the proof. Extending the path can only add
new variable bindings, because an attempt to add a different binding for an already bound
variable results in a failure of uniﬁcation. When a path in the search fails, Prolog will back
up to a previous choice point, and then it might have to unbind some variables. This is done
by keeping track of all the variables that have been bound in a stack called the trail. As each
TRAIL
and it is time to back up to a previous choice point, each of the variables is unbound as it is
removed from the trail.
tions per inference step because of the cost of index lookup, uniﬁcation, and building the
gram before; for example, it has to ﬁnd clauses that match the goal. A compiled PrologBackward Chaining
procedure APPEND(ax,y,az,continuation)
if ax = [ ] and UNIFY(y,az) then CALL(continuation)
if UNIFY(ax,[a | x]) and UNIFY(az,[a | z]) then APPEND(x,y,z,continuation)
Pseudocode representing the result of compiling the Append predicate. The
The procedure CALL(continuation) continues execution with the speciﬁed continuation.
program, on the other hand, is an inference procedure for a speciﬁc set of clauses, so it knows
what clauses match the goal. Prolog basically generates a miniature theorem prover for each
The instruction sets of today’s computers give a poor match with Prolog’s semantics,
chine language. The most popular intermediate language is the Warren Abstract Machine,
piler. The WAM is an abstract instruction set that is suitable for Prolog and can be either
level language such as Lisp or C and then use that language’s compiler to translate to machine
language. For example, the deﬁnition of the Append predicate can be compiled into the code
come a procedure and the inferences are carried out simply by calling the procedure.
• As described earlier, the current variable bindings are kept on a trail. The ﬁrst step of the
if the ﬁrst clause fails. This will undo any bindings generated by the ﬁrst call to UNIFY.
• The trickiest part is the use of continuations to implement choice points. You can think
CONTINUATION
of a continuation as packaging up a procedure and a list of arguments that together
deﬁne what should be done next whenever the current goal succeeds. It would not
do just to return from a procedure like APPEND when the goal succeeds, because it
could succeed in several ways, and each of them has to be explored. The continuation
argument solves this problem because it can be called each time the goal succeeds. In
the APPEND code, if the ﬁrst argument is empty and the second argument uniﬁes with
the third, then the APPEND predicate has succeeded. We then CALL the continuation,
with the appropriate bindings on the trail, to do whatever should be done next. For
example, if the call to APPEND were at the top level, the continuation would print the
bindings of the variables.Chapter
Before Warren’s work on the compilation of inference in Prolog, logic programming was
too slow for general use. Compilers by Warren and others allowed Prolog code to achieve
Of course, the fact that one can write a planner or natural language parser in a few dozen
research projects.
Parallelization can also provide substantial speedup. There are two principal sources of
with many different clauses in the knowledge base. Each gives rise to an independent branch
in the search space that can lead to a potential solution, and all such branches can be solved
achieve, because solutions for the whole conjunction require consistent bindings for all the
variables. Each conjunctive branch must communicate with the other branches to ensure a
global solution.
Redundant inference and inﬁnite loops
gram that decides if a path exists between two points on a directed graph:
path(X,Z) :- link(X,Z).
path(X,Z) :- path(X,Y), link(Y,Z).
path(X,Z) :- path(X,Y), link(Y,Z).
path(X,Z) :- link(X,Z).
as a theorem prover for deﬁnite clauses—even for Datalog programs, as this example shows—
because, for some knowledge bases, it fails to prove sentences that are entailed. Notice that
forward chaining does not suffer from this problem: once path(a,b), path(b,c), and
path(a,c) are inferred, forward chaining halts.
most of which involve ﬁnding all possible paths to nodes from which the goal is unreachable.
inference can be exponential in the number of ground facts that are generated. If we apply
forward chaining instead, at most n2 path(X,Y) facts can be generated linking n nodes.
Forward chaining on graph search problems is an example of dynamic programming,
DYNAMIC
PROGRAMMING
in which the solutions to subproblems are constructed incrementally from those of smallerBackward Chaining
(a)
(b)
A
B
C
A1
J4
(a) Finding a path from A to C can lead Prolog into an inﬁnite loop. (b) A
graph in which each node is connected to two random successors in the next layer. Finding a
path(a,c)
fail
{
}
/
Y b
{ }
link(a,c)
path(a,Y)
link(a,Y)
link(b,c)
path(a,c)
path(a,Y)
link(Y,c)
path(a,Y’)
link(Y’,Y)
(a)
(b)
(a) Proof that a path exists from A to C. (b) Inﬁnite proof tree generated
when the clauses are in the “wrong” order.
subproblems and are cached to avoid recomputation. We can obtain a similar effect in a
backward chaining system using memoization—that is, caching solutions to subgoals as
TABLED LOGIC
PROGRAMMING
tems, which use efﬁcient storage and retrieval mechanisms to perform memoization. Tabled
programming efﬁciency of forward chaining. It is also complete for Datalog knowledge
sible to get an inﬁnite loop with predicates like father(X,Y) that refer to a potentially
unbounded number of objects.)
Database semantics of Prolog
says that every Prolog constant and every ground term refers to a distinct object, and the
closed world assumption says that the only sentences that are true are those that are entailedChapter
by the knowledge base. There is no way to assert that a sentence is false in Prolog. This makes
and more concise. Consider the following Prolog assertions about some course offerings:
are no other courses, so there are exactly four courses. But if these were assertions in FOL
rather than in Prolog, then all we could say is that there are somewhere between one and
inﬁnity courses. That’s because the assertions (in FOL) do not deny the possibility that other
unmentioned courses are also offered, nor do they say that the courses mentioned are different
Course(d, n)
⇔
COMPLETION
at most four courses. To express in FOL the idea that there are at least four courses, we need
to write the completion of the equality predicate:
x = y
⇔
The completion is useful for understanding database semantics, but for practical purposes, if
your problem can be described with database semantics, it is more efﬁcient to reason with
soning with a full FOL theorem prover.
Constraint logic programming
problems (CSPs) can be encoded as deﬁnite clauses. Standard Prolog solves such problems
domain CSPs. In Prolog terms, there must be a ﬁnite number of solutions for any goal
with unbound variables. (For example, the goal diff(Q,SA), which says that Queensland
and South Australia must be different colors, has six solutions if three colors are allowed.)
ferent algorithms, such as bounds propagation or linear programming.
Consider the following example. We deﬁne triangle(X,Y,Z) as a predicate that
holds if the three arguments are numbers that satisfy the triangle inequality:
triangle(X,Y,Z) :-Resolution
Constraint logic programming (CLP) allows variables to be constrained rather than
CONSTRAINT LOGIC
PROGRAMMING
bound. A CLP solution is the most speciﬁc set of constraints on the query variables that can
CLP in which the solution constraints must be equality constraints—that is, bindings.
variables might include a linear programming algorithm for solving those constraints. CLP
systems also adopt a much more ﬂexible approach to solving standard logic programming
backjumping, cutset conditioning, and so on. CLP systems therefore combine elements of
constraint satisfaction algorithms, logic programming, and deductive databases.
allows the programmer to write metarules to determine which conjuncts are tried ﬁrst. The
METARULE
user could write a rule saying that the goal with the fewest variables should be tried ﬁrst or
RESOLUTION
normal form (CNF)—that is, a conjunction of clauses, where each clause is a disjunction of
example, the sentence
∀x American(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x)
becomes, in CNF,
¬American(x) ∨¬Weapon(y) ∨¬Sells(x, y, z) ∨¬Hostile(z) ∨Criminal(x) .
sentence. In particular, the CNF sentence will be unsatisﬁable just when the original sentence
is unsatisﬁable, so we have a basis for doing proofs by contradiction on the CNF sentences.Chapter
The procedure for conversion to CNF is similar to the propositional case, which we saw
We illustrate the procedure by translating the sentence “Everyone who loves all animals is
loved by someone,” or
∀x [∀y Animal(y) ⇒Loves(x, y)] ⇒[∃y Loves(y, x)] .
The steps are as follows:
• Eliminate implications:
∀x [¬∀y ¬Animal(y) ∨Loves(x, y)] ∨[∃y Loves(y, x)] .
• Move ¬ inwards: In addition to the usual rules for negated connectives, we need rules
for negated quantiﬁers. Thus, we have
¬∀x p
becomes
∃x ¬p
¬∃x p
becomes
∀x ¬p .
Our sentence goes through the following transformations:
∀x [∃y ¬(¬Animal(y) ∨Loves(x, y))] ∨[∃y Loves(y, x)] .
∀x [∃y ¬¬Animal(y) ∧¬Loves(x, y)] ∨[∃y Loves(y, x)] .
∀x [∃y Animal(y) ∧¬Loves(x, y)] ∨[∃y Loves(y, x)] .
Notice how a universal quantiﬁer (∀y) in the premise of the implication has become
an existential quantiﬁer. The sentence now reads “Either there is some animal that x
doesn’t love, or (if this is not the case) someone loves x.” Clearly, the meaning of the
original sentence has been preserved.
• Standardize variables: For sentences like (∃x P(x))∨(∃x Q(x)) which use the same
variable name twice, change the name of one of the variables. This avoids confusion
later when we drop the quantiﬁers. Thus, we have
∀x [∃y Animal(y) ∧¬Loves(x, y)] ∨[∃z Loves(z, x)] .
SKOLEMIZATION
only parts of the sentence match the pattern. If we blindly apply the rule to the two
matching parts we get
∀x [Animal(A) ∧¬Loves(x, A)] ∨Loves(B, x) ,
ticular animal A or is loved by some particular entity B. In fact, our original sentence
allows each person to fail to love a different animal or to be loved by a different person.
Thus, we want the Skolem entities to depend on x and z:
∀x [Animal(F(x)) ∧¬Loves(x, F(x))] ∨Loves(G(z), x) .
Here F and G are Skolem functions. The general rule is that the arguments of the
SKOLEM FUNCTION
tential quantiﬁer appears. As with Existential Instantiation, the Skolemized sentence is
satisﬁable exactly when the original sentence is satisﬁable.Resolution
• Drop universal quantiﬁers: At this point, all remaining variables must be universally
tiﬁers have been moved to the left. We can therefore drop the universal quantiﬁers:
[Animal(F(x)) ∧¬Loves(x, F(x))] ∨Loves(G(z), x) .
• Distribute ∨over ∧:
[Animal(F(x)) ∨Loves(G(z), x)] ∧[¬Loves(x, F(x)) ∨Loves(G(z), x)] .
This step may also require ﬂattening out nested conjunctions and disjunctions.
The sentence is now in CNF and consists of two clauses. It is quite unreadable. (It may
help to explain that the Skolem function F(x) refers to the animal potentially unloved by x,
whereas G(z) refers to someone who might love x.) Fortunately, humans seldom need look
at CNF sentences—the translation process is easily automated.
The resolution inference rule
complementary if one uniﬁes with the negation of the other. Thus, we have
ℓ1 ∨· · · ∨ℓk,
m1 ∨· · · ∨mn
where UNIFY(ℓi, ¬mj) = θ. For example, we can resolve the two clauses
[Animal(F(x)) ∨Loves(G(x), x)]
and
[¬Loves(u, v) ∨¬Kills(u, v)]
by eliminating the complementary literals Loves(G(x), x) and ¬Loves(u, v), with uniﬁer
[Animal(F(x)) ∨¬Kills(G(x), x)] .
This rule is called the binary resolution rule because it resolves exactly two literals. The
BINARY RESOLUTION
lution rule resolves subsets of literals in each clause that are uniﬁable. An alternative approach
two literals to one if they are uniﬁable. The uniﬁer must be applied to the entire clause. The
combination of binary resolution and factoring is complete.
Example proofs
Resolution proves that KB |= α by proving KB ∧¬α unsatisﬁable, that is, by deriving the
empty clause. The algorithmic approach is identical to the propositional case, described inChapter
¬American(x)    ¬Weapon(y)    ¬Sells(x,y,z)   ¬Hostile(z)   Criminal(x)
¬Criminal(West)
¬Enemy(Nono,America)
Enemy(Nono,America)
¬Missile(x)   Weapon(x)
¬Weapon(y)   ¬Sells(West,y,z)   ¬Hostile(z)
Missile(M1)
¬Missile(y)   ¬Sells(West,y,z)   ¬Hostile(z)
¬Missile(x)   ¬Owns(Nono,x)    Sells(West,x,Nono)
¬Sells(West,M1,z)   ¬Hostile(z)
¬American(West)   ¬Weapon(y)   ¬Sells(West,y,z)   ¬Hostile(z)
American(West)
¬Missile(M1)   ¬Owns(Nono,M1)   ¬Hostile(Nono)
Missile(M1)
¬Owns(Nono,M1)   ¬Hostile(Nono)
Owns(Nono,M1)
¬Enemy(x,America)   Hostile(x)
¬Hostile(Nono)
^
^
^
^
^
^
^
^
^
^
^
^
^
^
^
^
^
^
^
A resolution proof that West is a criminal. At each step, the literals that unify
are in bold.
¬American(x) ∨¬Weapon(y) ∨¬Sells(x, y, z) ∨¬Hostile(z) ∨Criminal(x)
¬Missile(x) ∨¬Owns(Nono, x) ∨Sells(West, x, Nono)
¬Enemy(x, America) ∨Hostile(x)
¬Missile(x) ∨Weapon(x)
Owns(Nono, M1)
Missile(M1)
American(West)
Enemy(Nono, America) .
clauses from the knowledge base until the empty clause is generated. This is characteristic
of resolution on Horn clause knowledge bases. In fact, the clauses along the main spine
positive literal uniﬁed with the leftmost literal of the “current” clause on the spine; this is
exactly what happens in backward chaining. Thus, backward chaining is just a special case
of resolution with a particular control strategy to decide which resolution to perform next.
inite clauses. This results in a somewhat more complex proof structure. In English, the
problem is as follows:
Everyone who loves all animals is loved by someone.
Anyone who kills an animal is loved by no one.
Jack loves all animals.
Either Jack or Curiosity killed the cat, who is named Tuna.
Did Curiosity kill the cat?Resolution
First, we express the original sentences, some background knowledge, and the negated goal
A.
∀x [∀y Animal(y) ⇒Loves(x, y)] ⇒[∃y Loves(y, x)]
B.
∀x [∃z Animal(z) ∧Kills(x, z)] ⇒[∀y ¬Loves(y, x)]
C.
∀x Animal(x) ⇒Loves(Jack, x)
D.
Kills(Jack, Tuna) ∨Kills(Curiosity, Tuna)
E.
Cat(Tuna)
F.
∀x Cat(x) ⇒Animal(x)
¬G.
¬Kills(Curiosity, Tuna)
Now we apply the conversion procedure to convert each sentence to CNF:
A1.
Animal(F(x)) ∨Loves(G(x), x)
A2.
¬Loves(x, F(x)) ∨Loves(G(x), x)
B.
¬Loves(y, x) ∨¬Animal(z) ∨¬Kills(x, z)
C.
¬Animal(x) ∨Loves(Jack, x)
D.
Kills(Jack, Tuna) ∨Kills(Curiosity, Tuna)
E.
Cat(Tuna)
F.
¬Cat(x) ∨Animal(x)
¬G.
¬Kills(Curiosity, Tuna)
could be paraphrased as follows:
Suppose Curiosity did not kill Tuna. We know that either Jack or Curiosity did; thus
Jack must have. Now, Tuna is a cat and cats are animals, so Tuna is an animal. Because
anyone who kills an animal is loved by no one, we know that no one loves Jack. On the
other hand, Jack loves all animals, so someone loves him; so we have a contradiction.
Therefore, Curiosity killed the cat.
¬Loves(y, Jack)
Loves(G(Jack), Jack)
¬Kills(Curiosity, Tuna)
Kills(Jack, Tuna)   Kills(Curiosity, Tuna)
¬Cat(x)   Animal(x)
Cat(Tuna)
¬Animal(F(Jack))   Loves(G(Jack), Jack)
Animal(F(x))   Loves(G(x), x) 
¬Loves(y, x)   ¬Kills(x, Tuna)
Kills(Jack, Tuna)
¬Loves(y, x)   ¬Animal(z)   ¬Kills(x, z)
Animal(Tuna)
¬Loves(x,F(x))   Loves(G(x), x)
¬Animal(x)   Loves(Jack, x)
^
^
^
^
^
^
^
^
^
A resolution proof that Curiosity killed the cat. Notice the use of factoring
in the derivation of the clause Loves(G(Jack), Jack). Notice also in the upper right, the
uniﬁcation of Loves(x, F(x)) and Loves(Jack, x) can only succeed after the variables have
been standardized apart.Chapter
The proof answers the question “Did Curiosity kill the cat?” but often we want to pose more
general questions, such as “Who killed the cat?” Resolution can do this, but it takes a little
steps. So, in this case, ﬁnding out who killed the cat is just a matter of keeping track of the
bindings for the query variables in the proof.
Unfortunately, resolution can produce nonconstructive proofs for existential goals.
NONCONSTRUCTIVE
PROOF
For example, ¬Kills(w, Tuna) resolves with Kills(Jack, Tuna) ∨Kills(Curiosity, Tuna)
to give Kills(Jack, Tuna), which resolves again with ¬Kills(w, Tuna) to yield the empty
clause. Notice that w has two different bindings in this proof; resolution is telling us that,
lution is to restrict the allowed resolution steps so that the query variables can be bound
ANSWER LITERAL
comes ¬Kills(w, Tuna) ∨Answer(w). Now, the resolution process generates an answer
Answer(Curiosity) ∨Answer(Jack), which does not constitute an answer.
Completeness of resolution
This section gives a completeness proof of resolution. It can be safely skipped by those who
are willing to take it on faith.
REFUTATION
COMPLETENESS
is unsatisﬁable, then resolution will always be able to derive a contradiction. Resolution
cannot be used to generate all logical consequences of a set of sentences, but it can be used
to establish that a given sentence is entailed by the set of sentences. Hence, it can be used to
ﬁnd all answers to a given question, Q(x), by proving that KB ∧¬Q(x) is unsatisﬁable.
ten as a set of clauses in CNF. This can be proved by induction on the form of the sentence,
prove the following: if S is an unsatisﬁable set of clauses, then the application of a ﬁnite
number of resolution steps to S will yield a contradiction.
Our proof sketch follows Robinson’s original proof with some simpliﬁcations from
instances of the clauses of S such that this set is also unsatisﬁable (Herbrand’s theorem).
propositional resolution is complete for ground sentences.Resolution
Resolution can find a contradiction in S'
There is a resolution proof for the contradiction in S'
Herbrand’s theorem
Some set S' of ground instances is unsatisfiable
Any set of sentences S is representable in clausal form
Assume S is unsatisfiable, and in clausal form
Lifting lemma
Ground resolution
theorem
Structure of a completeness proof for resolution.
To carry out the ﬁrst step, we need three new concepts:
• Herbrand universe: If S is a set of clauses, then HS, the Herbrand universe of S, is
HERBRAND
UNIVERSE
the set of all ground terms constructable from the following:
a. The function symbols in S, if any.
b. The constant symbols in S, if any; if none, then the constant symbol A.
For example, if S contains just the clause ¬P(x, F(x, A)) ∨¬Q(x, A) ∨R(x, B), then
HS is the following inﬁnite set of ground terms:
{A, B, F(A, A), F(A, B), F(B, A), F(B, B), F(A, F(A, A)), . . .} .
• Saturation: If S is a set of clauses and P is a set of ground terms, then P(S), the
SATURATION
saturation of S with respect to P, is the set of all ground clauses obtained by applying
all possible consistent substitutions of ground terms in P with variables in S.
HERBRAND BASE
verse is called the Herbrand base of S, written as HS(S). For example, if S contains
solely the clause just given, then HS(S) is the inﬁnite set of clauses
{¬P(A, F(A, A)) ∨¬Q(A, A) ∨R(A, B),
¬P(B, F(B, A)) ∨¬Q(B, A) ∨R(B, B),
¬P(F(A, A), F(F(A, A), A)) ∨¬Q(F(A, A), A) ∨R(F(A, A), B),
¬P(F(A, B), F(F(A, B), A)) ∨¬Q(F(A, B), A) ∨R(F(A, B), B), . . . }
HERBRAND’S
THEOREM
If a set S of clauses is unsatisﬁable, then there exists a ﬁnite subset of HS(S) that
is also unsatisﬁable.
Let S′ be this ﬁnite subset of ground sentences. Now, we can appeal to the ground resolution
That is, running propositional resolution to completion on S′ will derive a contradiction.
Now that we have established that there is always a resolution proof involving some
ﬁnite subset of the Herbrand base of S, the next step is to show that there is a resolutionChapter
G ¨ODEL’S INCOMPLETENESS THEOREM
pleteness theorem, that there are true arithmetic sentences that cannot be proved.
The proof of the incompleteness theorem is somewhat beyond the scope of
begin with the logical theory of numbers. In this theory, there is a single constant,
the natural numbers. The vocabulary also includes the function symbols +, ×, and
Expt (exponentiation) and the usual set of logical connectives and quantiﬁers. The
ﬁrst step is to notice that the set of sentences that we can write in this language can
be enumerated. (Imagine deﬁning an alphabetical order on the symbols and then
so on.) We can then number each sentence α with a unique natural number #α
(the G¨odel number). This is crucial: number theory contains a name for each of
its own sentences. Similarly, we can number each possible proof P with a G¨odel
number G(P), because a proof is simply a ﬁnite sequence of sentences.
Now suppose we have a recursively enumerable set A of sentences that are
true statements about the natural numbers. Recalling that A can be named by a
given set of integers, we can imagine writing in our language a sentence α(j, A) of
the following sort:
∀i i is not the G¨odel number of a proof of the sentence whose G¨odel
number is j, where the proof uses only premises in A.
ability from A. (That this sentence always exists is true but not entirely obvious.)
Now we make the following ingenious argument: Suppose that σ is provable
from A; then σ is false (because σ says it cannot be proved). But then we have a
false sentence that is provable from A, so A cannot consist of only true sentences—
a violation of our premise. Therefore, σ is not provable from A. But this is exactly
what σ itself claims; hence σ is a true sentence.
tences that cannot be proved from those axioms. This establishes, among other
things, that we can never prove all the theorems of mathematics within any given
system of axioms. Clearly, this was an important discovery for mathematics. Its
signiﬁcance for AI has been widely debated, beginning with speculations by G¨odelResolution
proof using the clauses of S itself, which are not necessarily ground clauses. We start by
considering a single application of the resolution rule. Robinson stated this lemma:
Let C1 and C2 be two clauses with no shared variables, and let C′
ground instances of C1 and C2. If C′ is a resolvent of C′
instance of C.
This is called a lifting lemma, because it lifts a proof step from ground clauses up to general
LIFTING LEMMA
cation and derive all of the properties of most general uniﬁers. Rather than repeat the proof
here, we simply illustrate the lemma:
C1 = ¬P(x, F(x, A)) ∨¬Q(x, A) ∨R(x, B)
C2 = ¬N(G(y), z) ∨P(H(y), z)
C′
C′
C′ = ¬N(G(B), F(H(B), A)) ∨¬Q(H(B), A) ∨R(H(B), B)
C = ¬N(G(y), F(H(y), A)) ∨¬Q(H(y), A) ∨R(H(y), B) .
We see that indeed C′ is a ground instance of C. In general, for C′
resolvents, they must be constructed by ﬁrst applying to C1 and C2 the most general uniﬁer
of a pair of complementary literals in C1 and C2. From the lifting lemma, it is easy to derive
a similar statement about any sequence of applications of the resolution rule:
tion closure of S such that C′ is a ground instance of C and the derivation of C is
the same length as the derivation of C′.
From this fact, it follows that if the empty clause appears in the resolution closure of S′, it
must also appear in the resolution closure of S. This is because the empty clause cannot be a
ground instance of any other clause. To recap: we have shown that if S is unsatisﬁable, then
there is a ﬁnite derivation of the empty clause using the resolution rule.
required to examine a huge number of arbitrary instantiations.
Equality
None of the inference methods described so far in this chapter handle an assertion of the form
x = y. Three distinct approaches can be taken. The ﬁrst approach is to axiomatize equality—
to write down sentences about the equality relation in the knowledge base. We need to say that
equality is reﬂexive, symmetric, and transitive, and we also have to say that we can substitute
equals for equals in any predicate or function. So we need three basic axioms, and then oneChapter
for each predicate and function:
∀x x = x
∀x, y x = y ⇒y = x
∀x, y, z x = y ∧y = z ⇒x = z
∀x, y x = y ⇒(P1(x) ⇔P1(y))
∀x, y x = y ⇒(P2(x) ⇔P2(y))
...
∀w, x, y, z w = y ∧x = z ⇒(F1(w, x) = F1(y, z))
∀w, x, y, z w = y ∧x = z ⇒(F2(w, x) = F2(y, z))
...
Given these sentences, a standard inference procedure such as resolution can perform tasks
requiring equality reasoning, such as solving mathematical equations. However, these axioms
will generate a lot of conclusions, most of them not helpful to a proof. So there has been a
search for more efﬁcient ways of handling equality. One alternative is to add inference rules
rather than axioms. The simplest rule, demodulation, takes a unit clause x = y and some
clause α that contains the term x, and yields a new clause formed by substituting y for x
within α. It works if the term within α uniﬁes with x; it need not be exactly equal to x.
Note that demodulation is directional; given x = y, the x always gets replaced with y, never
vice versa. That means that demodulation can be used for simplifying expressions using
Father(Father(x)) = PaternalGrandfather (x)
we can conclude by demodulation
More formally, we have
• Demodulation: For any terms x, y, and z, where z appears somewhere in literal mi
DEMODULATION
and where UNIFY(x, z) = θ,
x = y,
m1 ∨· · · ∨mn
SUB(SUBST(θ, x), SUBST(θ, y), m1 ∨· · · ∨mn) .
where SUBST is the usual substitution of a binding list, and SUB(x, y, m) means to
replace x with y everywhere that x occurs within m.
• Paramodulation: For any terms x, y, and z, where z appears somewhere in literal mi,
PARAMODULATION
and where UNIFY(x, z) = θ,
ℓ1 ∨· · · ∨ℓk ∨x = y,
m1 ∨· · · ∨mn
SUB(SUBST(θ, x), SUBST(θ, y), SUBST(θ, ℓ1 ∨· · · ∨ℓk ∨m1 ∨· · · ∨mn) .
For example, from
P(F(x, B), x) ∨Q(x)
and
F(A, y) = y ∨R(y)Resolution
lation the sentence
P(B, A) ∨Q(A) ∨R(B) .
A third approach handles equality reasoning entirely within an extended uniﬁcation
algorithm. That is, terms are uniﬁable if they are provably equal under some substitution,
normally are not uniﬁable, but a uniﬁcation algorithm that knows that x + y = y + x could
unify them with the empty substitution. Equational uniﬁcation of this kind can be done with
EQUATIONAL
UNIFICATION
efﬁcient algorithms designed for the particular axioms used (commutativity, associativity, and
so on) rather than through explicit inference with those axioms. Theorem provers using this
Resolution strategies
We know that repeated applications of the resolution inference rule will eventually ﬁnd a
proof if one exists. In this subsection, we examine strategies that help ﬁnd proofs efﬁciently.
Unit preference: This strategy prefers to do resolutions where one of the sentences is a single
UNIT PREFERENCE
literal (also known as a unit clause). The idea behind the strategy is that we are trying to
produce an empty clause, so it might be a good idea to prefer inferences that produce shorter
clauses. Resolving a unit sentence (such as P) with any other sentence (such as ¬P ∨¬Q∨R)
always yields a clause (in this case, ¬Q ∨R) that is shorter than the other clause. When
dramatic speedup, making it feasible to prove theorems that could not be handled without the
preference. Unit resolution is a restricted form of resolution in which every resolution step
must involve a unit clause. Unit resolution is incomplete in general, but complete for Horn
clauses. Unit resolution proofs on Horn clauses resemble forward chaining.
the “weight” of each clause, where lighter clauses are preferred. The exact choice of heuristic
is up to the user, but generally, the weight of a clause should be correlated with its size or
difﬁculty. Unit clauses are treated as light; the search can thus be seen as a generalization of
the unit preference strategy.
Set of support: Preferences that try certain resolutions ﬁrst are helpful, but in general it is
SET OF SUPPORT
more effective to try to eliminate some potential resolutions altogether. For example, we can
insist that every resolution step involve at least one element of a special set of clauses—the
set of support. The resolvent is then added into the set of support. If the set of support is
small relative to the whole knowledge base, the search space will be reduced dramatically.
We have to be careful with this approach because a bad choice for the set of support
will make the algorithm incomplete. However, if we choose the set of support S so that the
For example, one can use the negated query as the set of support, on the assumption that theChapter
original knowledge base is consistent. (After all, if it is not consistent, then the fact that the
Input resolution: In this strategy, every resolution combines one of the input sentences (from
INPUT RESOLUTION
tences combining onto the spine. Clearly, the space of proof trees of this shape is smaller
than the space of all proof graphs. In Horn knowledge bases, Modus Ponens is a kind of
input resolution strategy, because it combines an implication from the original KB with some
other sentences. Thus, it is no surprise that input resolution is complete for knowledge bases
that are in Horn form, but incomplete in the general case. The linear resolution strategy is a
LINEAR RESOLUTION
slight generalization that allows P and Q to be resolved together either if P is in the original
KB or if P is an ancestor of Q in the proof tree. Linear resolution is complete.
Subsumption: The subsumption method eliminates all sentences that are subsumed by (that
SUBSUMPTION
is, more speciﬁc than) an existing sentence in the KB. For example, if P(x) is in the KB, then
there is no sense in adding P(A) and even less sense in adding P(A) ∨Q(B). Subsumption
helps keep the KB small and thus helps keep the search space small.
Practical uses of resolution theorem provers
Theorem provers can be applied to the problems involved in the synthesis and veriﬁcation
SYNTHESIS
VERIFICATION
hardware design, programming languages, and software engineering—not just in AI.
In the case of software, reasoning about programs is quite similar to reasoning about
The formal synthesis of algorithms was one of the ﬁrst uses of theorem provers, as outlined
is to constructively prove a theorem to the effect that “there exists a program p satisfying a
certain speciﬁcation.” Although fully automated deductive synthesis, as it is called, has not
DEDUCTIVE
SYNTHESIS
purpose programs, such as scientiﬁc computing code, is also an active area of research.
Similar techniques are now being applied to software veriﬁcation by systems such as theSummary
SUMMARY
rithms for doing it.
tiation) to propositionalize the inference problem. Typically, this approach is slow,
unless the domain is small.
• The use of uniﬁcation to identify appropriate substitutions for variables eliminates the
• A lifted version of Modus Ponens uses uniﬁcation to provide a natural and powerful
chaining algorithms apply this rule to sets of deﬁnite clauses.
• Generalized Modus Ponens is complete for deﬁnite clauses, although the entailment
deﬁnite clauses, entailment is decidable.
lational database operations. It is also used in production systems, which perform
efﬁcient updates with very large rule sets. Forward chaining is complete for Datalog
and runs in polynomial time.
cated compiler technology to provide very fast inference. Backward chaining suffers
from redundant inferences and inﬁnite loops; these can be alleviated by memoization.
and negation as failure. These make Prolog a more practical programming language,
but bring it further from pure logic.
order logic, using knowledge bases in conjunctive normal form.
• Several strategies exist for reducing the search space of a resolution system without
compromising completeness. One of the most important issues is dealing with equality;
we showed how demodulation and paramodulation can be used.
matical theorems and to verify and synthesize software and hardware.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
on a collection of valid schemas plus a single inference rule, Modus Ponens. WhiteheadChapter
developed the key idea of letting the quest for propositional inconsistency drive the search,
lish propositional inconsistency. After further development by other researchers, this idea led
tures that would allow efﬁcient retrieval of facts; this work is covered in AI programming
olution. AI applications typically involved large numbers of rules, so it was important to
nology for production systems was developed to support such applications. The production
RETE
brought together experts in logical inference and database systems (Gallaire and Minker,
of Datalog as a standard language for deductive databases. The development of the magic sets
log for the purpose of parsing natural language—Prolog’s clauses were intended initially
oretical background for logic programming was developed by Robert Kowalski, workingBibliographical and Historical Notes
overview. Efﬁcient Prolog compilers are generally based on the Warren Abstract Machine
showed that Prolog programs can be competitive with C programs in terms of speed.
Methods for avoiding unnecessary looping in recursive logic programs were developed
included memoization for logic programs, a method developed extensively as tabled logic
to handle tabling, enabling Datalog programs to execute an order of magnitude faster than
gramming was the journal of record; it has now been replaced by Theory and Practice of
Logic Programming. Logic programming conferences include the International Conference
on Logic Programming (ICLP) and the International Logic Programming Symposium (ILPS).
Research into mathematical theorem proving began even before the ﬁrst complete
A number of control strategies have been proposed for resolution, beginning with the
thorough analysis of a wide variety of control strategies.
theorem provers. The most successful theorem prover in recent annual competitions has beenChapter
ness. Theorem provers have been used to automatically synthesize and verify software for
Theorem provers have come up with novel mathematical results that eluded human
AURA program has also answered open questions in several areas of mathematics (Wos and
Natarajan Shankar to give the ﬁrst fully rigorous formal proof of G¨odel’s Incompleteness
set of axioms—the Robbins algebra—that appeared to deﬁne Boolean algebra, but no proof
ROBBINS ALGEBRA
Many early papers in mathematical logic are to be found in From Frege to G¨odel:
mated Deduction (CADE) and the International Joint Conference on Automated Reasoning
of theorem proving for the popular audience.
EXERCISES
Prove that Universal Instantiation is sound and that Existential Instantiation produces
an inferentially equivalent knowledge base.
Write down a general inference rule, Existential Introduction, that sanctions this inference.
EXISTENTIAL
INTRODUCTION
State carefully the conditions that must be satisﬁed by the variables and terms involved.Chapter
∃y
stant rather than by a Skolem function of x. Show that an inference engine that uses
P(q, q) to be inferred from the premise
d. A common error among students is to suppose that, in uniﬁcation, one is allowed to
substitute a term for a Skolem constant instead of for a variable. For instance, they will
Give an example where this leads to an invalid inference.
Suppose you are given the following axioms:
only the axioms given here, not anything else you may know about arithmetic.) Show
only the steps that leads to success, not the irrelevant steps.
that lead to success.
A popular children’s riddle is “Brothers and sisters have I none, but that man’s father
who that man is. You may apply any of the inference methods described in this chapter. Why
do you think that this riddle is difﬁcult?
curity numbers as identifying constants for each person. Thus, George’s age is given by
solution for which of the queries Q1–Q4 (assuming normal backward chaining)?
• S1: an index for each atom in each position.
• S2: an index for each ﬁrst argument.
• S3: an index for each predicate atom.
• S4: an index for each combination of predicate and ﬁrst argument.Chapter
ming. A rewrite rule (or demodulator in OTTER terminology) is an equation with a speciﬁed
ing systems. Use the predicate rewrite(X,Y) to represent rewrite rules. For example, the
a. Write a deﬁnition of a predicate simplify(X,Y), that is true when Y is a simpliﬁed
version of X—that is, when no further rewrite rules apply to any subexpression of Y.
b. Write a collection of rules for the simpliﬁcation of expressions involving arithmetic
operators, and apply your simpliﬁcation algorithm to some sample expressions.
c. Write a collection of rewrite rules for symbolic differentiation, and use them along with
your simpliﬁcation rules to differentiate and simplify expressions involving arithmetic
expressions, including exponentiation.
This exercise considers the implementation of search algorithms in Prolog. Suppose
that successor(X,Y) is true when state Y is a successor of state X; and that goal(X)
is true when X is a goal state. Write a deﬁnition for solve(X,P), which means that P is a
path (list of states) beginning with X, ending in a goal state, and consisting of a sequence of
to do this. How easy would it be to add heuristic search control?
Ancestor(Mother(x), x)
Ancestor(x, y) ∧Ancestor(y, z) ⇒Ancestor(x, z)
Consider a forward chaining algorithm that, on the jth iteration, terminates if the KB contains
a sentence that uniﬁes with the query, else adds to the KB every atomic sentence that can be
(i) Ancestor(Mother(y), John)
(ii) Ancestor(Mother(Mother(y)), John)
(iii) Ancestor(Mother(Mother(Mother(y))), Mother(y))
(iv) Ancestor(Mother(John), Mother(Mother(John)))
inal knowledge base? Explain how, or why not.
gorithm with inference rules for equality. Now what is the answer to (b)?
Assume a domain of people.CLASSICAL PLANNING
In which we see how an agent can take advantage of the structure of a problem to
construct complex plans of action.
We have deﬁned AI as the study of rational action, which means that planning—devising a
plan of action to achieve one’s goals—is a critical part of AI. We have seen two examples
problems that scales up to problems that could not be handled by those earlier approaches.
take advantage of this representation, primarily through accurate heuristics that can be derived
automatically from the structure of the representation. (This is analogous to the way in which
search for a plan more efﬁcient. We then describe a few of the other approaches to planning,
and conclude by comparing the various approaches.
This chapter covers fully observable, deterministic, static environments with single
with multiple agents.
DEFINITION OF CLASSICAL PLANNING
inference, which means that it may be swamped when there are many actions and states. For
example, in the wumpus world, the simple action of moving a step forward had to be repeated
for all four agent orientations, T time steps, and n2 current locations.Deﬁnition of Classical Planning
In response to this, planning researchers have settled on a factored representation—
one in which a state of the world is represented by a collection of variables. We use a language
called PDDL, the Planning Domain Deﬁnition Language, that allows us to express all 4Tn2
PDDL
actions with one action schema. There have been several versions of PDDL; we select a
how PDDL describes the four things we need to deﬁne a search problem: the initial state, the
actions that are available in a state, the result of applying an action, and the goal test.
Each state is represented as a conjunction of ﬂuents that are ground, functionless atoms.
For example, Poor ∧Unknown might represent the state of a hapless agent, and a state
¬Poor (because it is a negation), and At(Father(Fred), Sydney) (because it uses a function
symbol). The representation of states is carefully designed so that a state can be treated
either as a conjunction of ﬂuents, which can be manipulated by logical inference, or as a set
of ﬂuents, which can be manipulated with set operations. The set semantics is sometimes
SET SEMANTICS
easier to deal with.
Actions are described by a set of action schemas that implicitly deﬁne the ACTIONS(s)
any system for action description needs to solve the frame problem—to say what changes and
what stays the same as the result of the action. Classical planning concentrates on problems
where most actions leave most things unchanged. Think of a world consisting of a bunch of
cation by a vector Δ. A concise description of the action should mention only Δ; it shouldn’t
have to mention all the objects that stay in place. PDDL does that by specifying the result of
an action in terms of what changes; everything that stays the same is left unmentioned.
ACTION SCHEMA
The schema is a lifted representation—it lifts the level of reasoning from propositional logic
plane from one location to another:
Action(Fly(p, from, to),
PRECOND:At(p, from) ∧Plane(p) ∧Airport(from) ∧Airport(to)
EFFECT:¬At(p, from) ∧At(p, to))
The schema consists of the action name, a list of all the variables used in the schema, a
precondition and an effect. Although we haven’t said yet how the action schema converts
PRECONDITION
EFFECT
into logical sentences, think of the variables as being universally quantiﬁed. We are free to
choose whatever values we want to instantiate the variables. For example, here is one ground
more restricted than PDDL: STRIPS preconditions and goals cannot contain negative literals.Chapter
Classical Planning
action that results from substituting values for all the variables:
Action(Fly(P1, SFO, JFK),
PRECOND:At(P1, SFO) ∧Plane(P1) ∧Airport(SFO) ∧Airport(JFK)
EFFECT:¬At(P1, SFO) ∧At(P1, JFK))
The precondition and effect of an action are each conjunctions of literals (positive or negated
atomic sentences). The precondition deﬁnes the states in which the action can be executed,
and the effect deﬁnes the result of executing the action. An action a can be executed in state
s if s entails the precondition of a. Entailment can also be expressed with the set semantics:
s |= q iff every positive literal in q is in s and every negated literal in q is not. In formal
notation we say
where any variables in a are universally quantiﬁed. For example,
∀p, from, to (Fly(p, from, to) ∈ACTIONS(s)) ⇔
s |= (At(p, from) ∧Plane(p) ∧Airport(from) ∧Airport(to))
We say that action a is applicable in state s if the preconditions are satisﬁed by s. When
APPLICABLE
an action schema a contains variables, it may have multiple applicable instantiations. For
Fly(P1, SFO, JFK) or as Fly(P2, JFK , SFO), both of which are applicable in the initial
state. If an action a has v variables, then, in a domain with k unique names of objects, it takes
O(vk) time in the worst case to ﬁnd the applicable ground actions.
Sometimes we want to propositionalize a PDDL problem—replace each action schema
PROPOSITIONALIZE
with a set of ground actions and then use a propositional solver such as SATPLAN to ﬁnd a
solution. However, this is impractical when v and k are large.
The result of executing action a in state s is deﬁned as a state s′ which is represented
by the set of ﬂuents formed by starting with s, removing the ﬂuents that appear as negative
literals in the action’s effects (what we call the delete list or DEL(a)), and adding the ﬂuents
DELETE LIST
that are positive literals in the action’s effects (what we call the add list or ADD(a)):
ADD LIST
RESULT(s, a) = (s −DEL(a)) ∪ADD(a) .
For example, with the action Fly(P1, SFO, JFK), we would remove At(P1, SFO) and add
At(P1, JFK). It is a requirement of action schemas that any variable in the effect must also
appear in the precondition. That way, when the precondition is matched against the state s,
all the variables will be bound, and RESULT(s, a) will therefore have only ground atoms. In
other words, ground states are closed under the RESULT operation.
In PDDL the times and states are implicit in the action schemas: the precondition always
A set of action schemas serves as a deﬁnition of a planning domain. A speciﬁc problem
within the domain is deﬁned with the addition of an initial state and a goal. The initialDeﬁnition of Classical Planning
Init(At(C1, SFO) ∧At(C2, JFK) ∧At(P1, SFO) ∧At(P2, JFK)
∧Cargo(C1) ∧Cargo(C2) ∧Plane(P1) ∧Plane(P2)
∧Airport(JFK) ∧Airport(SFO))
Goal(At(C1, JFK) ∧At(C2, SFO))
Action(Load(c, p, a),
PRECOND: At(c, a) ∧At(p, a) ∧Cargo(c) ∧Plane(p) ∧Airport(a)
EFFECT: ¬ At(c, a) ∧In(c, p))
Action(Unload(c, p, a),
PRECOND: In(c, p) ∧At(p, a) ∧Cargo(c) ∧Plane(p) ∧Airport(a)
EFFECT: At(c, a) ∧¬ In(c, p))
Action(Fly(p, from, to),
PRECOND: At(p, from) ∧Plane(p) ∧Airport(from) ∧Airport(to)
EFFECT: ¬ At(p, from) ∧At(p, to))
A PDDL description of an air cargo transportation planning problem.
INITIAL STATE
used, which means that any atoms that are not mentioned are false.) The goal is just like a
GOAL
precondition: a conjunction of literals (positive or negative) that may contain variables, such
as At(p, SFO) ∧Plane(p). Any variables are treated as existentially quantiﬁed, so this goal
is to have any plane at SFO. The problem is solved when we can ﬁnd a sequence of actions
that end in a state s that entails the goal. For example, the state Rich ∧Famous ∧Miserable
entails the goal Rich ∧Famous, and the state Plane(Plane1) ∧At(Plane1, SFO) entails
the goal At(p, SFO) ∧Plane(p).
Now we have deﬁned planning as a search problem: we have an initial state, an ACTIONS
function, a RESULT function, and a goal test. We’ll look at some example problems before
investigating efﬁcient search algorithms.
Example: Air cargo transport
ﬂying it from place to place. The problem can be deﬁned with three actions: Load, Unload,
and Fly. The actions affect two predicates: In(c, p) means that cargo c is inside plane p, and
At(x, a) means that object x (either plane or cargo) is at airport a. Note that some care must
be taken to make sure the At predicates are maintained properly. When a plane ﬂies from
be easy to quantify over all objects that are inside the plane. But basic PDDL does not have
a universal quantiﬁer, so we need a different solution. The approach we use is to say that a
piece of cargo ceases to be At anywhere when it is In a plane; the cargo only becomes At the
new airport when it is unloaded. So At really means “available for use at a given location.”
The following plan is a solution to the problem:
[Load(C1, P1, SFO), Fly(P1, SFO, JFK ), Unload(C1, P1, JFK),
Load(C2, P2, JFK ), Fly(P2, JFK , SFO), Unload(C2, P2, SFO)] .Chapter
Classical Planning
Finally, there is the problem of spurious actions such as Fly(P1, JFK, JFK ), which should
include At(P1, JFK) ∧¬At(P1, JFK)). It is common to ignore such problems, because
they seldom cause incorrect plans to be produced. The correct approach is to add inequality
preconditions saying that the from and to airports must be different; see another example of
Example: The spare tire problem
tire properly mounted onto the car’s axle, where the initial state has a ﬂat tire on the axle and
a good spare tire in the trunk. To keep it simple, our version of the problem is an abstract
one, with no sticky lug nuts or other complications. There are just four actions: removing the
spare from the trunk, removing the ﬂat tire from the axle, putting the spare on the axle, and
leaving the car unattended overnight. We assume that the car is parked in a particularly bad
neighborhood, so that the effect of leaving it overnight is that the tires disappear. A solution
to the problem is [Remove(Flat, Axle), Remove(Spare, Trunk), PutOn(Spare, Axle)].
Init(Tire(Flat) ∧Tire(Spare) ∧At(Flat, Axle) ∧At(Spare, Trunk))
Goal(At(Spare, Axle))
Action(Remove(obj , loc),
PRECOND: At(obj , loc)
EFFECT: ¬ At(obj , loc) ∧At(obj , Ground))
Action(PutOn(t, Axle),
PRECOND: Tire(t) ∧At(t, Ground) ∧¬ At(Flat, Axle)
EFFECT: ¬ At(t, Ground) ∧At(t, Axle))
Action(LeaveOvernight,
PRECOND:
EFFECT: ¬ At(Spare, Ground) ∧¬ At(Spare, Axle) ∧¬ At(Spare, Trunk)
∧¬ At(Flat, Ground) ∧¬ At(Flat, Axle) ∧¬ At(Flat, Trunk))
The simple spare tire problem.
Example: The blocks world
One of the most famous planning domains is known as the blocks world. This domain
BLOCKS WORLD
only one block can ﬁt directly on top of another. A robot arm can pick up a block and move
it to another position, either on the table or on top of another block. The arm can pick up
only one block at a time, so it cannot pick up a block that has another one on it. The goal will
always be to build one or more stacks of blocks, speciﬁed in terms of what blocks are on topDeﬁnition of Classical Planning
Init(On(A, Table) ∧On(B, Table) ∧On(C, A)
∧Block(A) ∧Block(B) ∧Block(C) ∧Clear(B) ∧Clear(C))
Goal(On(A, B) ∧On(B, C))
Action(Move(b, x, y),
PRECOND: On(b, x) ∧Clear(b) ∧Clear(y) ∧Block(b) ∧Block(y) ∧
(b̸=x) ∧(b̸=y) ∧(x̸=y),
EFFECT: On(b, y) ∧Clear(x) ∧¬On(b, x) ∧¬Clear(y))
Action(MoveToTable(b, x),
PRECOND: On(b, x) ∧Clear(b) ∧Block(b) ∧(b̸=x),
EFFECT: On(b, Table) ∧Clear(x) ∧¬On(b, x))
solution is the sequence [MoveToTable(C, A), Move(B, Table, C), Move(A, Table, B)].
Start State
Goal State
B
A
C
A
B
C
of what other blocks. For example, a goal might be to get block A on B and block B on C
We use On(b, x) to indicate that block b is on x, where x is either another block or the
table. The action for moving block b from the top of x to the top of y will be Move(b, x, y).
this would be ¬∃x On(x, b) or, alternatively, ∀x ¬On(x, b). Basic PDDL does not allow
quantiﬁers, so instead we introduce a predicate Clear(x) that is true when nothing is on x.
The action Move moves a block b from x to y if both b and y are clear. After the move
is made, b is still clear but y is not. A ﬁrst attempt at the Move schema is
Action(Move(b, x, y),
PRECOND:On(b, x) ∧Clear(b) ∧Clear(y),
EFFECT:On(b, y) ∧Clear(x) ∧¬On(b, x) ∧¬Clear(y)) .
Unfortunately, this does not maintain Clear properly when x or y is the table. When x is the
Table, this action has the effect Clear(Table), but the table should not become clear; and
when y = Table, it has the precondition Clear(Table), but the table does not have to be clearChapter
Classical Planning
for us to move a block onto it. To ﬁx this, we do two things. First, we introduce another
action to move a block b from x to the table:
Action(MoveToTable(b, x),
PRECOND:On(b, x) ∧Clear(b),
EFFECT:On(b, Table) ∧Clear(x) ∧¬On(b, x)) .
Second, we take the interpretation of Clear(x) to be “there is a clear space on x to hold a
block.” Under this interpretation, Clear(Table) will always be true. The only problem is that
nothing prevents the planner from using Move(b, x, Table) instead of MoveToTable(b, x).
not lead to incorrect answers—or we could introduce the predicate Block and add Block(b)∧
Block(y) to the precondition of Move.
The complexity of classical planning
In this subsection we consider the theoretical complexity of planning and distinguish two
decision problems. PlanSAT is the question of whether there exists any plan that solves a
PLANSAT
planning problem. Bounded PlanSAT asks whether there is a solution of length k or less;
BOUNDED PLANSAT
this can be used to ﬁnd an optimal plan.
The ﬁrst result is that both decision problems are decidable for classical planning. The
proof follows from the fact that the number of states is ﬁnite. But if we add function symbols
to the language, then the number of states becomes inﬁnite, and PlanSAT becomes only
semidecidable: an algorithm exists that will terminate with the correct answer for any solvable
problem, but may not terminate on unsolvable problems. The Bounded PlanSAT problem
remains decidable even in the presence of function symbols. For proofs of the assertions in
Both PlanSAT and Bounded PlanSAT are in the complexity class PSPACE, a class that
is larger (and hence more difﬁcult) than NP and refers to problems that can be solved by a
deterministic Turing machine with a polynomial amount of space. Even if we make some
rather severe restrictions, the problems remain quite difﬁcult. For example, if we disallow
preconditions, PlanSAT reduces to the class P.
which can be much easier than the theoretical worst case. For many domains (including theNow we turn our attention to planning algorithms. We saw how the description of a planning
problem deﬁnes a search problem: we can search from the initial state through the space
of states, looking for a goal. One of the nice advantages of the declarative representation of
action schemas is that we can also search backward from the goal, looking for the initial state.
Now that we have shown how a planning problem maps into a search problem, we can solve
come up with reasons why.
First, forward search is prone to exploring irrelevant actions. Consider the noble task
of buying a copy of AI: A Modern Approach from an online bookseller. Suppose there is an
(a)
(b)
At(P1, A)
Fly(P1, A, B)
Fly(P2, A, B)
Fly(P1, A, B)
Fly(P2, A, B)
At(P2, A)
At(P1, B)
At(P2, A)
At(P1, A)
At(P2, B)
At(P1, B)
At(P2, B)
At(P1, B)
At(P2, A)
At(P1, A)
At(P2, B)
Two approaches to searching for a plan. (a) Forward (progression) search
through the space of states, starting in the initial state and using the problem’s actions to
search forward for a member of the set of goal states. (b) Backward (regression) search
through sets of relevant states, starting at the set of states representing the goal and using the
inverse of the actions to search backward for the initial state.Chapter
Classical Planning
Second, planning problems often have large state spaces. Consider an air cargo problem
pieces of cargo into one of the planes at A, ﬂy the plane to B, and unload the cargo. Finding
it is loaded) or loaded into any plane at its airport (if it is unloaded). So in any state there
Clearly, even this relatively small problem instance is hopeless without an accurate
can be derived automatically; that is what makes forward search feasible.
In regression search we start at the goal and apply the actions backward until we ﬁnd a
We start with the goal, which is a conjunction of literals forming a description of a set of
states—for example, the goal ¬Poor ∧Famous describes those states in which Poor is false,
Famous is true, and any other ﬂuent can have any value. If there are n ground ﬂuents in a
domain, then there are 2n ground states (each ﬂuent can be true or false), but 3n descriptions
of sets of goal states (each ﬂuent can be positive, negative, or not mentioned).
In general, backward search works only when we know how to regress from a state
description to the predecessor state description. For example, it is hard to search backwards
are one move away from the goal. Happily, the PDDL representation was designed to make
it easy to regress actions—if a domain can be expressed in PDDL, then we can do regression
search on it. Given a ground goal description g and a ground action a, the regression from g
over a gives us a state description g′ deﬁned by
g′ = (g −ADD(a)) ∪Precond(a) .
That is, the effects that were added by the action need not have been true before, and also
the preconditions must have held before, or else the action could not have been executed.
Note that DEL(a) does not appear in the formula; that’s because while we know the ﬂuents
in DEL(a) are no longer true after the action, we don’t know whether or not they were true
before, so there’s nothing to be said about them.ciﬁc piece of cargo to SFO: At(C2, SFO). That suggests the action Unload(C2, p′, SFO):
Action(Unload(C2, p′, SFO),
PRECOND:In(C2, p′) ∧At(p′, SFO) ∧Cargo(C2) ∧Plane(p′) ∧Airport(SFO)
EFFECT:At(C2, SFO) ∧¬In(C2, p′) .
(Note that we have standardized variable names (changing p to p′ in this case) so that there
will be no confusion between variable names if we happen to use the same action schema
This represents unloading the package from an unspeciﬁed plane at SFO; any plane will do,
representations: a single description summarizes the possibility of using any of the planes by
implicitly quantifying over p′. The regressed state description is
g′ = In(C2, p′) ∧At(p′, SFO) ∧Cargo(C2) ∧Plane(p′) ∧Airport(SFO) .
tion we chose actions that were applicable—those actions that could be the next step in the
plan. In backward search we want actions that are relevant—those actions that could be the
RELEVANCE
last step in a plan leading up to the current goal state.
For an action to be relevant to a goal it obviously must contribute to the goal: at least
one of the action’s effects (either positive or negative) must unify with an element of the goal.
What is less obvious is that the action must not have any effect (positive or negative) that
negates an element of the goal. Now, if the goal is A ∧B ∧C and an action has the effect
A∧B∧¬C then there is a colloquial sense in which that action is very relevant to the goal—it
because this action could not be the ﬁnal step of a solution—we would always need at least
one more step to achieve C.
Given the goal At(C2, SFO), several instantiations of Unload are relevant: we could
chose any speciﬁc plane to unload from, or we could leave the plane unspeciﬁed by using
the action Unload(C2, p′, SFO). We can reduce the branching factor without ruling out any
solutions by always using the action formed by substituting the most general uniﬁer into the
(standardized) action schema.
A = Action(Buy(i), PRECOND:ISBN (i), EFFECT:Own(i)) .
initial state, so we are done.Chapter
Classical Planning
We can make this more formal. Assume a goal description g which contains a goal
literal gi and an action schema A that is standardized to produce A′. If A′ has an effect literal
e′
j where Unify(gi, e′
j) = θ and where we deﬁne a′ = SUBST(θ, A′) and if there is no effect
in a′ that is the negation of a literal in g, then a′ is a relevant action towards g.
lem domains. However, the fact that backward search uses state sets rather than individual
states makes it harder to come up with good heuristics. That is the main reason why the
majority of current systems favor forward search.
Heuristics for planning
Neither forward nor backward search is efﬁcient without a good heuristic function. Recall
goal and that if we can derive an admissible heuristic for this distance—one that does not
overestimate—then we can use A∗search to ﬁnd optimal solutions. An admissible heuristic
can be derived by deﬁning a relaxed problem that is easier to solve. The exact cost of a
solution to this easier problem then becomes the heuristic for the original problem.
By deﬁnition, there is no way to analyze an atomic state, and thus it it requires some
with atomic states. Planning uses a factored representation for states and action schemas.
Think of a search problem as a graph where the nodes are states and the edges are
actions. The problem is to ﬁnd a path connecting the initial state to a goal state. There are
two ways we can relax this problem to make it easier: by adding more edges to the graph,
making it strictly easier to ﬁnd a path, or by grouping multiple nodes together, forming an
abstraction of the state space that has fewer states, and thus is easier to search.
conditions heuristic drops all preconditions from actions. Every action becomes applicable
IGNORE
PRECONDITIONS
HEURISTIC
ble action—if not, the problem is impossible). This almost implies that the number of steps
required to solve the relaxed problem is the number of unsatisﬁed goals—almost but not
except those that are literals in the goal. Then, we count the minimum number of actions
required such that the union of those actions’ effects satisﬁes the goal. This is an instance
PROBLEM
Fortunately a simple greedy algorithm is guaranteed to return a set covering whose size is
within a factor of log n of the true minimum covering, where n is the number of literals in
the goal. Unfortunately, the greedy algorithm loses the guarantee of admissibility.problem involving tiles with a single schema Slide:
Action(Slide(t, s1, s2),
PRECOND:On(t, s1) ∧Tile(t) ∧Blank(s2) ∧Adjacent(s1, s2)
EFFECT:On(t, s2) ∧Blank(s1) ∧¬On(t, s1) ∧¬Blank(s2))
see how these heuristics could be derived automatically from the action schema description.
The ease of manipulating the schemas is the great advantage of the factored representation of
planning problems, as compared with the atomic representation of search problems.
Another possibility is the ignore delete lists heuristic. Assume for a moment that all
IGNORE DELETE
LISTS
goals and preconditions contain only positive literals3 We want to create a relaxed version of
the original problem that will be easier to solve, and where the length of the solution will serve
as a good heuristic. We can do that by removing the delete lists from all actions (i.e., removing
all negative literals from effects). That makes it possible to make monotonic progress towards
hard to ﬁnd the optimal solution to this relaxed problem, but an approximate solution can be
the edges actions, and the height of each dot above the bottom plane represents the heuristic
value. States on the bottom plane are solutions. In both these problems, there is a wide path
to the goal. There are no dead ends, so no need for backtracking; a simple hillclimbing search
will easily ﬁnd a solution to these problems (although it may not be an optimal solution).
STATE ABSTRACTION
to the abstract representation.
The easiest form of state abstraction is to ignore some ﬂuents. For example, consider
and all packages at a given airport have the same destination. Then a useful abstraction of the
problem is to drop all the At ﬂuents except for the ones involving one plane and one package
abstract state space will be shorter than a solution in the original space (and thus will be an
admissible heuristic), and the abstract solution is easy to extend to a solution to the original
problem (by adding additional Load and Unload actions).
in a goal or precondition with a new positive literal, P ′.Chapter
Classical Planning
tic. The height above the bottom plane is the heuristic score of a state; states on the bottom
plane are goals. There are no local minima, so search for the goal is straightforward. From
DECOMPOSITION
SUBGOAL
INDEPENDENCE
sumption is that the cost of solving a conjunction of subgoals is approximated by the sum
of the costs of solving each subgoal independently. The subgoal independence assumption
can be optimistic or pessimistic. It is optimistic when there are negative interactions between
the subplans for each subgoal—for example, when an action in one subplan deletes a goal
achieved by another subplan. It is pessimistic, and therefore inadmissible, when subplans
contain redundant actions—for instance, two actions that could be replaced by a single action
in the merged plan.
Suppose the goal is a set of ﬂuents G, which we divide into disjoint subsets G1, . . . , Gn.
We then ﬁnd plans P1, . . . , Pn that solve the respective subgoals. What is an estimate of the
cost of the plan for achieving all of G? We can think of each Cost(Pi) as a heuristic estimate,
and we know that if we combine estimates by taking their maximum value, we always get an
admissible heuristic. So maxi COST(Pi) is admissible, and sometimes it is exactly correct:
it could be that P1 serendipitously achieves all the Gi. But in most cases, in practice the
estimate is too low. Could we sum the costs instead? For many problems that is a reasonable
estimate, but it is not admissible. The best case is when we can determine that Gi and Gj are
independent. If the effects of Pi leave all the preconditions and goals of Pj unchanged, then
the estimate COST(Pi) + COST(Pj) is admissible, and more accurate than the max estimate.
stractions. The trick is choosing the right abstractions and using them in a way that makes
the total cost—deﬁning an abstraction, doing an abstract search, and mapping the abstractionPlanning Graphs
the pattern database can be amortized over multiple problem instances.
An example of a system that makes use of effective heuristics is FF, or FASTFORWARD
When it hits a plateau or local maximum—when no action leads to a state with better heuristic
score—then FF uses iterative deepening search until it ﬁnds a state that is better, or it gives
PLANNING GRAPHS
All of the heuristics we have suggested can suffer from inaccuracies. This section shows
how a special data structure called a planning graph can be used to give better heuristic
PLANNING GRAPH
estimates. These heuristics can be applied to any of the search techniques we have seen so
far. Alternatively, we can search for a solution over the space formed by the planning graph,
using an algorithm called GRAPHPLAN.
A planning problem asks if we can reach a goal state from the initial state. Suppose we
tion “can we reach state G from state S0” immediately, just by looking it up. Of course, the
size approximation to this tree that can be constructed quickly. The planning graph can’t
answer deﬁnitively whether G is reachable from S0, but it can estimate how many steps it
takes to reach G. The estimate is always correct when it reports the goal is not reachable, and
it never overestimates the number of steps, so it is an admissible heuristic.
A planning graph is a directed graph organized into levels: ﬁrst a level S0 for the initial
LEVEL
state, consisting of nodes representing each ﬂuent that holds in S0; then a level A0 consisting
of nodes for each ground action that might be applicable in S0; then alternating levels Si
followed by Ai; until we reach a termination condition (to be discussed later).
Roughly speaking, Si contains all the literals that could hold at time i, depending on
the actions executed at preceding time steps. If it is possible that either P or ¬P could hold,
then both will be represented in Si. Also roughly speaking, Ai contains all the actions that
could have their preconditions satisﬁed at time i. We say “roughly speaking” because the
planning graph records only a restricted subset of the possible negative interactions among
actions; therefore, a literal might show up at level Sj when actually it could not be true until
a later level, if at all. (A literal will never show up too late.) Despite the possible error, the
level j at which a literal ﬁrst appears is a good estimate of how difﬁcult it is to achieve the
literal from the initial state.Chapter
Classical Planning
Init(Have(Cake))
Goal(Have(Cake) ∧Eaten(Cake))
Action(Eat(Cake)
PRECOND: Have(Cake)
EFFECT: ¬ Have(Cake) ∧Eaten(Cake))
Action(Bake(Cake)
PRECOND: ¬ Have(Cake)
EFFECT: Have(Cake))
The “have cake and eat cake too” problem.
Bake(Cake)
Eat(Cake)
Have(Cake)
S0
A0
S1
A1
S2
Have(Cake)
Have(Cake)
Have(Cake)
Have(Cake)
Eaten(Cake)
Eaten(Cake)
Eaten(Cake)
Eaten(Cake)
Eaten(Cake)
Eat(Cake)
¬
¬
¬
¬
¬
The planning graph for the “have cake and eat cake too” problem up to level
S2. Rectangles indicate actions (small squares indicate persistence actions), and straight
lines indicate preconditions and effects. Mutex links are shown as curved gray lines. Not all
mutex links are shown, because the graph would be too cluttered. In general, if two literals
are mutex at Si, then the persistence actions for those literals will be mutex at Ai and we
need not draw that mutex link.
tion schemas. Despite the resulting increase in the size of the problem description, planning
graphs have proved to be effective tools for solving hard planning problems.
So a literal appears because an action caused it, but we also want to say that a literal can
persist if no action negates it. This is represented by a persistence action (sometimes called
PERSISTENCE
ACTION
persistence actions drawn as small square boxes.
Level A0 contains all the actions that could occur in state S0, but just as important it
records conﬂicts between actions that would prevent them from occurring together. The gray
MUTUAL EXCLUSION
MUTEX
mutually exclusive with the persistence of either Have(Cake) or ¬Eaten(Cake). We shall
see shortly how mutex links are computed.
Level S1 contains all the literals that could result from picking any subset of the actions
in A0, as well as mutex links (gray lines) indicating literals that could not appear together,
regardless of the choice of actions. For example, Have(Cake) and Eaten(Cake) are mutex:Planning Graphs
depending on the choice of actions in A0, either, but not both, could be the result. In other
words, S1 represents a belief state: a set of possible states. The members of this set are all
subsets of the literals such that there is no mutex link between any members of the subset.
We continue in this way, alternating between state level Si and action level Ai until we
reach a point where two consecutive levels are identical. At this point, we say that the graph
LEVELED OFF
What we end up with is a structure where every Ai level contains all the actions that are
applicable in Si, along with constraints saying that two actions cannot both be executed at the
same level. Every Si level contains all the literals that could result from any possible choice
It is important to note that the process of constructing the planning graph does not require
choosing among actions, which would entail combinatorial search. Instead, it just records the
impossibility of certain choices using mutex links.
We now deﬁne mutex links for both actions and literals. A mutex relation holds between
two actions at a given level if any of the following three conditions holds:
• Inconsistent effects: one action negates an effect of the other. For example, Eat(Cake)
and the persistence of Have(Cake) have inconsistent effects because they disagree on
the effect Have(Cake).
• Interference: one of the effects of one action is the negation of a precondition of the
ing its precondition.
• Competing needs: one of the preconditions of one action is mutually exclusive with a
precondition of the other. For example, Bake(Cake) and Eat(Cake) are mutex because
they compete on the value of the Have(Cake) precondition.
A mutex relation holds between two literals at the same level if one is the negation of the other
or if each possible pair of actions that could achieve the two literals is mutually exclusive.
This condition is called inconsistent support. For example, Have(Cake) and Eaten(Cake)
are mutex in S1 because the only way of achieving Have(Cake), the persistence action, is
mutex with the only way of achieving Eaten(Cake), namely Eat(Cake). In S2 the two
literals are not mutex, because there are new ways of achieving them, such as Bake(Cake)
and the persistence of Eaten(Cake), that are not mutex.
A planning graph is polynomial in the size of the planning problem. For a planning
problem with l literals and a actions, each Si has no more than l nodes and l2 mutex links,
Planning graphs for heuristic estimation
A planning graph, once constructed, is a rich source of information about the problem. First,
if any goal literal fails to appear in the ﬁnal level of the graph, then the problem is unsolvable.
Second, we can estimate the cost of achieving any goal literal gi from state s as the level at
which gi ﬁrst appears in the planning graph constructed from initial state s. We call this theChapter
Classical Planning
LEVEL COST
goals. The estimate might not always be accurate, however, because planning graphs allow
several actions at each level, whereas the heuristic counts just the level and not the number
of actions. For this reason, it is common to use a serial planning graph for computing
SERIAL PLANNING
GRAPH
heuristics. A serial graph insists that only one action can actually occur at any given time
step; this is done by adding mutex links between every pair of nonpersistence actions. Level
costs extracted from serial graphs are often quite reasonable estimates of actual costs.
To estimate the cost of a conjunction of goals, there are three simple approaches. The
ble, but not necessarily accurate.
The level sum heuristic, following the subgoal independence assumption, returns the
LEVEL SUM
sum of the level costs of the goals; this can be inadmissible but works well in practice
so bad. A more serious error is that if Bake(Cake) were not in the set of actions, then the
goal appear in the planning graph without any pair of them being mutually exclusive. This
extremely well on tasks in which there is a good deal of interaction among subplans. It is not
perfect, of course; for example, it ignores interactions among three or more literals.
As a tool for generating accurate heuristics, we can view the planning graph as a relaxed
problem that is efﬁciently solvable. To understand the nature of the relaxed problem, we
need to understand exactly what it means for a literal g to appear at level Si in the planning
graph. Ideally, we would like it to be a guarantee that there exists a plan with i action levels
that achieves g, and also that if g does not appear, there is no such plan. Unfortunately,
making that guarantee is as difﬁcult as solving the original planning problem. So the planning
graph makes the second half of the guarantee (if g does not appear, there is no plan), but
if g does appear, then all the planning graph promises is that there is a plan that possibly
achieves g and has no “obvious” ﬂaws. An obvious ﬂaw is deﬁned as a ﬂaw that can be
detected by considering two actions or two literals at a time—in other words, by looking at
the mutex relations. There could be more subtle ﬂaws involving three, four, or more actions,
but experience has shown that it is not worth the computational effort to keep track of these
possible ﬂaws. This is similar to a lesson learned from constraint satisfaction problems—that
One example of an unsolvable problem that cannot be recognized as such by a planning
A. This is an impossible goal; a tower with the bottom on top of the top. But a planning graphPlanning Graphs
cannot detect the impossibility, because any two of the three subgoals are achievable. There
are no mutexes between any pair of literals, only between the three as a whole. To detect that
this problem is impossible, we would have to search over the planning graph.
The GRAPHPLAN algorithm
This subsection shows how to extract a plan directly from the planning graph, rather than just
the problem. If that fails, it expands another level and tries again, terminating with failure
when there is no reason to go on.
function GRAPHPLAN(problem) returns solution or failure
goals ←CONJUNCTS(problem.GOAL)
nogoods ←an empty hash table
if solution ̸= failure then return solution
if graph and nogoods have both leveled off then return failure
problem description’s initial state are shown, as are the relevant negative ﬂuents. Not shown
are the unchanging positive literals (such as Tire(Spare)) and the irrelevant negative literals.
actions whose preconditions exist at level S0 (i.e., all the actions except PutOn(Spare, Axle)),
along with persistence actions for all the literals in S0. The effects of the actions are added at
some of the examples of mutex relations and their causes:
• Inconsistent effects: Remove(Spare, Trunk) is mutex with LeaveOvernight because
one has the effect At(Spare, Ground) and the other has its negation.Chapter
Classical Planning
S0
A1
S2
At(Spare,Trunk)
At(Spare,Trunk)
At(Flat,Axle)
At(Flat,Axle)
At(Spare,Axle)
At(Flat,Ground)
At(Flat,Ground)
At(Spare,Ground)
At(Spare,Ground)
At(Spare,Trunk)
At(Spare,Trunk)
At(Flat,Axle)
At(Flat,Axle)
At(Spare,Axle)
At(Flat,Ground)
At(Flat,Ground)
At(Spare,Ground)
At(Spare,Ground)
At(Spare,Axle)
At(Spare,Trunk)
At(Flat,Axle)
At(Spare,Axle)
At(Flat,Ground)
At(Spare,Ground)
PutOn(Spare,Axle)
LeaveOvernight
Remove(Flat,Axle)
Remove(Spare,Trunk)
Remove(Spare,Trunk)
Remove(Flat,Axle)
LeaveOvernight
¬
¬
¬
¬
¬
¬
¬
¬
¬
¬
¬
¬
¬
A0
S1
The planning graph for the spare tire problem after expansion to level S2.
Mutex links are shown as gray lines. Not all links are shown, because the graph would be too
cluttered if we showed them all. The solution is indicated by bold lines and outlines.
• Interference: Remove(Flat, Axle) is mutex with LeaveOvernight because one has the
precondition At(Flat, Axle) and the other has its negation as an effect.
• Competing needs: PutOn(Spare, Axle) is mutex with Remove(Flat, Axle) because
one has At(Flat, Axle) as a precondition and the other has its negation.
• Inconsistent support: At(Spare, Axle) is mutex with At(Flat, Axle) in S2 because the
only way of achieving At(Spare, Axle) is by PutOn(Spare, Axle), and that is mutex
with the persistence action that is the only way of achieving At(Flat, Axle). Thus, the
mutex relations detect the immediate conﬂict that arises from trying to put two objects
in the same place at the same time.
This time, when we go back to the start of the loop, all the literals from the goal are present
in S2, and none of them is mutex with any other. That means that a solution might exist,
Boolean constraint satisfaction problem (CSP) where the variables are the actions at each
level, the values for each variable are in or out of the plan, and the constraints are the mutexes
and the need to satisfy each goal and precondition.
isﬁed goals. We deﬁne this search problem as follows:
• The initial state is the last level of the planning graph, Sn, along with the set of goals
from the planning problem.
“conﬂict free,” we mean a set of actions such that no two of them are mutex and no two
of their preconditions are mutex.Planning Graphs
• The goal is to reach a state at level S0 such that all the goals are satisﬁed.
For this particular problem, we start at S2 with the goal At(Spare, Axle). The only choice we
have for achieving the goal set is PutOn(Spare, Axle). That brings us to a search state at S1
with goals At(Spare, Ground) and ¬At(Flat, Axle). The former can be achieved only by
Remove(Spare, Trunk), and the latter by either Remove(Flat, Axle) or LeaveOvernight.
But LeaveOvernight is mutex with Remove(Spare, Trunk), so the only solution is to choose
Remove(Spare, Trunk) and Remove(Flat, Axle). That brings us to a search state at S0 with
the goals At(Spare, Trunk) and At(Flat, Axle). Both of these are present in the state, so
we have a solution: the actions Remove(Spare, Trunk) and Remove(Flat, Axle) in level
A0, followed by PutOn(Spare, Axle) in A1.
takes polynomial time, so it must be the case that solution extraction is intractable in the worst
case. Therefore, we will need some heuristic guidance for choosing among actions during the
backward search. One approach that works well in practice is a greedy algorithm based on
the level cost of the literals. For any set of goals, we proceed in the following order:
such that the sum (or maximum) of the level costs of its preconditions is smallest.
Termination of GRAPHPLAN
So far, we have skated over the question of termination. Here we show that GRAPHPLAN will
in fact terminate and return failure when there is no solution.
The ﬁrst thing to understand is why we can’t stop expanding the graph as soon as it has
leveled off. Consider an air cargo domain with one plane and n pieces of cargo at airport
A, all of which have airport B as their destination. In this version of the problem, only one
fact that for any single piece of cargo, we can load it, ﬂy it, and unload it at the destination in
and for all but the last piece we need to ﬂy back to airport A to get the next piece.
How long do we have to keep expanding after the graph has leveled off? If the function
failure because there is no possibility of a subsequent change that could add a solution.Chapter
Classical Planning
key to this proof is that certain properties of planning graphs are monotonically increasing or
(not necessarily proper) of the set at level i. The properties are as follows:
• Literals increase monotonically: Once a literal appears at a given level, it will appear
at all subsequent levels. This is because of the persistence actions; once a literal shows
up, persistence actions cause it to stay forever.
• Actions increase monotonically: Once an action appears at a given level, it will appear
at all subsequent levels. This is a consequence of the monotonic increase of literals; if
the preconditions of an action appear at one level, they will appear at subsequent levels,
and thus so will the action.
• Mutexes decrease monotonically: If two actions are mutex at a given level Ai, then they
will also be mutex for all previous levels at which they both appear. The same holds for
mutexes between literals. It might not always appear that way in the ﬁgures, because
the ﬁgures have a simpliﬁcation: they display neither literals that cannot hold at level
Si nor actions that cannot be executed at level Ai. We can see that “mutexes decrease
monotonically” is true if you consider that these invisible literals and actions are mutex
with everything.
The proof can be handled by cases: if actions A and B are mutex at level Ai, it
must be because of one of the three types of mutex. The ﬁrst two, inconsistent effects
and interference, are properties of the actions themselves, so if the actions are mutex
at Ai, they will be mutex at every level. The third case, competing needs, depends on
conditions at level Si: that level must contain a precondition of A that is mutex with
a precondition of B. Now, these two preconditions can be mutex if they are negations
of each other (in which case they would be mutex in every level) or if all actions for
achieving one are mutex with all actions for achieving the other. But we already know
that the available actions are increasing monotonically, so, by induction, the mutexes
must be decreasing.
then they are not achievable in any previous level. The proof is by contradiction: if they
were achievable at some previous level, then we could just add persistence actions to
make them achievable at a subsequent level.
Because the actions and literals increase monotonically and because there are only a ﬁnite
number of actions and literals, there must come a level that has the same number of actions
state, then if one of the goals is missing or is mutex with another goal, then we can stop the
GRAPHPLAN algorithm and return failure. That concludes a sketch of the proof; for moreOther Classical Planning Approaches
Year
Track
Winning Systems (approaches)
Optimal
GAMER (model checking, bidirectional search)
Satisﬁcing
LAMA (fast downward search with FF heuristic)
Optimal
SATPLAN, MAXPLAN (Boolean satisﬁability)
Satisﬁcing
SGPLAN (forward search; partitions into independent subproblems)
Optimal
SATPLAN (Boolean satisﬁability)
Satisﬁcing
FAST DIAGONALLY DOWNWARD (forward search with causal graph)
Automated
LPG (local search, planning graphs converted to CSPs)
TLPLAN (temporal action logic with control rules for forward search)
Automated
FF (forward search)
TALPLANNER (temporal action logic with control rules for forward search)
Automated
IPP (planning graphs); HSP (forward search)
tition. Each year there are various tracks: “Optimal” means the planners must produce the
OTHER CLASSICAL PLANNING APPROACHES
Currently the most popular and effective approaches to fully automated planning are:
• Translating to a Boolean satisﬁability (SAT) problem
tion to a satisﬁability problem and then describe three other inﬂuential approaches: planning
Classical planning as Boolean satisﬁability
sitional logic. Here we show how to translate a PDDL description into a form that can be
processed by SATPLAN. The translation is a series of straightforward steps:
• Propositionalize the actions: replace each action schema with a set of ground actions
formed by substituting constants for each of the variables. These ground actions are not
part of the translation, but will be used in subsequent steps.
¬F for every ﬂuent not mentioned in the initial state.
• Propositionalize the goal: for every variable in the goal, replace the literals that contain
the variable with a disjunction over constants. For example, the goal of having block AChapter
Classical Planning
on another block, On(A, x) ∧Block(x) in a world with objects A, B and C, would be
replaced by the goal
(On(A, A) ∧Block(A)) ∨(On(A, B) ∧Block(B)) ∨(On(A, C) ∧Block(C)) .
where ActionCausesF is a disjunction of all the ground actions that have F in their
add list, and ActionCausesNotF is a disjunction of all the ground actions that have F
in their delete list.
• Add precondition axioms: For each ground action A, add the axiom At ⇒PRE(A)t,
that is, if an action is taken at time t, then the preconditions must have been true.
• Add action exclusion axioms: say that every action is distinct from every other action.
The resulting translation is in the form that we can hand to SATPLAN to ﬁnd a solution.
plexity of the algorithms that operate on it. But some problems remain difﬁcult to express in
PDDL. For example, we can’t express the goal “move all the cargo from A to B regardless
as “no more than four robots can be in the same place at the same time.” PDDL can only say
this with repetitious preconditions on every possible action that involves a move.
The propositional logic representation of planning problems also has limitations, such
as the fact that the notion of time is tied directly to ﬂuents. For example, South2 means
of linear time with a notion of branching situations, using a representation called situation
calculus that works like this:
SITUATION
CALCULUS
• The initial state is called a situation.
If s is a situation and a is an action, then
SITUATION
responds to a sequence, or history, of actions. You can also think of a situation as the
result of applying the actions, but note that two situations are the same only if their start
and actions are the same: (RESULT(s, a) = RESULT(s′, a′)) ⇔(s = s′ ∧a = a′).
tion, the situation s is always the last argument to the ﬂuent, for example At(x, l, s) is a
relational ﬂuent that is true when object x is at location l in situation s, and Location is a
functional ﬂuent such that Location(x, s) = l holds in the same situations as At(x, l, s).
• Each action’s preconditions are described with a possibility axiom that says when the
POSSIBILITY AXIOM
action can be taken. It has the form Φ(s) ⇒Poss(a, s) where Φ(s) is some formulaOther Classical Planning Approaches
PIT
PIT
PIT
Gold
PIT
PIT
PIT
Gold
PIT
PIT
PIT
Gold
S0
Forward
Result(S0, Forward)
Result(Result(S0, Forward),
Turn(Right))
Turn(Right)
Situations as the results of actions in the wumpus world.
involving s that describes the preconditions. An example from the wumpus world says
that it is possible to shoot if the agent is alive and has an arrow:
Alive(Agent, s) ∧Have(Agent, Arrow, s) ⇒Poss(Shoot, s)
ﬂuent, depending on what action is taken. This is similar to the approach we took for
propositional logic. The axiom has the form
Action is possible ⇒
(Fluent is true in result state ⇔Action’s effect made it true
∨It was true before and action left it alone) .
For example, the axiom for the relational ﬂuent Holding says that the agent is holding
some gold g after executing a possible action if and only if the action was a Grab of g
or if the agent was already holding g and the action was not releasing it:
Poss(a, s) ⇒
(Holding(Agent, g, Result(a, s)) ⇔
a = Grab(g) ∨(Holding(Agent, g, s) ∧a ̸= Release(g))) .
• We need unique action axioms so that the agent can deduce that, for example, a ̸=
UNIQUE ACTION
AXIOMS
Release(g). For each distinct pair of action names Ai and Aj we have an axiom that
says the actions are different:
Ai(x, . . .) ̸= Aj(y, . . .)Chapter
Classical Planning
and for each action name Ai we have an axiom that says two uses of that action name
are equal if and only if all their arguments are equal:
Ai(x1, . . . , xn) = Ai(y1, . . . , yn) ⇔x1 = y1 ∧. . . ∧xn = yn .
• A solution is a situation (and hence a sequence of actions) that satisﬁes the goal.
Work in situation calculus has done a lot to deﬁne the formal semantics of planning and to
planning programs based on logical deduction over the situation calculus. This is in part
because of the difﬁculty of doing efﬁcient inference in FOL, but is mainly because the ﬁeld
has not yet developed effective heuristics for planning with situation calculus.
Planning as constraint satisfaction
We have seen that constraint satisfaction has a lot in common with Boolean satisﬁability, and
we have seen that CSP techniques are effective for scheduling problems, so it is not surprising
that it is possible to encode a bounded planning problem (i.e., the problem of ﬁnding a plan of
length k) as a constraint satisfaction problem (CSP). The encoding is similar to the encoding
need only a single variable, Actiont, whose domain is the set of possible actions. We no
longer need one variable for every action, and we don’t need the action exclusion axioms. It
Planning as reﬁnement of partially ordered plans
All the approaches we have seen so far construct totally ordered plans consisting of a strictly
linear sequences of actions. This representation ignores the fact that many subproblems are
independent. A solution to an air cargo problem consists of a totally ordered sequence of
being loaded onto another at another airport, it seems pointless to come up with a strict linear
An alternative is to represent plans as partially ordered structures: a plan is a set of
actions and a set of constraints of the form Before(ai, aj) saying that one action occurs
to the spare tire problem. Actions are boxes and ordering constraints are arrows. Note that
Remove(Spare, Trunk) and Remove(Flat, Axle) can be done in either order as long as they
are both completed before the PutOn(Spare, Axle) action.
Partially ordered plans are created by a search through the space of plans rather than
through the state space. We start with the empty plan consisting of just the initial state and
looks for a ﬂaw in the plan, and makes an addition to the plan to correct the ﬂaw (or if no
FLAW
correction can be made, the search backtracks and tries something else). A ﬂaw is anything
that keeps the partial plan from being a solution. For example, one ﬂaw in the empty plan is
that no action achieves At(Spare, Axle). One way to correct the ﬂaw is to insert into the planOther Classical Planning Approaches
Finish
At(Spare,Axle)
Start
At(Flat,Axle)
At(Spare,Trunk)
(a)
Remove(Spare,Trunk)
At(Spare,Trunk)
PutOn(Spare,Axle)
At(Spare,Ground)
At(Flat,Axle)
Finish
At(Spare,Axle)
Start
At(Flat,Axle)
At(Spare,Trunk)
¬
(b)
Start
Remove(Spare,Trunk)
At(Spare,Trunk)
Remove(Flat,Axle)
At(Flat,Axle)
PutOn(Spare,Axle)
At(Spare,Ground)
At(Flat,Axle)
Finish
At(Spare,Axle)
At(Flat,Axle)
At(Spare,Trunk)
¬
(c)
(a) the tire problem expressed as an empty plan. (b) an incomplete partially
ordered plan for the tire problem. Boxes represent actions and arrows indicate that one action
the action PutOn(Spare, Axle). Of course that introduces some new ﬂaws: the preconditions
of the new action are not achieved. The search keeps adding to the plan (backtracking if
make the least commitment possible to ﬁx the ﬂaw. For example, in adding the action
LEAST COMMITMENT
Remove(Spare, Trunk) we need to commit to having it occur before PutOn(Spare, Axle),
but we make no other commitment that places it before or after other actions. If there were a
variable in the action schema that could be left unbound, we would do so.
plicitly represents independent branches of a plan. On the other hand, it has the disadvantage
fully automated classical planning problems.
portant for humans to understand the plans. Operational plans for spacecraft and Mars rovers
uploaded to the vehicles for execution. The plan reﬁnement approach makes it easier for the
humans to understand what the planning algorithms are doing and verify that they are correct.Chapter
Classical Planning
ANALYSIS OF PLANNING APPROACHES
Planning combines the two major areas of AI we have covered so far: search and logic. A
has led both to improvements in performance amounting to several orders of magnitude in
the last decade and to an increased use of planners in industrial applications. Unfortunately,
we do not yet have a clear understanding of which techniques work best on which kinds of
problems. Quite possibly, new techniques will emerge that dominate existing methods.
Planning is foremost an exercise in controlling combinatorial explosion. If there are n
erful weapon. In the best case—full decomposability of the problem—we get an exponential
speedup. Decomposability is destroyed, however, by negative interactions between actions.
resents a similar range of mutex relations, but does so by using the general CNF form rather
than a speciﬁc data structure. Forward search addresses the problem heuristically by trying
to ﬁnd patterns (subsets of propositions) that cover the independent subproblems. Since this
approach is heuristic, it can work even when the subproblems are not completely independent.
Sometimes it is possible to solve a problem efﬁciently by recognizing that negative
interactions can be ruled out. We say that a problem has serializable subgoals if there exists
SERIALIZABLE
SUBGOAL
an order of subgoals such that the planner can achieve them in that order without having to
undo any of the previously achieved subgoals. For example, in the blocks world, if the goal
is to build a tower (e.g., A on B, which in turn is on C, which in turn is on the Table, as in
C on Table, we will never have to undo it while we are achieving the other subgoals. A
backtracking (although it might not always ﬁnd the shortest plan).
As a more complex example, for the Remote Agent planner that commanded NASA’s
ing a spacecraft are serializable. This is perhaps not too surprising, because a spacecraft is
designed by its engineers to be as easy as possible to control (subject to other constraints).
Taking advantage of the serialized ordering of goals, the Remote Agent planner was able to
eliminate most of the search. This meant that it was fast enough to control the spacecraft in
real time, something previously considered impossible.
Planners such as GRAPHPLAN, SATPLAN, and FF have moved the ﬁeld of planning
sentational and combinatorial issues involved, and by the development of useful heuristics.
However, there is a question of how far these techniques will scale. It seems likely that further
progress on larger problems cannot rely only on factored and propositional representations,
the efﬁcient heuristics currently in use.Summary
SUMMARY
In this chapter, we deﬁned the problem of planning in deterministic, fully observable, static
environments. We described the PDDL representation for planning problems and several
algorithmic approaches for solving them. The points to remember:
ble the derivation of effective heuristics and the development of powerful and ﬂexible
algorithms for solving problems.
• PDDL, the Planning Domain Deﬁnition Language, describes the initial and goal states
as conjunctions of literals, and actions in terms of their preconditions and effects.
direction (regression). Effective heuristics can be derived by subgoal independence
assumptions and by various relaxations of the planning problem.
• A planning graph can be constructed incrementally, starting from the initial state. Each
layer contains a superset of all the literals or actions that could occur at that time step
and can be used directly in the GRAPHPLAN algorithm.
a planning problem as a Boolean satisﬁability problem or as a constraint satisfaction
problem; and explicitly searching through the space of partially ordered plans.
resulted in signiﬁcant gains in efﬁciency for planning systems.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
theory and from the practical needs of robotics, scheduling, and other domains. STRIPS (Fikes
ences. STRIPS was designed as the planning component of the software for the Shakey robot
project at SRI. Its overall control structure was modeled on that of GPS, the General Problem
more recent planning efforts.
gorithmic approach; what we call the “classical” language is close to what STRIPS used.Chapter
Classical Planning
schemes for compiling ADL into STRIPS. The Problem Domain Description Language, or
lem decomposition was achieved by computing a subplan for each subgoal and then stringing
the subplans together in some order. This approach, called linear planning by Sacerdoti
LINEAR PLANNING
leaving of actions from different subplans within a single sequence. The notion of serializable
INTERLEAVING
planners are complete.
which steps in a totally ordered plan are reordered so as to avoid conﬂict between subgoals.
ming language (Prolog) and is one of the best examples of the remarkable economy that can
fraction of the size of comparable planners of the time.
construction of partially ordered plans (then called task networks) was pioneered by the
order planning fell out of favor in the late 1990s as faster methods emerged. Nguyen and
rived from a planning graph, their REPOP planner scales up much better than GRAPHPLAN
ning at the time; McDermott suspected that other approaches were not getting the attentionBibliographical and Historical Notes
native representation which makes some of the constraints more explicit. FASTDOWNWARD
for sliding blocks puzzles, which can be thought of as a planning domain, but Hoffmann et al.
planning graphs is based partly on this work and on lecture notes and articles by Subbarao
graph can be used in many different ways to guide the search for a solution. The winner
planning graphs using a local search technique inspired by WALKSAT.
ing that the most compact forms did not necessarily lead to the fastest solution times. A
matic “compiler” for generating propositional representations from PDDL problems. The
Most recently, there has been interest in the representation of plans as binary decision
diagrams, compact data structures for Boolean expressions widely studied in the hardware
BINARY DECISION
DIAGRAM
for proving properties of binary decision diagrams, including the property of being a solutionChapter
Classical Planning
integer programming for planning.
The jury is still out, but there are now some interesting comparisons of the various
can be found without backtracking. GRAPHPLAN and SATPLAN have trouble in domains
with many objects because that means they must create many actions. In some cases the
problem can be delayed or avoided by generating the propositionalized actions dynamically,
only as needed, rather than instantiating them all before the search begins.
the 1990s. It is interesting to see the change in the ﬁve years between the two surveys:
planning, with extensive coverage of robot motion planning.
Planning research has been central to AI since its inception, and papers on planning are
a staple of mainstream AI journals and conferences. There are also specialized conferences
such as the International Conference on AI Planning Systems, the International Workshop on
Planning and Scheduling for Space, and the European Conference on Planning.
EXERCISES
Describe the differences and similarities between problem solving and planning.
concrete instances of Fly(p, from, to) in the state described by
At(P1, JFK) ∧At(P2, SFO) ∧Plane(P1) ∧Plane(P2)
∧Airport(JFK ) ∧Airport(SFO) ?
bananas hanging out of reach from the ceiling. A box is available that will enable the monkey
to reach the bananas if he climbs on it. Initially, the monkey is at A, the bananas at B, and the
box at C. The monkey and box have height Low, but if the monkey climbs onto the box he
will have height High, the same as the bananas. The actions available to the monkey include
Go from one place to another, Push an object from one place to another, ClimbUp onto or
ClimbDown from an object, and Grasp or Ungrasp an object. The result of a Grasp is that
the monkey holds the object if the monkey and object are in the same place at the same height.
a. Write down the initial state description.Chapter
Classical Planning
objects (such as boxes), and turning light switches on and off. The robot itself could not climb
on a box or toggle a switch, but the planner was capable of ﬁnding and printing out plans that
were beyond the robot’s abilities. Shakey’s six actions are the following:
• Go(x, y, r), which requires that Shakey be At x and that x and y are locations In the
same room r. By convention a door between two rooms is in both of them.
• Push a box b from location x to location y within the same room: Push(b, x, y, r). You
will need the predicate Box and constants for the boxes.
• Climb onto a box from position x: ClimbUp(x, b); climb down from a box to position
x: ClimbDown(b, x). We will need the predicate On and the constant Floor.
• Turn a light switch on or off: TurnOn(s, b); TurnOﬀ(s, b). To turn a light on or off,
Shakey must be on top of a box at the light switch’s location.
one of a ﬁnite number of symbols. One cell has a read and write head above it. There is a
ﬁnite set of states the machine can be in, one of which is the accept state. At each time step,
depending on the symbol on the cell under the head and the machine’s current state, there are
a set of actions we can choose from. Each action involves writing a symbol to the cell under
the head, transitioning the machine to a state, and optionally moving the head left or right.
The mapping that determines which actions are allowed is the Turing machine’s program.
Your goal is to control the machine into the accept state.
Represent the Turing machine acceptance problem as a planning problem. If you can
do this, it demonstrates that determining whether a planning problem has a solution is at least
lem results in a relaxed problem.
anomaly. The problem was considered anomalous because the noninterleaved planners of
SUSSMAN ANOMALY
the early 1970s could not solve it. Write a deﬁnition of the problem and solve it, either by
hand or with a planning program. A noninterleaved planner is a planner that, when given two
subgoals G1 and G2, produces either a plan for G1 concatenated with a plan for G2, or vice
versa. Explain why a noninterleaved planner cannot solve this problem.
Prove that backward search with PDDL problems is complete.
Prove the following assertions about planning graphs:
a. A literal that does not appear in the ﬁnal level of the graph cannot be achieved.Chapter
Classical Planning
c. Now suppose there is an additional method of travel: Teleport(p, from, to). It has
the additional precondition ¬Warped(p) and the additional effect Warped(p). Explain
how the situation calculus knowledge base must be modiﬁed.
satisﬁability algorithm is called only once, with the goal g0 ∨g1 ∨· · · ∨gTmax.
a. Will this always return a plan if one exists with length less than or equal to Tmax?
b. Does this approach introduce any new spurious “solutions”?
c. Discuss how one might modify a satisﬁability algorithm such as WALKSAT so that it
ﬁnds short solutions (if they exist) when given a disjunctive goal of this form.PLANNING AND ACTING
IN THE REAL WORLD
In which we see how more expressive representations and more interactive agent
architectures lead to planners that are useful in the real world.
The previous chapter introduced the most basic concepts, representations, and algorithms for
ations of spacecraft, factories, and military campaigns are more complex; they extend both
the representation language and the way the planner interacts with the environment. This
structing plans that are organized hierarchically. This allows human experts to communicate
to the planner what they know about how to solve the problem. Hierarchy also lends itself to
efﬁcient plan construction because the planner can solve a problem at an abstract level before
TIME, SCHEDULES, AND RESOURCES
sentation cannot talk about time: how long an action takes and when it occurs. For example,
assigned to which ﬂights, but we really need to know departure and arrival times as well. This
is the subject matter of scheduling. The real world also imposes many resource constraints;
for example, an airline has a limited number of staff—and staff who are on one ﬂight cannot
be on another at the same time. This section covers methods for representing and solving
planning problems that include temporal and resource constraints.
The approach we take in this section is “plan ﬁrst, schedule later”: that is, we divide
the overall problem into a planning phase in which actions are selected, with some ordering
ral information is added to the plan to ensure that it meets resource and deadline constraints.Chapter
Planning and Acting in the Real World
Jobs({AddEngine1 ≺AddWheels1 ≺Inspect1 },
{AddEngine2 ≺AddWheels2 ≺Inspect2})
straints. The notation A ≺B means that action A must precede action B.
also be used for the planning phase, provided that they produce plans with just the minimal
minimal ordering constraints.
Representing temporal and resource constraints
set of jobs, each of which consists a collection of actions with ordering constraints among
JOB
them. Each action has a duration and a set of resource constraints required by the action.
DURATION
Each constraint speciﬁes a type of resource (e.g., bolts, wrenches, or pilots), the number
of that resource required, and whether that resource is consumable (e.g., the bolts are no
CONSUMABLE
longer available for use) or reusable (e.g., a pilot is occupied during a ﬂight but is available
REUSABLE
poral ordering constraints and resource constraints. As with search and planning problems,
solutions can be evaluated according to a cost function; this can be quite complicated, with
that the cost function is just the total duration of the plan, which is called the makespan.
MAKESPAN
problem consists of two jobs, each of the form [AddEngine, AddWheels, Inspect]. Then theTime, Schedules, and Resources
Resources statement declares that there are four types of resources, and gives the number
nuts. The action schemas give the duration and resource needs of each action. The lug nuts
are consumed as wheels are added to the car, whereas the other resources are “borrowed” at
the start of an action and released at the action’s end.
than as named entities, such as Inspector(I1) and Inspector(I2), is an example of a very
general technique called aggregation. The central idea of aggregation is to group individual
AGGREGATION
objects into quantities when the objects are all indistinguishable with respect to the purpose
at hand. In our assembly problem, it does not matter which inspector inspects the car, so there
are available. With inspectors represented as quantities, a failure is detected immediately and
the algorithm backtracks to try another schedule. With inspectors represented as individuals,
Solving scheduling problems
We begin by considering just the temporal scheduling problem, ignoring resource constraints.
To minimize makespan (plan duration), we must ﬁnd the earliest start times for all the actions
consistent with the ordering constraints supplied with the problem. It is helpful to view these
apply the critical path method (CPM) to this graph to determine the possible start and end
CRITICAL PATH
METHOD
ordered sequence of actions beginning with Start and ending with Finish. (For example,
The critical path is that path whose total duration is longest; the path is “critical”
CRITICAL PATH
because it determines the duration of the entire plan—shortening other paths doesn’t shorten
the plan as a whole, but delaying the start of any action on the critical path slows down the
whole plan. Actions that are off the critical path have a window of time in which they can be
executed. The window is speciﬁed in terms of an earliest possible start time, ES, and a latest
possible start time, LS. The quantity LS – ES is known as the slack of an action. We can
SLACK
Together the ES and LS times for all the actions constitute a schedule for the problem.
SCHEDULE
The following formulas serve as a deﬁnition for ES and LS and also as the outline of a
that A comes before B:
ES(B) = maxA ≺B ES(A) + Duration(A)
LS(Finish) = ES(Finish)
LS(A) = minB ≻A LS(B) −Duration(A) .Chapter
Planning and Acting in the Real World
Start
AddEngine1
AddWheels1
Inspect1
Finish
Inspect2
AddWheels2
AddEngine2
AddEngine1
AddWheels1
Inspect1
AddWheels2
Inspect2
AddEngine2
In solving the problem, we compute the earliest and latest start times as the pair [ES, LS],
displayed in the upper left. The difference between these two numbers is the slack of an
action; actions with zero slack are on the critical path, shown with bold arrows. Bottom: the
same solution shown as a timeline. Grey rectangles represent time intervals during which an
action may be executed, provided that the ordering constraints are respected. The unoccupied
portion of a gray rectangle indicates the slack.
B such that all the actions that come immediately before B have ES values assigned, we
set ES(B) to be the maximum of the earliest ﬁnish times of those immediately preceding
actions, where the earliest ﬁnish time of an action is deﬁned as the earliest start time plus the
duration. This process repeats until every action has been assigned an ES value. The LS
values are computed in a similar manner, working backward from the Finish action.
The complexity of the critical path algorithm is just O(Nb), where N is the number of
actions and b is the maximum branching factor into or out of an action. (To see this, note that
the LS and ES computations are done once for each action, and each computation iterates
ordering on the actions and no resource constraints, is quite easy.
ﬁned as a conjunction of linear inequalities on the start and end times. When we introduceTime, Schedules, and Resources
AddEngine1
AddWheels1
Inspect1
AddWheels2
Inspect2
AddEngine2
ble schedules, depending on which assembly uses the engine hoist ﬁrst; we’ve shown the
require the same EngineHoist and so cannot overlap. The “cannot overlap” constraint is a
disjunction of two linear inequalities, one for each possible ordering. The introduction of
Notice that there is no time at which both inspectors are required, so we can immediately
move one of our two inspectors to a more productive position.
The complexity of scheduling with resource constraints is often seen in practice as
bound, simulated annealing, tabu search, constraint satisfaction, and other techniques from
MINIMUM SLACK
each iteration, schedule for the earliest possible start whichever unscheduled action has all
its predecessors scheduled and has the least slack; then update the ES and LS times for each
heuristic in constraint satisfaction. It often works well in practice, but for our assembly
Up to this point, we have assumed that the set of actions and ordering constraints is
ﬁxed. Under these assumptions, every scheduling problem can be solved by a nonoverlapping
sequence that avoids all resource conﬂicts, provided that each action is feasible by itself. If
a scheduling problem is proving very difﬁcult, however, it may not be a good idea to solve
it this way—it may be better to reconsider the actions and constraints, in case that leads to a
much easier scheduling problem. Thus, it makes sense to integrate planning and scheduling
same way they detect conﬂicts with causal links. Heuristics can be devised to estimate the
total completion time of a plan. This is currently an active area of research.Chapter
Planning and Acting in the Real World
HIERARCHICAL PLANNING
set of atomic actions. Actions can be strung together into sequences or branching networks;
For plans executed by the human brain, atomic actions are muscle activations. In very
To bridge this gap, AI systems will probably have to do what humans appear to do: plan
at higher levels of abstraction. A reasonable plan for the Hawaii vacation might be “Go to
the action “Go to San Francisco airport” can be viewed as a planning task in itself, with a
Each of these actions, in turn, can be decomposed further, until we reach the level of actions
that can be executed without deliberation to generate the required motor control sequences.
In this example, we see that planning can occur both before and during the execution
of the plan; for example, one would probably defer the problem of planning a route from a
been found during execution. Thus, that particular action will remain at an abstract level
concentrate on the aspect of hierarchical decomposition, an idea that pervades almost all
HIERARCHICAL
DECOMPOSITION
attempts to manage complexity. For example, complex software is created from a hierarchy
porations have hierarchies of departments, subsidiaries, and branch ofﬁces. The key beneﬁt
of hierarchical structure is that, at each level of the hierarchy, a computational task, military
mission, or administrative function is reduced to a small number of activities at the next lower
level, so the computational cost of ﬁnding the correct way to arrange those activities for the
current problem is small. Nonhierarchical methods, on the other hand, reduce a task to a
The basic formalism we adopt to understand hierarchical decomposition comes from the area
HIERARCHICAL TASK
NETWORK
assume full observability and determinism and the availability of a set of actions, now called
primitive actions, with standard precondition–effect schemas. The key additional concept is
PRIMITIVE ACTIONHierarchical Planning
Reﬁnement(Go(Home, SFO),
STEPS: [Drive(Home, SFOLongTermParking),
Shuttle(SFOLongTermParking, SFO)] )
Reﬁnement(Go(Home, SFO),
STEPS: [Taxi(Home, SFO)] )
Reﬁnement(Navigate([a, b], [x, y]),
PRECOND: a = x ∧b = y
STEPS: [ ] )
Reﬁnement(Navigate([a, b], [x, y]),
Reﬁnement(Navigate([a, b], [x, y]),
. . .
Francisco airport and navigating in the vacuum world. In the latter case, note the recursive
nature of the reﬁnements and the use of preconditions.
example given earlier. Each HLA has one or more possible reﬁnements, into a sequence1
REFINEMENT
of actions, each of which may be an HLA or a primitive action (which has no reﬁnements
by deﬁnition). For example, the action “Go to San Francisco airport,” represented formally
same ﬁgure shows a recursive reﬁnement for navigation in the vacuum world: to get to a
destination, take a step, and then go to the destination.
about how to do things. For instance, the reﬁnements for Go(Home, SFO) say that to get to
the airport you can drive or take a taxi; buying milk, sitting down, and moving the knight to
e4 are not to be considered.
An HLA reﬁnement that contains only primitive actions is called an implementation
IMPLEMENTATION
of the HLA. For example, in the vacuum world, the sequences [Right, Right, Down] and
HLA in the sequence. Given the precondition–effect deﬁnitions of each primitive action, it is
least one of its implementations achieves the goal from that state. The “at least one” in this
deﬁnition is crucial—not all implementations need to achieve the goal, because the agent gets
different HLAs in a plan to share actions. We omit these important complications in the interest of understanding
the basic concepts of hierarchical planning.Chapter
Planning and Acting in the Real World
to decide which implementation it will execute. Thus, the set of possible implementations in
HTN planning—each of which may have a different outcome—is not the same as the set of
possible outcomes in nondeterministic planning. There, we required that a plan work for all
outcomes because the agent doesn’t get to choose the outcome; nature does.
The simplest case is an HLA that has exactly one implementation. In that case, we
can compute the preconditions and effects of the HLA from those of the implementation
can be shown that the right collection of HLAs can result in the time complexity of blind
though devising such a collection of HLAs may be a nontrivial task in itself. When HLAs
have multiple possible implementations, there are two options: one is to search among the
latter method enables the derivation of provably correct abstract plans, without the need to
consider their implementations.
Searching for primitive solutions
HTN planning is often formulated with a single “top level” action called Act, where the aim is
to ﬁnd an implementation of Act that achieves the goal. This approach is entirely general. For
example, classical planning problems can be deﬁned as follows: for each primitive action ai,
provide one reﬁnement of Act with steps [ai, Act]. That creates a recursive deﬁnition of Act
that lets us add actions. But we need some way to stop the recursion; we do that by providing
one more reﬁnement for Act, one with an empty list of steps and with a precondition equal
to the goal of the problem. This says that if the goal is already achieved, then the right
implementation is to do nothing.
The approach leads to a simple algorithm: repeatedly choose an HLA in the current
plan and replace it with one of its reﬁnements, until the plan achieves the goal. One possible
ered in order of depth of nesting of the reﬁnements, rather than number of primitive steps. It
iterative deepening versions.
In essence, this form of hierarchical search explores the space of sequences that conform
to the knowledge contained in the HLA library about how things are to be done. A great deal
of knowledge can be encoded, not just in the action sequences speciﬁed in each reﬁnement but
also in the preconditions for the reﬁnements. For some domains, HTN planners have been
aspect of HTN plans is that they are, by deﬁnition, hierarchically structured; usually this
makes them easy for humans to understand.Hierarchical Planning
frontier ←a FIFO queue with [Act] as the only element
loop do
if EMPTY?(frontier) then return failure
plan ←POP(frontier) /* chooses the shallowest plan in frontier */
hla ←the ﬁrst HLA in plan, or null if none
preﬁx,suﬃx ←the action subsequences before and after hla in plan
if hla is null then /* so plan is primitive and outcome is its result */
if outcome satisﬁes problem.GOAL then return plan
else for each sequence in REFINEMENTS(hla,outcome,hierarchy) do
frontier ←INSERT(APPEND(preﬁx,sequence,suﬃx),frontier)
initial plan supplied to the algorithm is [Act]. The REFINEMENTS function returns a set of
action sequences, one for each reﬁnement of the HLA whose preconditions are satisﬁed by
the speciﬁed state, outcome.
alized case. Suppose that a planning problem has a solution with d primitive actions. For
ular reﬁnement structure: each nonprimitive action has r possible reﬁnements, each into
k actions at the next lower level. We want to know how many different reﬁnement trees
there are with this structure. Now, if there are d actions at the primitive level, then the
number of levels below the root is logk d, so the number of internal reﬁnement nodes is
this formula, we see that keeping r small and k large can result in huge savings: essentially
we are taking the kth root of the nonhierarchical cost, if b and r are comparable. Small r and
large k means a library of HLAs with a small number of reﬁnements each yielding a long
sible: long action sequences that are usable across a wide range of problems are extremely
precious.
The key to HTN planning, then, is the construction of a plan library containing known
perience of constructing a plan from scratch, the agent can save the plan in the library as a
become more and more competent over time as new methods are built on top of old methods.
One important aspect of this learning process is the ability to generalize the methods that
are constructed, eliminating detail that is speciﬁc to the problem instance (e.g., the name ofChapter
Planning and Acting in the Real World
the builder or the address of the plot of land) and keeping just the key elements of the plan.
inconceivable that humans could be as competent as they are without some such mechanism.
Searching for abstract solutions
tive action sequences to determine if a plan is workable. This contradicts common sense: one
[Drive(Home, SFOLongTermParking), Shuttle(SFOLongTermParking, SFO)]
gets one to the airport without having to determine a precise route, choice of parking spot,
and so on. The solution seems obvious: write precondition–effect descriptions of the HLAs,
just as we write down what the primitive actions do. From the descriptions, it ought to be
work on the problem of reﬁning each step of the plan. This gives us the exponential reduction
achieve the goal (by virtue of the descriptions of its steps) does in fact achieve the goal in
the sense deﬁned earlier: it must have at least one implementation that does achieve the goal.
This property has been called the downward reﬁnement property for HLA descriptions.
DOWNWARD
REFINEMENT
PROPERTY
the goal must in fact do so—otherwise, the descriptions are making some false claim about
what the HLAs do. We have already seen how to write true descriptions for HLAs that have
implementations. How can we describe the effects of an action that can be implemented in
many different ways?
One safe answer (at least for problems where all preconditions and goals are positive) is
to include only the positive effects that are achieved by every implementation of the HLA and
the negative effects of any implementation. Then the downward reﬁnement property would
be satisﬁed. Unfortunately, this semantics for HLAs is much too conservative. Consider again
ment, a simple world in which one can always drive to the airport and park, but taking a taxi
requires Cash as a precondition. In that case, Go(Home, SFO) doesn’t always get you to
the airport. In particular, it fails if Cash is false, and so we cannot assert At(Agent, SFO) as
an effect of the HLA. This makes no sense, however; if the agent didn’t have Cash, it would
drive itself. Requiring that an effect hold for every implementation is equivalent to assuming
our case, the agent itself will choose the implementation.
DEMONIC
NONDETERMINISMHierarchical Planning
(a)
(b)
Schematic examples of reachable sets. The set of goal states is shaded. Black
able set of an HLA h1 in a state s. (b) The reachable set for the sequence [h1, h2]. Because
this intersects the goal set, the sequence achieves the goal.
terminism, where the agent itself makes the choices. We borrow this term to deﬁne angelic
ANGELIC
NONDETERMINISM
ANGELIC SEMANTICS
mantics is the reachable set of an HLA: given a state s, the reachable set for an HLA h,
REACHABLE SET
written as REACH(s, h), is the set of states reachable by any of the HLA’s implementations.
The key idea is that the agent can choose which element of the reachable set it ends up in
when it executes the HLA; thus, an HLA with multiple reﬁnements is more “powerful” than
the same HLA with fewer reﬁnements. We can also deﬁne the reachable set of a sequences of
HLAs. For example, the reachable set of a sequence [h1, h2] is the union of all the reachable
sets obtained by applying h2 in each state in the reachable set of h1:
REACH(s, [h1, h2]) =

s′∈REACH(s, h1)
REACH(s′, h2) .
reachable set intersects the set of goal states. (Compare this to the much stronger condition
for demonic semantics, where every member of the reachable set has to be a goal state.)
Conversely, if the reachable set doesn’t intersect the goal, then the plan deﬁnitely doesn’t
level plans, looking for one whose reachable set intersects the goal; once that happens, the
algorithm can commit to that abstract plan, knowing that it works, and focus on reﬁning
the plan further. We will come back to the algorithmic issues later; ﬁrst, we consider the
question of how the effects of an HLA—the reachable set for each possible initial state—areChapter
Planning and Acting in the Real World
made to each ﬂuent. Think of a ﬂuent as a state variable. A primitive action can add or delete
fourth possibility: ﬂipping a variable to its opposite.)
An HLA under angelic semantics can do more: it can control the value of a variable,
setting it to true or false depending on which implementation is chosen. In fact, an HLA can
have nine different effects on a variable: if the variable starts out true, it can always keep
it true, always make it false, or have a choice; if the variable starts out false, it can always
keep it false, always make it true, or have a choice; and the three choices for each case can
be combined arbitrarily, making nine. Notationally, this is a bit challenging. We’ll use the 
symbol to mean “possibly, if the agent so chooses.” Thus, an effect +A means “possibly add
A,” that is, either leave A unchanged or make it true. Similarly, −A means “possibly delete
A” and ±A means “possibly add or delete A.” For example, the HLA Go(Home, SFO),
take a taxi), so it should have the effect −Cash. Thus, we see that the descriptions of HLAs
are derivable, in principle, from the descriptions of their reﬁnements—in fact, this is required
if we want true HLA descriptions, such that the downward reﬁnement property holds. Now,
suppose we have the following schemas for the HLAs h1 and h2:
Action(h1, PRECOND:¬A, EFFECT:A ∧−B) ,
Action(h2, PRECOND:¬B, EFFECT: +A ∧±C) .
That is, h1 adds A and possible deletes B, while h2 possibly adds A and has full control over
C. Now, if only B is true in the initial state and the goal is A ∧C then the sequence [h1, h2]
achieves the goal: we choose an implementation of h1 that makes B false, then choose an
implementation of h2 that leaves A true and makes C true.
The preceding discussion assumes that the effects of an HLA—the reachable set for
any given initial state—can be described exactly by describing the effect on each variable. It
fects because an HLA may have inﬁnitely many implementations and may produce arbitrarily
possibly adds At(Car, SFOLongTermParking); but it cannot do both—in fact, it must do
exactly one. As with belief states, we may need to write approximate descriptions. We will
OPTIMISTIC
DESCRIPTION
overstate the reachable set, while a pessimistic description REACH−(s,h) may understate
PESSIMISTIC
DESCRIPTION
the reachable set. Thus, we have
For example, an optimistic description of Go(Home, SFO) says that it possible deletes Cash
and possibly adds At(Car, SFOLongTermParking). Another good example arises in the
the exact reachable set is quite wiggly.
With approximate descriptions, the test for whether a plan achieves the goal needs to
be modiﬁed slightly. If the optimistic reachable set for the plan doesn’t intersect the goal,Hierarchical Planning
(a)
(b)
set of goal states is shaded. For each plan, the pessimistic (solid lines) and optimistic (dashed
lines) reachable sets are shown. (a) The plan indicated by the black arrow deﬁnitely achieves
the goal, while the plan indicated by the gray arrow deﬁnitely doesn’t. (b) A plan that would
need to be reﬁned further to determine if it really does achieve the goal.
then the plan doesn’t work; if the pessimistic reachable set intersects the goal, then the plan
with approximate descriptions, there is a middle ground: if the optimistic set intersects the
When this circumstance arises, the uncertainty can be resolved by reﬁning the plan. This is
a very common situation in human reasoning. For example, in planning the aforementioned
Prudence would indicate that this ambitious plan needs to be reﬁned by adding details of
An algorithm for hierarchical planning with approximate angelic descriptions is shown
When a workable abstract plan is found, the algorithm decomposes the original problem into
subproblems, one for each step of the plan. The initial state and goal for each subproblemChapter
Planning and Acting in the Real World
frontier ←a FIFO queue with initialPlan as the only element
loop do
if EMPTY?(frontier) then return fail
plan ←POP(frontier) /* chooses the shallowest node in frontier */
if plan is primitive then return plan
ﬁnalState ←any element of guaranteed
hla ←some HLA in plan
preﬁx,suﬃx ←the action subsequences before and after hla in plan
for each sequence in REFINEMENTS(hla,outcome,hierarchy) do
frontier ←INSERT(APPEND(preﬁx,sequence,suﬃx),frontier)
function DECOMPOSE(hierarchy,s0,plan,sf ) returns a solution
solution ←an empty plan
while plan is not empty do
sf ←si
return solution
A hierarchical planning algorithm that uses angelic semantics to identify and
goal through the ﬁnal action.
CleanWholeRoom. (Cleaning the room could be implemented with the repeated application
of another HLA to clean each row.) Since there are ﬁve actions in this domain, the costPlanning and Acting in Nondeterministic Domains
quence and prunes away the other options. Notice that cleaning a set of rooms by cleaning
each room in turn is hardly rocket science: it is easy for humans precisely because of the
hierarchical structure of the task. When we consider how difﬁcult humans ﬁnd it to solve
plex problems derives to a great extent from their skill in abstracting and decomposing the
problem to eliminate combinatorics.
notion of reachable set. Instead of a state being reachable or not, it has a cost for the most
simistic descriptions bound these costs. In this way, angelic search can ﬁnd provably optimal
abstract plans without considering their implementations. The same approach can be used to
obtain effective hierarchical lookahead algorithms for online search, in the style of LRTA∗
HIERARCHICAL
LOOKAHEAD
as planning a vacation to Hawaii—consideration of alternatives is done initially at an abstract
level over long time scales; some parts of the plan are left quite abstract until execution time,
such as how to spend two lazy days on Molokai, while others parts are planned in detail, such
as the ﬂights to be taken and lodging to be reserved—without these reﬁnements, there is no
guarantee that the plan would be feasible.
PLANNING AND ACTING IN NONDETERMINISTIC DOMAINS
also similar: sensorless planning (also known as conformant planning) for environments
with no observations; contingency planning for partially observable and nondeterministic
environments; and online planning and replanning for unknown environments.
ferences. These arise because planners deal with factored representations rather than atomic
vation and the way we represent belief states—the sets of possible physical states the agent
search heuristics.
Consider this problem: given a chair and a table, the goal is to have them match—have
the same color. In the initial state we have two cans of paint, but the colors of the paint and
the furniture are unknown. Only the table is initially in the agent’s ﬁeld of view:
Init(Object(Table) ∧Object(Chair) ∧Can(C1) ∧Can(C2) ∧InView(Table))
Goal(Color(Chair, c) ∧Color(Table, c))
There are two actions: removing the lid from a paint can and painting an object using the
paint from an open can. The action schemas are straightforward, with one exception: we now
allow preconditions and effects to contain variables that are not part of the action’s variableChapter
Planning and Acting in the Real World
list. That is, Paint(x, can) does not mention the variable c, representing the color of the
paint in the can. In the fully observable case, this is not allowed—we would have to name
the action Paint(x, can, c). But in the partially observable case, we might or might not
know what color is in the can. (The variable c is universally quantiﬁed, just like all the other
variables in an action schema.)
Action(RemoveLid(can),
PRECOND:Can(can)
EFFECT:Open(can))
Action(Paint(x, can),
PRECOND:Object(x) ∧Can(can) ∧Color(can, c) ∧Open(can)
EFFECT:Color(x, c))
To solve a partially observable problem, the agent will have to reason about the percepts it will
obtain when it is executing the plan. The percept will be supplied by the agent’s sensors when
this model was given by a function, PERCEPT(s). For planning, we augment PDDL with a
new type of schema, the percept schema:
PERCEPT SCHEMA
Percept(Color(x, c),
PRECOND:Object(x) ∧InView(x)
Percept(Color(can, c),
PRECOND:Can(can) ∧InView(can) ∧Open(can)
The ﬁrst schema says that whenever an object is in view, the agent will perceive the color
of the object (that is, for the object x, the agent will learn the truth value of Color(x, c) for
all c). The second schema says that if an open can is in view, then the agent perceives the
color of the paint in the can. Because there are no exogenous events in this world, the color
of an object will remain the same, even if it is not being perceived, until the agent performs
an action to change the object’s color. Of course, the agent will need an action that causes
objects (one at a time) to come into view:
Action(LookAt(x),
PRECOND:InView(y) ∧(x ̸= y)
EFFECT:InView(x) ∧¬InView(y))
For a fully observable environment, we would have a Percept axiom with no preconditions
for each ﬂuent. A sensorless agent, on the other hand, has no Percept axioms at all. Note
that even a sensorless agent can solve the painting problem. One solution is to open any can
of paint and apply it to both chair and table, thus coercing them to be the same color (even
though the agent doesn’t know what the color is).
A contingent planning agent with sensors can generate a better plan. First, look at the
table and chair to obtain their colors; if they are already the same then the plan is done. If
not, look at the paint cans; if the paint in a can is the same color as one piece of furniture,
then apply that paint to the other piece. Otherwise, paint both pieces with any color.
Finally, an online planning agent might generate a contingent plan with fewer branches
at ﬁrst—perhaps ignoring the possibility that no cans match any of the furniture—and dealPlanning and Acting in Nondeterministic Domains
with problems when they arise by replanning. It could also deal with incorrectness of its
action schemas. Whereas a contingent planner simply assumes that the effects of an action
always succeed—that painting the chair does the job—a replanning agent would check the
result and make an additional plan to ﬁx any unexpected failure, such as an unpainted area or
the original color showing through.
In the real world, agents use a combination of approaches. Car manufacturers sell spare
tires and air bags, which are physical embodiments of contingent plan branches designed
to handle punctures or crashes. On the other hand, most car drivers never consider these
possibilities; when a problem arises they respond as replanning agents. In general, agents
plan only for contingencies that have important consequences and a nonnegligible chance
of happening. Thus, a car driver contemplating a trip across the Sahara desert should make
explicit contingency plans for breakdowns, whereas a trip to the supermarket requires less
advance planning. We next look at each of the three approaches in more detail.
Sensorless planning
ences are that the underlying physical transition model is represented by a collection of action
schemas and the belief state can be represented by a logical formula instead of an explicitly
enumerated set of states. For simplicity, we assume that the underlying planning problem is
deterministic.
The initial belief state for the sensorless painting problem can ignore InView ﬂuents
because the agent has no sensors.
Furthermore, we take as given the unchanging facts
lief state. The agent doesn’t know the colors of the cans or the objects, or whether the cans
b0 = Color(x, C(x)) .
and negative ﬂuents, and if a ﬂuent does not appear, its value is unknown. Thus, the belief
state corresponds exactly to the set of possible worlds that satisfy the formula. Given this
initial belief state, the following action sequence is a solution:
[RemoveLid(Can1), Paint(Chair, Can1), Paint(Table, Can1)] .
We now show how to progress the belief state through the action sequence to show that the
ﬁnal belief state satisﬁes the goal.
conditions are satisﬁed by b. (The other actions cannot be used because the transition model
doesn’t deﬁne the effects of actions whose preconditions might be unsatisﬁed.) AccordingChapter
Planning and Acting in the Real World
applicable action a in a deterministic world is as follows:
where RESULTP deﬁnes the physical transition model. For the time being, we assume that the
the new belief state b′, we must consider what happens to each literal ℓin each physical state
s in b when action a is applied. For literals whose truth value is already known in b, the truth
value in b′ is computed from the current value and the add list and delete list of the action.
(For example, if ℓis in the delete list of the action, then ¬ℓis added to b′.) What about a
literal whose truth value is unknown in b? There are three cases:
will not appear in b′.
Hence, we see that the calculation of b′ is almost identical to the observable case, which was
b′ = RESULT(b, a) = (b −DEL(a)) ∪ADD(a) .
that RESULT(b, a) is computed by starting with b, setting any atom that appears in DEL(a)
to false, and setting any atom that appears in ADD(a) to true. For example, if we apply
RemoveLid(Can1) to the initial belief state b0, we get
b1 = Color(x, C(x)) ∧Open(Can1) .
When we apply the action Paint(Chair, Can1), the precondition Color(Can1, c) is satisﬁed
state is
b2 = Color(x, C(x)) ∧Open(Can1) ∧Color(Chair, C(Can1)) .
Finally, we apply the action Paint(Table, Can1) to obtain
b3 = Color(x, C(x)) ∧Open(Can1) ∧Color(Chair, C(Can1))
∧Color(Table, C(Can1)) .
The ﬁnal belief state satisﬁes the goal, Color(Table, c) ∧Color(Chair, c), with the variable
c bound to C(Can1).
The preceding analysis of the update rule has shown a very important fact: the family
of belief states deﬁned as conjunctions of literals is closed under updates deﬁned by PDDL
action schemas. That is, if the belief state starts as a conjunction of literals, then any update
will yield a conjunction of literals. That means that in a world with n ﬂuents, any belief
state can be represented by a conjunction of size O(n). This is a very comforting result,
considering that there are 2n states in the world. It says we can compactly represent all the
subsets of those 2n states that we will ever need. Moreover, the process of checking for beliefPlanning and Acting in Nondeterministic Domains
states that are subsets or supersets of previously visited belief states is also easy, at least in
the propositional case.
The ﬂy in the ointment of this pleasant picture is that it only works for action schemas
that have the same effects for all states in which their preconditions are satisﬁed. It is this
Let the ﬂuents be AtL and AtR for the location of the robot and CleanL and CleanR for
the state of the squares. According to the deﬁnition of the problem, the Suck action has no
cation: when the robot is AtL, the result is CleanL, but when it is AtR, the result is CleanR.
For such actions, our action schemas will need something new: a conditional effect. These
CONDITIONAL
EFFECT
pared against the current state, and effect is a formula describing the resulting state. For the
vacuum world, we have
Action(Suck,
EFFECT:when AtL: CleanL ∧when AtR: CleanR) .
When applied to the initial belief state True, the resulting belief state is (AtL ∧CleanL) ∨
ﬂuents in a belief state, leading to belief states of exponential size in the worst case.
ate the resulting state; if none are satisﬁed, then the resulting state is unchanged. On the other
hand, if a precondition is unsatisﬁed, then the action is inapplicable and the resulting state
is undeﬁned. From the point of view of sensorless planning, it is better to have conditional
effects than an inapplicable action. For example, we could split Suck into two actions with
unconditional effects as follows:
Action(SuckL,
PRECOND:AtL; EFFECT:CleanL)
Action(SuckR,
PRECOND:AtR; EFFECT:CleanR) .
nately, we cannot determine the applicability of SuckL and SuckR in the initial belief state.
It seems inevitable, then, that nontrivial problems will involve wiggly belief states, just
like those encountered when we considered the problem of state estimation for the wumpus
if it contains all literals whose truth values can be determined and treats all other literals as
unknown. While this approach is sound, in that it never generates an incorrect plan, it is
incomplete because it may be unable to ﬁnd solutions to problems that necessarily involve
interactions among literals. To give a trivial example, if the goal is for the robot to be onChapter
Planning and Acting in the Real World
states will not ﬁnd it.
Perhaps a better solution is to look for action sequences that keep the belief state
as simple as possible. For example, in the sensorless vacuum world, the action sequence
[Right, Suck, Left, Suck] generates the following sequence of belief states:
b0 = True
b1 = AtR
b2 = AtR ∧CleanR
b3 = AtL ∧CleanR
b4 = AtL ∧CleanR ∧CleanL
not lost on humans: we are always performing little actions (checking the time, patting our
pockets to make sure we have the car keys, reading street signs as we navigate through a city)
to eliminate uncertainty and keep our belief state manageable.
lief states: don’t bother computing them at all. Suppose the initial belief state is b0 and we
would like to know the belief state resulting from the action sequence [a1, . . . , am]. Instead
ambiguous representation of the belief state, and it’s quite concise—O(n + m) where n is
ever: determining whether the goal is satisﬁed, or an action is applicable, may require a lot
of computation.
then the plan achieves the goal if b0 ∧Am |= Gm, that is, if b0 ∧Am ∧¬Gm is unsatisﬁable.
Given a modern SAT solver, it may be possible to do this much more quickly than computing
the full belief state. For example, if none of the actions in the sequence has a particular goal
ﬂuent in its add list, the solver will detect this immediately. It also helps if partial results
about the belief state—for example, ﬂuents known to be true or false—are cached to simplify
subsequent computations.
The ﬁnal piece of the sensorless planning puzzle is a heuristic function to guide the
mate (perhaps admissible) of the cost of achieving the goal from the given belief state. With
belief states, we have one additional fact: solving any subset of a belief state is necessarily
easier than solving the belief state:
Hence, any admissible heuristic computed for a subset is admissible for the belief state itself.
The most obvious candidates are the singleton subsets, that is, individual physical states. WePlanning and Acting in Nondeterministic Domains
can take any random collection of states s1, . . . , sN that are in the belief state b, apply any
H(b) = max{h(s1), . . . , h(sN)}
as the heuristic estimate for solving b. We could also use a planning graph directly on b itself:
b. For example, if b is in disjunctive normal form (DNF), each term of the DNF formula is
a conjunction of literals that entails b and can form the initial layer of a planning graph. As
before, we can take the maximum of the heuristics obtained from each set of literals. We can
seems to work quite well in practice.
Contingent planning
determinism, or both. For the partially observable painting problem with the percept axioms
given earlier, one possible contingent solution is as follows:
[LookAt(Table), LookAt(Chair),
if Color(Table, c) ∧Color(Chair, c) then NoOp
else [RemoveLid(Can1), LookAt(Can1), RemoveLid(Can2), LookAt(Can2),
if Color(Table, c) ∧Color(can, c) then Paint(Chair, can)
else if Color(Chair, c) ∧Color(can, c) then Paint(Table, can)
else [Paint(Chair, Can1), Paint(Table, Can1)]]]
Variables in this plan should be considered existentially quantiﬁed; the second line says
that if there exists some color c that is the color of the table and the chair, then the agent
agent can maintain its belief state as a logical formula and evaluate each branch condition
by determining if the belief state entails the condition formula or its negation. (It is up to
conditions, the formula may be satisﬁed in more than one way; for example, the condition
both cans are the same color as the table. In that case, the agent can choose any satisfying
substitution to apply to the rest of the plan.
quent percept is done in two stages. The ﬁrst stage calculates the belief state after the action,
just as for the sensorless agent:
ˆb = (b −DEL(a)) ∪ADD(a)
where, as before, we have assumed a belief state represented as a conjunction of literals. The
second stage is a little trickier. Suppose that percept literals p1, . . . , pk are received. One
might think that we simply need to add these into the belief state; in fact, we can also inferChapter
Planning and Acting in the Real World
that the preconditions for sensing are satisﬁed. Now, if a percept p has exactly one percept
axiom, Percept(p, PRECOND:c), where c is a conjunction of literals, then those literals can
be thrown into the belief state along with p. On the other hand, if p has more than one percept
axiom whose preconditions might hold according to the predicted belief state ˆb, then we have
to add in the disjunction of the preconditions. Obviously, this takes the belief state outside
classes of solutions.
Given a mechanism for computing exact or approximate belief states, we can generate
contingent plans with an extension of the AND–OR forward search over belief states used
disjunction in the EFFECT of the action schema—can be accommodated with minor changes
function, many of the methods suggested for sensorless planning are also applicable in the
partially observable, nondeterministic case.
Online replanning
pressive, the robot probably does not seem at all intelligent because the motion is a ﬁxed,
ingful sense. Now suppose that a poorly attached door falls off the car just as the robot is
picks up the door, checks it for scratches, reattaches it to the car, sends an email to the ﬂoor
supervisor, switches back to the welding actuator, and resumes its work. All of a sudden,
the robot’s behavior seems purposive rather than rote; we assume it results not from a vast,
precomputed contingent plan but from an online replanning process—which means that the
robot does need to know what it’s trying to do.
Replanning presupposes some form of execution monitoring to determine the need for
EXECUTION
MONITORING
a new plan. One such need arises when a contingent planning agent gets tired of planning
of a partially constructed contingent plan can simply say Replan; if such a branch is reached
during execution, the agent reverts to planning mode. As we mentioned earlier, the decision
as to how much of the problem to solve in advance and how much to leave to replanning
is one that involves tradeoffs among possible events with different costs and probabilities of
occurring. Nobody wants to have their car break down in the middle of the Sahara desert and
only then think about having enough water.
the Mbale meteorite hit a small boy on the head; fortunately, its descent was slowed by banana leaves (Jenniskens
injuries resulted from any of these incidents, suggesting that the need for preplanning against such contingencies
is sometimes overstated.Planning and Acting in Nondeterministic Domains
whole plan
plan
repair
S
P
O
E
G
continuation
Before execution, the planner comes up with a plan, here called whole plan,
to get from S to G. The agent executes steps of the plan until it expects to be in state E, but
observes it is actually in O. The agent then replans for the minimal repair plus continuation
to reach G.
Replanning may also be needed if the agent’s model of the world is incorrect. The model
for an action may have a missing precondition—for example, the agent may not know that
MISSING
PRECONDITION
removing the lid of a paint can often requires a screwdriver; the model may have a missing
effect—for example, painting an object may get paint on the ﬂoor as well; or the model may
MISSING EFFECT
have a missing state variable—for example, the model given earlier has no notion of the
MISSING STATE
VARIABLE
amount of paint in a can, of how its actions affect this amount, or of the need for the amount
to be nonzero. The model may also lack provision for exogenous events such as someone
EXOGENOUS EVENT
knocking over the paint can. Exogenous events can also include changes in the goal, such
as the addition of the requirement that the table and chair not be painted black. Without the
ability to monitor and replan, an agent’s behavior is likely to be extremely fragile if it relies
on absolute correctness of its model.
guish three levels:
ACTION MONITORING
tions still hold.
• Plan monitoring: before executing an action, the agent veriﬁes that the remaining plan
PLAN MONITORING
will still succeed.
• Goal monitoring: before executing an action, the agent checks to see if there is a better
GOAL MONITORING
set of goals it could be trying to achieve.
original plan, wholeplan, and the part of the plan that has not been executed yet, which is
denoted by plan. After executing the ﬁrst few steps of the plan, the agent expects to be in
state E. But the agent observes it is actually in state O. It then needs to repair the plan by
ﬁnding some point P on the original plan that it can get back to. (It may be that P is the goal
state, G.) The agent tries to minimize the total cost of the plan: the repair part (from O to P)
plus the continuation (from P to G).Chapter
Planning and Acting in the Real World
Now let’s return to the example problem of achieving a chair and table of matching
color. Suppose the agent comes up with this plan:
[LookAt(Table), LookAt(Chair),
if Color(Table, c) ∧Color(Chair, c) then NoOp
else [RemoveLid(Can1), LookAt(Can1),
if Color(Table, c) ∧Color(Can1, c) then Paint(Chair, Can1)
else REPLAN]] .
Now the agent is ready to execute the plan. Suppose the agent observes that the table and
can of paint are white and the chair is black. It then executes Paint(Chair, Can1). At this
point a classical planner would declare victory; the plan has been executed. But an online
execution monitoring agent needs to check the preconditions of the remaining empty plan—
that the table and chair are the same color. Suppose the agent perceives that they do not
have the same color—in fact, the chair is now a mottled gray because the black paint is
showing through. The agent then needs to ﬁgure out a position in whole plan to aim for
and a repair action sequence to get there. The agent notices that the current state is identical
to the precondition before the Paint(Chair, Can1) action, so the agent chooses the empty
sequence for repair and makes its plan be the same [Paint] sequence that it just attempted.
With this new plan in place, execution monitoring resumes, and the Paint action is retried.
This behavior will loop until the chair is perceived to be completely painted. But notice that
the loop is created by a process of plan–execute–replan, rather than by an explicit loop in a
plan. Note also that the original plan need not cover every contingency. If the agent reaches
the step marked REPLAN, it can then generate a new plan (perhaps involving Can2).
Action monitoring is a simple method of execution monitoring, but it can sometimes
lead to less than intelligent behavior. For example, suppose there is no black or white paint,
and the agent constructs a plan to solve the painting problem by painting both the chair and
table red. Suppose that there is only enough red paint for the chair. With action monitoring,
the agent would go ahead and paint the chair red, then notice that it is out of paint and cannot
paint the table, at which point it would replan a repair—perhaps painting both chair and table
remaining plan no longer works. Thus, it would not waste time painting the chair red. Plan
monitoring achieves this by checking the preconditions for success of the entire remaining
plan—that is, the preconditions of each step in the plan, except those preconditions that are
achieved by another step in the remaining plan. Plan monitoring cuts off execution of a
Plan monitoring also allows for serendipity—accidental success. If someone comes along
and paints the table red at the same time that the agent is painting the chair red, then the ﬁnal
plan preconditions are satisﬁed (the goal has been achieved), and the agent can go home early.
It is straightforward to modify a planning algorithm so that each action in the plan
is annotated with the action’s preconditions, thus enabling action monitoring. It is slightly
to get another ball and plug its hole.Multiagent Planning
the advantage that they have already built up structures that contain the relations necessary
done by careful bookkeeping as the goal ﬂuents are regressed through the plan.
Now that we have described a method for monitoring and replanning, we need to ask,
“Does it work?” This is a surprisingly tricky question. If we mean, “Can we guarantee that
the agent will always achieve the goal?” then the answer is no, because the agent could
inadvertently arrive at a dead end from which there is no repair. For example, the vacuum
agent might have a faulty model of itself and not know that its batteries can run out. Once
they do, it cannot repair any plans. If we rule out dead ends—assume that there exists a plan
to reach the goal from any state in the environment—and assume that the environment is
really nondeterministic, in the sense that such a plan always has some chance of success on
any given execution attempt, then the agent will eventually reach the goal.
Trouble occurs when an action is actually not nondeterministic, but rather depends on
some precondition that the agent does not know about. For example, sometimes a paint
can may be empty, so painting from that can has no effect. No amount of retrying is going to
rather than to try the same one each time. In this case, the repair plan of opening another can
might work. A better approach is to learn a better model. Every prediction failure is an
opportunity for learning; an agent should be able to modify its model of the world to accord
with its percepts. From then on, the replanner will be able to come up with a repair that gets
at the root problem, rather than relying on luck to choose a good repair. This kind of learning
MULTIAGENT PLANNING
So far, we have assumed that only one agent is doing the sensing, planning, and acting.
When there are multiple agents in the environment, each agent faces a multiagent planning
problem in which it tries to achieve its own goals with the help or hindrance of others.
MULTIAGENT
PLANNING PROBLEM
lems that exhibit various degrees of decomposition of the monolithic agent. An agent with
multiple effectors that can operate concurrently—for example, a human who can type and
speak at the same time—needs to do multieffector planning to manage each effector while
MULTIEFFECTOR
PLANNING
handling positive and negative interactions among the effectors.
When the effectors are
physically decoupled into detached units—as in a ﬂeet of delivery robots in a factory—
MULTIBODY
PLANNING
can be pooled—either centrally or within each body—to form a common estimate of the
ies act as a single body. When communication constraints make this impossible, we haveChapter
Planning and Acting in the Real World
DECENTRALIZED
PLANNING
cause the planning phase is centralized but the execution phase is at least partially decoupled.
tive actions with other bodies. For example, multiple reconnaissance robots covering a wide
area may often be out of radio contact with each other and should share their ﬁndings during
times when communication is feasible.
When a single entity is doing the planning, there is really only one goal, which all the
bodies necessarily share. When the bodies are distinct agents that do their own planning, they
may still share identical goals; for example, two human tennis players who form a doubles
team share the goal of winning the match. Even with shared goals, however, the multibody
and multiagent cases are quite different. In a multibody robotic doubles team, a single plan
agent doubles team, on the other hand, each agent decides what to do; without some method
for coordination, both agents may decide to cover the same part of the court and each may
COORDINATION
leave the ball for the other to hit.
The clearest case of a multiagent problem, of course, is when the agents have different
is a signiﬁcant factor and can be inﬂuenced by the players’ conduct; otherwise, they can be
treated as an aspect of nature—just like the weather—that is assumed to be indifferent to the
ample, a delivery company may do centralized, ofﬂine planning for the routes of its trucks
and planes each day, but leave some aspects open for autonomous decisions by drivers and
pilots who can respond individually to trafﬁc and weather situations. Also, the goals of the
company and its employees are brought into alignment, to some extent, by the payment of
incentives (salaries and bonuses)—a sure sign that this is a true multiagent system.
INCENTIVE
The issues involved in multiagent planning can be divided roughly into two sets. The
and competition arising in true multiagent settings.
Planning with multiple simultaneous actions
For the time being, we will treat the multieffector, multibody, and multiagent settings in the
same way, labeling them generically as multiactor settings, using the generic term actor to
MULTIACTOR
ACTOR
cover effectors, bodies, and agents. The goal of this section is to work out how to deﬁne
transition models, correct plans, and efﬁcient planning algorithms for the multiactor setting.
A correct plan is one that, if executed by the actors, achieves the goal. (In the true multiagent
setting, of course, the agents may not agree to execute any particular plan, but at least they
guarantees rain.Multiagent Planning
Actors(A, B)
Init(At(A, LeftBaseline) ∧At(B, RightNet) ∧
Approaching(Ball, RightBaseline)) ∧Partner(A, B) ∧Partner(B, A)
Goal(Returned(Ball) ∧(At(a, RightNet) ∨At(a, LeftNet))
Action(Hit(actor, Ball),
PRECOND:Approaching(Ball, loc) ∧At(actor, loc)
EFFECT:Returned(Ball))
Action(Go(actor, to),
PRECOND:At(actor,loc) ∧to ̸= loc,
EFFECT:At(actor, to) ∧¬ At(actor, loc))
The doubles tennis problem. Two actors A and B are playing together and
can be in one of four locations: LeftBaseline, RightBaseline, LeftNet, and RightNet. The
ball can be returned only if a player is in the right place. Note that each action must include
the actor as an argument.
will know what plans would work if they did agree to execute them.) For simplicity, we
assume perfect synchronization: each action takes the same amount of time and actions at
SYNCHRONIZATION
each point in the joint plan are simultaneous.
We begin with the transition model; for the deterministic case, this is the function
but action schemas provide a concise representation nonetheless. In the multiactor setting
with n actors, the single action a is replaced by a joint action ⟨a1, . . . , an⟩, where ai is the
JOINT ACTION
action taken by the ith actor. Immediately, we see two problems: ﬁrst, we have to describe
the transition model for bn different joint actions; second, we have a joint planning problem
with a branching factor of bn.
Having put the actors together into a multiactor system with a huge branching factor,
the principal focus of research on multiactor planning has been to decouple the actors to
the extent possible, so that the complexity of the problem grows linearly with n rather than
exponentially. If the actors have no interaction with one another—for example, n actors each
playing a game of solitaire—then we can simply solve n separate problems. If the actors are
loosely coupled, can we attain something close to this exponential improvement? This is, of
LOOSELY COUPLED
course, a central question in many areas of AI. We have seen it explicitly in the context of
pletely decoupled and then ﬁx up the interactions. For the transition model, this means writing
action schemas as if the actors acted independently. Let’s see how this works for the doubles
tennis problem. Let’s suppose that at one point in the game, the team has the goal of returning
the ball that has been hit to them and ensuring that at least one of them is covering the net.Chapter
Planning and Acting in the Real World
easy to see that the following joint plan plan works:
JOINT PLAN
A : [Go(A, RightBaseline), Hit(A, Ball)]
B : [NoOp(B), NoOp(B)] .
Problems arise, however, when a plan has both agents hitting the ball at the same time. In the
real world, this won’t work, but the action schema for Hit says that the ball will be returned
successfully. Technically, the difﬁculty is that preconditions constrain the state in which an
action can be executed successfully, but do not constrain other actions that might mess it up.
We solve this by augmenting action schemas with one new feature: a concurrent action list
CONCURRENT
ACTION LIST
stating which actions must or must not be executed concurrently. For example, the Hit action
could be described as follows:
Action(Hit(a, Ball),
CONCURRENT:b ̸= a ⇒¬Hit(b, Ball)
PRECOND:Approaching(Ball, loc) ∧At(a, loc)
EFFECT:Returned(Ball)) .
In other words, the Hit action has its stated effect only if no other Hit action by another
agent occurs at the same time. (In the SATPLAN approach, this would be handled by a
partial action exclusion axiom.) For some actions, the desired effect is achieved only when
another action occurs concurrently. For example, two agents are needed to carry a cooler full
of beverages to the tennis court:
Action(Carry(a, cooler, here, there),
CONCURRENT:b ̸= a ∧Carry(b, cooler, here, there)
PRECOND:At(a, here) ∧At(cooler, here) ∧Cooler(cooler)
EFFECT:At(a, there) ∧At(cooler, there) ∧¬At(a, here) ∧¬At(cooler, here)).
can be adapted with only minor modiﬁcations to generate multiactor plans. To the extent that
the coupling among subplans is loose—meaning that concurrency constraints come into play
agent planning to also be effective in the multiactor context. We could extend this approach
cution monitoring, and replanning—but that is beyond the scope of this book.
Planning with multiple agents: Cooperation and coordination
Now let us consider the true multiagent setting in which each agent makes its own plan. To
start with, let us assume that the goals and knowledge base are shared. One might think
that this reduces to the multibody case—each agent simply computes the joint solution and
executes its own part of that solution. Alas, the “the” in “the joint solution” is misleading.
For our doubles team, more than one joint solution exists:
A : [Go(A, LeftNet), NoOp(A)]
B : [Go(B, RightBaseline), Hit(B, Ball)] .Multiagent Planning
they coordinate to make sure they agree on the plan?
One option is to adopt a convention before engaging in joint activity. A convention is
CONVENTION
any constraint on the selection of joint plans. For example, the convention “stick to your side
a road face the problem of not colliding with each other; this is (partially) solved by adopting
the convention “stay on the right side of the road” in most countries; the alternative, “stay
on the left side,” works equally well as long as all agents in an environment agree. Similar
considerations apply to the development of human language, where the important thing is not
which language each individual should speak, but the fact that a community all speaks the
same language. When conventions are widespread, they are called social laws.
SOCIAL LAWS
In the absence of a convention, agents can use communication to achieve common
knowledge of a feasible joint plan. For example, a tennis player could shout “Mine!” or
“Yours!” to indicate a preferred joint plan. We cover mechanisms for communication in more
verbal exchange. For example, one player can communicate a preferred joint plan to the other
simply by executing the ﬁrst part of it. If agent A heads for the net, then agent B is obliged
A’s heading for the net. This approach to coordination, sometimes called plan recognition,
PLAN RECOGNITION
works when a single action (or short sequence of actions) is enough to determine a joint plan
unambiguously. Note that communication can work as well with competitive agents as with
cooperative ones.
harvester ants are social creatures that evolved from the less social wasps. Colonies of ants
cording to the local conditions it observes. For example, foragers travel away from the nest,
search for a seed, and when they ﬁnd one, bring it back immediately. Thus, the rate at which
foragers return to the nest is an approximation of the availability of food today. When the
rate is high, other ants abandon their current role and take on the role of scavenger. The ants
appear to have a convention on the importance of roles—foraging is the most important—and
ants will easily switch into the more important roles, but not into the less important. There is
some learning mechanism: a colony learns to make more successful and prudent actions over
One ﬁnal example of cooperative multiagent behavior appears in the ﬂocking behavior
of birds. We can obtain a reasonable simulation of a ﬂock if each bird agent (sometimes
called a boid) observes the positions of its nearest neighbors and then chooses the heading
BOID
and acceleration that maximizes the weighted sum of these three components:Chapter
Planning and Acting in the Real World
(a)
(b)
(c)
(a) A simulated ﬂock of birds, using Reynold’s boids model. Image courtesy
Giuseppe Randazzo, novastructura.net. (b) An actual ﬂock of starlings. Image by Eduardo
(pastaboy sleeps on ﬂickr). (c) Two competitive teams of agents attempting to capture the
towers in the NERO game. Image courtesy Risto Miikkulainen.
If all the boids execute this policy, the ﬂock exhibits the emergent behavior of ﬂying as a
EMERGENT
BEHAVIOR
pseudorigid body with roughly constant density that does not disperse over time, and that
and compare it to an actual ﬂock in (b). As with ants, there is no need for each agent to
possess a joint plan that models the actions of other agents.
The most difﬁcult multiagent problems involve both cooperation with members of one’s
ods for efﬁcient planning in these kinds of environments—for example, taking advantage of
loose coupling—are in their infancy.
SUMMARY
This chapter has addressed some of the complications of planning and acting in the real world.
The main points:
• Many actions consume resources, such as money, gas, or raw materials. It is convenient
to treat these resources as numeric measures in a pool rather than try to reason about,
say, each individual coin and bill in the world. Actions can generate and consume
resources, and it is usually cheap and effective to check partial plans for satisfaction of
resource constraints before attempting further reﬁnements.
ing algorithms, or scheduling can be integrated with planning.Bibliographical and Historical Notes
• Hierarchical task network (HTN) planning allows the agent to take advice from the
istic, fully observable environments. Many domains violate this assumption.
• Contingent plans allow the agent to sense the world during execution to decide what
branch of the plan to follow. In some cases, sensorless or conformant planning can be
used to construct a plan that works without the need for perception. Both conformant
and contingent plans can be constructed by search in the space of belief states. Efﬁcient
representation or computation of belief states is a key problem.
• An online planning agent uses execution monitoring and splices in repairs as needed
to recover from unexpected situations, which can be due to nondeterministic actions,
exogenous events, or incorrect models of the environment.
• Multiagent planning is necessary when there are other agents in the environment with
which to cooperate or compete. Joint plans can be constructed, but must be augmented
with some form of coordination if two agents are to agree on which joint plan to execute.
• This chapter extends classic planning to cover nondeterministic environments (where
describes techniques for stochastic environments (in which outcomes of actions have
probabilities associated with them): Markov decision processes, partially observable
learning allows an agent to learn how to behave from past successes and failures.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
planning at Jaguar Cars.
with durations and resources. An alternative is to use very expressive action languages, butChapter
Planning and Acting in the Real World
show that this can lead to incompleteness on certain problems. There is a long history of
vation planning, respectively, at the European Space Agency. SPIKE (Johnston and Adorf,
the ﬁrst autonomous planner–scheduler to control a spacecraft when it ﬂew onboard the Deep
MACROPS
ABSTRACTION
HIERARCHY
preconditions of actions in order to derive the general structure of a working plan. Austin
planner as well as a range of complexity results for pure HTN planners. Our presentation of
have proposed an approach in which decompositions are just another form of plan reﬁnement,
planning has been the reuse of previous planning experience in the form of generalized plans.
applied in several systems as a means of generalizing previously computed plans, including
to store previously computed plans in their original form and then reuse them to solve new,
similar problems by analogy to the original problem. This is the approach taken by the ﬁeld
PLANNINGBibliographical and Historical Notes
Early planners lacked conditionals and loops, but some could use coercion to form
and described a sensorless plan that can move a tool into a speciﬁc position on a table by a
sequence of tilting actions, regardless of the initial position.
sorless plans are often effective even if the agent has sensors. The ﬁrst moderately efﬁcient
search in the space of belief states, drawing on ideas ﬁrst developed in the 1960s for partially
Currently, there are three main approaches to conformant planning. The ﬁrst two use
the lazy approach of computing precondition and goal tests on demand using a SAT solver.
sensorless planning problem as a quantiﬁed Boolean formula (QBF) and solves it using a
be generalized to generate good heuristics for conformant and contingent planning.
There has been some confusion in the literature between the terms “conditional” and
mean a plan (or action) that has different effects depending on the actual state of the world,
and “contingent” to mean a plan in which the agent can choose different actions depending
on the results of sensing. The problem of contingent planning received more attention after
the publication of Drew McDermott’s (1978a) inﬂuential article, Planning and Acting.
to do conformant and contingent planning.
In retrospect, it is now possible to see how the major classical planning algorithms ledChapter
Planning and Acting in the Real World
which worked with the STRIPS planner to control the robot Shakey. The NASL planner
(McDermott, 1978a) treated a planning problem simply as a speciﬁcation for carrying out a
complex action, so that execution and planning were completely uniﬁed. SIPE (System for
to deal systematically with the problem of replanning. It has been used in demonstration
projects in several domains, including planning operations on the ﬂight deck of an aircraft
REACTIVE PLANNING
by using Boolean circuits combined with a “visual” representation of current goals and the
table method for reactive planning, but turned out to be a rediscovery of the idea of policies
POLICY
a policy) contains a mapping from any state to the action that should be taken in that state.
Multiagent planning has leaped in popularity in recent years, although it does have
sential if agents are to execute a joint plan, comes from work on communicative acts (Cohen
tiactor planning algorithm whose complexity grows only linearly with the number of actors,
We have barely skimmed the surface of work on negotiation in multiagent planning.
teammates in the competitive, dynamic, partially observable environment of robotic soccer. In
proaches has led to many multiagent systems being designed by ad hoc methods.
of computing resources, the best architecture for an agent is an ensemble of subagents, each
of which tries to optimize its own objective, and all of which are in conﬂict with one another.Chapter
Planning and Acting in the Real World
by the speciﬁcation of the program. Therefore, planning algorithms can be used to construct
programs that achieve a given speciﬁcation.
a. Write an action schema for the assignment operator (assigning the value of one variable
to another). Remember that the original value will be overwritten!
b. Show how object creation can be used by a planner to produce a plan for exchanging
the values of two variables by using a temporary variable.
Suppose the Flip action always changes the truth value of variable L. Show how
to deﬁne its effects by using an action schema with conditional effects. Show that, despite
Flip.
In the blocks world we were forced to introduce two action schemas, Move and
MoveToTable, in order to maintain the Clear predicate properly. Show how conditional
effects can be used to represent both of these cases with a single action.
Conditional effects were illustrated for the Suck action in the vacuum world—which
square becomes clean depends on which square the robot is in. Can you think of a new set of
tions, and demonstrate that they sufﬁce to describe all possible states of the world.
Find a suitably dirty carpet, free of obstacles, and vacuum it. Draw the path taken
by the vacuum cleaner as accurately as you can. Explain it, with reference to the forms of
planning discussed in this chapter.
To the medication problem in the previous exercise, add a Test action that has the
conditional effect CultureGrowth when Disease is true and in any case has the perceptual
effect Known(CultureGrowth). Diagram a conditional plan that solves the problem and
minimizes the use of the Medicate action.KNOWLEDGE
REPRESENTATION
aspects of the real world, such as action, space, time, thoughts, and shopping.
agents that use these logics. In this chapter we address the question of what content to put
into such an agent’s knowledge base—how to represent facts about the world.
about beliefs. We then return to consider the technology for reasoning with this content:
knowledge together in the context of an Internet shopping environment.
ONTOLOGICAL ENGINEERING
In “toy” domains, the choice of representation is not that important; many choices will work.
Complex domains such as shopping on the Internet or driving a car in trafﬁc require more
general and ﬂexible representations. This chapter shows how to create these representations,
concentrating on general concepts—such as Events, Time, Physical Objects, and Beliefs—
that occur in many different domains. Representing these abstract concepts is sometimes
called ontological engineering.
ONTOLOGICAL
ENGINEERING
The prospect of representing everything in the world is daunting. Of course, we won’t
actually write a complete description of everything—that would be far too much for even a
can ﬁt in. For example, we will deﬁne what it means to be a physical object, and the details of
different types of objects—robots, televisions, books, or whatever—can be ﬁlled in later. This
the Java Swing graphical framework) deﬁne general concepts like Window, expecting users toChapter
Knowledge Representation
Anything
AbstractObjects
Sets
Numbers RepresentationalObjects
Interval
Places
Processes
PhysicalObjects
Humans
Categories
Sentences Measurements
Moments
Things
Stuff
Times
Weights
Animals Agents
Solid Liquid Gas
GeneralizedEvents
The upper ontology of the world, showing the topics to be covered later in
the chapter. Each link indicates that the lower concept is a specialization of the upper one.
Specializations are not necessarily disjoint; a human is both an animal and an agent, for
use these to deﬁne more speciﬁc concepts like SpreadsheetWindow. The general framework
of concepts is called an upper ontology because of the convention of drawing graphs with
UPPER ONTOLOGY
Before considering the ontology further, we should state one important caveat. We
although certain aspects of the real world are hard to capture in FOL. The principal difﬁculty
is that most generalizations have exceptions or hold only to a degree. For example, although
“tomatoes are red” is a useful rule, some tomatoes are green, yellow, or orange. Similar
exceptions can be found to almost all the rules in this chapter. The ability to handle exceptions
and uncertainty is extremely important, but is orthogonal to the task of understanding the
It makes many simplifying assumptions: time is omitted completely; signals are ﬁxed and do
not propagate; the structure of the circuit remains constant. A more general ontology would
lays. This would allow us to simulate the timing properties of the circuit, and indeed such
esting classes of gates, for example, by describing the technology (TTL, CMOS, and so on)
as well as the input–output speciﬁcation. If we wanted to discuss reliability or diagnosis, we
would include the possibility that the structure of the circuit or the properties of the gates
might change spontaneously. To account for stray capacitances, we would need to represent
where the wires are on the board.Ontological Engineering
If we look at the wumpus world, similar considerations apply. Although we do represent
time, it has a simple structure: Nothing happens except when the agent acts, and all changes
are instantaneous. A more general ontology, better suited for the real world, would allow for
simultaneous changes extended over time. We also used a Pit predicate to say which squares
have pits. We could have allowed for different kinds of pits by having several individuals
belonging to the class of pits, each having different properties. Similarly, we might want to
allow for other animals besides wumpuses. It might not be possible to pin down the exact
species from the available percepts, so we would need to build up a biological taxonomy to
toward greater generality. An obvious question then arises: do all these ontologies converge
tional issue can be ﬁnessed or brushed under the carpet.
• In any sufﬁciently demanding domain, different areas of knowledge must be uniﬁed,
because reasoning and problem solving could involve several areas simultaneously. A
trical connectivity and physical layout, and about time, both for circuit timing analysis
and estimating labor costs. The sentences describing time therefore must be capable
of being combined with those describing spatial layout and must work equally well for
nanoseconds and minutes and for angstroms and meters.
We should say up front that the enterprise of general ontological engineering has so far had
considerations can make it difﬁcult for competing parties to agree on an ontology. As Tom
some common motive in sharing.” When competing concerns outweigh the motivation for
sharing, there can be no common ontology. Those ontologies that do exist have been created
along four routes:Chapter
Knowledge Representation
CATEGORIES AND OBJECTS
CATEGORY
ing takes place at the level of categories. For example, a shopper would normally have the
goal of buying a basketball, rather than a particular basketball such as BB9. Categories also
serve to make predictions about objects once they are classiﬁed. One infers the presence of
ties of the objects, and then uses category information to make predictions about the objects.
ﬂesh, black seeds, and presence in the fruit aisle, one can infer that an object is a watermelon;
from this, one infers that it would be useful for fruit salad.
objects. That is, we can use the predicate Basketball(b), or we can reify1 the category as
REIFICATION
Subset(Basketballs, Balls), abbreviated as Basketballs ⊂Balls, to say that Basketballs is
a subcategory of Balls. We will use subcategory, subclass, and subset interchangeably.
SUBCATEGORY
Categories serve to organize and simplify the knowledge base through inheritance. If
INHERITANCE
we say that all instances of the category Food are edible, and if we assert that Fruit is a
subclass of Food and Apples is a subclass of Fruit, then we can infer that every apple is
edible. We say that the individual apples inherit the property of edibility, in this case from
their membership in the Food category.
TAXONOMY
onomies have been used explicitly for centuries in technical ﬁelds. The largest such taxonomy
erarchy; library science has developed a taxonomy of all ﬁelds of knowledge, encoded as the
oped extensive taxonomies of occupations and commercial products. Taxonomies are also an
important aspect of general commonsense knowledge.
jects to categories or by quantifying over their members. Here are some types of facts, with
examples of each:
• An object is a member of a category.
• A category is a subclass of another category.
Basketballs ⊂Balls
• All members of a category have some properties.
proposed the term “thingiﬁcation,” but it never caught on.Categories and Objects
• Members of a category can be recognized by some properties.
• A category as a whole has some properties.
Notice that because Dogs is a category and is a member of DomesticatedSpecies, the latter
must be a category of categories. Of course there are exceptions to many of the above rules
(punctured basketballs are not spherical); we deal with these exceptions later.
Although subclass and member relations are the most important ones for categories,
we also want to be able to state relations between categories that are not subclasses of each
other. For example, if we just say that Males and Females are subclasses of Animals, then
we have not said that a male cannot be a female. We say that two or more categories are
disjoint if they have no members in common. And even if we know that males and females
DISJOINT
are disjoint, we will not know that an animal that is not a male must be a female, unless
we say that males and females constitute an exhaustive decomposition of the animals. A
EXHAUSTIVE
DECOMPOSITION
disjoint exhaustive decomposition is known as a partition. The following examples illustrate
PARTITION
these three concepts:
Disjoint({Animals, Vegetables})
ExhaustiveDecomposition({Americans, Canadians, Mexicans},
NorthAmericans)
Partition({Males, Females}, Animals) .
(Note that the ExhaustiveDecomposition of NorthAmericans is not a Partition, because
some people have dual citizenship.) The three predicates are deﬁned as follows:
Partition(s, c) ⇔Disjoint(s) ∧ExhaustiveDecomposition(s, c) .
Categories can also be deﬁned by providing necessary and sufﬁcient conditions for
membership. For example, a bachelor is an unmarried adult male:
gories are neither always possible nor always necessary.
Physical composition
The idea that one object can be part of another is a familiar one. One’s nose is part of one’s
head, Romania is part of Europe, and this chapter is part of this book. We use the general
PartOf relation to say that one thing is part of another. Objects can be grouped into PartOf
hierarchies, reminiscent of the Subset hierarchy:
PartOf (Bucharest, Romania)
PartOf (Romania, EasternEurope)
PartOf (EasternEurope, Europe)
PartOf (Europe, Earth) .Chapter
Knowledge Representation
The PartOf relation is transitive and reﬂexive; that is,
PartOf (x, y) ∧PartOf (y, z) ⇒PartOf (x, z) .
PartOf (x, x) .
Therefore, we can conclude PartOf (Bucharest, Earth).
Categories of composite objects are often characterized by structural relations among
COMPOSITE OBJECT
parts. For example, a biped has two legs attached to a body:
Biped(a)
⇒
∃l1, l2, b Leg(l1) ∧Leg(l2) ∧Body(b) ∧
PartOf (l1, a) ∧PartOf (l2, a) ∧PartOf (b, a) ∧
Attached(l1, b) ∧Attached(l2, b) ∧
l1 ̸= l2 ∧[∀l3 Leg(l3) ∧PartOf (l3, a) ⇒(l3 = l1 ∨l3 = l2)] .
The notation for “exactly two” is a little awkward; we are forced to say that there are two
legs, that they are not the same, and that if anyone proposes a third leg, it must be the same
makes it easier to represent constraints like “exactly two.”
ite object is the sum of the masses of the parts. Notice that this is not the case with categories,
which have no mass, even though their elements might.
ture. For example, we might want to say “The apples in this bag weigh two pounds.” The
temptation would be to ascribe this weight to the set of apples in the bag, but this would be
a mistake because the set is an abstract mathematical concept that has elements but does not
have weight. Instead, we need a new concept, which we will call a bunch. For example, if
BUNCH
the apples are Apple1, Apple2, and Apple3, then
BunchOf ({Apple1, Apple2, Apple3})
denotes the composite object with the three apples as parts (not elements). We can then use the
bunch as a normal, albeit unstructured, object. Notice that BunchOf ({x}) = x. Furthermore,
BunchOf (Apples) is the composite object consisting of all apples—not to be confused with
Apples, the category or set of all apples.
We can deﬁne BunchOf in terms of the PartOf relation. Obviously, each element of
s is part of BunchOf (s):
Furthermore, BunchOf (s) is the smallest object satisfying this condition. In other words,
BunchOf (s) must be part of any object that has all the elements of s as parts:
These axioms are an example of a general technique called logical minimization, which
LOGICAL
MINIMIZATION
means deﬁning an object as the smallest one satisfying certain conditions.Categories and Objects
NATURAL KINDS
Some categories have strict deﬁnitions: an object is a triangle if and only if it is
a polygon with three sides. On the other hand, most categories in the real world
tomatoes tend to be a dull scarlet; roughly spherical; with an indentation at the top
where the stem was; about two to four inches in diameter; with a thin but tough
skin; and with ﬂesh, seeds, and juice inside. There is, however, variation: some
tomatoes are yellow or orange, unripe tomatoes are green, some are smaller or
larger than average, and cherry tomatoes are uniformly small. Rather than having
a complete deﬁnition of tomatoes, we have a set of features that serves to identify
objects that are clearly typical tomatoes, but might not be able to decide for other
objects. (Could there be a tomato that is fuzzy like a peach?)
This poses a problem for a logical agent. The agent cannot be sure that an
tain which of the properties of typical tomatoes this one has. This problem is an
inevitable consequence of operating in partially observable environments.
gory from what is true only of typical instances. So in addition to the category
Tomatoes, we will also have the category Typical(Tomatoes). Here, the Typical
function maps a category to the subclass that contains only typical instances:
Typical(c) ⊆c .
Most knowledge about natural kinds will actually be about their typical instances:
tions. The difﬁculty of providing exact deﬁnitions for most natural categories was
that members of a category shared “family resemblances” rather than necessary
taire, and dodgeball?
The utility of the notion of strict deﬁnition was also challenged by
married adult male is suspect; one might, for example, question a statement such
licitous because it induces unintended inferences on the part of the listener. The
tension could perhaps be resolved by distinguishing between logical deﬁnitions
suitable for internal knowledge representation and the more nuanced criteria for
felicitous linguistic usage. The latter may be achieved by “ﬁltering” the assertions
derived from the former. It is also possible that failures of linguistic usage serve as
feedback for modifying internal deﬁnitions, so that ﬁltering becomes unnecessary.Chapter
Knowledge Representation
Measurements
In both scientiﬁc and commonsense theories of the world, objects have height, mass, cost,
and so on.
The values that we assign for these properties are called measures.
MEASURE
ment:
the same length has different names in our language.We represent the length with a units
function that takes a number as argument.
UNITS FUNCTION
Conversion between units is done by equating multiples of one unit to another:
Similar axioms can be written for pounds and kilograms, seconds and days, and dollars and
cents. Measures can be used to describe objects as follows:
Simple, quantitative measures are easy to represent. Other measures present more of a
problem, because they have no agreed scale of values. Exercises have difﬁculty, desserts have
deliciousness, and poems have beauty, yet numbers cannot be assigned to these qualities. One
might, in a moment of pure accountancy, dismiss such properties as useless for the purpose of
logical reasoning; or, still worse, attempt to impose a numerical scale on beauty. This would
be a grave mistake, because it is unnecessary. The most important aspect of measures is not
the particular numerical values, but the fact that measures can be ordered.
Although measures are not numbers, we can still compare them, using an ordering
symbol such as >. For example, we might well believe that Norvig’s exercises are tougher
than Russell’s, and that one scores less on tougher exercises:
Diﬃculty(e1) > Diﬃculty(e2) .
ExpectedScore(e1) < ExpectedScore(e2) .
This is enough to allow one to decide which exercises to do, even though no numerical values
cises.) These sorts of monotonic relationships among measures form the basis for the ﬁeld of
qualitative physics, a subﬁeld of AI that investigates how to reason about physical systems
without plunging into detailed equations and numerical simulations. Qualitative physics is
discussed in the historical notes section.Categories and Objects
Objects: Things and stuff
The real world can be seen as consisting of primitive objects (e.g., atomic particles) and
composite objects built from them. By reasoning at the level of large objects such as apples
and cars, we can overcome the complexity involved in dealing with vast numbers of primitive
objects individually. There is, however, a signiﬁcant portion of reality that seems to defy any
obvious individuation—division into distinct objects. We give this portion the generic name
INDIVIDUATION
stuff. For example, suppose I have some butter and an aardvark in front of me. I can say
STUFF
the major distinction between stuff and things. If we cut an aardvark in half, we do not get
two aardvarks (unfortunately).
vark,” but, except in pretentious California restaurants, one cannot say “a butter.” Linguists
distinguish between count nouns, such as aardvarks, holes, and theorems, and mass nouns,
COUNT NOUNS
MASS NOUN
tion. Here we describe just one; the others are covered in the historical notes section.
To represent stuff properly, we begin with the obvious. We need to have as objects in
our ontology at least the gross “lumps” of stuff we interact with. For example, we might
recognize a lump of butter as the one left on the table the night before; we might pick it up,
weigh it, sell it, or whatever. In these senses, it is an object just like the aardvark. Let us
call it Butter3. We also deﬁne the category Butter. Informally, its elements will be all those
things of which one might say “It’s butter,” including Butter3. With some caveats about very
ture, has a high fat content, and so on. On the other hand, butter has no particular size, shape,
or weight. We can deﬁne more specialized categories of butter such as UnsaltedButter,
which is also a kind of stuff. Note that the category PoundOfButter, which includes as
butter in half, we do not, alas, get two pounds of butter.
What is actually going on is this: some properties are intrinsic: they belong to the very
INTRINSIC
substance of the object, rather than to the object as a whole. When you cut an instance of
stuff in half, the two pieces retain the intrinsic properties—things like density, boiling point,
ﬂavor, color, ownership, and so on. On the other hand, their extrinsic properties—weight,
EXTRINSIC
length, shape, and so on—are not retained under subdivision. A category of objects that
includes in its deﬁnition only intrinsic properties is then a substance, or mass noun; a class
that includes any extrinsic properties in its deﬁnition is a count noun. The category Stuﬀis
the most general substance category, specifying no intrinsic properties. The category Thing
is the most general discrete object category, specifying no extrinsic properties.Chapter
Knowledge Representation
EVENTS
Situation calculus is limited in its applicability: it was designed to describe a world in which
actions are discrete, instantaneous, and happen one at a time. Consider a continuous action,
such as ﬁlling a bathtub. Situation calculus can say that the tub is empty before the action and
full when the action is done, but it can’t talk about what happens during the action. It also
can’t describe two actions happening at the same time—such as brushing one’s teeth while
waiting for the tub to ﬁll. To handle such cases we introduce an alternative formalism known
EVENT CALCULUS
ject that refers to the fact of Shankar being in Berkeley, but does not by itself say anything
about whether it is true. To assert that a ﬂuent is actually true at some point in time we use
the predicate T, as in T(At(Shankar, Berkeley), t).
from San Francisco to Washington, D.C. is described as
ﬂying events and say
We then use Happens(E1, i) to say that the event E1 took place over the time interval i, and
we say the same thing in functional form with Extent(E1) = i. We represent time intervals
by a (start, end) pair of times; that is, i = (t1, t2) is the time interval that starts at t1 and ends
at t2. The complete set of predicates for one version of the event calculus is
T(f, t)
Fluent f is true at time t
Happens(e, i)
Event e happens over the time interval i
Initiates(e, f, t)
Event e causes ﬂuent f to start to hold at time t
Terminates(e, f, t)
Event e causes ﬂuent f to cease to hold at time t
Clipped(f, i)
Fluent f ceases to be true at some point during time interval i
Restored(f, i)
Fluent f becomes true sometime during time interval i
We assume a distinguished event, Start, that describes the initial state by saying which ﬂuents
are initiated or terminated at the start time. We deﬁne T by saying that a ﬂuent holds at a point
in time if the ﬂuent was initiated by an event at some time in the past and was not made false
(clipped) by an intervening event. A ﬂuent does not hold if it was terminated by an event and
“event” connotes the possibility of agentless actions.Events
not made true (restored) by another event. Formally, the axioms are:
Happens(e, (t1, t2)) ∧Initiates(e, f, t1) ∧¬Clipped(f, (t1, t)) ∧t1 < t ⇒
T(f, t)
Happens(e, (t1, t2)) ∧Terminates(e, f, t1) ∧¬Restored(f, (t1, t)) ∧t1 < t ⇒
¬T(f, t)
where Clipped and Restored are deﬁned by
Clipped(f, (t1, t2)) ⇔
Restored(f, (t1, t2)) ⇔
It is convenient to extend T to work over intervals as well as time points; a ﬂuent holds over
an interval if it holds on every point within the interval:
arrow is at the start, and the only way to use up an arrow is to shoot it:
Initiates(e, HaveArrow(a), t) ⇔e = Start
By reifying events we make it possible to add any amount of arbitrary information about
them. For example, we can say that Shankar’s ﬂight was bumpy with Bumpy(E1). In an
We can extend event calculus to make it possible to represent simultaneous events (such
as two people being necessary to ride a seesaw), exogenous events (such as the wind blowing
and changing the location of an object), continuous events (such as the level of water in the
bathtub continuously rising) and other complications.
Processes
DISCRETE EVENTS
ture. Shankar’s trip has a beginning, middle, and end. If interrupted halfway, the event would
be something different—it would not be a trip from San Francisco to Washington, but instead
a trip from San Francisco to somewhere over Kansas. On the other hand, the category of
events denoted by Flyings has a different quality. If we take a small interval of Shankar’s
event is still a member of Flyings. In fact, this is true for any subinterval.
Categories of events with this property are called process categories or liquid event
PROCESS
LIQUID EVENT
categories. Any process e that happens over an interval also happens over any subinterval:
The distinction between liquid and nonliquid events is exactly analogous to the difference
between substances, or stuff, and individual objects, or things. In fact, some have called
liquid events temporal substances, whereas substances like butter are spatial substances.
TEMPORAL
SUBSTANCE
SPATIAL SUBSTANCEChapter
Knowledge Representation
Time intervals
Event calculus opens us up to the possibility of talking about time, and time intervals. We
will consider two kinds of time intervals: moments and extended intervals. The distinction is
that only moments have zero duration:
Partition({Moments, ExtendedIntervals}, Intervals)
solute times. The time scale is arbitrary; we measure it in seconds and say that the moment
the earliest and latest moments in an interval, and the function Time delivers the point on the
time scale for a moment. The function Duration gives the difference between the end time
and the start time.
Interval(i) ⇒Duration(i) = (Time(End(i)) −Time(Begin(i))) .
To make these numbers easier to read, we also introduce a function Date, which takes six
arguments (hours, minutes, seconds, day, month, and year) and returns a time point:
and logically below:
Meet(i, j)
⇔
End(i) = Begin(j)
Before(i, j)
⇔
End(i) < Begin(j)
After(j, i)
⇔
Before(i, j)
During(i, j)
⇔
Begin(j) < Begin(i) < End(i) < End(j)
Overlap(i, j)
⇔
Begin(i) < Begin(j) < End(i) < End(j)
Begins(i, j)
⇔
Begin(i) = Begin(j)
Finishes(i, j)
⇔
End(i) = End(j)
Equals(i, j)
⇔
Begin(i) = Begin(j) ∧End(i) = End(j)
These all have their intuitive meaning, with the exception of Overlap: we tend to think of
overlap as symmetric (if i overlaps j then j overlaps i), but in this deﬁnition, Overlap(i, j)
only holds if i begins before j. To say that the reign of Elizabeth II immediately followed that
of George VI, and the reign of Elvis overlapped with the 1950s, we can write the following:
Meets(ReignOf (GeorgeVI ), ReignOf (ElizabethII )) .
Overlap(Fifties, ReignOf (Elvis)) .
Begin(Fifties) = Begin(AD1950) .
End(Fifties) = End(AD1959) .Events
Predicates on time intervals.
time
Washington
Adams
Jefferson
existence.
Fluents and objects
Physical objects can be viewed as generalized events, in the sense that a physical object is
a chunk of space–time. For example, USA can be thought of as an event that began in,
describe the changing properties of USA using state ﬂuents, such as Population(USA). A
property of the USA that changes every four or eight years, barring mishaps, is its president.
One might propose that President(USA) is a logical term that denotes a different object
at different times. Unfortunately, this is not possible, because a term denotes exactly one
object in a given model structure. (The term President(USA, t) can denote different objects,
depending on the value of t, but our ontology keeps time indices separate from ﬂuents.) TheChapter
Knowledge Representation
only possibility is that President(USA) denotes a single object that consists of different
T(Equals(President(USA), GeorgeWashington), AD1790) .
We use the function symbol Equals rather than the standard logical predicate =, because
we cannot have a predicate as an argument to T, and because the interpretation is not that
not something that can change over time. The identity is between the subevents of each object
MENTAL EVENTS AND MENTAL OBJECTS
The agents we have constructed so far have beliefs and can deduce new beliefs. Yet none
of them has any knowledge about beliefs or about deduction. Knowledge about one’s own
knowledge and reasoning processes is useful for controlling inference. For example, suppose
“think harder,” Bob should realize that with some more thought, this question can in fact
be answered. On the other hand, if the question were “Is your mother sitting down right
now?” then Bob should realize that thinking harder is unlikely to help. Knowledge about
the knowledge of other agents is also important; Bob should realize that his mother knows
whether she is sitting or not, and that asking her would be a way to ﬁnd out.
thing’s knowledge base) and of the mental processes that manipulate those mental objects.
The model does not have to be detailed. We do not have to be able to predict how many
milliseconds it will take for a particular agent to make a deduction. We will be happy just to
be able to conclude that mother knows whether or not she is sitting.
PROPOSITIONAL
ATTITUDE
jects: attitudes such as Believes, Knows, Wants, Intends, and Informs. The difﬁculty is
that these attitudes do not behave like “normal” predicates. For example, suppose we try to
assert that Lois knows that Superman can ﬂy:
Knows(Lois, CanFly(Superman)) .
One minor issue with this is that we normally think of CanFly(Superman) as a sentence, but
here it appears as a term. That issue can be patched up just be reifying CanFly(Superman);
making it a ﬂuent. A more serious problem is that, if it is true that Superman is Clark Kent,
then we must conclude that Lois knows that Clark can ﬂy:
(Superman = Clark) ∧Knows(Lois, CanFly(Superman))
|= Knows(Lois, CanFly(Clark)) .
This is a consequence of the fact that equality reasoning is built into logic. Normally that isMental Events and Mental Objects
REFERENTIAL
TRANSPARENCY
term a logic uses to refer to an object, what matters is the object that the term names. But for
propositional attitudes like believes and knows, we would like to have referential opacity—the
MODAL LOGIC
gle modality, the modality of truth, allowing us to express “P is true.” Modal logic includes
special modal operators that take sentences (rather than terms) as arguments. For example,
edge. It takes two arguments, an agent (written as the subscript) and a sentence. The syntax
modal operators.
tains a set of objects and an interpretation that maps each name to the appropriate object,
relation, or function. In modal logic we want to be able to consider both the possibility that
plicated model, one that consists of a collection of possible worlds rather than just one true
POSSIBLE WORLD
world. The worlds are connected in a graph by accessibility relations, one relation for each
ACCESSIBILITY
RELATIONS
modal operator. We say that world w1 is accessible from world w0 with respect to the modal
operator KA if everything in w1 is consistent with what A knows in w0, and we write this
tween possible worlds. As an example, in the real world, Bucharest is the capital of Romania,
but for an agent that did not know that, other possible worlds are accessible, including ones
not be accessible to any agent.
In general, a knowledge atom KAP is true in world w if and only if P is true in every
be used to reason about nested knowledge sentences: what one agent knows about another
agent’s knowledge. For example, we can say that, even though Lois doesn’t know whether
Superman’s secret identity is Clark Kent, she does know that Clark knows:
KLois[KClarkIdentity(Superman, Clark) ∨KClark¬Identity(Superman, Clark)]
and Superman.
tity, and neither he nor Lois has seen the weather report. So in w0 the worlds w0 and w2 are
cessible from each other; she doesn’t know anything about the report or if Clark is Superman.
But she does know that Superman knows whether he is Clark, because in every world that is
accessible to Lois, either Superman knows I, or he knows ¬I. Lois does not know which is
the case, but either way she knows Superman knows.
report. So in w4 she knows rain is predicted and in w6 she knows rain is not predicted.Chapter
Knowledge Representation
(a)
(b)
(c)
w0: I,R
w2: I,¬R
w3: ¬I,¬R
w1: ¬I,R
w4: I,R
w6: I,¬R
w7: ¬I,¬R
w5: ¬I,R
w0: I,R
w2: I,¬R
w3: ¬I,¬R
w1: ¬I,R
w4: I,R
w5: ¬I,R
w6: I,¬R
w7: ¬I,¬R
Possible worlds with accessibility relations KSuperman (solid arrows) and
KLois (dotted arrows). The proposition R means “the weather report for tomorrow is rain”
selves; the arrows from a world to itself are not shown.
Superman does not know the report, but he knows that Lois knows, because in every world
that is accessible to him, either she knows R or she knows ¬R.
In the BOTTOM diagram we represent the scenario where it is common knowledge that
Superman knows his identity, and Lois might or might not have seen the weather report. We
represent this by combining the two top scenarios, and adding arrows to show that Superman
does not know which scenario actually holds. Lois does know, so we don’t need to add any
arrows for her. In w0 Superman still knows I but not R, and now he does not know whether
Lois knows R. From what Superman knows, he might be in w0 or w2, in which case Lois
does not know whether R is true, or he could be in w4, in which case she knows R, or w6, in
which case she knows ¬R.
There are an inﬁnite number of possible worlds, so the trick is to introduce just the ones
you need to represent what you are trying to model. A new possible world is needed to talk
about different possible facts (e.g., rain is predicted or not), or to talk about different states
of knowledge (e.g., does Lois know that rain is predicted). That means two possible worlds,
in their accessibility relations, and therefore in facts about knowledge.
Modal logic solves some tricky issues with the interplay of quantiﬁers and knowledge.
The English sentence “Bond knows that someone is a spy” is ambiguous. The ﬁrst reading isReasoning Systems for Categories
that there is a particular someone who Bond knows is a spy; we can write this as
∃x KBondSpy(x) ,
which in modal logic means that there is an x that, in all accessible worlds, Bond knows to
be a spy. The second reading is that Bond just knows that there is at least one spy:
The modal logic interpretation is that in each accessible world there is an x that is a spy, but
it need not be the same x in each world.
Now that we have a modal operator for knowledge, we can write axioms for it. First,
we can say that agents are able to draw deductions; if an agent knows P and knows that P
implies Q, then the agent knows Q:
(KaP ∧Ka(P ⇒Q)) ⇒KaQ .
From this (and a few other rules about logical identities) we can establish that KA(P ∨¬P)
is a tautology; every agent knows every proposition P is either true or false. On the other
hand, (KAP) ∨(KA¬P) is not a tautology; in general, there will be lots of propositions that
an agent does not know to be true and does not know to be false.
It is said (going back to Plato) that knowledge is justiﬁed true belief. That is, if it is
true, if you believe it, and if you have an unassailably good reason, then you know it. That
means that if you know something, it must be true, and we have the axiom:
KaP ⇒P .
Furthermore, logical agents should be able to introspect on their own knowledge. If they
know something, then they know that they know it:
KaP ⇒Ka(KaP) .
We can deﬁne similar axioms for belief (often denoted by B) and other modalities. However,
one problem with the modal logic approach is that it assumes logical omniscience on the
LOGICAL
OMNISCIENCE
part of agents. That is, if an agent knows a set of axioms, then it knows all consequences of
those axioms. This is on shaky ground even for the somewhat abstract notion of knowledge,
but it seems even worse for belief, because belief has more connotation of referring to things
that are physically represented in the agent, not just potentially derivable. There have been
attempts to deﬁne a form of limited rationality for agents; to say that agents believe those
assertions that can be derived with the application of no more than k reasoning steps, or no
more than s seconds of computation. These attempts have been generally unsatisfactory.
REASONING SYSTEMS FOR CATEGORIES
ical aids for visualizing a knowledge base and efﬁcient algorithms for inferring propertiesChapter
Knowledge Representation
mal language for constructing and combining category deﬁnitions and efﬁcient algorithms
for deciding subset and superset relationships between categories.
Semantic networks
EXISTENTIAL
GRAPHS
form of logic. The notation that semantic networks provide for certain kinds of sentences
is often more convenient, but if we strip away the “human interface” issues, the underlying
concepts—objects, relations, quantiﬁcation, and so on—are the same.
tation displays object or category names in ovals or boxes, and connects them with labeled
nect categories using SubsetOf links, and so on. It is such fun drawing bubbles and arrows
that one can get carried away. For example, we know that persons have female persons as
mothers, so can we draw a HasMother link from Persons to FemalePersons? The answer
is no, because HasMother is a relation between a person and his or her mother, and categories
This link asserts that
We might also want to assert that persons have two legs—that is,
The semantic network notation makes it convenient to perform inheritance reasoning
the property of having two legs. Thus, to ﬁnd out how many legs Mary has, the inheritance
algorithm follows the MemberOf link from Mary to the category she belongs to, and then
follows SubsetOf links up the hierarchy until it ﬁnds a category for which there is a boxed
Legs link—in this case, the Persons category. The simplicity and efﬁciency of this inference
article “Artiﬁcial Intelligence Meets Natural Stupidity.” Another common problem was the use of IsA links for
both subset and membership relations, in correspondence with English usage: “a cat is a mammal” and “Fiﬁis aReasoning Systems for Categories
Mammals
John
Mary
Persons
Male
Persons
Female
Persons
SubsetOf
SubsetOf
SubsetOf
MemberOf
MemberOf
SisterOf
Legs
Legs
HasMother
gories. Relations are denoted by labeled links.
MemberOf
FlyEvents
Fly17
Shankar
NewYork
NewDelhi
Yesterday
Agent
Origin
Destination
During
A fragment of a semantic network showing the representation of the logical
assertion Fly(Shankar, NewYork, NewDelhi, Yesterday).
mechanism, compared with logical theorem proving, has been one of the main attractions of
semantic networks.
Inheritance becomes complicated when an object can belong to more than one category
heritance. In such cases, the inheritance algorithm might ﬁnd two or more conﬂicting values
MULTIPLE
INHERITANCE
programming (OOP) languages, such as Java, that use inheritance in a class hierarchy. It is
tions by reifying the proposition itself as an event belonging to an appropriate event category.
restriction to binary relations forces the creation of a rich ontology of reiﬁed concepts.Chapter
Knowledge Representation
sally quantiﬁed sentences can be asserted using inverse links and the singly boxed and doubly
logic. Negation, disjunction, nested function symbols, and existential quantiﬁcation are all
in Peirce’s existential graphs—but doing so negates one of the main advantages of semantic
networks, which is the simplicity and transparency of the inference processes. Designers can
build a large network and still have a good idea about what queries will be efﬁcient, because
(a) it is easy to visualize the steps that the inference procedure will go through and (b) in some
cases the query language is so simple that difﬁcult queries cannot be posed. In cases where
the expressive power proves to be too limiting, many semantic network systems provide for
procedural attachment to ﬁll in the gaps. Procedural attachment is a technique whereby
a query about (or sometimes an assertion of) a certain relation results in a call to a special
procedure designed for that relation rather than a general inference algorithm.
One of the most important aspects of semantic networks is their ability to represent
DEFAULT VALUE
leg, despite the fact that he is a person and all persons have two legs. In a strictly logical KB,
this would be a contradiction, but in a semantic network, the assertion that all persons have
two legs has only default status; that is, a person is assumed to have two legs unless this is
contradicted by more speciﬁc information. The default semantics is enforced naturally by the
inheritance algorithm, because it follows links upwards from the object itself (John in this
case) and stops as soon as it ﬁnds a value. We say that the default is overridden by the more
OVERRIDING
speciﬁc value. Notice that we could also override the default number of legs by creating a
category of OneLeggedPersons, a subset of Persons of which John is a member.
tion for Persons includes an exception for John:
For a ﬁxed network, this is semantically adequate but will be much less concise than the
network notation itself if there are lots of exceptions. For a network that will be updated with
more assertions, however, such an approach fails—we really want to say that any persons as
and on default reasoning in general.
Description logics
scription logics are notations that are designed to make it easier to describe deﬁnitions and
DESCRIPTION LOGIC
sponse to pressure to formalize what the networks mean while retaining the emphasis on
taxonomic structure as an organizing principle.
The principal inference tasks for description logics are subsumption (checking if one
SUBSUMPTION
category is a subset of another by comparing their deﬁnitions) and classiﬁcation (checking
CLASSIFICATION
gory deﬁnition—whether the membership criteria are logically satisﬁable.Reasoning Systems for Categories
Concept
→
Thing | ConceptName
|
And(Concept, . . .)
|
All(RoleName, Concept)
|
AtLeast(Integer, RoleName)
|
AtMost(Integer, RoleName)
|
Fills(RoleName, IndividualName, . . .)
|
SameAs(Path, Path)
|
OneOf(IndividualName, . . .)
Path
→
[RoleName, . . .]
The syntax of descriptions in a subset of the CLASSIC language.
unmarried adult males we would write
Bachelor = And(Unmarried, Adult, Male) .
Bachelor(x) ⇔Unmarried(x) ∧Adult(x) ∧Male(x) .
Notice that the description logic has an an algebra of operations on predicates, which of
For example, to describe the set of men with at least three sons who are all unemployed
and married to doctors, and at most two daughters who are all professors in physics or math
departments, we would use
All(Son, And(Unemployed, Married, All(Spouse, Doctor))),
All(Daughter, And(Professor, Fills(Department, Physics, Math)))) .
Perhaps the most important aspect of description logics is their emphasis on tractability
of inference. A problem instance is solved by describing it and then asking if it is subsumed
tation to detour around sets of sentences that seem to be causing the system to take several
weeks to solve a problem. The thrust in description logics, on the other hand, is to ensure that
another. This is a deliberate policy: subsumption between categories must be derivable from some aspects of the
descriptions of the categories. If not, then something is missing from the descriptions.Chapter
Knowledge Representation
This sounds wonderful in principle, until one realizes that it can only have one of two
consequences: either hard problems cannot be stated at all, or they require exponentially
large descriptions! However, the tractability results do shed light on what sorts of constructs
cause problems and thus help the user to understand how different representations behave.
order logical systems to go through a potentially exponential case analysis in order to ensure
completeness. CLASSIC allows only a limited form of disjunction in the Fills and OneOf
scriptions. With disjunctive descriptions, nested deﬁnitions can lead easily to an exponential
number of alternative routes by which one category can subsume another.
REASONING WITH DEFAULT INFORMATION
In the preceding section, we saw a simple example of an assertion with default status: people
have two legs. This default can be overridden by more speciﬁc information, such as that
Long John Silver has one leg. We saw that the inheritance mechanism in semantic networks
implements the overriding of defaults in a simple and natural way. In this section, we study
defaults more generally, with a view toward understanding the semantics of defaults rather
than just providing a procedural mechanism.
Circumscription and default logic
We have seen two examples of reasoning processes that violate the monotonicity property of
proposition α is not mentioned in KB then KB |= ¬α, but KB ∧α |= α.
Simple introspection suggests that these failures of monotonicity are widespread in
commonsense reasoning. It seems that humans often “jump to conclusions.” For example,
when one sees a car parked on the street, one is normally willing to believe that it has four
wheels even though only three are visible. Now, probability theory can certainly provide a
bility of the car’s not having four wheels does not arise unless some new evidence presents
any reason to doubt it. If new evidence arrives—for example, if one sees the owner carrying
a wheel and notices that the car is jacked up—then the conclusion can be retracted. This kind
of reasoning is said to exhibit nonmonotonicity, because the set of beliefs does not grow
NONMONOTONICITY
monotonically over time as new evidence arrives. Nonmonotonic logics have been devised
NONMONOTONIC
LOGIC
with modiﬁed notions of truth and entailment in order to capture such behavior. We will look
at two such logics that have been studied extensively: circumscription and default logic.
KB. That is, if KB |= α then KB ∧β |= α.Reasoning with Default Information
CIRCUMSCRIPTION
world assumption. The idea is to specify particular predicates that are assumed to be “as false
as possible”—that is, false for every object except those for which they are known to be true.
For example, suppose we want to assert the default rule that birds ﬂy. We would introduce a
predicate, say Abnormal1(x), and write
Bird(x) ∧¬Abnormal1(x) ⇒Flies(x) .
clusion Flies(Tweety) to be drawn from the premise Bird(Tweety), but the conclusion no
longer holds if Abnormal1(Tweety) is asserted.
Circumscription can be viewed as an example of a model preference logic. In such
MODEL
PREFERENCE
logics, a sentence is entailed (with default status) if it is true in all preferred models of the KB,
as opposed to the requirement of truth in all models in classical logic. For circumscription,
works in the context of multiple inheritance in semantic networks. The standard example for
which multiple inheritance is problematic is called the “Nixon diamond.” It arises from the
observation that Richard Nixon was both a Quaker (and hence by default a paciﬁst) and a
Republican (and hence by default not a paciﬁst). We can write this as follows:
Republican(Nixon) ∧Quaker(Nixon) .
Republican(x) ∧¬Abnormal2(x) ⇒¬Paciﬁst(x) .
Quaker(x) ∧¬Abnormal3(x) ⇒Paciﬁst(x) .
If we circumscribe Abnormal2 and Abnormal3, there are two preferred models: one in
and ¬Paciﬁst(Nixon) hold. Thus, the circumscriptive reasoner remains properly agnostic as
to whether Nixon was a paciﬁst. If we wish, in addition, to assert that religious beliefs take
precedence over political beliefs, we can use a formalism called prioritized circumscription
PRIORITIZED
CIRCUMSCRIPTION
to give preference to models where Abnormal3 is minimized.
DEFAULT LOGIC
DEFAULT RULES
gent, nonmonotonic conclusions. A default rule looks like this:
Bird(x) : Flies(x)/Flies(x) .
This rule means that if Bird(x) is true, and if Flies(x) is consistent with the knowledge base,
then Flies(x) may be concluded by default. In general, a default rule has the form
where P is called the prerequisite, C is the conclusion, and Ji are the justiﬁcations—if any
one of them can be proven false, then the conclusion cannot be drawn. Any variable thatChapter
Knowledge Representation
in default logic with one fact and two default rules:
Republican(Nixon) ∧Quaker(Nixon) .
Republican(x) : ¬Paciﬁst(x)/¬Paciﬁst(x) .
Quaker(x) : Paciﬁst(x)/Paciﬁst(x) .
To interpret what the default rules mean, we deﬁne the notion of an extension of a default
EXTENSION
theory to be a maximal set of consequences of the theory. That is, an extension S consists
of the original known facts and a set of conclusions from the default rules, such that no
additional conclusions can be drawn from S and the justiﬁcations of every default conclusion
in S are consistent with S. As in the case of the preferred models in circumscription, we have
two possible extensions for the Nixon diamond: one wherein he is a paciﬁst and one wherein
he is not. Prioritized schemes exist in which some default rules can be given precedence over
others, allowing some ambiguities to be resolved.
has been made in understanding their mathematical properties. There are still unresolved
questions, however. For example, if “Cars have four wheels” is false, what does it mean
to have it in one’s knowledge base? What is a good set of default rules to have? If we
cannot decide, for each rule separately, whether it belongs in our knowledge base, then we
have a serious problem of nonmodularity. Finally, how can beliefs that have default status be
used to make decisions? This is probably the hardest issue for default reasoning. Decisions
often involve tradeoffs, and one therefore needs to compare the strengths of belief in the
outcomes of different actions, and the costs of making a wrong decision. In cases where the
same kinds of decisions are being made repeatedly, it is possible to interpret default rules
as “threshold probability” statements. For example, the default rule “My brakes are always
OK” really means “The probability that my brakes are OK, given no other information, is
sufﬁciently high that the optimal decision is for me to drive without checking them.” When
the decision context changes—for example, when one is driving a heavily laden truck down a
steep mountain road—the default rule suddenly becomes inappropriate, even though there is
no new evidence of faulty brakes. These considerations have led some researchers to consider
how to embed default reasoning within probability theory or utility theory.
Truth maintenance systems
We have seen that many of the inferences drawn by a knowledge representation system will
BELIEF REVISION
ating a contradiction, we must ﬁrst execute RETRACT(KB, P). This sounds easy enough.
a change in the world rather than new information about a ﬁxed world. Belief update combines belief revisionReasoning with Default Information
Problems arise, however, if any additional sentences were inferred from P and asserted in
the KB. For example, the implication P
⇒Q might have been used to add Q. The obvious
“solution”—retracting all sentences inferred from P—fails because such sentences may have
other justiﬁcations besides P. For example, if R and R ⇒Q are also in the KB, then Q
does not have to be removed after all. Truth maintenance systems, or TMSs, are designed
TRUTH
MAINTENANCE
SYSTEM
to handle exactly these kinds of complications.
tences are told to the knowledge base by numbering them from P1 to Pn. When the call
RETRACT(KB, Pi) is made, the system reverts to the state just before Pi was added, thereby
Pn can then be added again. This is simple, and it guarantees that the knowledge base will
be consistent, but retracting Pi requires retracting and reasserting n −i sentences as well as
undoing and redoing all the inferences drawn from those sentences. For systems to which
many facts are being added—such as large commercial databases—this is impractical.
JTMS
In a JTMS, each sentence in the knowledge base is annotated with a justiﬁcation consisting
JUSTIFICATION
of the set of sentences from which it was inferred. For example, if the knowledge base
already contains P
⇒Q, then TELL(P) will cause Q to be added with the justiﬁcation
{P, P
tions make retraction efﬁcient. Given the call RETRACT(P), the JTMS will delete exactly
those sentences for which P is a member of every justiﬁcation. So, if a sentence Q had
ﬁcation {P, P ∨R ⇒Q}, it would still be removed; but if it also had the justiﬁcation
{R, P ∨R ⇒Q}, then it would be spared. In this way, the time required for retraction of P
depends only on the number of sentences derived from P rather than on the number of other
sentences added since P entered the knowledge base.
The JTMS assumes that sentences that are considered once will probably be considered
again, so rather than deleting a sentence from the knowledge base entirely when it loses
all justiﬁcations, we merely mark the sentence as being out of the knowledge base. If a
subsequent assertion restores one of the justiﬁcations, then we mark the sentence as being
back in. In this way, the JTMS retains all the inference chains that it uses and need not
rederive sentences when a justiﬁcation becomes valid again.
In addition to handling the retraction of incorrect information, TMSs can be used to
speed up the analysis of multiple hypothetical situations. Suppose, for example, that the
sis be Site(Swimming, Pitesti), Site(Athletics, Bucharest), and Site(Equestrian, Arad).
A great deal of reasoning must then be done to work out the logistical consequences and
stead, the TMS avoids the need to start again from scratch.
Instead, we simply retract
Site(Athletics, Bucharest) and assert Site(Athletics, Sibiu) and the TMS takes care of the
necessary revisions. Inference chains generated from the choice of Bucharest can be reused
with Sibiu, provided that the conclusions are the same.Chapter
Knowledge Representation
ATMS
switching between hypothetical worlds particularly efﬁcient. In a JTMS, the maintenance of
tions and assertions, but at any time only one state is represented. An ATMS represents all the
states that have ever been considered at the same time. Whereas a JTMS simply labels each
sentence as being in or out, an ATMS keeps track, for each sentence, of which assumptions
would cause the sentence to be true. In other words, each sentence has a label that consists of
a set of assumption sets. The sentence holds just in those cases in which all the assumptions
in one of the assumption sets hold.
Truth maintenance systems also provide a mechanism for generating explanations.
EXPLANATION
Technically, an explanation of a sentence P is a set of sentences E such that E entails P.
sis for proving that P must be the case. But explanations can also include assumptions—
ASSUMPTION
sentences that are not known to be true, but would sufﬁce to prove P if they were true. For
example, one might not have enough information to prove that one’s car won’t start, but a
reasonable explanation might include the assumption that the battery is dead. This, combined
with knowledge of how cars operate, explains the observed nonbehavior. In most cases, we
will prefer an explanation E that is minimal, meaning that there is no proper subset of E that
is also an explanation. An ATMS can generate explanations for the “car won’t start” problem
by making assumptions (such as “gas in car” or “battery dead”) in any order we like, even if
some assumptions are contradictory. Then we look at the label for the sentence “car won’t
start” to read off the sets of assumptions that would justify the sentence.
cated, and we do not cover them here. The computational complexity of the truth maintenance
you should not expect truth maintenance to be a panacea. When used carefully, however, a
TMS can provide a substantial increase in the ability of a logical system to handle complex
environments and hypotheses.
THE INTERNET SHOPPING WORLD
In this ﬁnal section we put together all we have learned to encode knowledge for a shopping
research agent that helps a buyer ﬁnd product offers on the Internet. The shopping agent is
given a product description by the buyer and has the task of producing a list of Web pages
that offer such a product for sale, and ranking which offers are best. In some cases the
buyer’s product description will be precise, as in Canon Rebel XTi digital camera, and the
task is then to ﬁnd the store(s) with the best offer. In other cases the description will be only
different products.
The shopping agent’s environment is the entire World Wide Web in its full complexity—
not a toy simulated environment. The agent’s percepts are Web pages, but whereas a humanThe Internet Shopping World
Example Online Store
Select from our ﬁne line of products:
• Computers
• Cameras
• Books
• Videos
• Music
A Web page from a generic online store in the form perceived by the human
user of a browser (top), and the corresponding HTML string as perceived by the browser or
the shopping agent (bottom). In HTML, characters between < and > are markup directives
to switch to italic font, display the word Select, and then end the use of italic font. A page
with the anchor text Books.
Web user would see pages displayed as an array of pixels on a screen, the shopping agent
volves extracting useful information from percepts of this kind.
Clearly, perception on Web pages is easier than, say, perception while driving a taxi in
Cairo. Nonetheless, there are complications to the Internet perception task. The Web page in
Javascript, Flash, robot exclusion protocols, malformed HTML, sound ﬁles, movies, and text
that appears only as part of a JPEG image. An agent that can deal with all of the Internet is
almost as complex as a robot that can move in the real world. We concentrate on a simple
agent that ignores most of these complications.
The agent’s ﬁrst task is to collect product offers that are relevant to a query. If the query
but if it doesn’t provide a way to buy, it isn’t an offer. For now, we can say a page is an offer
if it contains the words “buy” or “price” or “add to cart” within an HTML link or form on theChapter
Knowledge Representation
page. For example, if the page contains a string of the form “<a . . . add to cart . . . </a”
to encode it into program code. We show how to do more sophisticated information extraction
Following links
The strategy is to start at the home page of an online store and consider all pages that can be
for example:
egories from their home page. Minor categories can be reached through a chain of relevant
links, and eventually we will reach offers. In other words, a page is relevant to the query if it
can be reached by a chain of zero or more relevant category links from a store’s home page,
and then from one more link to the product offer. We can deﬁne relevance:
Relevant(page, query) ⇔
∧∃url, url2 RelevantChain(home, url2, query) ∧Link(url2, url)
∧page = Contents(url) .
Here the predicate Link(from, to) means that there is a hyperlink from the from URL to
the to URL. To deﬁne what counts as a RelevantChain, we need to follow not just any old
hyperlinks, but only those links whose associated anchor text indicates that the link is relevant
to the product query. For this, we use LinkText(from, to, text) to mean that there is a link
between from and to with text as the anchor text. A chain of links between two URLs, start
and end, is relevant to a description d if the anchor text of each link is a relevant category
name for d. The existence of the chain itself is determined by a recursive deﬁnition, with the
empty chain (start = end) as the base case:
RelevantChain(start, end, query) ⇔(start = end)
∨(∃u, text LinkText(start, u, text) ∧RelevantCategoryName(query, text)
∧RelevantChain(u, end, query)) .
Now we must deﬁne what it means for text to be a RelevantCategoryName for query.
First, we need to relate strings to the categories they name. This is done using the predicate
Name(s, c), which says that string s is a name for category c—for example, we might assert
that Name(“laptops”, LaptopComputers). Some more examples of the Name predicate
RelevantCategoryName(query, text) is true when one of the following holds:
• The text and query name the same category—e.g., “notebooks” and “laptops.”The Internet Shopping World
Books ⊂Products
MusicRecordings ⊂Products
MusicCDs ⊂MusicRecordings
Electronics ⊂Products
DigitalCameras ⊂Electronics
StereoEquipment ⊂Electronics
Computers ⊂Electronics
DesktopComputers ⊂Computers
LaptopComputers ⊂Computers
. . .
Name(“books”, Books)
Name(“music”, MusicRecordings)
Name(“CDs”, MusicCDs)
Name(“electronics”, Electronics)
Name(“digital cameras”, DigitalCameras)
Name(“stereos”, StereoEquipment)
Name(“computers”, Computers)
Name(“desktops”, DesktopComputers)
Name(“laptops”, LaptopComputers)
Name(“notebooks”, LaptopComputers)
. . .
(a)
(b)
(a) Taxonomy of product categories. (b) Names for those categories.
• The text names a supercategory such as “computers.”
• The text names a subcategory such as “ultralight notebooks.”
The logical deﬁnition of RelevantCategoryName is as follows:
RelevantCategoryName(query, text) ⇔
Otherwise, the anchor text is irrelevant because it names a category outside this line, such as
“clothes” or “lawn & garden.”
list all possible shopping categories, because a buyer could always come up with some new
desire and manufacturers will always come out with new products to satisfy them (electric
kneecap warmers?). Nonetheless, an ontology of about a thousand categories will serve as a
very useful tool for most buyers.
In addition to the product hierarchy itself, we also need to have a rich vocabulary of
names for categories.
dence between categories and the character strings that name them. We have already seen
the problem of synonymy—two names for the same category, such as “laptop computers”
and “laptops.” There is also the problem of ambiguity—one name for two or more different
categories. For example, if we add the sentence
Name(“CDs”, CertiﬁcatesOfDeposit)
Synonymy and ambiguity can cause a signiﬁcant increase in the number of paths that
the agent has to follow, and can sometimes make it difﬁcult to determine whether a given
page is indeed relevant. A much more serious problem is the very broad range of descriptions
that a user can type and category names that a store can use. For example, the link might say
“laptop” when the knowledge base has only “laptops” or the user might ask for “a computerChapter
Knowledge Representation
tional reasoning in some cases to determine if the Name relation holds. In the worst case, this
tice, a few simple rules—such as allowing “laptop” to match a category named “laptops”—go
into online stores.
Given the logical deﬁnitions from the preceding paragraphs and suitable knowledge
bases of product categories and naming conventions, are we ready to apply an inference
algorithm to obtain a set of relevant offers for our query? Not quite! The missing element
is the Contents(url) function, which refers to the HTML page at a given URL. The agent
doesn’t have the page contents of every URL in its knowledge base; nor does it have explicit
rules for deducing what those contents might be. Instead, we can arrange for the right HTTP
procedure to be executed whenever a subgoal involves the Contents function. In this way, it
appears to the inference engine as if the entire Web is inside the knowledge base. This is an
example of a general technique called procedural attachment, whereby particular predicates
PROCEDURAL
ATTACHMENT
Comparing offers
Let us assume that the reasoning processes of the preceding section have produced a set of
vant information—price, speed, disk size, weight, and so on—from the offer pages. This can
be a difﬁcult task with real Web pages, for all the reasons mentioned previously. A common
way of dealing with this problem is to use programs called wrappers to extract information
WRAPPER
now we assume that wrappers exist, and when given a page and a knowledge base, they add
assertions to the knowledge base. Typically, a hierarchy of wrappers would be applied to a
page: a very general one to extract dates and prices, a more speciﬁc one to extract attributes
particular store. Given a page on the example.com site with the text
Our price:
followed by various technical speciﬁcations, we would like a wrapper to extract information
such as the following:
Manufacturer(c, IBM ) ∧Model(c, ThinkBook970 ) ∧
OﬀeredProduct(oﬀer, c) ∧Store(oﬀer, GenStore) ∧
This example illustrates several issues that arise when we take seriously the task of knowledge
engineering for commercial transactions. For example, notice that the price is an attribute ofSummary
the offer, not the product itself. This is important because the offer at a given store may
change from day to day even for the same individual laptop; for some categories—such as
houses and paintings—the same individual object may even be offered simultaneously by
different intermediaries at different prices. There are still more complications that we have
not handled, such as the possibility that the price depends on the method of payment and on
the buyer’s qualiﬁcations for certain discounts. The ﬁnal task is to compare the offers that
have been extracted. For example, consider these three offers:
C is dominated by A; that is, A is cheaper and faster, and they are otherwise the same. In
general, X dominates Y if X has a better value on at least one attribute, and is not worse on
any attribute. But neither A nor B dominates the other. To decide which is better we need
to know how the buyer weighs CPU speed and price against memory and disk space. The
our shopping agent will simply return a list of all undominated offers that meet the buyer’s
description. In this example, both A and B are undominated. Notice that this outcome relies
on the assumption that everyone prefers cheaper prices, faster processors, and more storage.
Some attributes, such as screen size on a notebook, depend on the user’s particular preference
(portability versus visibility); for these, the shopping agent will just have to ask the user.
The shopping agent we have described here is a simple one; many reﬁnements are
actually be of use to a shopper. Because of its declarative construction, it extends easily to
more complex applications. The main point of this section is to show that some knowledge
representation—in particular, the product hierarchy—is necessary for such an agent, and that
once we have some knowledge in this form, the rest follows naturally.
SUMMARY
By delving into the details of how one represents a variety of knowledge, we hope we have
given the reader a sense of how real knowledge bases are constructed and a feeling for the
interesting philosophical issues that arise. The major points are as follows:
and tie together the various speciﬁc domains of knowledge.
capable, in principle, of handling any domain.
fully realized, although current frameworks seem to be quite robust.
• We presented an upper ontology based on categories and the event calculus.
We
covered categories, subcategories, parts, structured objects, measurements, substances,
events, time and space, change, and beliefs.Chapter
Knowledge Representation
• Natural kinds cannot be deﬁned completely in logic, but properties of natural kinds can
be represented.
• Actions, events, and time can be represented either in situation calculus or in more
expressive representations such as event calculus. Such representations enable an agent
to construct plans by logical inference.
• We presented a detailed analysis of the Internet shopping domain, exercising the general
ontology and showing how the domain knowledge can be used by a shopping agent.
logics, have been devised to help in organizing a hierarchy of categories. Inheritance
is an important form of inference, allowing the properties of objects to be deduced from
their membership in categories.
way to avoid having to specify lots of negative information. It is best interpreted as a
default that can be overridden by additional information.
ture default reasoning in general.
• Truth maintenance systems handle knowledge updates and revisions efﬁciently.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
nium B.C. In the West, the use of deﬁnitions of terms in ancient Greek mathematics can be
regarded as the earliest instance: Aristotle’s Metaphysics (literally, what comes after the book
ogy in any ﬁeld can be regarded as a form of knowledge representation.
Early discussions of representation in AI tended to focus on “problem representation”
domain knowledge, match or exceed the performance of human experts on narrowly deﬁned
lyze the structure of organic chemical compounds) as accurately as expert chemists. Although
the success of DENDRAL was instrumental in convincing the AI research community of the
importance of knowledge representation, the representational formalisms used in DENDRAL
are highly speciﬁc to the domain of chemistry. Over time, researchers became interested in
standardized knowledge representation formalisms and ontologies that could streamline the
process of creating new expert systems. In so doing, they ventured into territory previously
explored by philosophers of science and of language. The discipline imposed in AI by the
need for one’s theories to “work” has led to more rapid and deeper progress than was the caseBibliographical and Historical Notes
when these problems were the exclusive domain of philosophy (although it has at times also
led to the repeated reinvention of the wheel).
The creation of comprehensive taxonomies or classiﬁcations dates back to ancient times.
Organon, a collection of works on logic assembled by his students after his death, included a
treatise called Categories in which he attempted to construct what we would now call an upper
Our present system of biological classiﬁcation, including the use of “binomial nomenclature”
(classiﬁcation via genus and species in the technical sense), was invented by the Swedish
of “cellular phone,” which in turn is a type of “consumer electronics,” “phone,” “wireless
communication device,” and other concepts. The DBPEDIA project extracts structured data
ﬁciently managing a very large ontology. A survey of techniques for extracting knowledge
allows for assertions to be made in the form of relational triples, and provides some means
that supports inferences over these triples. So far, usage seems to be inversely proportional to
Web content, followed by the simplest representation schemes, such as microformats (Khare,
add attributes to literal text. Usage of sophisticated RDF and OWL ontologies is not yet
been realized. The conferences on Formal Ontology in Information Systems (FOIS) contain
The taxonomy used in this chapter was developed by the authors and is based in part
Successful deep ontologies within a speciﬁc ﬁeld include the Gene Ontology projectChapter
Knowledge Representation
Doubts about the feasibility of a single ontology for all knowledge are expressed by
initial project of building one single ontology . . . has . . . largely been abandoned.”
the logic of events maps onto the language we use to talk about events. An alternative to the
ral than situations for reasoning about extended and concurrent events. Peter Ladkin (1986a,
nary “convex” time intervals) and applied the techniques of mathematical abstract algebra to
The histories in Pat Hayes’s (1985a) ontology of liquids and the chronicles in McDermott’s
The question of the ontological status of substances has a long history. Plato proposed
that substances were abstract entities entirely distinct from physical objects; he would say
sition adopted in this chapter, in which substances are categories of objects, was championed
ter, in which butter is one object consisting of all buttery objects in the universe, was proposed
MEREOLOGY
the Greek word for “part”) used the part–whole relation as a substitute for mathematical set
theory, with the aim of eliminating abstract entities such as sets. A more readable exposition
rate inheritance mechanism based on part–whole relations—the approach gained the support
Mental objects have been the subject of intensive study in philosophy and AI. There
are three main approaches. The one taken in this chapter, based on modal logic and possible
third approach is a syntactic theory, in which mental objects are represented by character
SYNTACTIC THEORYBibliographical and Historical Notes
strings. A string is just a complex term denoting a list of symbols, so CanFly(Clark) can
be represented by the list of symbols [C, a, n, F, l, y, (, C, l, a, r, k, )]. The syntactic theory
comparison of the syntactic and modal theories of knowledge.
developed existential graphs as the ﬁrst semantic network formalism using modern logic.
presented a version of semantic networks called frames; a frame was a representation of
tion of semantics arose quite acutely with respect to Quillian’s semantic networks (and those
be applied to all knowledge representation formalisms. This remains a controversial idea;
constructs—notably, certain uses of disjunction and negation—were primarily responsible
have argued that restricting the expressiveness of a language either makes it impossible to
solve certain problems or encourages the user to circumvent the language restrictions through
nonlogical means.
The three main formalisms for dealing with nonmonotonic inference—circumscriptionChapter
Knowledge Representation
scription; the underlying theory of stable model semantics was introduced by Gelfond and
discusses the application of answer set programming to planning. A variety of nonmonotonic
reasoning systems based on logic programming are documented in the proceedings of the
conferences on Logic Programming and Nonmonotonic Reasoning (LPNMR).
operations of a NASA spacecraft in real time.
This chapter could not cover every area of knowledge representation in depth. The three
principal topics omitted are the following:
Qualitative physics: Qualitative physics is a subﬁeld of knowledge representation concerned
QUALITATIVE
PHYSICS
speciﬁcally with constructing a logical, nonnumeric theory of physical objects and processes.
towers of blocks. Fahlman discovered in the process of designing it that most of the effort
stability of various subassemblies of blocks, rather than into planning per se. He sketches a
Davidson’s events—to construct a fairly complex naive physics of liquids. Hayes was the
ﬁrst to prove that a bath with the plug in will eventually overﬂow if the tap keeps running and
ontology of liquids that describes the pouring of liquids into containers.
pendently and almost simultaneously developed systems that can reason about a physical
system based on qualitative abstractions of the underlying equations. Qualitative physics
soon developed to the point where it became possible to analyze an impressive variety ofChapter
Knowledge Representation
as many real categories as possible. A good way to do this is to cover all the things in your
everyday life. This includes objects and events. Start with waking up, and proceed in an
orderly fashion noting everything that you see, touch, do, and think about. For example,
a random sampling produces music, news, milk, walking, driving, gas, Soda Hall, carpet,
You should produce both a single hierarchy chart (on a large sheet of paper) and a
listing of objects and categories with the relations satisﬁed by members of each category.
Every object should be in a category, and every category should be in the hierarchy.
computer interface. In particular, your representation should be able to describe:
• The state of a window: minimized, displayed, or nonexistent.
• Which window (if any) is the active window.
• The position of every window at a given time.
• The order (front to back) of overlapping windows.
• The actions of creating, destroying, resizing, and moving windows; changing the state
of a window; and bringing a window to the front. Treat these actions as atomic; that is,
do not deal with the issue of relating them to mouse actions. Give axioms describing
the effects of actions on ﬂuents. You may use either event or situation calculus.
Assume an ontology containing situations, actions, integers (for x and y coordinates) and
windows. Deﬁne a language over this ontology; that is, a list of constants, function symbols,
and predicates with an English description of each. If you need to add more categories to the
(and should) use symbols deﬁned in the text, but be sure to list these explicitly.
State the following in the language you developed for the previous exercise:
a. In situation S0, window W1 is behind W2 but sticks out on the left and right. Do not
state exact coordinates for these; describe the general situation.
b. If a window is displayed, then its top edge is higher than its bottom edge.
c. After you create a window w, it is displayed.
d. A window can be minimized if it is displayed.
(Adapted from an example by Doug Lenat.) Your mission is to capture, in logical
form, enough knowledge to answer a series of questions about the following simple scenario:
Yesterday John went to the North Berkeley Safeway supermarket and bought two
pounds of tomatoes and a pound of ground beef.
Start by trying to represent the content of the sentence as a series of assertions. You should
write sentences that have straightforward logical structure (e.g., statements that objects have
erty satisfy another). The following might help you get started:Chapter
Knowledge Representation
veloped in the chapter:
c. The water in John’s water bottle is frozen.
d. Perrier is a kind of water.
e. John has Perrier in his water bottle.
f. All liquids have a freezing point.
g. A liter of water weighs more than a liter of alcohol.
Write deﬁnitions for the following:
a. ExhaustivePartDecomposition
b. PartPartition
c. PartwiseDisjoint
These should be analogous to the deﬁnitions for ExhaustiveDecomposition, Partition, and
Disjoint. Is it the case that PartPartition(s, BunchOf (s))? If so, prove it; if not, give a
counterexample and deﬁne sufﬁcient conditions under which it does hold.
An alternative scheme for representing measures involves applying the units function
to an abstract length object. In such a scheme, one would write Inches(Length(L1)) =
Add sentences to extend the deﬁnition of the predicate Name(s, c) so that a string
such as “laptop computer” matches the appropriate category names from a variety of stores.
Try to make your deﬁnition general. Test it by looking at ten online stores, and at the category
names they give for three different categories. For example, for the category of laptops, we
found the names “Notebooks,” “Laptops,” “Notebook Computers,” “Notebook,” “Laptops
and Notebooks,” and “Notebook PCs.” Some of these can be covered by explicit Name facts,
while others could be covered by sentences for handling plurals, conjunctions, etc.
Write event calculus axioms to describe the actions in the wumpus world.
world events:
LK: The life of President Kennedy.
IK: The infancy of President Kennedy.
PK: The presidency of President Kennedy.
LJ: The life of President Johnson.
PJ: The presidency of President Johnson.
LO: The life of President Obama.Chapter
Knowledge Representation
c. A shopping agent in an environment in which there are costs of gathering information.
d. Reasoning about public key cryptography, which rests on the intractability of certain
computational problems.
logic, and comment on the result:
All(Son, And(Unemployed, Married, All(Spouse, Doctor))),
All(Daughter, And(Professor, Fills(Department, Physics, Math)))) .
Recall that inheritance information in semantic networks can be captured logically
by suitable implication sentences. This exercise investigates the efﬁciency of using such
sentences for inheritance.
chaining theorem prover such as Prolog?
with the inheritance method used in semantic nets.
d. Describe a situation in which neither forward nor backward chaining on the sentences
will allow the price query for an individual car to be handled efﬁciently.
e. Can you suggest a solution enabling this type of query to be solved efﬁciently in all
cases in logic systems? (Hint: Remember that two cars of the same year and model
have the same price.)
One might suppose that the syntactic distinction between unboxed links and singly
tached to categories; an inheritance algorithm could simply assume that an unboxed link
attached to a category is intended to apply to all members of that category. Show that this
argument is fallacious, giving examples of errors that would arise.
One part of the shopping process that was not covered in this chapter is checking
for compatibility between items. For example, if a digital camera is ordered, what accessory
batteries, memory cards, and cases are compatible with the camera? Write a knowledge base
that can determine the compatibility of a set of items and suggest replacements or additional
items if the shopper makes a choice that is not compatible. The knowledge base should works
with at least one line of products and extend easily to other lines.
A complete solution to the problem of inexact matches to the buyer’s description
in shopping is very difﬁcult and requires a full array of natural language processing andQUANTIFYING
UNCERTAINTY
In which we see how an agent can tame uncertainty with degrees of belief.
ACTING UNDER UNCERTAINTY
UNCERTAINTY
ism, or a combination of the two. An agent may never know for certain what state it’s in or
where it will end up after a sequence of actions.
designed to handle uncertainty by keeping track of a belief state—a representation of the set
dles every possible eventuality that its sensors may report during execution. Despite its many
virtues, however, this approach has signiﬁcant drawbacks when taken literally as a recipe for
creating agent programs:
ically possible explanation for the observations, no matter how unlikely. This leads to
• A correct contingent plan that handles every eventuality can grow arbitrarily large and
must consider arbitrarily unlikely contingencies.
• Sometimes there is no plan that is guaranteed to achieve the goal—yet the agent must
act. It must have some way to compare the merits of plans that are not guaranteed.
minutes before the ﬂight departs and driving at a reasonable speed. Even though the airport
that “Plan A90 will get us to the airport in time.” Instead, it reaches the weaker conclusion
“Plan A90 will get us to the airport in time, as long as the car doesn’t break down or run out
of gas, and I don’t get into an accident, and there are no accidents on the bridge, and the plane
doesn’t leave early, and no meteorite hits the car, and . . . .” None of these conditions can beActing under Uncertainty
deduced for sure, so the plan’s success cannot be inferred. This is the qualiﬁcation problem
Nonetheless, in some sense A90 is in fact the right thing to do. What do we mean by
ative to the agent’s knowledge about the environment). The performance measure includes
getting to the airport in time for the ﬂight, avoiding a long, unproductive wait at the airport,
and avoiding speeding tickets along the way. The agent’s knowledge cannot guarantee any of
these outcomes for A90, but it can provide some degree of belief that they will be achieved.
Other plans, such as A180, might increase the agent’s belief that it will get to the airport on
time, but also increase the likelihood of a long wait. The right thing to do—the rational
hood that, and degree to which, they will be achieved. The remainder of this section hones
these ideas, in preparation for the development of the general theories of uncertain reasoning
and rational decisions that we present in this and subsequent chapters.
Summarizing uncertainty
Let’s consider an example of uncertain reasoning: diagnosing a dental patient’s toothache.
Diagnosis—whether for medicine, automobile repair, or whatever—almost always involves
uncertainty. Let us try to write rules for dental diagnosis using propositional logic, so that we
can see how the logical approach breaks down. Consider the following simple rule:
Toothache ⇒Cavity .
The problem is that this rule is wrong. Not all patients with toothaches have cavities; some
of them have gum disease, an abscess, or one of several other problems:
Toothache ⇒Cavity ∨GumProblem ∨Abscess . . .
Unfortunately, in order to make the rule true, we have to add an almost unlimited list of
possible problems. We could try turning the rule into a causal rule:
Cavity ⇒Toothache .
But this rule is not right either; not all cavities cause pain. The only way to ﬁx the rule
required for a cavity to cause a toothache. Trying to use logic to cope with a domain like
medical diagnosis thus fails for three main reasons:
• Laziness: It is too much work to list the complete set of antecedents or consequents
LAZINESS
needed to ensure an exceptionless rule and too hard to use such rules.
• Theoretical ignorance: Medical science has no complete theory for the domain.
THEORETICAL
IGNORANCE
• Practical ignorance: Even if we know all the rules, we might be uncertain about a
PRACTICAL
IGNORANCE
particular patient because not all the necessary tests have been or can be run.
The connection between toothaches and cavities is just not a logical consequence in either
direction. This is typical of the medical domain, as well as most other judgmental domains:
law, business, design, automobile repair, gardening, dating, and so on. The agent’s knowledgeChapter
Quantifying Uncertainty
can at best provide only a degree of belief in the relevant sentences. Our main tool for
DEGREE OF BELIEF
PROBABILITY
THEORY
ontological commitments of logic and probability theory are the same—that the world is
composed of facts that do or do not hold in any particular case—but the epistemological
commitments are different: a logical agent believes each sentence to be true or false or has
ness and ignorance, thereby solving the qualiﬁcation problem. We might not know for sure
that out of all the situations that are indistinguishable from the current situation as far as our
some general dental knowledge, or from a combination of evidence sources.
One confusing point is that at the time of our diagnosis, there is no uncertainty in the
actual world: the patient either has a cavity or doesn’t. So what does it mean to say the
statements are made with respect to a knowledge state, not with respect to the real world. We
later learn that the patient has a history of gum disease, we can make a different statement:
“The probability that the patient has a cavity, given that she has a toothache and a history of
these statements do not contradict each other; each is a separate assertion about a different
knowledge state.
Uncertainty and rational decisions
of catching our ﬂight. Does this mean it is a rational choice? Not necessarily: there might
be other plans, such as A180, with higher probabilities. If it is vital not to miss the ﬂight,
then it is worth risking the longer wait at the airport. What about A1440, a plan that involves
although it almost guarantees getting there on time, it involves an intolerable wait—not to
mention a possibly unpleasant diet of airport food.
PREFERENCE
sible outcomes of the various plans. An outcome is a completely speciﬁed state, including
OUTCOME
such factors as whether the agent arrives on time and the length of the wait at the airport. We
use utility theory to represent and reason with preferences. (The term utility is used here in
UTILITY THEORY
the sense of “the quality of being useful,” not in the sense of the electric company or water
works.) Utility theory says that every state has a degree of usefulness, or utility, to an agent
and that the agent will prefer states with higher utility.Basic Probability Notation
The utility of a state is relative to an agent. For example, the utility of a state in which
White has checkmated Black in a game of chess is obviously high for the agent playing White,
are dictated by the rules of tournament chess—some players (including the authors) might be
thrilled with a draw against the world champion, whereas other players (including the former
world champion) might not. There is no accounting for taste or preferences: you might think
or even misguided, but you could not say the agent is irrational. A utility function can account
for any set of preferences—quirky or typical, noble or perverse. Note that utilities can account
for altruism, simply by including the welfare of others as one of the factors.
Preferences, as expressed by utilities, are combined with probabilities in the general
theory of rational decisions called decision theory:
DECISION THEORY
Decision theory = probability theory + utility theory .
The fundamental idea of decision theory is that an agent is rational if and only if it chooses
the action that yields the highest expected utility, averaged over all the possible outcomes
of the action. This is called the principle of maximum expected utility (MEU). Note that
MAXIMUM EXPECTED
UTILITY
“expected” might seem like a vague, hypothetical term, but as it is used here it has a precise
meaning: it means the “average,” or “statistical mean” of the outcomes, weighted by the
brieﬂy on optimal decisions in backgammon; it is in fact a completely general principle.
maintain a belief state reﬂecting the history of percepts to date. The primary difference is
states but also their probabilities. Given the belief state, the agent can make probabilistic
predictions of action outcomes and hence select the action with highest expected utility. This
chapter and the next concentrate on the task of representing and computing with probabilistic
actions in uncertain environments.
BASIC PROBABILITY NOTATION
For our agent to represent and use probabilistic information, we need a formal language.
ematicians to other human mathematicians. Appendix A includes a standard introduction to
elementary probability theory; here, we take an approach more suited to the needs of AI and
more consistent with the concepts of formal logic.Chapter
Quantifying Uncertainty
persistent: belief state, probabilistic beliefs about the current state of the world
action, the agent’s action
update belief state based on action and percept
calculate outcome probabilities for actions,
given action descriptions and current belief state
select action with highest expected utility
given probabilities of outcomes and utility information
return action
What probabilities are about
Like logical assertions, probabilistic assertions are about possible worlds. Whereas logical
assertions say which possible worlds are strictly ruled out (all those in which the assertion is
false), probabilistic assertions talk about how probable the various worlds are. In probability
theory, the set of all possible worlds is called the sample space. The possible worlds are
SAMPLE SPACE
mutually exclusive and exhaustive—two possible worlds cannot both be the case, and one
possible world must be the case. For example, if we are about to roll two (distinguishable)
(uppercase omega) is used to refer to the sample space, and ω (lowercase omega) refers to
elements of the space, that is, particular possible worlds.
A fully speciﬁed probability model associates a numerical probability P(ω) with each
PROBABILITY MODEL


ω∈Ω
For example, if we assume that each die is fair and the rolls don’t interfere with each other,
might have higher probabilities, leaving the others with lower probabilities.
Probabilistic assertions and queries are not usually about particular possible worlds, but
about sets of them. For example, we might be interested in the cases where the two dice add
EVENT
the sets are always described by propositions in a formal language. (One such language is
possible worlds in which the proposition holds. The probability associated with a proposition
certain complications that are less relevant for most purposes in AI.Basic Probability Notation
is deﬁned to be the sum of the probabilities of the worlds in which it holds:
For any proposition φ, P(φ) =


ω∈φ
P(ω) .
of the probabilities of each possible world. For example, if we believe the dice conspire to
the underlying probability model without fully determining it.
UNCONDITIONAL
PROBABILITY
PRIOR PROBABILITY
sitions in the absence of any other information. Most of the time, however, we have some
information, usually called evidence, that has already been revealed. For example, the ﬁrst
EVIDENCE
stop spinning. In that case, we are interested not in the unconditional probability of rolling
doubles, but the conditional or posterior probability (or just “posterior” for short) of rolling
CONDITIONAL
PROBABILITY
POSTERIOR
PROBABILITY
the “ | ” is pronounced “given.” Similarly, if I am going to the dentist for a regular checkup,
such that any expression of the form P(. . . | . . .) always means P((. . .)|(. . .)).
served; it just isn’t especially useful. When making decisions, an agent needs to condition
ample, if we had the further information that the dentist found no cavities, we deﬁnitely
tional probabilities as follows: for any propositions a and b, we have
P(a | b) = P(a ∧b)
P(b)
,
.
The deﬁnition makes sense if you remember that observing b rules out all those possible
worlds where b is false, leaving a set whose total probability is just P(b). Within that set, theChapter
Quantifying Uncertainty
form called the product rule:
PRODUCT RULE
P(a ∧b) = P(a | b)P(b) ,
The product rule is perhaps easier to remember: it comes from the fact that, for a and b to be
true, we need b to be true, and we also need a to be true given b.
The language of propositions in probability assertions
In this chapter and the next, propositions describing sets of possible worlds are written in a
notation that combines elements of propositional logic and constraint satisfaction notation. In
Variables in probability theory are called random variables and their names begin with
RANDOM VARIABLE
an uppercase letter. Thus, in the dice example, Total and Die1 are random variables. Every
random variable has a domain—the set of possible values it can take on. The domain of
DOMAIN
random variable has the domain {true, false} (notice that values are always lowercase); for
vention, propositions of the form A = true are abbreviated simply as a, while A = false is
abbreviated as ¬a. (The uses of doubles, cavity, and toothache in the preceding section are
abbreviations of this kind.) As in CSPs, domains can be sets of arbitrary tokens; we might
choose the domain of Age to be {juvenile, teen, adult} and the domain of Weather might
be {sunny, rain, cloudy, snow}. When no ambiguity is possible, it is common to use a value
by itself to stand for the proposition that a particular variable has that value; thus, sunny can
stand for Weather = sunny.
The preceding examples all have ﬁnite domains. Variables can have inﬁnite domains,
too—either discrete (like the integers) or continuous (like the reals). For any variable with an
viated forms for Boolean variables) by using the connectives of propositional logic. For
example, we can express “The probability that the patient has a cavity, given that she is a
Sometimes we will want to talk about the probabilities of all the possible values of a random
variable. We could write:
but as an abbreviation we will allowBasic Probability Notation
deﬁned ordering ⟨sunny, rain, cloudy, snow⟩on the domain of Weather. We say that the
PROBABILITY
DISTRIBUTION
tion is also used for conditional distributions: P(X | Y ) gives the values of P(X = xi | Y = yj)
for each possible i, j pair.
For continuous variables, it is not possible to write out the entire distribution as a vector,
because there are inﬁnitely many values. Instead, we can deﬁne the probability that a random
variable takes on some value x as a parameterized function of x. For example, the sentence
P(NoonTemp = x) = Uniform[18C,26C](x)
degrees Celsius. We call this a probability density function.
PROBABILITY
DENSITY FUNCTION
Probability density functions (sometimes called pdfs) differ in meaning from discrete
distributions. Saying that the probability density is uniform from 18C to 26C means that
density for a continuous random variable X at value x as P(X = x) or just P(x); the intuitive
deﬁnition of P(x) is the probability that X falls within an arbitrarily small region beginning
at x, divided by the width of the region:
P(x) = lim
For NoonTemp we have
P(NoonTemp = x) = Uniform[18C,26C](x) =

,
8C , note
that
8C is not a probability, it is a probability density. The probability that NoonTemp is
sion seldom arises and the equations are usually identical. Note that probabilities are unitless
numbers, whereas density functions are measured with a unit, in this case reciprocal degrees.
In addition to distributions on single variables, we need notation for distributions on
multiple variables. Commas are used for this. For example, P(Weather, Cavity) denotes
table of probabilities called the joint probability distribution of Weather and Cavity. We
JOINT PROBABILITY
DISTRIBUTION
vector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity.
The P notation makes certain expressions much more concise than they might otherwise be.
For example, the product rules for all possible values of Weather and Cavity can be written
as a single equation:
P(Weather, Cavity) = P(Weather | Cavity)P(Cavity) ,Chapter
Quantifying Uncertainty
P(W = sunny ∧C = true) = P(W = sunny|C = true) P(C = true)
P(W = rain ∧C = true) = P(W = rain|C = true) P(C = true)
P(W = cloudy ∧C = true) = P(W = cloudy|C = true) P(C = true)
P(W = snow ∧C = true) = P(W = snow|C = true) P(C = true)
P(W = sunny ∧C = false) = P(W = sunny|C = false) P(C = false)
P(W = rain ∧C = false) = P(W = rain|C = false) P(C = false)
P(W = cloudy ∧C = false) = P(W = cloudy|C = false) P(C = false)
P(W = snow ∧C = false) = P(W = snow|C = false) P(C = false) .
tor that is the probability of a sunny day with a cavity, which could also be written as
P(sunny, cavity) or P(sunny ∧cavity). We will sometimes use P notation to derive results
Now we have deﬁned a syntax for propositions and probability assertions and we have
of the probabilities of worlds in which it holds. To complete the semantics, we need to say
what the worlds are and how to determine whether a proposition holds in a world. We borrow
this part directly from the semantics of propositional logic, as follows. A possible world is
deﬁned to be an assignment of values to all of the random variables under consideration. It is
easy to see that this deﬁnition satisﬁes the basic requirement that possible worlds be mutually
truth of any given proposition, no matter how complex, can be determined easily in such
worlds using the same recursive deﬁnition of truth as for formulas in propositional logic.
From the preceding deﬁnition of possible worlds, it follows that a probability model is
full joint probability distribution. For example, if the variables are Cavity, Toothache,
FULL JOINT
PROBABILITY
DISTRIBUTION
and Weather, then the full joint distribution is given by P(Cavity, Toothache, Weather).
proposition’s probability is a sum over possible worlds, a full joint distribution sufﬁces, in
principle, for calculating the probability of any proposition.
Probability axioms and their reasonableness
ple, we can derive the familiar relationship between the probability of a proposition and the
probability of its negation:
P(¬a) = 
ω∈¬a P(ω)
= 
ω∈¬a P(ω) + 
ω∈a P(ω) −
ω∈a P(ω)
= 
ω∈Ω P(ω) −
ω∈a P(ω)
grouping the ﬁrst two termsBasic Probability Notation
called the inclusion–exclusion principle:
INCLUSION–
EXCLUSION
PRINCIPLE
P(a ∨b) = P(a) + P(b) −P(a ∧b) .
This rule is easily remembered by noting that the cases where a holds, together with the cases
where b holds, certainly cover all the cases where a ∨b holds; but summing the two sets of
cases counts their intersection twice, so we need to subtract P(a ∧b). The proof is left as an
KOLMOGOROV’S
AXIOMS
sian mathematician Andrei Kolmogorov, who showed how to build up the rest of probability
theory from this simple foundation and how to handle the difﬁculties caused by continuous
lated propositions. This is analogous to the fact that a logical agent cannot simultaneously
believe A, B, and ¬(A ∧B), because there is no possible world in which all three are true.
With probabilities, however, statements refer not to the world directly, but to the agent’s own
state of knowledge. Why, then, can an agent not hold the following set of beliefs (even though
they violate Kolmogorov’s axioms)?
This kind of question has been the subject of decades of intense debate between those who
advocate the use of probabilities as the only legitimate form for degrees of belief and those
who advocate alternative approaches.
belief in a proposition a, then the agent should be able to state odds at which it is indifferent
observe the outcome of a, and whoever is right collects the money. If an agent’s degrees of
belief do not accurately reﬂect the world, then you would expect that it would tend to lose
money over the long run to an opposing agent whose beliefs more accurately reﬂect the state
of the world.
of belief that violate the axioms of probability theory then there is a combination of bets byChapter
Quantifying Uncertainty
regardless of the outcomes for a and b. De Finetti’s theorem implies that no rational agent
can have beliefs that violate the axioms of probability.
Proposition
Belief
Bet
Stakes
a, b
a, ¬b
¬a, b
¬a, ¬b
a
a
b
b
a ∨b
¬(a ∨b)
trived. For example, what if one refuses to bet? Does that end the argument? The answer is
agent is unavoidably involved at every moment. Every action (including inaction) is a kind
of bet, and every outcome can be seen as a payoff of the bet. Refusing to bet is like refusing
to allow time to pass.
Other strong philosophical arguments have been put forward for the use of probabilities,
set of axioms for reasoning with degrees of beliefs: no contradictions, correspondence with
ordinary logic (for example, if belief in A goes up, then belief in ¬A must go down), and so
on. The only controversial axiom is that degrees of belief must be numbers, or at least act
like numbers in that they must be transitive (if belief in A is greater than belief in B, which is
greater than belief in C, then belief in A must be greater than C) and comparable (the belief
in A must be one of equal to, greater than, or less than belief in B). It can then be proved that
probability is the only approach that satisﬁes these axioms.
The world being the way it is, however, practical demonstrations sometimes speak
louder than proofs. The success of reasoning systems based on probability theory has been
much more effective in making converts. We now look at how the axioms can be deployed to
make inferences.
INFERENCE USING FULL JOINT DISTRIBUTIONS
PROBABILISTIC
INFERENCE
tation of posterior probabilities for query propositions given observed evidence. We use the
rived. Along the way we also introduce several useful techniques for manipulating equations
involving probabilities.Inference Using Full Joint Distributions
WHERE DO PROBABILITIES COME FROM?
There has been endless debate over the source and status of probability numbers.
The frequentist position is that the numbers can come only from experiments: if
in the limit of inﬁnitely many samples. From any ﬁnite sample, we can estimate
the true fraction and also calculate how accurate our estimate is likely to be.
The objectivist view is that probabilities are real aspects of the universe—
tions of an observer’s degree of belief. For example, the fact that a fair coin comes
quentist measurements are attempts to observe these propensities. Most physicists
agree that quantum phenomena are objectively probabilistic, but uncertainty at the
macroscopic scale—e.g., in coin tossing—usually arises from ignorance of initial
conditions and does not seem consistent with the propensity view.
The subjectivist view describes probabilities as a way of characterizing an
propositions, but then insists on proper Bayesian updating as evidence arrives.
ity of a particular experiment, the frequentist has to place it in a reference class of
mate in practice is that of an event that has never occurred before.” For example,
given a particular patient, a frequentist who wants to estimate the probability of a
cavity will consider a reference class of other patients who are similar in important
ways—age, symptoms, diet—and see what proportion of them had a cavity. If the
dentist considers everything that is known about the patient—weight to the nearest
gram, hair color, mother’s maiden name—then the reference class becomes empty.
This has been a vexing problem in the philosophy of science.
sitions that are syntactically “symmetric” with respect to the evidence should be
accorded equal probability. Various reﬁnements have been proposed, culminating
ble of computing the correct probability for any proposition from any collection of
observations. Currently, it is believed that no unique inductive logic exists; rather,
any such logic rests on a subjective prior probability distribution whose effect is
diminished as more observations are collected.Chapter
Quantifying Uncertainty
toothache
¬toothache
catch
¬catch
catch
¬catch
cavity
¬cavity
A full joint distribution for the Toothache, Cavity, Catch world.
We begin with a simple example: a domain consisting of just the three Boolean variables
Toothache, Cavity, and Catch (the dentist’s nasty steel probe catches in my tooth). The full
ity of any proposition, simple or complex: simply identify those possible worlds in which the
proposition is true and add up their probabilities. For example, there are six possible worlds
in which cavity ∨toothache holds:
One particularly common task is to extract the distribution over some subset of variables or
a single variable. For example, adding the entries in the ﬁrst row gives the unconditional or
marginal probability4 of cavity:
MARGINAL
PROBABILITY
MARGINALIZATION
ities for each possible value of the other variables, thereby taking them out of the equation.
We can write the following general marginalization rule for any sets of variables Y and Z:
P(Y) =


P(Y, z) ,
where 
Z. We sometimes abbreviate this as 
z, leaving Z implicit. We just used the rule as
P(Cavity) =


P(Cavity, z) .
A variant of this rule involves conditional probabilities instead of joint probabilities, using
the product rule:
P(Y) =


z
P(Y | z)P(z) .
This rule is called conditioning. Marginalization and conditioning turn out to be useful rules
CONDITIONING
for all kinds of derivations involving probability expressions.
ables, given evidence about others. Conditional probabilities can be found by ﬁrst using
margins of insurance tables.Inference Using Full Joint Distributions
uating the expression from the full joint distribution.
For example, we can compute the
probability of a cavity, given evidence of a toothache, as follows:
P(cavity | toothache) = P(cavity ∧toothache)
P(toothache)
=
Just to check, we can also compute the probability that there is no cavity, given a toothache:
P(¬cavity | toothache) = P(¬cavity ∧toothache)
P(toothache)
=
it can be viewed as a normalization constant for the distribution P(Cavity | toothache),
NORMALIZATION
denote such constants. With this notation, we can write the two preceding equations in one:
P(Cavity | toothache) = α P(Cavity, toothache)
= α [P(Cavity, toothache, catch) + P(Cavity, toothache, ¬catch)]
In other words, we can calculate P(Cavity | toothache) even if we don’t know the value of
probability calculations, both to make the computation easier and to allow us to proceed when
some probability assessment (such as P(toothache)) is not available.
From the example, we can extract a general inference procedure. We begin with the
case in which the query involves a single variable, X (Cavity in the example). Let E be the
list of evidence variables (just Toothache in the example), let e be the list of observed values
for them, and let Y be the remaining unobserved variables (just Catch in the example). The
query is P(X | e) and can be evaluated as
P(X | e) = α P(X, e) = α


y
P(X, e, y) ,
where the summation is over all possible ys (i.e., all possible combinations of values of the
plete set of variables for the domain, so P(X, e, y) is simply a subset of probabilities from the
full joint distribution.
queries for discrete variables. It does not scale well, however: for a domain described by n
Boolean variables, it requires an input table of size O(2n) and takes O(2n) time to process theChapter
Quantifying Uncertainty
full joint distribution in tabular form is just not a practical tool for building reasoning systems.
Instead, it should be viewed as the theoretical foundation on which more effective approaches
may be built, just as truth tables formed a theoretical foundation for more practical algorithms
like DPLL. The remainder of this chapter introduces some of the basic ideas required in
INDEPENDENCE
The full joint distribution then becomes P(Toothache, Catch, Cavity, Weather), which has
one for each kind of weather. What relationship do these editions have to each other and to
and P(toothache, catch, cavity) related? We can use the product rule:
P(toothache, catch, cavity, cloudy)
= P(cloudy | toothache, catch, cavity)P(toothache, catch, cavity) .
Now, unless one is in the deity business, one should not imagine that one’s dental problems
inﬂuence the weather. And for indoor dentistry, at least, it seems safe to say that the weather
does not inﬂuence the dental variables. Therefore, the following assertion seems reasonable:
P(cloudy | toothache, catch, cavity) = P(cloudy) .
From this, we can deduce
P(toothache, catch, cavity, cloudy) = P(cloudy)P(toothache, catch, cavity) .
A similar equation exists for every entry in P(Toothache, Catch, Cavity, Weather). In fact,
we can write the general equation
P(Toothache, Catch, Cavity, Weather) = P(Toothache, Catch, Cavity)P(Weather) .
INDEPENDENCE
dependence and absolute independence). In particular, the weather is independent of one’s
dental problems. Independence between propositions a and b can be written as
P(a | b) = P(a)
or
P(b | a) = P(b)
or
P(a ∧b) = P(a)P(b) .
can be written as follows (again, these are all equivalent):
P(X | Y ) = P(X)
or
P(Y | X) = P(Y )
or
P(X, Y ) = P(X)P(Y ) .
Independence assertions are usually based on knowledge of the domain. As the toothache–
essary to specify the full joint distribution. If the complete set of variables can be dividedBayes’ Rule and Its Use
Weather
Toothache
Catch
Cavity
decomposes
      into
Weather
Toothache
Catch
Cavity
decomposes
       into
Coin1
Coinn
Coin1
Coinn
(a)
(b)
Two examples of factoring a large joint distribution into smaller distributions,
using absolute independence. (a) Weather and dental problems are independent. (b) Coin
ﬂips are independent.
into independent subsets, then the full joint distribution can be factored into separate joint
distributions on those subsets. For example, the full joint distribution on the outcome of n
dentistry and meteorology is a good thing, because otherwise the practice of dentistry might
require intimate knowledge of meteorology, and vice versa.
When they are available, then, independence assertions can help in reducing the size of
the domain representation and the complexity of the inference problem. Unfortunately, clean
separation of entire sets of variables by independence is quite rare. Whenever a connection,
however indirect, exists between two variables, independence will fail to hold. Moreover,
even independent subsets can be quite large—for example, dentistry might involve dozens of
diseases and hundreds of symptoms, all of which are interrelated. To handle such problems,
we need more subtle methods than the straightforward concept of independence.
BAYES’ RULE AND ITS USE
P(a ∧b) = P(a | b)P(b)
and
P(a ∧b) = P(b | a)P(a) .
P(b | a) = P(a | b)P(b)
P(a)
.
This equation is known as Bayes’ rule (also Bayes’ law or Bayes’ theorem). This simple
BAYES’ RULE
equation underlies most modern AI systems for probabilistic inference.Chapter
Quantifying Uncertainty
The more general case of Bayes’ rule for multivalued variables can be written in the P
notation as follows:
P(Y | X) = P(X | Y )P(Y )
P(X)
,
ues of the variables. We will also have occasion to use a more general version conditionalized
on some background evidence e:
P(Y | X, e) = P(X | Y, e)P(Y | e)
P(X | e)
.
Applying Bayes’ rule: The simple case
On the surface, Bayes’ rule does not seem very useful. It allows us to compute the single
term P(b | a) in terms of three terms: P(a | b), P(b), and P(a). That seems like two steps
backwards, but Bayes’ rule is useful in practice because there are many cases where we do
have good probability estimates for these three numbers and need to compute the fourth.
Often, we perceive as evidence the effect of some unknown cause and we would like to
determine that cause. In that case, Bayes’ rule becomes
P(cause | effect) = P(effect | cause)P(cause)
P(effect)
.
CAUSAL
tion, whereas P(cause | effect) describes the diagnostic direction. In a task such as medical
DIAGNOSTIC
diagnosis, we often have conditional probabilities on causal relationships (that is, the doctor
knows P(symptoms | disease)) and want to derive a diagnosis, P(disease | symptoms). For
example, a doctor knows that the disease meningitis causes the patient to have a stiff neck,
proposition that the patient has meningitis, we have
P(m | s) = P(s | m)P(m)
P(s)
probability of meningitis in the patient remains small. This is because the prior probability of
stiff necks is much higher than that of meningitis.
of the evidence (here, P(s)) by instead computing a posterior probability for each value ofBayes’ Rule and Its Use
the query variable (here, m and ¬m) and then normalizing the results. The same process can
be applied when using Bayes’ rule. We have
P(M | s) = α ⟨P(s | m)P(m), P(s | ¬m)P(¬m)⟩.
Thus, to use this approach we need to estimate P(s | ¬m) instead of P(s). There is no free
lunch—sometimes this is easier, sometimes it is harder. The general form of Bayes’ rule with
normalization is
P(Y | X) = α P(X | Y )P(Y ) ,
One obvious question to ask about Bayes’ rule is why one might have available the
conditional probability in one direction, but not the other. In the meningitis domain, perhaps
has quantitative information in the diagnostic direction from symptoms to causes. Such a
doctor has no need to use Bayes’ rule. Unfortunately, diagnostic knowledge is often more
fragile than causal knowledge. If there is a sudden epidemic of meningitis, the unconditional
bility P(m | s) directly from statistical observation of patients before the epidemic will have
no idea how to update the value, but the doctor who computes P(m | s) from the other three
values will see that P(m | s) should go up proportionately with P(m). Most important, the
causal information P(s | m) is unaffected by the epidemic, because it simply reﬂects the way
the crucial robustness needed to make probabilistic systems feasible in the real world.
Using Bayes’ rule: Combining evidence
We have seen that Bayes’ rule can be useful for answering probabilistic queries conditioned
on one piece of evidence—for example, the stiff neck. In particular, we have argued that
probabilistic information is often available in the form P(effect | cause). What happens when
we have two or more pieces of evidence? For example, what can a dentist conclude if her
nasty steel probe catches in the aching tooth of a patient? If we know the full joint distribution
We know, however, that such an approach does not scale up to larger numbers of variables.
We can try using Bayes’ rule to reformulate the problem:
P(Cavity | toothache ∧catch)
= α P(toothache ∧catch | Cavity) P(Cavity) .
tion toothache ∧catch for each value of Cavity. That might be feasible for just two evidence
variables, but again it does not scale up. If there are n possible evidence variables (X rays,
diet, oral hygiene, etc.), then there are 2n possible combinations of observed values for which
we would need to know conditional probabilities. We might as well go back to using the
full joint distribution. This is what ﬁrst led researchers away from probability theory towardChapter
Quantifying Uncertainty
approximate methods for evidence combination that, while giving incorrect answers, require
fewer numbers to give any answer at all.
Rather than taking this route, we need to ﬁnd some additional assertions about the
independent, but they are not: if the probe catches in the tooth, then it is likely that the tooth
ever, given the presence or the absence of a cavity. Each is directly caused by the cavity, but
neither has a direct effect on the other: toothache depends on the state of the nerves in the
tooth, whereas the probe’s accuracy depends on the dentist’s skill, to which the toothache is
P(toothache ∧catch | Cavity) = P(toothache | Cavity)P(catch | Cavity) .
This equation expresses the conditional independence of toothache and catch given Cavity.
CONDITIONAL
INDEPENDENCE
P(Cavity | toothache ∧catch)
= α P(toothache | Cavity) P(catch | Cavity) P(Cavity) .
dence separately: the prior probability P(Cavity) for the query variable and the conditional
probability of each effect, given its cause.
The general deﬁnition of conditional independence of two variables X and Y , given a
third variable Z, is
P(X, Y | Z) = P(X | Z)P(Y | Z) .
In the dentist domain, for example, it seems reasonable to assert conditional independence of
the variables Toothache and Catch, given Cavity:
dence only for speciﬁc values of Toothache and Catch. As with absolute independence in
P(X | Y, Z) = P(X | Z)
and
P(Y | X, Z) = P(Y | Z)
sertions allow a decomposition of the full joint distribution into much smaller pieces. It turns
out that the same is true for conditional independence assertions. For example, given the
P(Toothache, Catch, Cavity)
= P(Toothache, Catch | Cavity)P(Cavity)
(product rule)
= P(Toothache | Cavity)P(Catch | Cavity)P(Cavity)
the original large table is decomposed into three smaller tables. The original table has sevenThe Wumpus World Revisited
dent). The smaller tables contain ﬁve independent numbers (for a conditional probability
dent number). Going from seven to ﬁve might not seem like a major triumph, but the point
is that, for n symptoms that are all conditionally independent given Cavity, the size of the
representation grows as O(n) instead of O(2n). That means that conditional independence
monly available than absolute independence assertions. Conceptually, Cavity separates
SEPARATION
Toothache and Catch because it is a direct cause of both of them. The decomposition of
large probabilistic domains into weakly connected subsets through conditional independence
is one of the most important developments in the recent history of AI.
The dentistry example illustrates a commonly occurring pattern in which a single cause
directly inﬂuences a number of effects, all of which are conditionally independent, given the
cause. The full joint distribution can be written as
P(Cause, Effect1, . . . , Effectn) = P(Cause)

i
P(Effecti | Cause) .
Such a probability distribution is called a naive Bayes model—“naive” because it is often
NAIVE BAYES
used (as a simplifying assumption) in cases where the “effect” variables are not actually
conditionally independent given the cause variable. (The naive Bayes model is sometimes
called a Bayesian classiﬁer, a somewhat careless usage that has prompted true Bayesians
to call it the idiot Bayes model.) In practice, naive Bayes systems can work surprisingly
methods for learning naive Bayes distributions from observations.
THE WUMPUS WORLD REVISITED
We can combine of the ideas in this chapter to solve probabilistic reasoning problems in the
arises in the wumpus world because the agent’s sensors give only partial information about
nothing about which square is most likely to be safe, so a logical agent might have to choose
randomly. We will see that a probabilistic agent can do much better than the logical agent.
Our aim is to calculate the probability that each of the three squares contains a pit. (For
this example we ignore the wumpus and the gold.) The relevant properties of the wumpus
variables we need:
• As in the propositional logic case, we want one Boolean variable Pij for each square,
which is true iff square [i, j] actually contains a pit.Chapter
Quantifying Uncertainty
OK
OK
OK
B
B
KNOWN
FRONTIER
QUERY
OTHER
(a)
(b)
no safe place to explore. (b) Division of the squares into Known, Frontier, and Other, for
• We also have Boolean variables Bij that are true iff square [i, j] is breezy; we include
plying the product rule, we have
This decomposition makes it easy to see what the joint probability values should be. The
ﬁrst term is the conditional probability distribution of a breeze conﬁguration, given a pit
second term is the prior probability of a pit conﬁguration. Each square contains a pit with

P(Pi,j) .
absence) in each square that is visited, combined with the fact that each such square contains
contains a pit, given the observations so far?The Wumpus World Revisited


unknown
The full joint probabilities have already been speciﬁed, so we are done—that is, unless we
care about computation.
Surely, one might ask, aren’t the other squares irrelevant?
ables. To use the insight, we manipulate the query formula into a form in which the breezes
are conditioned on all the other variables, and then we apply conditional independence:
= α


unknown
= α


unknown
(by the product rule)
= α


frontier


other
= α


frontier


other
where the ﬁnal step uses conditional independence: b is independent of other given known,
variables, so we can move the summation inward:
= α


frontier


other
can be reordered:
= α


frontier


other


frontier


other
P(other)


frontierChapter
Quantifying Uncertainty
OK
OK
OK
B
B
OK
OK
OK
B
B
OK
OK
OK
B
B
OK
OK
OK
B
B
OK
OK
OK
B
B
(a)
(b)
where the last step folds P(known) into the normalizing constant and uses the fact that

other squares from consideration.
how likely it is.
mulated precisely in probability theory and solved with simple algorithms. To get efﬁcient
solutions, independence and conditional independence relationships can be used to simplify
the summations required. These relationships often correspond to our natural understanding
tations for such relationships as well as algorithms that operate on those representations to
perform probabilistic inference efﬁciently.Summary
SUMMARY
This chapter has suggested probability theory as a suitable foundation for uncertain reasoning
and provided a gentle introduction to its use.
• Uncertainty arises because of both laziness and ignorance. It is inescapable in complex,
nondeterministic, or partially observable environments.
• Probabilities express the agent’s inability to reach a deﬁnite decision regarding the truth
of a sentence. Probabilities summarize the agent’s beliefs relative to the evidence.
• Decision theory combines the agent’s beliefs and desires, deﬁning the best action as the
one that maximizes expected utility.
• Basic probability statements include prior probabilities and conditional probabilities
over simple and complex propositions.
sitions. An agent that violates the axioms must behave irrationally in some cases.
signment of values to random variables. It is usually too large to create or use in its
explicit form, but when it is available it can be used to answer queries simply by adding
up entries for the possible worlds corresponding to the query propositions.
tribution to be factored into smaller joint distributions, greatly reducing its complexity.
Absolute independence seldom occurs in practice.
• Bayes’ rule allows unknown probabilities to be computed from known conditional
probabilities, usually in the causal direction. Applying Bayes’ rule with many pieces of
evidence runs into the same scaling problems as does the full joint distribution.
• Conditional independence brought about by direct causal relationships in the domain
butions. The naive Bayes model assumes the conditional independence of all effect
variables, given a single cause variable, and grows linearly with the number of effects.
thereby improving on the decisions of a purely logical agent. Conditional independence
makes these calculations tractable.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
the Indian mathematician Mahaviracarya described how to arrange a set of bets that can’t lose
(what we now call a Dutch book). In Europe, the ﬁrst signiﬁcant systematic analyses were
By that time, probability had been established as a mathematical discipline due to a series ofChapter
Quantifying Uncertainty
results established in a famous correspondence between Blaise Pascal and Pierre de Fermat
impossible for a Die, with such determin’d force and direction, not to fall on such determin’d
side, only I don’t know the force and direction which makes it fall on such determin’d side,
and therefore I call it Chance, which is nothing but the want of art...”
was the ﬁrst to use the example “take two urns, A and B, the ﬁrst containing four white and
sidered the case of uniform priors; it was Laplace who independently developed the general
presentation that took conditional probability, rather than absolute probability, as primitive.
erty of the world based on symmetry or relative frequency, and the subjective interpretation,
based on degree of belief—the former in his analyses of probabilities in games of chance, the
ever, Pascal did not clearly realize the distinction between these two interpretations. The
Leibniz introduced the “classical” notion of probability as a proportion of enumerated,
tion and the subjective interpretation. The cases can be thought to be equally probable either
because of a natural, physical symmetry between them, or simply because we do not have
any knowledge that would lead us to consider one more probable than another. The use of
this latter, subjective consideration to justify assigning equal probabilities is known as the
principle of indifference. The principle is often attributed to Laplace, but he never isolated
PRINCIPLE OF
INDIFFERENCE
the principle explicitly. George Boole and John Venn both referred to it as the principle of
PRINCIPLE OF
INSUFFICIENT
REASON
The debate between objectivists and subjectivists became sharper in the 20th century.
“propensity” interpretation traces relative frequencies to an underlying physical symmetry.
ties and to behavior—speciﬁcally, to the willingness to place bets. Rudolf Carnap, following
Leibniz and Laplace, offered a different kind of subjective interpretation of probability—
not as any actual individual’s degree of belief, but as the degree of belief that an idealized
individual should have in a particular proposition a, given a particular body of evidence e.Bibliographical and Historical Notes
Carnap attempted to go further than Leibniz or Laplace by making this notion of degree of
conﬁrmation mathematically precise, as a logical relation between a and e. The study of this
CONFIRMATION
INDUCTIVE LOGIC
ial arguments that some fundamental difﬁculties would prevent a strict extension to languages
capable of expressing arithmetic.
of assumptions is equivalent to probability theory. This gave renewed conﬁdence to those
who already favored probability, but others were not convinced, pointing to the assumptions
(primarily that belief must be represented by a single number, and thus the belief in ¬p must
The question of reference classes is closely tied to the attempt to ﬁnd an inductive logic.
The approach of choosing the “most speciﬁc” reference class of sufﬁcient size was formally
fallacies that arise with Reichenbach’s rule, but such approaches remain somewhat ad hoc.
with nonmonotonic logic.
Bayesian probabilistic reasoning has been used in AI since the 1960s, especially in
medical diagnosis. It was used not only to make a diagnosis from available evidence, but also
outperformed human experts in the diagnosis of acute abdominal illnesses (de Dombal et al.,
number of problems, however. Because they lacked any theoretical model of the conditions
they were diagnosing, they were vulnerable to unrepresentative data occurring in situations
quisition, storage, and processing of enormous tables of probabilistic data. Because of these
difﬁculties, probabilistic methods for coping with uncertainty fell out of favor in AI from theChapter
Quantifying Uncertainty
dence assumptions are clearly violated.
There are many good introductory textbooks on probability theory, including those by
tion to probability theory from the standpoint of a propensity interpretation based on physical
EXERCISES
Using the axioms of probability, prove that any probability distribution on a discrete
For each of the following statements, either prove it is true or give a counterexample.
a. If P(a | b, c) = P(b | a, c), then P(a | c) = P(b | c)
b. If P(a | b, c) = P(a), then P(b | c) = P(b)
c. If P(a | b) = P(a), then P(a | b, c) = P(a | c)
why it is rational to have this probability, even though the table shows one case that is a loss
each of the four cases, especially the case that is a loss?)
assignments to all random variables. We will work with propositions that correspond to
exactly one possible world because they pin down the assignments of all the variables. In
probability theory, such propositions are called atomic events. For example, with Boolean
ATOMIC EVENT
variables X1, X2, X3, the proposition x1 ∧¬x2 ∧¬x3 ﬁxes the assignment of the variables;
in the language of propositional logic, we would say it has exactly one model.
a. Prove, for the case of n Boolean variables, that any two distinct atomic events are
mutually exclusive; that is, their conjunction is equivalent to false.
b. Prove that the disjunction of all possible atomic events is logically equivalent to true.
c. Prove that any proposition is logically equivalent to the disjunction of the atomic events
that entail its truth.Chapter
Quantifying Uncertainty
if at most one bit in the entire message (including the parity bit) has been corrupted. Suppose
positive rate. The two tests use independent methods of identifying the virus. The virus is
and that test comes back positive for carrying the virus. Which test returning positive is more
indicative of someone really carrying the virus? Justify your answer mathematically.
Suppose you are given a coin that lands heads with probability x and tails with
other given that you know the value of x? Are the outcomes of successive ﬂips of the coin
independent of each other if you do not know the value of x? Justify your answer.
After your yearly checkup, the doctor has bad news and good news. The bad news
testing negative when you don’t have the disease). The good news is that this is a rare disease,
What are the chances that you actually have the disease?
It is quite often useful to consider the effect of some speciﬁc propositions in the
context of some general background evidence that remains ﬁxed, rather than in the complete
absence of information. The following questions ask you to prove more general versions of
the product rule and Bayes’ rule, with respect to some background evidence e:
a. Prove the conditionalized version of the general product rule:
P(X, Y | e) = P(X | Y, e)P(Y | e) .
Show that the statement of conditional independence
P(X, Y | Z) = P(X | Z)P(Y | Z)
is equivalent to each of the statements
P(X | Y, Z) = P(X | Z)
and
P(B | X, Z) = P(Y | Z) .
these coins are normal, with heads on one side and tails on the other, whereas one coin is a
fake, with heads on both sides.
a. Suppose you reach into the bag, pick out a coin at random, ﬂip it, and get a head. What
is the (conditional) probability that the coin you chose is the fake coin?PROBABILISTIC
REASONING
In which we explain how to build network models to reason under uncertainty
according to the laws of probability theory.
ships explicitly in the form of Bayesian networks. We deﬁne the syntax and semantics of
ral and efﬁcient way. We then show how probabilistic inference, although computationally
intractable in the worst case, can be done efﬁciently in many practical situations. We also
describe a variety of approximate inference algorithms that are often applicable when exact
inference is infeasible. We explore ways in which probability theory can be applied to worlds
Finally, we survey alternative approaches to uncertain reasoning.
REPRESENTING KNOWLEDGE IN AN UNCERTAIN DOMAIN
the domain, but can become intractably large as the number of variables grows. Furthermore,
specifying probabilities for possible worlds one by one is unnatural and tedious.
ables can greatly reduce the number of probabilities that need to be speciﬁed in order to deﬁne
the full joint distribution. This section introduces a data structure called a Bayesian network1
BAYESIAN NETWORK
to represent the dependencies among variables. Bayesian networks can represent essentially
any full joint probability distribution and in many cases can do so very concisely.
work, causal network, and knowledge map. In statistics, the term graphical model refers to a somewhat
broader class that includes Bayesian networks. An extension of Bayesian networks called a decision network orRepresenting Knowledge in an Uncertain Domain
tive probability information. The full speciﬁcation is as follows:
X to node Y , X is said to be a parent of Y. The graph has no directed cycles (and hence
is a directed acyclic graph, or DAG.
tiﬁes the effect of the parents on the node.
dence relationships that hold in the domain, in a way that will be made precise shortly. The
intuitive meaning of an arrow is typically that X has a direct inﬂuence on Y, which suggests
that causes should be parents of effects. It is usually easy for a domain expert to decide what
abilities themselves. Once the topology of the Bayesian network is laid out, we need only
specify a conditional probability distribution for each variable, given its parents. We will
see that the combination of the topology and the conditional distributions sufﬁces to specify
(implicitly) the full joint distribution for all the variables.
ables; furthermore, we argued that Toothache and Catch are conditionally independent,
given Cavity. These relationships are represented by the Bayesian network structure shown
Cavity, is indicated by the absence of a link between Toothache and Catch. Intuitively, the
network represents the fact that Cavity is a direct cause of Toothache and Catch, whereas
no direct causal relationship exists between Toothache and Catch.
Now consider the following example, which is just a little more complex. You have
a new burglar alarm installed at home. It is fairly reliable at detecting a burglary, but also
responds on occasion to minor earthquakes. (This example is due to Judea Pearl, a resident
of Los Angeles—hence the acute interest in earthquakes.) You also have two neighbors, John
and Mary, who have promised to call you at work when they hear the alarm. John nearly
always calls when he hears the alarm, but sometimes confuses the telephone ringing with
Weather
Cavity
Toothache
Catch
A simple Bayesian network in which Weather is independent of the other
three variables and Toothache and Catch are conditionally independent, given Cavity.Chapter
Probabilistic Reasoning
P(B)
Alarm
Earthquake
MaryCalls
JohnCalls
Burglary
A
P(J)
t
f
B
t
t
f
f
E
t
f
t
f
P(A)
P(E)
A P(M)
t
f
A typical Bayesian network, showing both the topology and the conditional
probability tables (CPTs). In the CPTs, the letters B, E, A, J, and M stand for Burglary,
Earthquake, Alarm, JohnCalls, and MaryCalls, respectively.
the alarm and calls then, too. Mary, on the other hand, likes rather loud music and often
misses the alarm altogether. Given the evidence of who has or has not called, we would like
to estimate the probability of a burglary.
shows that burglary and earthquakes directly affect the probability of the alarm’s going off,
but whether John and Mary call depends only on the alarm. The network thus represents
quakes, and they do not confer before calling.
table, or CPT. (This form of table can be used for discrete variables; other representations,
CONDITIONAL
PROBABILITY TABLE
in a CPT contains the conditional probability of each node value for a conditioning case.
CONDITIONING CASE
contains 2k independently speciﬁable probabilities. A node with no parents has only one row,
representing the prior probabilities of each possible value of the variable.
Notice that the network does not have nodes corresponding to Mary’s currently listening
to loud music or to the telephone ringing and confusing John. These factors are summarized
in the uncertainty associated with the links from Alarm to JohnCalls and MaryCalls. This
shows both laziness and ignorance in operation: it would be a lot of work to ﬁnd out why those
factors would be more or less likely in any particular case, and we have no reasonable way to
obtain the relevant information anyway. The probabilities actually summarize a potentiallyThe Semantics of Bayesian Networks
inﬁnite set of circumstances in which the alarm might fail to go off (high humidity, power
failure, dead battery, cut wires, a dead mouse stuck inside the bell, etc.) or John or Mary
might fail to call and report it (out to lunch, on vacation, temporarily deaf, passing helicopter,
etc.). In this way, a small agent can cope with a very large world, at least approximately. The
degree of approximation can be improved if we introduce additional relevant information.
THE SEMANTICS OF BAYESIAN NETWORKS
The previous section described what a network is, but not what it means. There are two
ways in which one can understand the semantics of Bayesian networks. The ﬁrst is to see
the network as a representation of the joint probability distribution. The second is to view
it as an encoding of a collection of conditional independence statements. The two views are
equivalent, but the ﬁrst turns out to be helpful in understanding how to construct networks,
whereas the second is helpful in designing inference procedures.
Representing the full joint distribution
Viewed as a piece of “syntax,” a Bayesian network is a directed acyclic graph with some
numeric parameters attached to each node. One way to deﬁne what the network means—its
semantics—is to deﬁne the way in which it represents a speciﬁc joint distribution over all the
rameters associated with each node. We said that those parameters correspond to conditional
probabilities P(Xi | Parents(Xi)); this is a true statement, but until we assign semantics to
A generic entry in the joint distribution is the probability of a conjunction of particular
assignments to each variable, such as P(X1 = x1 ∧. . . ∧Xn = xn). We use the notation
P(x1, . . . , xn) as an abbreviation for this. The value of this entry is given by the formula
P(x1, . . . , xn) =
n

θ(xi | parents(Xi)) ,
where parents(Xi) denotes the values of Parents(Xi) that appear in x1, . . . , xn. Thus,
each entry in the joint distribution is represented by the product of the appropriate elements
of the conditional probability tables (CPTs) in the Bayesian network.
exactly the conditional probabilities P(Xi | Parents(Xi)) implied by the joint distribution
P(x1, . . . , xn) =
n

P(xi | parents(Xi)) .
To illustrate this, we can calculate the probability that the alarm has sounded, but neither
a burglary nor an earthquake has occurred, and both John and Mary call. We multiply entriesChapter
Probabilistic Reasoning
P(j, m, a, ¬b, ¬e) = P(j | a)P(m | a)P(a | ¬b ∧¬e)P(¬b)P(¬e)
the domain. If a Bayesian network is a representation of the joint distribution, then it too can
how to do this, but also describes methods that are much more efﬁcient.
A method for constructing Bayesian networks
how to construct a Bayesian network in such a way that the resulting joint distribution is a
conditional independence relationships that can be used to guide the knowledge engineer in
constructing the topology of the network. First, we rewrite the entries in the joint distribution
Then we repeat the process, reducing each conjunctive probability to a conditional probability
and a smaller conjunction. We end up with one big product:
=
n

This identity is called the chain rule. It holds for any set of random variables. Comparing it
CHAIN RULE
general assertion that, for every variable Xi in the network,
the nodes in a way that is consistent with the partial order implicit in the graph structure.
the domain only if each node is conditionally independent of its other predecessors in the
node ordering, given its parents. We can satisfy this condition with this methodology:
order them, {X1, . . . , Xn}. Any order will work, but the resulting network will be more
compact if the variables are ordered such that causes precede effects.
• For each parent insert a link from the parent to Xi.
• CPTs: Write down the conditional probability table, P(Xi|Parents(Xi)).The Semantics of Bayesian Networks
except for the choice of parents for MaryCalls. MaryCalls is certainly inﬂuenced by whether
there is a Burglary or an Earthquake, but not directly inﬂuenced. Intuitively, our knowledge
of the domain tells us that these events inﬂuence Mary’s calling behavior only through their
effect on the alarm. Also, given the state of the alarm, whether John calls has no inﬂuence on
Mary’s calling. Formally speaking, we believe that the following conditional independence
statement holds:
P(MaryCalls | JohnCalls, Alarm, Earthquake, Burglary) = P(MaryCalls | Alarm) .
Thus, Alarm will be the only parent node for MaryCalls.
tees that the network is acyclic. Another important property of Bayesian networks is that they
contain no redundant probability values. If there is no redundancy, then there is no chance
for inconsistency: it is impossible for the knowledge engineer or domain expert to create a
Bayesian network that violates the axioms of probability.
Compactness and node ordering
work can often be far more compact than the full joint distribution. This property is what
works is an example of a general property of locally structured (also called sparse) systems.
LOCALLY
STRUCTURED
SPARSE
In a locally structured system, each subcomponent interacts directly with only a bounded
number of other components, regardless of the total number of components. Local structure
is usually associated with linear rather than exponential growth in complexity. In the case of
Bayesian networks, it is reasonable to suppose that in most domains each random variable
is directly inﬂuenced by at most k others, for some constant k. If we assume n Boolean
variables for simplicity, then the amount of information needed to specify each conditional
probability table will be at most 2k numbers, and the complete network can be speciﬁed by
n2k numbers. In contrast, the joint distribution contains 2n numbers. To make this concrete,
There are domains in which each variable can be inﬂuenced directly by all the others,
quires the same amount of information as specifying the joint distribution. In some domains,
there will be slight dependencies that should strictly be included by adding a new link. But
if these dependencies are tenuous, then it may not be worth the additional complexity in the
work on the grounds that if there is an earthquake, then John and Mary would not call even
if they heard the alarm, because they assume that the earthquake is the cause. Whether to
add the link from Earthquake to JohnCalls and MaryCalls (and thus enlarge the tables)
depends on comparing the importance of getting more accurate probabilities with the cost of
specifying the extra information.Chapter
Probabilistic Reasoning
JohnCalls
MaryCalls
Alarm
Burglary
Earthquake
MaryCalls
Alarm
Earthquake
Burglary
JohnCalls
(a)
(b)
Network structure depends on order of introduction. In each network, we
Even in a locally structured domain, we will get a compact Bayesian network only if
der? Consider the burglary example again. Suppose we decide to add the nodes in the order
MaryCalls, JohnCalls, Alarm, Burglary, Earthquake. We then get the somewhat more
• Adding MaryCalls: No parents.
• Adding JohnCalls: If Mary calls, that probably means the alarm has gone off, which
of course would make it more likely that John calls.
Therefore, JohnCalls needs
MaryCalls as a parent.
• Adding Alarm: Clearly, if both call, it is more likely that the alarm has gone off than if
just one or neither calls, so we need both MaryCalls and JohnCalls as parents.
• Adding Burglary: If we know the alarm state, then the call from John or Mary might
give us information about our phone ringing or Mary’s music, but not about burglary:
P(Burglary | Alarm, JohnCalls, MaryCalls) = P(Burglary | Alarm) .
Hence we need just Alarm as parent.
quake. (The alarm is an earthquake detector of sorts.) But if we know that there has
been a burglary, then that explains the alarm, and the probability of an earthquake would
be only slightly above normal. Hence, we need both Alarm and Burglary as parents.
quires three more probabilities to be speciﬁed. What’s worse, some of the links representThe Semantics of Bayesian Networks
sessing the probability of Earthquake, given Burglary and Alarm. This phenomenon is
links from symptoms to causes (as from MaryCalls to Alarm or Alarm to Burglary), we
end up having to specify additional dependencies between otherwise independent causes (and
often between separately occurring symptoms as well). If we stick to a causal model, we end
up having to specify fewer numbers, and the numbers will often be easier to come up with. In
that expert physicians prefer to give probability judgments for causal rules rather than for
diagnostic ones.
same number as the full joint distribution. It is important to realize, however, that any of the
three networks can represent exactly the same joint distribution. The last two versions simply
fail to represent all the conditional independence relationships and hence end up specifying a
lot of unnecessary numbers instead.
Conditional independence relations in Bayesian networks
method for constructing Bayesian networks, we were led to the consequence that a node is
conditionally independent of its other predecessors, given its parents. It turns out that we
can also go in the other direction. We can start from a “topological” semantics that speciﬁes
the conditional independence relationships encoded by the graph structure, and from this we
DESCENDANT
as speciﬁcations of conditional probabilities P(Xi | Parents(Xi)), the full joint distribution
the “topological” semantics are equivalent.
Another important independence property is implied by the topological semantics: a
node is conditionally independent of all other nodes in the network, given its parents, children,
MARKOV BLANKET
this.) For example, Burglary is independent of JohnCalls and MaryCalls, given Alarm and
conditionally independent of another set Y, given a third set Z. The criterion is rather complicated and is notChapter
Probabilistic Reasoning
. . .
. . .
U1
X
Um
Yn
Znj
Y1
Z1j
. . .
. . .
U1
Um
Yn
Znj
Y1
Z1j
X
(a)
(b)
Zijs) given its parents (the Uis shown in the gray area). (b) A node X is conditionally
independent of all other nodes in the network given its Markov blanket (the gray area).
EFFICIENT REPRESENTATION OF CONDITIONAL DISTRIBUTIONS
Even if the maximum number of parents k is smallish, ﬁlling in the CPT for a node requires
up to O(2k) numbers and perhaps a great deal of experience with all the possible conditioning
the child is completely arbitrary. Usually, such relationships are describable by a canonical
distribution that ﬁts some standard pattern. In such cases, the complete table can be speciﬁed
CANONICAL
DISTRIBUTION
by naming the pattern and perhaps supplying a few parameters—much easier than supplying
an exponential number of parameters.
The simplest example is provided by deterministic nodes. A deterministic node has
DETERMINISTIC
NODES
its value speciﬁed exactly by the values of its parents, with no uncertainty. The relationship
can be a logical one: for example, the relationship between the parent nodes Canadian, US,
Mexican and the child node NorthAmerican is simply that the child is the disjunction of
the parents. The relationship can also be numerical: for example, if the parent nodes are
the prices of a particular model of car at several dealers and the child node is the price that
a bargain hunter ends up paying, then the child node is the minimum of the parent values;
or if the parent nodes are a lake’s inﬂows (rivers, runoff, precipitation) and outﬂows (rivers,
evaporation, seepage) and the child is the change in the water level of the lake, then the value
of the child is the sum of the inﬂow parents minus the sum of the outﬂow parents.
ical OR. In propositional logic, we might say that Fever is true if and only if Cold, Flu, or
ent to cause the child to be true—the causal relationship between parent and child may beEfﬁcient Representation of Conditional Distributions
inhibited, and so a patient could have a cold, but not exhibit a fever. The model makes two
assumptions. First, it assumes that all the possible causes are listed. (If some are missing,
LEAK NODE
assumes that inhibition of each parent is independent of inhibition of any other parents: for
example, whatever inhibits Malaria from causing a fever is independent of whatever inhibits
Flu from causing a fever. Given these assumptions, Fever is false if and only if all its true
parents are inhibited, and the probability of this is the product of the inhibition probabilities
q for each parent. Let us suppose these individual inhibition probabilities are as follows:
general rule is that

{j:Xj = true}
qj ,
where the product is taken over the parents that are set to true for that row of the CPT. The
following table illustrates this calculation:
Cold
Flu
Malaria P(Fever) P(¬Fever)
F
F
F
F
F
T
F
T
F
F
T
T
T
F
F
T
F
T
T
T
F
T
T
T
scribed using O(k) parameters instead of O(2k) for the full conditional probability table.
Bayesian nets with continuous variables
uous. By deﬁnition, continuous variables have an inﬁnite number of possible values, so it is
impossible to specify conditional probabilities explicitly for each value. One possible way to
handle continuous variables is to avoid them by using discretization—that is, dividing up the
DISCRETIZATIONChapter
Probabilistic Reasoning
Harvest
Subsidy
Buys
Cost
A simple network with discrete variables (Subsidy and Buys) and continuous
variables (Harvest and Cost).
possible values into a ﬁxed set of intervals. For example, temperatures could be divided into
(<0oC), (0oC−100oC), and (>100oC). Discretization is sometimes an adequate solution,
mon solution is to deﬁne standard families of probability density functions (see Appendix A)
that are speciﬁed by a ﬁnite number of parameters. For example, a Gaussian (or normal)
PARAMETER
solution—sometimes called a nonparametric representation—is to deﬁne the conditional
NONPARAMETRIC
distribution implicitly with a collection of instances, each containing speciﬁc values of the
A network with both discrete and continuous variables is called a hybrid Bayesian
network. To specify a hybrid network, we have to specify two new kinds of distributions:
HYBRID BAYESIAN
NETWORK
the conditional distribution for a continuous variable given discrete or continuous parents;
and the conditional distribution for a discrete variable given continuous parents. Consider the
which depends in turn on the size of the harvest and whether the government’s subsidy scheme
is operating. The variable Cost is continuous and has continuous and discrete parents; the
variable Buys is discrete and has a continuous parent.
For the Cost variable, we need to specify P(Cost | Harvest, Subsidy). The discrete
parent is handled by enumeration—that is, by specifying both P(Cost | Harvest, subsidy)
and P(Cost | Harvest, ¬subsidy). To handle Harvest, we specify how the distribution over
the cost c depends on the continuous value h of Harvest. In other words, we specify the
parameters of the cost distribution as a function of h. The most common choice is the linear
LINEAR GAUSSIAN
linearly with the value of the parent and whose standard deviation σ is ﬁxed. We need two
distributions, one for subsidy and one for ¬subsidy, with different parameters:
P(c | h, subsidy) = N(ath + bt, σ2
t )(c) =
σt
√
σt
P(c | h, ¬subsidy) = N(afh + bf, σ2
f)(c) =
σf
√
2π e
„
σf
.
For this example, then, the conditional distribution for Cost is speciﬁed by naming the linearEfﬁcient Representation of Conditional Distributions
Cost c
Harvest h
P(c | h, subsidy)
Cost c
Harvest h
P(c | h, ¬subsidy)
Cost c
Harvest h
P(c | h)
(a)
(b)
(c)
The graphs in (a) and (b) show the probability distribution over Cost as a
function of Harvest size, with Subsidy true and false, respectively. Graph (c) shows the
distribution P(Cost | Harvest), obtained by summing over the two subsidy cases.
and (b) show these two relationships. Notice that in each case the slope is negative, because
cost decreases as supply increases. (Of course, the assumption of linearity implies that the
cost becomes negative at some point; the linear model is reasonable only if the harvest size is
that even with very simple models, quite interesting distributions can be represented.
The linear Gaussian conditional distribution has some special properties. A network
When discrete variables are added as parents (not as children) of continuous variables, the
network deﬁnes a conditional Gaussian, or CG, distribution: given any assignment to the
CONDITIONAL
GAUSSIAN
discrete variables, the distribution over the continuous variables is a multivariate Gaussian.
customer will buy if the cost is low and will not buy if it is high and that the probability of
tion is like a “soft” threshold function. One way to make soft thresholds is to use the integral
of the standard normal distribution:
Φ(x) =
 x
−∞
Then the probability of Buys given Cost might be
P(buys | Cost = c) = Φ((−c + μ)/σ) ,Chapter
Probabilistic Reasoning
P(c)
Cost c
P(buys | c)
Cost c
Logit
Probit
(a)
(b)
(a) A normal (Gaussian) distribution for the cost threshold, centered on
PROBIT
DISTRIBUTION
old, but that the precise location of the threshold is subject to random Gaussian noise.
LOGIT DISTRIBUTION
LOGISTIC FUNCTION
P(buys | Cost = c) =
σ
) .
Both probit and logit can be generalized to handle multiple continuous parents by taking a
linear combination of the parent values.
EXACT INFERENCE IN BAYESIAN NETWORKS
The basic task for any probabilistic inference system is to compute the posterior probability
EVENT
ment of values to a set of evidence variables. To simplify the presentation, we will consider
denotes the set of evidence variables E1, . . . , Em, and e is a particular observed event; Y will
denotes the nonevidence, nonquery variables Y1, . . . , Yl (called the hidden variables). Thus,
HIDDEN VARIABLE
probability distribution P(X | e).Exact Inference in Bayesian Networks
In the burglary network, we might observe the event in which JohnCalls = true and
MaryCalls = true. We could then ask for, say, the probability that a burglary has occurred:
In this section we discuss exact algorithms for computing posterior probabilities and will
Inference by enumeration
from the full joint distribution. More speciﬁcally, a query P(X | e) can be answered using
P(X | e) = α P(X, e) = α


y
P(X, e, y) .
Now, a Bayesian network gives a complete representation of the full joint distribution. More
bution can be written as products of conditional probabilities from the network. Therefore, a
tional probabilities from the network.
Consider the query P(Burglary | JohnCalls = true, MaryCalls = true). The hidden
letters for the variables to shorten the expressions, we have4
P(B | j, m) = α P(B, j, m) = α


e


a
P(B, j, m, e, a, ) .
of CPT entries. For simplicity, we do this just for Burglary = true:
P(b | j, m) = α


e


a
P(b)P(e)P(a | b, e)P(j | a)P(m | a) .
To compute this expression, we have to add four terms, each computed by multiplying ﬁve
numbers. In the worst case, where we have to sum out almost all the variables, the complexity
of the algorithm for a network with n Boolean variables is O(n2n).
An improvement can be obtained from the following simple observations: the P(b)
term is a constant and can be moved outside the summations over a and e, and the P(e) term
can be moved outside the summation over a. Hence, we have
P(b | j, m) = α P(b)


e
P(e)


a
P(a | b, e)P(j | a)P(m | a) .
This expression can be evaluated by looping through the variables in order, multiplying CPT
entries as we go. For each summation, we also need to loop over the variable’s possible
e P(a, e) means to sum P(A = a, E = e) for all possible values of e. When E is
Boolean, there is an ambiguity in that P(e) is used to mean both P(E = true) and P(E = e), but it should be
clear from context which is intended; in particular, in the context of a sum the latter is intended.Chapter
Probabilistic Reasoning
fortunately, its time complexity for a network with n Boolean variables is always O(2n)—
better than the O(n 2n) for the simple approach described earlier, but still rather grim.
ated by the algorithm. The products P(j | a)P(m | a) and P(j | ¬a)P(m | ¬a) are computed
twice, once for each value of e. The next section describes a general method that avoids such
wasted computations.
The variable elimination algorithm
sions of this approach; we present the variable elimination algorithm, which is the simplest.
VARIABLE
ELIMINATION
each variable are done only for those portions of the expression that depend on the variable.
Let us illustrate this process for the burglary network. We evaluate the expression
P(B | j, m) = α P(B)
 

 

f1(B)


e
P(e)




f2(E)


a
P(a | B, e)






f3(A,B,E)
P(j | a)
 

 

f4(A)
P(m | a)






f5(A)
.
Notice that we have annotated each part of the expression with the name of the corresponding
factor; each factor is a matrix indexed by the values of its argument variables. For example,
FACTOR
the factors f4(A) and f5(A) corresponding to P(j | a) and P(m | a) depend just on A because
f4(A) =

 P(j | a)
P(j | ¬a)

=


f5(A) =

 P(m | a)
P(m | ¬a)

=


.
factors, the query expression is written as
P(B | j, m) = α f1(B) ×


e
f2(E) ×


a
f3(A, B, E) × f4(A) × f5(A)Exact Inference in Bayesian Networks
P(j|a)
P(m|a)
P(m|¬a)
P( j|¬a)
P( j|a)
P(m|a)
P(m|¬a)
P( j|¬a)
P(b)
P(e)
P(¬e)
P(a|b,e)
P(¬a|b,¬e)
P(¬a|b,e)
P(a|b,¬e)
proceeds top down, multiplying values along each path and summing at the “+” nodes. Notice
the repetition of the paths for j and m.
inputs: X , the query variable
e, observed values for variables E
/* Y = hidden variables */
Q(X ) ←a distribution over X , initially empty
for each value xi of X do
where exi is e extended with X = xi
return NORMALIZE(Q(X))
Y ←FIRST(vars)
if Y has value y in e
else return 
where ey is e extended with Y = y
The enumeration algorithm for answering queries on Bayesian networks.Chapter
Probabilistic Reasoning
uct operation, to be described shortly.
POINTWISE
PRODUCT
The process of evaluation is a process of summing out variables (right to left) from
pointwise products of factors to produce new factors, eventually yielding a factor that is the
solution, i.e., the posterior distribution over the query variable. The steps are as follows:
f6(B, E) whose indices range over just B and E:
f6(B, E) =


a
f3(A, B, E) × f4(A) × f5(A)
= (f3(a, B, E) × f4(a) × f5(a)) + (f3(¬a, B, E) × f4(¬a) × f5(¬a)) .
Now we are left with the expression
P(B | j, m) = α f1(B) ×


e
f2(E) × f6(B, E) .
• Next, we sum out E from the product of f2 and f6:
f7(B) =


e
f2(E) × f6(B, E)
= f2(e) × f6(B, e) + f2(¬e) × f6(B, ¬e) .
This leaves the expression
P(B | j, m) = α f1(B) × f7(B)
which can be evaluated by taking the pointwise product and normalizing the result.
wise product of a pair of factors, and summing out a variable from a product of factors. The
next section describes each of these operations.
Operations on factors
The pointwise product of two factors f1 and f2 yields a new factor f whose variables are
the union of the variables in f1 and f2 and whose elements are given by the product of the
corresponding elements in the two factors. Suppose the two factors have variables Y1, . . . , Yk
in common. Then we have
f(X1 . . . Xj, Y1 . . . Yk, Z1 . . . Zl) = f1(X1 . . . Xj, Y1 . . . Yk) f2(Y1 . . . Yk, Z, . . . Zl).
For example, given two factors f1(A, B) and
variables than any of the factors being multiplied and that the size of a factor is exponential in
the number of variables. This is where both space and time complexity arise in the variable
elimination algorithm.Exact Inference in Bayesian Networks
A
B
f1(A, B)
B
C
f2(B, C)
A
B
C
f3(A, B, C)
T
T
T
T
T
T
T
T
F
T
F
T
T
F
F
T
F
T
T
F
T
F
F
F
F
T
F
F
F
T
T
F
T
F
F
F
T
F
F
F
Illustrating pointwise multiplication: f1(A, B) × f2(B, C) = f3(A, B, C).
Summing out a variable from a product of factors is done by adding up the submatrices
formed by ﬁxing the variable to each of its values in turn. For example, to sum out A from
f3(A, B, C), we write
f(B, C) =


a
f3(A, B, C) = f3(a, B, C) + f3(¬a, B, C)
=


+


=


.
The only trick is to notice that any factor that does not depend on the variable to be summed
out can be moved outside the summation. For example, if we were to sum out E ﬁrst in the
burglary network, the relevant part of the expression would be


e
f2(E) × f3(A, B, E) × f4(A) × f5(A) = f4(A) × f5(A) ×


e
f2(E) × f3(A, B, E) .
Now the pointwise product inside the summation is computed, and the variable is summed
out of the resulting matrix.
Notice that matrices are not multiplied until we need to sum out a variable from the
accumulated product. At that point, we multiply just those matrices that include the variable
to be summed out. Given functions for pointwise product and summing out, the variable
Variable ordering and variable relevance
for the variables. Every choice of ordering yields a valid algorithm, but different orderings
cause different intermediate factors to be generated during the calculation. For example, in
the calculation shown previously, we eliminated A before E; if we do it the other way, the
calculation becomes
P(B | j, m) = α f1(B) ×


a
f4(A) × f5(A) ×


e
f2(E) × f3(A, B, E) ,
during which a new factor f6(A, B) will be generated.
In general, the time and space requirements of variable elimination are dominated by
the size of the largest factor constructed during the operation of the algorithm. This in turnChapter
Probabilistic Reasoning
inputs: X , the query variable
e, observed values for variables E
bn, a Bayesian network specifying joint distribution P(X1, . . . , Xn)
factors ←[ ]
for each var in ORDER(bn.VARS) do
The variable elimination algorithm for inference in Bayesian networks.
is determined by the order of elimination of variables and by the structure of the network.
It turns out to be intractable to determine the optimal ordering, but several good heuristics
are available. One fairly effective method is a greedy one: eliminate whichever variable
minimizes the size of the next factor to be constructed.
Let us consider one more query: P(JohnCalls | Burglary = true). As usual, the ﬁrst
step is to write out the nested summation:
P(J | b) = α P(b)


e
P(e)


a
P(a | b, e)P(J | a)


m
P(m | a) .
Evaluating this expression from right to left, we notice something interesting: 
m P(m | a)
able M is irrelevant to this query. Another way of saying this is that the result of the query
P(JohnCalls | Burglary = true) is unchanged if we remove MaryCalls from the network
altogether. In general, we can remove any leaf node that is not a query variable or an evidence
vant. Continuing this process, we eventually ﬁnd that every variable that is not an ancestor
of a query variable or evidence variable is irrelevant to the query. A variable elimination
algorithm can therefore remove all these variables before evaluating the query.
The complexity of exact inference
The complexity of exact inference in Bayesian networks depends strongly on the structure of
there is at most one undirected path between any two nodes in the network. These are called
singly connected networks or polytrees, and they have a particularly nice property: The time
SINGLY CONNECTED
POLYTREE
and space complexity of exact inference in polytrees is linear in the size of the network. Here,
the size is deﬁned as the number of CPT entries; if the number of parents of each node is
bounded by a constant, then the complexity will also be linear in the number of nodes.
MULTIPLY
CONNECTED
can have exponential time and space complexity in the worst case, even when the number
of parents per node is bounded. This is not surprising when one considers that because itExact Inference in Bayesian Networks
C
P(R)
t
f
C
P(S)
t
f
S
R
t
t
t
f
f
t
f
f
Cloudy
Rain
Sprinkler
 Wet
Grass
P(W)
t
f
P(W)
t t
t f
f t
f f
Cloudy
 Wet
Grass
C
t t
t f
f t
f f
(a)
(b)
(a) A multiply connected network with conditional probability tables. (b) A
clustered equivalent of the multiply connected network.
includes inference in propositional logic as a special case, inference in Bayesian networks is
puting the number of satisfying assignments for a propositional logic formula. This means
There is a close connection between the complexity of Bayesian network inference and
the difﬁculty of solving a discrete CSP is related to how “treelike” its constraint graph is.
Measures such as tree width, which bound the complexity of solving a CSP, can also be
applied directly to Bayesian networks. Moreover, the variable elimination algorithm can be
generalized to solve CSPs as well as Bayesian networks.
Clustering algorithms
The variable elimination algorithm is simple and efﬁcient for answering individual queries. If
we want to compute posterior probabilities for all the variables in a network, however, it can
be less efﬁcient. For example, in a polytree network, one would need to issue O(n) queries
costing O(n) each, for a total of O(n2) time. Using clustering algorithms (also known as
CLUSTERING
join tree algorithms), the time can be reduced to O(n). For this reason, these algorithms are
JOIN TREE
widely used in commercial Bayesian network tools.
ter nodes in such a way that the resulting network is a polytree. For example, the multiply
possible values: tt, tf, ft, and ff. The meganode has only one parent, the Boolean variable
Cloudy, so there are two conditioning cases. Although this example doesn’t show it, the
process of clustering often produces meganodes that share some variables.Chapter
Probabilistic Reasoning
because ordinary inference methods cannot handle meganodes that share variables with each
ables that they have in common. With careful bookkeeping, this algorithm is able to compute
posterior probabilities for all the nonevidence nodes in the network in time linear in the size
network requires exponential time and space with variable elimination, then the CPTs in the
clustered network will necessarily be exponentially large.
APPROXIMATE INFERENCE IN BAYESIAN NETWORKS
tial to consider approximate inference methods. This section describes randomized sampling
algorithms, also called Monte Carlo algorithms, that provide approximate answers whose
MONTE CARLO
accuracy depends on the number of samples generated. Monte Carlo algorithms, of which
timate quantities that are difﬁcult to calculate exactly. In this section, we are interested in
sampling applied to the computation of posterior probabilities. We describe two families of
algorithms: direct sampling and Markov chain sampling. Two other approaches—variational
methods and loopy propagation—are mentioned in the notes at the end of the chapter.
Direct sampling methods
The primitive element in any sampling algorithm is the generation of samples from a known
probability distribution. For example, an unbiased coin can be thought of as a random variable
The simplest kind of random sampling process for Bayesian networks generates events
from a network that has no evidence associated with it. The idea is to sample each variable
in turn, in topological order. The probability distribution from which the value is sampled is
conditioned on the values already assigned to the variable’s parents. This algorithm is shown
an ordering [Cloudy, Sprinkler, Rain, WetGrass]:Approximate Inference in Bayesian Networks
inputs: bn, a Bayesian network specifying joint distribution P(X1, . . . , Xn)
x ←an event with n elements
foreach variable Xi in X1, . . . , Xn do
x[i] ←a random sample from P(Xi | parents(Xi))
return x
A sampling algorithm that generates events from a Bayesian network. Each
variable is sampled according to the conditional distribution given the values already sampled
for the variable’s parents.
speciﬁed by the network. First, let SPS(x1, . . . , xn) be the probability that a speciﬁc event is
SPS(x1 . . . xn) =
n

P(xi | parents(Xi))
because each sampling step depends only on the parent values. This expression should look
SPS(x1 . . . xn) = P(x1 . . . xn) .
This simple fact makes it easy to answer questions by using samples.
In any sampling algorithm, the answers are computed by counting the actual samples
generated. Suppose there are N total samples, and let NPS(x1, . . . , xn) be the number of
times the speciﬁc event x1, . . . , xn occurs in the set of samples. We expect this number, as a
fraction of the total, to converge in the limit to its expected value according to the sampling
probability:
lim
NPS(x1, . . . , xn)
N
= SPS(x1, . . . , xn) = P(x1, . . . , xn) .
For example, consider the event produced earlier: [true, false, true, true]. The sampling
probability for this event is
Whenever we use an approximate equality (“≈”) in what follows, we mean it in exactly
estimate is called consistent. For example, one can produce a consistent estimate of the
CONSISTENT
P(x1, . . . , xm) ≈NPS(x1, . . . , xm)/N .
That is, the probability of the event can be estimated as the fraction of all complete events
generated by the sampling process that match the partially speciﬁed event. For example, ifChapter
Probabilistic Reasoning
Rejection sampling in Bayesian networks
REJECTION
SAMPLING
by the network. Then, it rejects all those that do not match the evidence. Finally, the estimate
ˆP(X = x | e) is obtained by counting how often X = x occurs in the remaining samples.
Let ˆP(X | e) be the estimated distribution that the algorithm returns. From the deﬁnition
of the algorithm, we have
ˆP(X | e) = α NPS(X, e) = NPS(X, e)
NPS(e)
.
ˆP(X | e) ≈P(X, e)
P(e)
= P(X | e) .
That is, rejection sampling produces a consistent estimate of the true probability.
the true answer. The standard deviation of the error in each probability will be proportional
The biggest problem with rejection sampling is that it rejects so many samples! The
dence variables grows, so the procedure is simply unusable for complex problems.
ties directly from the real world. For example, to estimate P(Rain | RedSkyAtNight = true),
one can simply count how often it rains after a red sky is observed the previous evening—
ignoring those evenings when the sky is not red. (Here, the world itself plays the role of
seldom red, and that is the weakness of rejection sampling.
Likelihood weighting
Likelihood weighting avoids the inefﬁciency of rejection sampling by generating only events
LIKELIHOOD
WEIGHTING
that are consistent with the evidence e. It is a particular instance of the general statistical
technique of importance sampling, tailored for inference in Bayesian networks. We begin by
IMPORTANCE
SAMPLINGApproximate Inference in Bayesian Networks
inputs: X , the query variable
e, observed values for variables E
bn, a Bayesian network
N , the total number of samples to be generated
local variables: N, a vector of counts for each value of X , initially zero
if x is consistent with e then
return NORMALIZE(N)
Bayesian network.
describing how the algorithm works; then we show that it works correctly—that is, generates
consistent probability estimates.
ated is consistent with the evidence. Not all events are equal, however. Before tallying the
counts in the distribution for the query variable, each event is weighted by the likelihood that
the event accords to the evidence, as measured by the product of the conditional probabilities
for each evidence variable, given its parents. Intuitively, events in which the actual evidence
appears unlikely should be given less weight.
Grass. (Any topological ordering will do.) The process goes as follows: First, the weight w
true.
this is tallied under Rain = true.
To understand why likelihood weighting works, we start by examining the samplingChapter
Probabilistic Reasoning
inputs: X , the query variable
e, observed values for variables E
bn, a Bayesian network specifying joint distribution P(X1, . . . , Xn)
N , the total number of samples to be generated
local variables: W, a vector of weighted counts for each value of X , initially zero
W[x] ←W[x] + w where x is the value of X in x
return NORMALIZE(W)
foreach variable Xi in X1, . . . , Xn do
if Xi is an evidence variable with value xi in e
then w ←w × P(Xi = xi | parents(Xi))
else x[i] ←a random sample from P(Xi | parents(Xi))
return x, w
distribution given the values already sampled for the variable’s parents, while a weight is
accumulated based on the likelihood for each evidence variable.
with values e. We call the nonevidence variables Z (including the query variable X). The
algorithm samples each variable in Z given its parent values:
SWS(z, e) =
l
P(zi | parents(Zi)) .
like the prior distribution P(z), the distribution SWS pays some attention to the evidence: the
ple, when sampling Sprinkler the algorithm pays attention to the evidence Cloudy = true in
its parent variable. On the other hand, SWS pays less attention to the evidence than does the
true posterior distribution P(z | e), because the sampled values for each Zi ignore evidence
ignores the evidence in the child variable WetGrass = true; this means it will generate many
samples with Sprinkler = false and Rain = false despite the fact that the evidence actually
rules out this case.
into account. This cannot be done efﬁciently, however. If it could, then we could approximate the desired
time approximation scheme can exist.Approximate Inference in Bayesian Networks
The likelihood weight w makes up for the difference between the actual and desired
sampling distributions. The weight for a given sample x, composed from z and e, is the
product of the likelihoods for each evidence variable given its parents (some or all of which
may be among the Zis):
w(z, e) =
m

P(ei | parents(Ei)) .
the particularly convenient form
SWS(z, e)w(z, e) =
l
P(zi | parents(Zi))
m

P(ei | parents(Ei))
= P(z, e)
Now it is easy to show that likelihood weighting estimates are consistent. For any
particular value x of X, the estimated posterior probability can be calculated as follows:
ˆP(x | e) = α


y
NWS(x, y, e)w(x, y, e)
≈α′ 

y
SWS(x, y, e)w(x, y, e)
for large N
= α′ 

y
P(x, y, e)
= α′P(x, e) = P(x | e) .
Hence, likelihood weighting returns consistent estimates.
ﬁcient than rejection sampling. It will, however, suffer a degradation in performance as the
number of evidence variables increases. This is because most samples will have very low
weights and hence the weighted estimate will be dominated by the tiny fraction of samples
that accord more than an inﬁnitesimal likelihood to the evidence. The problem is exacerbated
if the evidence variables occur late in the variable ordering, because then the nonevidence
ples. This means the samples will be simulations that bear little resemblance to the reality
suggested by the evidence.
Inference by Markov chain simulation
MARKOV CHAIN
MONTE CARLO
gorithms generate each sample by making a random change to the preceding sample. It is
fying a value for every variable and generating a next state by making random changes to theChapter
Probabilistic Reasoning
ticular form of MCMC called Gibbs sampling, which is especially well suited for Bayesian
GIBBS SAMPLING
networks. (Other forms, some of them signiﬁcantly more powerful, are discussed in the notes
at the end of the chapter.) We will ﬁrst describe what the algorithm does, then we will explain
why it works.
Gibbs sampling in Bayesian networks
The Gibbs sampling algorithm for Bayesian networks starts with an arbitrary state (with the
evidence variables ﬁxed at their observed values) and generates a next state by randomly
sampling a value for one of the nonevidence variables Xi. The sampling for Xi is done
conditioned on the current values of the variables in the Markov blanket of Xi. (Recall from
parents.) The algorithm therefore wanders randomly around the state space—the space of
possible complete assignments—ﬂipping one variable at a time, but keeping the evidence
variables ﬁxed.
observed values and the nonevidence variables Cloudy and Rain are initialized randomly—
let us say to true and false respectively. Thus, the initial state is [true, true, false, true].
Now the nonevidence variables are sampled repeatedly in an arbitrary order. For example:
case, we sample from P(Cloudy | Sprinkler = true, Rain = false). (Shortly, we will
show how to calculate this distribution.) Suppose the result is Cloudy = false. Then
the new current state is [false, true, false, true].
pose this yields Rain = true. The new current state is [false, true, true, true].
Each state visited during this process is a sample that contributes to the estimate for the query
Why Gibbs sampling works
ities. The material in this section is quite technical, but the basic claim is straightforward:
time spent in each state is exactly proportional to its posterior probability. This remarkable
property follows from the speciﬁc transition probability with which the process moves from
TRANSITION
PROBABILITY
one state to another, as deﬁned by the conditional distribution given the Markov blanket of
the variable being sampled.Approximate Inference in Bayesian Networks
local variables: N, a vector of counts for each value of X , initially zero
Z, the nonevidence variables in bn
x, the current state of the network, initially copied from e
initialize x with random values for the variables in Z
for each Zi in Z do
set the value of Zi in x by sampling from P(Zi|mb(Zi))
return NORMALIZE(N)
works; this version cycles through the variables, but choosing variables at random also works.
state x′. This transition probability deﬁnes what is called a Markov chain on the state space.
MARKOV CHAIN
the Markov chain for t steps, and let πt(x) be the probability that the system is in state x at
the probability of being in that state times the probability of making the transition to x′:


x
STATIONARY
DISTRIBUTION
stationary distribution π; its deﬁning equation is therefore
π(x′) =


x
for all x′ .
Provided the transition probability distribution q is ergodic—that is, every state is reachable
ERGODIC
from every other and there are no strictly periodic cycles—there is exactly one distribution π
satisfying this equation for any given q.
its current “population”) is equal to the expected “inﬂow” from all the states. One obvious
way to satisfy this relationship is if the expected ﬂow between any pair of states is the same
in both directions; that is,
for all x, x′ .
DETAILED BALANCE
We can show that detailed balance implies stationarity simply by summing over x in


x


x
π(x′)q(x′ →x) = π(x′)


x
q(x′ →x) = π(x′)Chapter
Probabilistic Reasoning
where the last step follows because a transition from x′ is guaranteed to occur.
actually a special case of the more general deﬁnition of Gibbs sampling, according to which
each variable is sampled conditionally on the current values of all the other variables. We
start by showing that this general deﬁnition of Gibbs sampling satisﬁes the detailed balance
equation with a stationary distribution equal to P(x | e), (the true posterior distribution on
the nonevidence variables). Then, we simply observe that, for Bayesian networks, sampling
conditionally on all variables is equivalent to sampling conditionally on the variable’s Markov
To analyze the general Gibbs sampler, which samples each Xi in turn with a transition
ables (except the evidence variables); their values in the current state are xi. If we sample a
new value x′
i for Xi conditionally on all the other variables, including the evidence, we have
i, xi)) = P(x′
i | xi, e) .
Now we show that the transition probability for each step of the Gibbs sampler is in detailed
balance with the true posterior:
i | xi, e) = P(xi, xi | e)P(x′
i | xi, e)
= P(xi | xi, e)P(xi | e)P(x′
i | xi, e)
(using the chain rule on the ﬁrst term)
= P(xi | xi, e)P(x′
i, xi | e)
(using the chain rule backward)
= π(x′)qi(x′ →x) .
probability q that is the sequential composition q1 ◦q2 ◦· · · ◦qn of the transition probabilities
π as its stationary distribution, then the sequential composition qi ◦qj does too; hence the
transition probability q for the whole loop has P(x | e) as its stationary distribution. Finally,
disconnected—it is easy to see that q is ergodic. Hence, the samples generated by Gibbs
sampling will eventually be drawn from the true posterior distribution.
The ﬁnal step is to show how to perform the general Gibbs sampling step—sampling
pendent of all other variables given its Markov blanket; hence,
P(x′
i | xi, e) = P(x′
i | mb(Xi)) ,
where mb(Xi) denotes the values of the variables in Xi’s Markov blanket, MB(Xi). As
to the probability of the variable given its parents times the probability of each child given its
respective parents:
P(x′
i | mb(Xi)) = α P(x′
i | parents(Xi)) ×

tions required is equal to the number of Xi’s children.Recommendation(C1, B1)
Honesty(C1)
Kindness(C1)
Quality(B1)
Recommendation(C1, B1)
Honesty(C1)
Kindness(C1)
Quality(B1)
Recommendation(C2, B1)
Honesty(C2)
Kindness(C2)
Quality(B2)
Recommendation(C1, B2)
Recommendation(C2, B2)
(a)
(b)
(a) Bayes net for a single customer C1 recommending a single book B1.
net with two customers and two books.
relations among them and can express facts about some or all of the objects in a domain. This
often results in representations that are vastly more concise than the equivalent propositional
ables is ﬁxed and ﬁnite, and each has a ﬁxed domain of possible values. This fact limits the
applicability of Bayesian networks. If we can ﬁnd a way to combine probability theory with
cally the range of problems that can be handled.
ations of products based on recommendations received from its customers. The evaluation
able evidence. The simplest solution to base the evaluation on the average recommendation,
perhaps with a variance determined by the number of recommendations, but this fails to take
into account the fact that some customers are kinder than others and some are less honest than
others. Kind customers tend to give high recommendations even to fairly mediocre books,
while dishonest customers give very high or very low recommendations for reasons other
For a single customer C1, recommending a single book B1, the Bayes net might look
such as Honest(C1) are just fancy symbols—in this case, fancy names for random variables.)Chapter
Probabilistic Reasoning
larger numbers of books and customers, it becomes completely impractical to specify the
network by hand.
Fortunately, the network has a lot of repeated structure. Each Recommendation(c, b)
variable has as its parents the variables Honest(c), Kindness(c), and Quality(b). Moreover,
the CPTs for all the Recommendation(c, b) variables are identical, as are those for all the
We would like to say something like
Recommendation(c, b) ∼RecCPT(Honest(c), Kindness(c), Quality(b))
with the intended meaning that a customer’s recommendation for a book depends on the
customer’s honesty and kindness and the book’s quality according to some ﬁxed CPT. This
section develops a language that lets us say exactly this, and a lot more besides.
Possible worlds
signments of values to variables; for the Boolean case in particular, the possible worlds are
relations among them and an interpretation that maps constant symbols to objects, predicate
The model also needs to deﬁne a probability for each such possible world, just as a Bayesian
network deﬁnes a probability for each assignment of values to variables.
Let us suppose, for a moment, that we have ﬁgured out how to do this. Then, as usual
over the possible worlds where it is true:
P(φ) =


ω:φ is true in ω
P(ω) .
Conditional probabilities P(φ | e) can be obtained similarly, so we can, in principle, ask any
question we want of our model—e.g., “Which books are most likely to be recommended
highly by dishonest customers?”—and get an answer. So far, so good.
consistent distribution over an inﬁnite set of worlds could be very difﬁcult.
assumption—here, we adopt it for the constant symbols. It also assumes domain closure—
there are no more objects than those that are named. We can then guarantee a ﬁnite set of
possible worlds by making the set of objects in each world be exactly the set of constantR
J
R
J
R
J
R
J
R
J
R
J
. . .
. . .
. . .
. . .
R
J
R
J
R
J
R
J
R
J
R
J
R
J
R
J
R
J
R
J
Top: Some members of the set of all possible worlds for a language with two
constant symbols, R and J, and one binary relation symbol, under the standard semantics for
of the constant symbols is ﬁxed, and there is a distinct object for each constant symbol.
mapping from symbols to objects or about the objects that exist. We will call models deﬁned
RELATIONAL
PROBABILITY MODEL
fact is false doesn’t make sense in a probabilistic reasoning system!
When the underlying assumptions of database semantics fail to hold, RPMs won’t work
well. For example, a book retailer might use an ISBN (International Standard Book Number)
as a constant symbol to name each book, even though a given “logical” book (e.g., “Gone
tions across multiple ISBNs, but the retailer may not know for sure which ISBNs are really
the same book. (Note that we are not reifying the individual copies of the book, which might
tiﬁed by a login ID, but a dishonest customer may have thousands of IDs! In the computer
SIBYL
SIBYL ATTACK
online domain involves both existence uncertainty (what are the real books and customers
EXISTENCE
UNCERTAINTY
underlying the observed data) and identity uncertainty (which symbol really refer to the
IDENTITY
UNCERTAINTY
same object). We need to bite the bullet and deﬁne probability models based on the standard
the underlying ideas are the same.Chapter
Probabilistic Reasoning
Relational probability models
be easier to view predicates as functions that return true or false.) We will also assume a
type signature for each function, that is, a speciﬁcation of the type of each argument and the
TYPE SIGNATURE
and Book, and the type signatures for the functions and predicates are as follows:
The constant symbols will be whatever customer and book names appear in the retailer’s data
tures, the random variables of the RPM are obtained by instantiating each function with each
possible combination of objects: Honest(C1), Quality(B2), Recommendation(C1, B2),
has only ﬁnitely many instances, the number of basic random variables is also ﬁnite.
To complete the RPM, we have to write the dependencies that govern these random
variables. There is one dependency statement for each function, where each argument of the
Recommendation(c, b) ∼RecCPT(Honest(c), Kindness(c), Quality(b))
INDEPENDENCE
independent of some of its parents given certain values of others; thus, Recommendation(c, b)
is independent of Kindness(c) and Quality(b) when Honest(c) = false:
Recommendation(c, b) ∼
if Honest(c) then
HonestRecCPT(Kindness(c), Quality(b))
the dependencies must be acyclic, otherwise the resulting Bayesian network will have cycles and will not deﬁneRecommendation(C1, B1)
Honesty(C1)
Kindness(C1)
Quality(B1)
Recommendation(C2, B1)
Quality(B2)
Fan(C1, A1)
Fan(C1, A2)
Author(B2)
Fragment of the equivalent Bayes net when Author(B2) is unknown.
This kind of dependency may look like an ordinary if–then–else statement on a programming
language, but there is a key difference: the inference engine doesn’t necessarily know the
value of the conditional test!
We can elaborate this model in endless ways to make it more realistic. For example,
regardless of quality:
Recommendation(c, b) ∼
if Honest(c) then
else HonestRecCPT(Kindness(c), Quality(b))
Again, the conditional test Fan(c, Author(b)) is unknown, but if a customer gives only 5s to
a particular author’s books and is not otherwise especially kind, then the posterior probability
that the customer is a fan of that author will be high. Furthermore, the posterior distribution
will tend to discount the customer’s 5s in evaluating the quality of that author’s books.
In the preceding example, we implicitly assumed that the value of Author(b) is known
for every b, but this may not be the case. How can the system reason about whether, say, C1
is a fan of Author(B2) when Author(B2) is unknown? The answer is that the system may
have to reason about all possible authors. Suppose (to keep things simple) that there are just
two authors, A1 and A2. Then Author(B2) is a random variable with two possible values,
A1 and A2, and it is a parent of Recommendation(C1, B2). The variables Fan(C1, A1) and
Fan(C1, A2) are parents too. The conditional distribution for Recommendation(C1, B2) is
then essentially a multiplexer in which the Author(B2) parent acts as a selector to choose
MULTIPLEXER
which of Fan(C1, A1) and Fan(C1, A2) actually gets to inﬂuence the recommendation. A
of Author(B2), which affects the dependency structure of the network, is an instance of
relational uncertainty.
RELATIONAL
UNCERTAINTY
In case you are wondering how the system can possibly work out who the author of
B2 is: consider the possibility that three other customers are fans of A1 (and have no other
tomers ﬁnd it quite dismal. In that case, it is extremely likely that A1 is the author of B2.Chapter
Probabilistic Reasoning
The emergence of sophisticated reasoning like this from an RPM model of just a few lines
nections among objects in the model. As more dependencies and more objects are added, the
picture conveyed by the posterior distribution often becomes clearer and clearer.
The next question is how to do inference in RPMs. One approach is to collect the
evidence and query and the constant symbols therein, construct the equivalent Bayes net,
and apply any of the inference methods discussed in this chapter. This technique is called
UNROLLING
more, if there are many candidate objects for an unknown relation or function—for example,
the unknown author of B2—then some variables in the network may have many parents.
Fortunately, much can be done to improve on generic inference algorithms. First, the
presence of repeated substructure in the unrolled Bayes net means that many of the factors
ing algorithms) will be identical; effective caching schemes have yielded speedups of three
MCMC inference algorithms have some interesting properties when applied to RPMs with
relational uncertainty. MCMC works by sampling complete possible worlds, so in each state
the relational structure is completely known. In the example given earlier, each MCMC state
ents of the recommendation nodes for B2. For MCMC, then, relational uncertainty causes no
increase in network complexity; instead, the MCMC process includes transitions that change
the relational structure, and hence the dependency structure, of the unrolled network.
All of the methods just described assume that the RPM has to be partially or completely
logic programming systems avoid propositionalizing by instantiating the logical variables
only as needed to make the inference go through; that is, they lift the inference process above
the level of ground propositional sentences and make each lifted step do the work of many
ground steps. The same idea applied in probabilistic inference. For example, in the variable
elimination algorithm, a lifted factor can represent an entire set of ground factors that assign
probabilities to random variables in the RPM, where those random variables differ only in the
constant symbols used to construct them. The details of this method are beyond the scope of
this book, but references are given at the end of the chapter.
We argued earlier that database semantics was appropriate for situations in which we know
ular, all observations about an object are correctly associated with the constant symbol that
(to which we will return in a moment), but the phenomenon is far more pervasive:• A vision system doesn’t know what exists, if anything, around the next corner, and may
not know if the object it sees now is the same one it saw a few minutes ago.
in a text, and must reason about whether phrases such as “Mary,” “Dr. Smith,” “she,”
“his cardiologist,” “his mother,” and so on refer to the same object.
• An intelligence analyst hunting for spies never knows how many spies there really are
and can only guess whether various pseudonyms, phone numbers, and sightings belong
to the same individual.
In fact, a major part of human cognition seems to require learning what objects exist and
being able to connect observations—which almost never come with unique IDs attached—to
hypothesized objects in the world.
OPEN UNIVERSE
while guaranteeing a unique, consistent probability distribution over the inﬁnite space of
possible worlds.
The basic idea is to understand how ordinary Bayesian networks and RPMs manage
essence, a Bayes net generates each possible world, event by event, in the topological order
deﬁned by the network structure, where each event is an assignment of a value to a variable.
An RPM extends this to entire sets of events, deﬁned by the possible instantiations of the
logical variables in a given predicate or function. OUPMs go further by allowing generative
steps that add objects to the possible world under construction, where the number and type
of objects may depend on the objects that are already in that world. That is, the event being
generated is not the assignment of a value to a variable, but the very existence of objects.
One way to do this in OUPMs is to add statements that deﬁne conditional distributions
domain, we might want to distinguish between customers (real people) and their login IDs.
We expect honest customers to have just one ID, whereas dishonest customers might have
# LoginID(Owner = c) ∼
This statement deﬁnes the number of login IDs for a given owner, who is a customer. The
Owner function is called an origin function because it says where each generated object
ORIGIN FUNCTION
elements in each possible world are actually generation histories (e.g., “the fourth login ID of
the seventh customer”) rather than simple tokens.Chapter
Probabilistic Reasoning
closely in the limit. There are some tricky issues involved in designing these algorithms.
For example, an MCMC algorithm cannot sample directly in the space of possible worlds
ing on the fact that only ﬁnitely many objects can be relevant to the query in distinct ways.
Moreover, transitions must allow for merging two objects into one or splitting one into two.
(Details are given in the references at the end of the chapter.) Despite these complications,
is well deﬁned and can be calculated.
order probabilistic reasoning yields a tremendous increase in the effectiveness of AI systems
at handling uncertain information. Potential applications include those mentioned above—
computer vision, text understanding, and intelligence analysis—as well as many other kinds
of sensor interpretation.
OTHER APPROACHES TO UNCERTAIN REASONING
Other sciences (e.g., physics, genetics, and economics) have long favored probability as a
ity which is, or ought to be, in a reasonable man’s mind.”
tives to probability. The earliest expert systems of the 1970s ignored uncertainty and used
abilistic techniques. Initial results were promising, but they did not scale up because of the
exponential number of probabilities required in the full joint distribution. (Efﬁcient Bayesian
network algorithms were unknown then.) As a result, probabilistic approaches fell out of
variety of reasons:
• One common view is that probability theory is essentially numerical, whereas human
judgmental reasoning is more “qualitative.” Certainly, we are not consciously aware
of doing numerical calculations of degrees of belief. (Neither are we aware of doing
uniﬁcation, yet we seem to be capable of some kind of logical reasoning.) It might be
that we have some kind of numerical degrees of belief encoded directly in strengths
of connections and activations in our neurons. In that case, the difﬁculty of consciousOther Approaches to Uncertain Reasoning
ing mechanisms can be built directly on top of probability theory, so the “no numbers”
argument against probability has little force. Nonetheless, some qualitative schemes
soning, which treats conclusions not as “believed to a certain degree,” but as “believed
until a better reason is found to believe something else.” Default reasoning is covered
and formed the basis for a large number of expert systems in medicine and other areas.
• One area that we have not addressed so far is the question of ignorance, as opposed
to uncertainty. Consider the ﬂipping of a coin. If we know that the coin is fair, then
two cases are different, yet the outcome probability seems not to distinguish them. The
knowledge of the probability of a proposition.
• Probability makes the same ontological commitment as logic: that propositions are true
or false in the world, even if the agent is uncertain as to which is the case. Researchers
in fuzzy logic have proposed an ontology that allows vagueness: that a proposition can
be “sort of” true. Vagueness and uncertainty are in fact orthogonal issues.
The next three subsections treat some of these approaches in slightly more depth. We will not
provide detailed technical material, but we cite references for further study.
desirable properties:
• Locality: In logical systems, whenever we have a rule of the form A ⇒B, we can
LOCALITY
conclude B, given evidence A, without worrying about any other rules. In probabilistic
systems, we need to consider all the evidence.
• Detachment: Once a logical proof is found for a proposition B, the proposition can be
DETACHMENT
used regardless of how it was derived. That is, it can be detached from its justiﬁcation.
In dealing with probabilities, on the other hand, the source of the evidence for a belief
is important for subsequent reasoning.
FUNCTIONALITY
the truth of the components. Probability combination does not work this way, except
under strong global independence assumptions.
There have been several attempts to devise uncertain reasoning schemes that retain these
advantages. The idea is to attach degrees of belief to propositions and rules and to devise
purely local schemes for combining and propagating those degrees of belief. The schemesChapter
Probabilistic Reasoning
in A and the belief in B.
functionality ﬁrst. Let H1 be the event that a fair coin ﬂip comes up heads, let T1 be the event
that the coin comes up tails on that same ﬂip, and let H2 be the event that the coin comes
But we can see that the probability of the disjunction depends on the events themselves and
not just on their probabilities:
P(A)
P(B)
P(A ∨B)
form A %→B that allow us to compute the belief in B as a function of the belief in the rule
in the rule is assumed to be constant and is usually speciﬁed by the knowledge engineer—for
able to do both causal and diagnostic reasoning, we would need the two rules
Rain %→WetGrass
and
WetGrass %→Rain .
These two rules form a feedback loop: evidence for Rain increases the belief in WetGrass,
which in turn increases the belief in Rain even more. Clearly, uncertain reasoning systems
need to keep track of the paths along which evidence is propagated.
Intercausal reasoning (or explaining away) is also tricky. Consider what happens when
we have the two rules
Sprinkler %→WetGrass
and
WetGrass %→Rain .
Suppose we see that the sprinkler is on. Chaining forward through our rules, this increases the
belief that the grass will be wet, which in turn increases the belief that it is raining. But this
is ridiculous: the fact that the sprinkler is on explains away the wet grass and should reduce
for uncertain reasoning is the certainty factors model, which was developed for the MYCIN
CERTAINTY FACTOR
medical diagnosis program and was widely used in expert systems of the late 1970s and
tic (as in MYCIN) or purely causal. Furthermore, evidence was entered only at the “roots”Other Approaches to Uncertain Reasoning
alent to Bayesian inference on polytrees. In other circumstances, certainty factors could yield
came larger, undesirable interactions between rules became more common, and practitioners
found that the certainty factors of many other rules had to be “tweaked” when new rules were
uncertain reasoning.
Representing ignorance: Dempster–Shafer theory
The Dempster–Shafer theory is designed to deal with the distinction between uncertainty
DEMPSTER–SHAFER
THEORY
and ignorance. Rather than computing the probability of a proposition, it computes the
probability that the evidence supports the proposition. This measure of belief is called a
belief function, written Bel(X).
BELIEF FUNCTION
We return to coin ﬂipping for an example of belief functions.
Suppose you pick a
coin from a magician’s pocket. Given that the coin might or might not be fair, what belief
should you ascribe to the event that it comes up heads? Dempster–Shafer theory says that
in a way that has some intuitive appeal. Now suppose you have an expert at your disposal
the evidence.
The mathematical underpinnings of Dempster–Shafer theory have a similar ﬂavor to
those of probability theory; the main difference is that, instead of assigning probabilities
to possible worlds, the theory assigns masses to sets of possible world, that is, to events.
MASS
masses for all events that are subsets of (i.e., that entail) A, including A itself. With this
As with default reasoning, there is a problem in connecting beliefs to actions. Whenever
there is a gap in the beliefs, then a decision problem can be deﬁned such that a Dempster–
Shafer system is unable to make a decision. In fact, the notion of utility in the Dempster–
not as a degree of belief in A but as the probability assigned to all the possible worlds (now
interpreted as logical theories) in which A is provable. While there are cases in which this
quantity might be of interest, it is not the same as the probability that A is true.
is necessary to handle such cases. The model would have two variables: the Bias of the coin
shows heads) and the outcome of the next Flip. The prior probability distribution for BiasChapter
Probabilistic Reasoning
would reﬂect our beliefs based on the source of the coin (the magician’s pocket): some small
probability that it is fair and some probability that it is heavily biased toward heads or tails.
The conditional distribution P(Flip | Bias) simply deﬁnes how the bias operates. If P(Bias)
P(Flip = heads) =
This is the same prediction as if we believe strongly that the coin is fair, but that does not
mean that probability theory treats the two situations identically. The difference arises after
the ﬂips in computing the posterior distribution for Bias. If the coin came from a bank, then
seeing it come up heads three times running would have almost no effect on our strong prior
belief in its fairness; but if the coin comes from the magician’s pocket, the same evidence
will lead to a stronger posterior belief that the coin is biased toward heads. Thus, a Bayesian
approach expresses our “ignorance” in terms of how our beliefs would change in the face of
future information gathering.
Representing vagueness: Fuzzy sets and fuzzy logic
Fuzzy set theory is a means of specifying how well an object satisﬁes a vague description.
FUZZY SET THEORY
people would hesitate to answer “true” or “false,” preferring to say, “sort of.” Note that this
is not a question of uncertainty about the external world—we are sure of Nate’s height. The
issue is that the linguistic term “tall” does not refer to a sharp demarcation of objects into two
classes—there are degrees of tallness. For this reason, fuzzy set theory is not a method for
uncertain reasoning at all. Rather, fuzzy set theory treats Tall as a fuzzy predicate and says
or false. The name “fuzzy set” derives from the interpretation of the predicate as implicitly
deﬁning a set of its members—a set that does not have sharp boundaries.
Fuzzy logic is a method for reasoning with logical expressions describing membership
FUZZY LOGIC
in fuzzy sets. For example, the complex sentence Tall(Nate) ∧Heavy(Nate) has a fuzzy
truth value that is a function of the truth values of its components. The standard rules for
evaluating the fuzzy truth, T, of a complex sentence are
T(A ∧B) = min(T(A), T(B))
T(A ∨B) = max(T(A), T(B))
among the component propositions.
Fuzzy control is a methodology for constructing control systems in which the mapping
FUZZY CONTROL
trol has been very successful in commercial products such as automatic transmissions, videoSummary
are successful because they have small rule bases, no chaining of inferences, and tunable
parameters that can be adjusted to improve the system’s performance. The fact that they are
implemented with fuzzy operators might be incidental to their success; the key is simply to
ity theory. One idea is to view assertions such as “Nate is Tall” as discrete observations made
ﬁes P(Observer says Nate is tall | Height), perhaps using a probit distribution as described
way, for example, if the model is part of a hybrid Bayesian network. Such an approach is not
P(Observer says Nate is tall and heavy | Height, Weight)
allows for interactions between height and weight in the causing of the observation. Thus,
Fuzzy predicates can also be given a probabilistic interpretation in terms of random
sets—that is, random variables whose possible values are sets of objects. For example, Tall
RANDOM SET
is a random set whose possible values are sets of people. The probability P(Tall = S1),
where S1 is some particular set of people, is the probability that exactly that set would be
identiﬁed as “tall” by an observer. Then the probability that “Nate is tall” is the sum of the
probabilities of all the sets of which Nate is a member.
Both the hybrid Bayesian network approach and the random sets approach appear to
capture aspects of fuzziness without introducing degrees of truth. Nonetheless, there remain
uous quantities—issues that have been neglected by most outside the fuzzy community.
SUMMARY
knowledge. Bayesian networks play a role roughly analogous to that of propositional logic
for deﬁnite knowledge.
• A Bayesian network is a directed acyclic graph whose nodes correspond to random
variables; each node has a conditional distribution for the node, given its parents.
tionships in the domain.
• A Bayesian network speciﬁes a full joint distribution; each joint entry is deﬁned as the
product of the corresponding entries in the local conditional distributions. A Bayesian
network is often exponentially smaller than an explicitly enumerated joint distribution.
• Many conditional distributions can be represented compactly by canonical families ofChapter
Probabilistic Reasoning
distributions. Hybrid Bayesian networks, which include both discrete and continuous
variables, use a variety of canonical distributions.
• Inference in Bayesian networks means computing the probability distribution of a set
of query variables, given a set of evidence variables. Exact inference algorithms, such
as variable elimination, evaluate sums of products of conditional probabilities as efﬁ-
ciently as possible.
• In polytrees (singly connected networks), exact inference takes time linear in the size
of the network. In the general case, the problem is intractable.
• Stochastic approximation techniques such as likelihood weighting and Markov chain
work and can cope with much larger networks than can exact algorithms.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The use of networks to represent probabilistic information began early in the 20th century,
Turing, developed probabilistic representations and Bayesian inference methods that could
be regarded as a forerunner of modern Bayesian networks—although the paper is not often
The inﬂuence diagram representation for decision problems, which incorporated a
DAG representation for random variables, was used in decision analysis in the late 1970s
applications in medicine included the MUNIN system for diagnosing neuromuscular disorders
(Clarke, 1968a), Good and Minsky are credited with making the breakthrough that led to the development of theBibliographical and Historical Notes
portion of the network.)
Applications in engineering include the Electric Power Research Institute’s work on
ﬁeld of network tomography, which aims to infer unobserved local properties of nodes and
tant application area is biology: Bayesian networks have been used for identifying human
a clustering algorithm for exact inference in general Bayesian networks, utilizing a conversion
to a directed polytree of clusters in which message passing was used to achieve consistency
over variables shared between clusters. A similar approach, developed by the statisticians
MARKOV NETWORK
proach is implemented in the HUGIN system, an efﬁcient and widely used tool for uncertain
independence in clustering algorithms.
The basic idea of variable elimination—that repeated computations within the overall
NONSERIAL DYNAMIC
PROGRAMMING
algorithmic approach that can be applied to solve a range of inference problems in Bayesian
networks—for example, ﬁnding the most likely explanation for a set of observations. This
connects Bayesian network algorithms to related methods for solving CSPs and gives a direct
measure of the complexity of exact inference in terms of the tree width of the network. Wexler
computed in variable elimination; their algorithm breaks down large factors into products of
smaller factors and simultaneously computes an error bound for the resulting approximation.
The inclusion of continuous random variables in Bayesian networks was considered
ing only continuous variables with linear Gaussian distributions. The inclusion of discreteChapter
Probabilistic Reasoning
been discovered several times in the 19th century. Bliss’s work was expanded considerably
clustering and variable elimination methods. The method of cutset conditioning, which was
Bayesian network, a cutset is a set of nodes that, when instantiated, reduces the remaining
nodes to a polytree that can be solved in linear time and space. The query is answered by
The development of fast approximation algorithms for Bayesian network inference is
a very active area, with contributions from statistics, computer science, and physics. The
rejection sampling method is a general technique that is long known to statisticians; it was
well even when the evidence has very low prior likelihood.
Markov chain Monte Carlo (MCMC) algorithms began with the Metropolis algorithm,
There are two very important families of approximation methods that we did not cover
in the chapter. The ﬁrst is the family of variational approximation methods, which can be
VARIATIONAL
APPROXIMATION
used to simplify complex calculations of all kinds. The basic idea is to propose a reduced
version of the original problem that is simple to work with, but that resembles the original
VARIATIONAL
PARAMETER
strict upper and lower bounds can be obtained. Variational methods have long been used in
MEAN FIELD
ational approximation in which the individual variables making up the model are assumedBibliographical and Historical Notes
ematical foundations for applying variational methods to Bayesian networks and obtained
bounds. Since these early papers, variational methods have been applied to many speciﬁc
fying theoretical analysis of the literature on variational methods.
A second important family of approximation algorithms is based on Pearl’s polytree
minate, but in many cases, the values obtained are close to the true values. Little attention
BELIEF
PROPAGATION
observed that message passing in a multiply connected Bayesian network was exactly the
TURBO DECODING
is that BP is both fast and accurate on the very large and very highly connected networks used
strong convergence results for BP on linear Gaussian networks. Weiss (2000b) shows how an
approximation called loopy belief propagation works, and when the approximation is correct.
statistical physics.
measures on possible worlds. Within AI, this idea was developed for propositional logic
tigation of knowledge representation issues in such languages was carried out by Bacchus
the distribution over possible worlds; one sentence entails another if it expresses a stronger
languages is quite difﬁcult and constructing a unique probability model nearly impossible
tences about conditional probabilities.
Beginning in the early 1990s, researchers working on complex applications noticed
the expressive limitations of Bayesian networks and developed various languages for writing
which combined Bayesian networks with the indexed random variable notation common in
INDEXED RANDOM
VARIABLEChapter
Probabilistic Reasoning
statistics. (In BUGS, an indexed random variable looks like X[i], where i has a deﬁned integer
programming language extended with a randomization primitive. Another important thread
less on knowledge representation and more on learning from large data sets.
Initially, inference in these models was performed by generating an equivalent Bayesian
computed factor for reuse by later computations involving the same relations but different
objects, thereby realizing some of the computational gains of lifting. The ﬁrst truly lifted
where certain aggregate probabilities can be computed in closed form, are described by Milch
of MCMC to avoid building the complete equivalent Bayes net in cases of relational and
probability models and their use in machine learning.
tics, the problem of record linkage arises when data records do not contain standard unique
RECORD LINKAGE
identiﬁers—for example, various citations of this book might name its ﬁrst author “Stuart
Russell” or “S. J. Russell” or even “Stewart Russle,” and other authors may use the some of
lems in ﬁnancial, medical, census, and other data. Probabilistic analysis goes back to work
history, work in symbolic AI assumed erroneously that sensors could supply sentences with
unique identiﬁers for objects. The issue was studied in the context of language understanding
for authors, papers, and citation strings, involving both relational and identity uncertainty,
(The program code faintly visible on the front cover of this book is part of a BLOG model
for detecting nuclear explosions from seismic signals as part of the UN Comprehensive TestBibliographical and Historical Notes
1970s, leaving a partial vacuum to be ﬁlled by alternative methods. Certainty factors were
both as an engineering solution and as a model of human judgment under uncertainty. The
bilistic results in some cases, but results in serious overcounting of evidence in other cases.
rules were justiﬁed by a (seldom tenable) global independence assumption.
a generalization of probability to interval values and a combination rule for using them. Later
between the Dempster–Shafer theory and standard probability theory.
a thorough introduction to fuzzy set theory; papers on fuzzy applications are collected in
incorrectly as a direct competitor to probability theory, whereas in fact it addresses a different
POSSIBILITY THEORY
connections between possibility theory and probability theory.
The resurgence of probability depended mainly on Pearl’s development of Bayesian
networks as a method for representing and using conditional independence information. This
of Probability” and his later article “An Inquiry into Computer Understanding” (Cheeseman,
introduce Bayesian networks to AI researchers. One of the principal philosophical objections
quire were not apparent to introspection and presumed an unrealistic level of precision in our
uncertain knowledge. The development of qualitative probabilistic networks (Wellman,
1990a) provided a purely qualitative abstraction of Bayesian networks, using the notion of
positive and negative inﬂuences between variables. Wellman shows that in many cases such
and evidence combination from probability theory and shows that they can also be applied inChapter
Probabilistic Reasoning
adoption of the technology.
ered in this chapter. New research on probabilistic reasoning appears both in mainstream
cialized journals, such as the International Journal of Approximate Reasoning. Many papers
on graphical models, which include Bayesian networks, appear in statistical journals. The
mation Processing Systems (NIPS), and Artiﬁcial Intelligence and Statistics (AISTATS) are
excellent sources for current research.
EXERCISES
We have a bag of three biased coins a, b, and c with probabilities of coming up heads
likelihood of drawing each of the three coins), and then the coin is ﬂipped three times to
generate the outcomes X1, X2, and X3.
a. Draw the Bayesian network corresponding to this setup and deﬁne the necessary CPTs.
b. Calculate which coin was most likely to have been drawn from the bag if the observed
ﬂips come out heads twice and tails once.
equivalence between the parameters and the conditional probabilities P(Xi | Parents(Xi))
from this deﬁnition.
P(z | y) as the ratio of two sums, each over entries in the joint distribution P(X, Y, Z).
c. Next, expand out the summations in your expression from part (b), writing out explicitly
the terms for the true and false values of each summed variable. Assuming that all
network parameters satisfy the constraint 
for any Bayesian network.Chapter
Probabilistic Reasoning
Hmother
Hfather
Hchild
mother
G
father
G
child
G
Hmother
Hfather
Hchild
mother
G
father
G
child
G
Hmother
Hfather
Hchild
mother
G
father
G
child
G
(a)
(b)
(c)
tance of handedness.
c. Which of the three networks is the best description of the hypothesis?
d. Write down the CPT for the Gchild node in network (a), in terms of s and m.
sion for P(Gchild = l) in terms of m and q only, by conditioning on its parent nodes.
f. Under conditions of genetic equilibrium, we expect the distribution of genes to be the
same across generations. Use this to calculate the value of q, and, given what you know
about handedness in humans, explain why the hypothesis described at the beginning of
this question must be wrong.
is independent of all other variables in the network, given its Markov blanket and derive
Radio
Battery
Ignition
Gas
Starts
Moves
A Bayesian network describing some features of a car’s electrical system
and engine. Each variable is Boolean, and the true value indicates that the corresponding
aspect of the vehicle is in working order.Chapter
Probabilistic Reasoning
N
N
(i)
(ii)
(iii)
F1
F1
F1
M1
M1
M1
F2
F2
F2
M2
M2
M2
N
Three possible networks for the telescope problem.
Two astronomers in different parts of the world make measurements M1 and M2 of
the number of stars N in some small region of the sky, using their telescopes. Normally, there
is a small possibility e of error by up to one star in each direction. Each telescope can also
(with a much smaller probability f) be badly out of focus (events F1 and F2), in which case
tations of the preceding information?
b. Which is the best network? Explain.
prior constraint on the values of N?
e. What is the most likely number of stars, given these observations? Explain how to
compute this, or if it is not possible to compute, explain what additional information is
needed and how it would affect the result.
a. Which of the following are asserted by the network structure?
(i) P(B, I, M) = P(B)P(I)P(M).
(ii) P(J | G) = P(J | G, I).
(iii) P(M | G, B, I) = P(M | G, B, I, J).Chapter
Probabilistic Reasoning
hard. (Hint: Consider a network with one variable for each proposition symbol, one for
each clause, and one for the conjunction of clauses.)
Consider the problem of generating a random sample from a speciﬁed distribution
on a single variable. Assume you have a random number generator that returns a random
CUMULATIVE
DISTRIBUTION
also Appendix A.) Explain how to calculate the cumulative distribution in O(k) time
and how to generate a single sample of X from it. Can the latter be done in less than
O(k) time?
b. Now suppose we want to generate N samples of X, where N ≫k. Explain how to do
this with an expected run time per sample that is constant (i.e., independent of k).
Gaussian). How can samples be generated from such a distribution?
algorithm such as LIKELIHOODWEIGHTING to do the inference. How would you have
a. How many states does the Markov chain have?
c. What does Q2, the square of the transition matrix, represent?
e. Explain how to do probabilistic inference in Bayesian networks, assuming that Qn is
available. Is this a practical way to do inference?
This exercise explores the stationary distribution for Gibbs sampling methods.
and then applies whichever is chosen. Prove that if q1 and q2 are in detailed balance
with π, then their convex composition is also in detailed balance with π. (Note: this
than sampled in a ﬁxed sequence.)
b. Prove that if each of q1 and q2 has π as its stationary distribution, then the sequential
composition q = q1 ◦q2 also has π as its stationary distribution.
The Metropolis–Hastings algorithm is a member of the MCMC family; as such, it is
METROPOLIS–
HASTINGS
designed to generate samples x (eventually) according to target probabilities π(x). (TypicallyPROBABILISTIC
REASONING OVER TIME
In which we try to interpret the present, understand the past, and perhaps predict
the future, even when very little is crystal clear.
Agents in partially observable environments must be able to keep track of the current state, to
agent maintains a belief state that represents which states of the world are currently possible.
From the belief state and a transition model, the agent can predict how the world might
evolve in the next time step. From the percepts observed and a sensor model, the agent can
logical formulas. Those approaches deﬁned belief states in terms of which world states were
possible, but could say nothing about which states were likely or unlikely. In this chapter, we
use probability theory to quantify the degree of belief in elements of the belief state.
changing world is modeled using a variable for each aspect of the world state at each point in
time. The transition and sensor models may be uncertain: the transition model describes the
probability distribution of the variables at time t, given the state of the world at past times,
while the sensor model describes the probability of each percept at time t, given the current
eral structure of inference algorithms for temporal models. Then we describe three speciﬁc
works (which include hidden Markov models and Kalman ﬁlters as special cases). Finally,
TIME AND UNCERTAINTY
We have developed our techniques for probabilistic reasoning in the context of static worlds,
in which each random variable has a single ﬁxed value. For example, when repairing a car,
we assume that whatever is broken remains broken during the process of diagnosis; our job
is to infer the state of the car from observed evidence, which also remains ﬁxed.Time and Uncertainty
Now consider a slightly different problem: treating a diabetic patient. As in the case of
ments, and other physical signs. The task is to assess the current state of the patient, including
sion about the patient’s food intake and insulin dose. Unlike the case of car repair, here the
dynamic aspects of the problem are essential. Blood sugar levels and measurements thereof
can change rapidly over time, depending on recent food intake and insulin doses, metabolic
activity, the time of day, and so on. To assess the current state from the history of evidence
and to predict the outcomes of treatment actions, we must model these changes.
The same considerations arise in many other contexts, such as tracking the location of
a robot, tracking the economic activity of a nation, and making sense of a spoken or written
sequence of words. How can dynamic situations like these be modeled?
States and observations
We view the world as a series of snapshots, or time slices, each of which contains a set of
TIME SLICE
same subset of variables is observable in each time slice (although this is not strictly necessary
in anything that follows). We will use Xt to denote the set of state variables at time t, which
are assumed to be unobservable, and Et to denote the set of observable evidence variables.
The observation at time t is Et = et for some set of values et.
ground installation. You want to know whether it’s raining today, but your only access to the
outside world occurs each morning when you see the director coming in with, or without, an
umbrella. For each day t, the set Et thus contains a single evidence variable Umbrellat or Ut
for short (whether the umbrella appears), and the set Xt contains a single state variable Raint
or Rt for short (whether it is raining). Other problems can involve larger sets of variables. In
the diabetes example, we might have evidence variables, such as MeasuredBloodSugar t and
PulseRatet, and state variables, such as BloodSugar t and StomachContents t. (Notice that
BloodSugar t and MeasuredBloodSugar t are not the same variable; this is how we deal with
noisy measurements of actual quantities.)
The interval between time slices also depends on the problem. For diabetes monitoring,
a suitable interval might be an hour rather than a day. In this chapter we assume the interval
between slices is ﬁxed, so we can label times by integers. We will assume that the state
R0, R1, R2, . . . and evidence variables U1, U2, . . .. We will use the notation a:b to denote
the sequence of integers from a to b (inclusive), and the notation Xa:b to denote the set ofChapter
Probabilistic Reasoning over Time
Xt
(a)
(b)
Xt
Transition and sensor models
With the set of state and evidence variables for a given problem decided on, the next step is
to specify how the world evolves (the transition model) and how the evidence variables get
their values (the sensor model).
The transition model speciﬁes the probability distribution over the latest state variables,
unbounded in size as t increases. We solve the problem by making a Markov assumption—
MARKOV
ASSUMPTION
isfying this assumption were ﬁrst studied in depth by the Russian statistician Andrei Markov
MARKOV PROCESS
MARKOV PROCESS
on the previous state and not on any earlier states. In other words, a state provides enough
information to make the future conditionally independent of the past, and we have
Even with the Markov assumption there is still a problem: there are inﬁnitely many
possible values of t. Do we need to specify a different distribution for each time step? We
avoid this problem by assuming that changes in the world state are caused by a stationary
process—that is, a process of change that is governed by laws that do not themselves change
STATIONARY
PROCESS
over time. (Don’t confuse stationary with static: in a static process, the state itself does not
same for all t, and we only have to specify one conditional probability table.
ables as well as the current state variables, but any state that’s worth its salt should sufﬁce to
generate the current sensor values. Thus, we make a sensor Markov assumption as follows:
SENSOR MARKOV
ASSUMPTION
shows both the transition model and the sensor model for the umbrella example. Notice theTime and Uncertainty
Raint
Umbrellat
t
P(R )
f
t
t
R
t
P(U  )
t
f
Bayesian network structure and conditional distributions describing the
umbrella world.
P(Umbrellat | Raint).
direction of the dependence between state and sensors: the arrows go from the actual state
of the world to sensor values because the state of the world causes the sensors to take on
particular values: the rain causes the umbrella to appear. (The inference process, of course,
goes in the other direction; the distinction between the direction of modeled dependencies
and the direction of inference is one of the principal advantages of Bayesian networks.)
For any t,
P(X0:t, E1:t) = P(X0)
t
assumed to depend only on whether it rained the previous day. Whether such an assumption
state variables contain all the information needed to characterize the probability distribution
for the next time slice. Sometimes the assumption is exactly true—for example, if a particle
assumption is only approximate, as in the case of predicting rain only on the basis of whether
it rained the previous day. There are two ways to improve the accuracy of the approximation:
more accurate predictions. For example, in Palo Alto, California, it very rarely rains
more than two days in a row.Chapter
Probabilistic Reasoning over Time
us to incorporate historical records of rainy seasons, or we could add Temperature t,
Humidityt and Pressuret (perhaps at a range of locations) to allow us to use a physical
model of rainy conditions.
reformulated as an increase in the set of state variables, keeping the order ﬁxed. Notice that
adding state variables might improve the system’s predictive power but also increases the
prediction requirements: we now have to predict the new variables as well. Thus, we are
the “physics” of the process being modeled. The requirement for accurate modeling of the
process is obviously lessened if we can add new sensors (e.g., measurements of temperature
and pressure) that provide information directly about the new state variables.
Consider, for example, the problem of tracking a robot wandering randomly on the X–Y
plane. One might propose that the position and velocity are a sufﬁcient set of state variables:
one can simply use Newton’s laws to calculate the new position, and the velocity may change
have a systematic effect on the change in velocity. Because this in turn depends on how much
power was used by all previous maneuvers, the Markov property is violated. We can restore
the Markov property by including the charge level Batteryt as one of the state variables that
make up Xt. This helps in predicting the motion of the robot, but in turn requires a model
reliably, but more often we ﬁnd that error accumulates over time. In that case, accuracy can
be improved by adding a new sensor for the battery level.
INFERENCE IN TEMPORAL MODELS
Having set up the structure of a generic temporal model, we can formulate the basic inference
tasks that must be solved:
• Filtering: This is the task of computing the belief state—the posterior distribution
FILTERING
BELIEF STATE
over the most recent state—given all evidence to date. Filtering2 is also called state
estimation. In our example, we wish to compute P(Xt | e1:t). In the umbrella example,
STATE ESTIMATION
this would mean computing the probability of rain today, given all the observations of
the umbrella carrier made so far. Filtering is what a rational agent does to keep track
of the current state so that rational decisions can be made. It turns out that an almost
identical calculation provides the likelihood of the evidence sequence, P(e1:t).
• Prediction: This is the task of computing the posterior distribution over the future state,
PREDICTION
In the umbrella example, this might mean computing the probability of rain three days
from now, given all the observations to date. Prediction is useful for evaluating possible
courses of action based on their expected outcomes.
is to ﬁlter out the noise in a signal by estimating its underlying properties.Inference in Temporal Models
• Smoothing: This is the task of computing the posterior distribution over a past state,
SMOOTHING
given all evidence up to the present. That is, we wish to compute P(Xk | e1:t) for some k
that it rained last Wednesday, given all the observations of the umbrella carrier made
up to today. Smoothing provides a better estimate of the state than was available at the
• Most likely explanation: Given a sequence of observations, we might wish to ﬁnd the
sequence of states that is most likely to have generated those observations. That is, we
wish to compute argmaxx1:t P(x1:t | e1:t). For example, if the umbrella appears on each
of the ﬁrst three days and is absent on the fourth, then the most likely explanation is that
it rained on the ﬁrst three days and did not rain on the fourth. Algorithms for this task
are useful in many applications, including speech recognition—where the aim is to ﬁnd
the most likely sequence of words, given a series of sounds—and the reconstruction of
bit strings transmitted over a noisy channel.
In addition to these inference tasks, we also have
• Learning: The transition and sensor models, if not yet known, can be learned from
observations. Just as with static Bayesian networks, dynamic Bayes net learning can be
actually occurred and of what states generated the sensor readings, and these estimates
can be used to update the models. The updated models provide new estimates, and the
ter estimates of the states of the process. Learning with ﬁltering can fail to converge correctly;
ness, smoothing is always required to infer what happened at the murder scene from the
observable variables.
The remainder of this section describes generic algorithms for the four inference tasks,
independent of the particular kind of model employed. Improvements speciﬁc to each model
are described in subsequent sections.
Filtering and prediction
state estimate and update it, rather than going back over the entire history of percepts for each
update. (Otherwise, the cost of each update increases as time goes by.) In other words, given
for some function f. This process is called recursive estimation. We can view the calculation
RECURSIVE
ESTIMATION
estimated trajectory than ﬁltering—hence the name.Chapter
Probabilistic Reasoning over Time
as being composed of two parts: ﬁrst, the current state distribution is projected forward from
simply when the formula is rearranged:
(dividing up the evidence)
(using Bayes’ rule)
(by the sensor Markov assumption).
Here and throughout this chapter, α is a normalizing constant used to make probabilities sum
conditioning on the current state Xt:


xt


xt
(Markov assumption).
Within the summation, the ﬁrst factor comes from the transition model and the second comes
from the current state distribution. Hence, we have the desired recursive formulation. We can
think of the ﬁltered estimate P(Xt | e1:t) as a “message” f1:t that is propagated forward along
the sequence, modiﬁed by each transition and updated by each new observation. The process
is given by
constant (i.e., independent of t), and the space required is also constant. (The constants
depend, of course, on the size of the state space and the speciﬁc type of the temporal model
in question.) The time and space requirements for updating must be constant if an agent with
limited memory is to keep track of the current state distribution over an unbounded sequence
of observations.
P(R1) =


r0
P(R1 | r0)P(r0)Inference in Temporal Models
P(R2 | u1) =


r1
P(R2 | r1)P(r1 | u1)
The task of prediction can be seen simply as ﬁltering without the addition of new
a prediction for t + k:


Naturally, this computation involves only the transition model and not the sensor model.
It is interesting to consider what happens as we try to predict further and further into
great deal is known about the properties of such distributions and about the mixing time—
MIXING TIME
roughly, the time taken to reach the ﬁxed point. In practical terms, this dooms to failure any
attempt to predict the actual state for a number of steps that is more than a small fraction of
the mixing time, unless the stationary distribution itself is strongly peaked in a small area of
the state space. The more uncertainty there is in the transition model, the shorter will be the
mixing time and the more the future is obscured.
In addition to ﬁltering and prediction, we can use a forward recursion to compute the
likelihood of the evidence sequence, P(e1:t). This is a useful quantity if we want to compare
different temporal models that might have produced the same evidence sequence (e.g., two
different models for the persistence of rain). For this recursion, we use a likelihood message
ℓ1:t(Xt) = P(Xt, e1:t). It is a simple exercise to show that the message calculation is identical
to that for ﬁltering:
Having computed ℓ1:t, we obtain the actual likelihood by summing out Xt:
L1:t = P(e1:t) =


xt
ℓ1:t(xt) .
Notice that the likelihood message represents the probabilities of longer and longer evidence
we shall not go into solutions here.Chapter
Probabilistic Reasoning over Time
X1
E1
X0
Xk
Ek
Xt
Et
Smoothing computes P(Xk | e1:t), the posterior distribution of the state at
Smoothing
As we said earlier, smoothing is the process of computing the distribution over past states
(using Bayes’ rule)
(using conditional independence)
runs backward from t:


=


(by conditional independence)
=


=


Of the three factors in this summation, the ﬁrst and third are obtained directly from the model,
and the second is the “recursive call.” Using the message notation, we have
recursion, the time and space needed for each update are constant and thus independent of t.Inference in Temporal Models
Let us now apply this algorithm to the umbrella example, computing the smoothed
P(R1 | u1, u2) = α P(R1 | u1) P(u2 | R1) .
scribed earlier. The second term can be computed by applying the backward recursion in
P(u2 | R1) =


r2
P(u2 | r2)P( | r2)P(r2 | R1)
Both the forward and backward recursions take a constant amount of time per step;
hence, the time complexity of smoothing with respect to evidence e1:t is O(t). This is the
quence, one obvious method is simply to run the whole smoothing process once for each
time step to be smoothed. This results in a time complexity of O(t2). A better approach
uses a simple application of dynamic programming to reduce the complexity to O(t). A clue
appears in the preceding analysis of the umbrella example, where we were able to reuse the
results of forward ﬁltering over the whole sequence. Then we run the backward recursion
FORWARD–
BACKWARD
ALGORITHM
timates for the entire sequence. It is now understood that the forward–backward algorithm
is in fact a special case of the polytree propagation algorithm used with clustering methods
(although the two were developed independently).
tions that deal with sequences of noisy observations. As described so far, it has two practical
drawbacks. The ﬁrst is that its space complexity can be too high when the state space is large
and the sequences are long. It uses O(|f|t) space where |f| is the size of the representation ofChapter
Probabilistic Reasoning over Time
The second drawback of the basic algorithm is that it needs to be modiﬁed to work
in an online setting where smoothed estimates must be computed for earlier time slices as
new observations are continuously added to the end of the sequence. The most common
SMOOTHING
P(Xt−d | e1:t) for ﬁxed d. That is, smoothing is done for the time slice d steps behind the
current time t; as t increases, the smoothing has to keep up. Obviously, we can run the
cases, be done in constant time per update, independent of the lag d.
Finding the most likely sequence
Suppose that [true, true, false, true, true] is the umbrella sequence for the security guard’s
ﬁrst ﬁve days on the job. What is the weather sequence most likely to explain this? Does
possible weather sequences we could pick. Is there a way to ﬁnd the most likely one, short of
enumerating all of them?
for the weather at each time step; then construct the sequence, using at each step the weather
that is most likely according to the posterior. Such an approach should set off alarm bells
prior, the prior distribution on the initial state, P(X0)
b, a representation of the backward message, initially all 1s
sv[i] ←NORMALIZE(fv[i] × b)
b ←BACKWARD(b, ev[i])
return sv
abilities of a sequence of states given a sequence of observations.
The FORWARD andInference in Temporal Models
Rain1
true
Rain5
true
Rain4
true
Rain3
false
Rain2
true
Umbrellat
(a)
(b)
true
false
true
false
true
false
true
false
true
false
(a) Possible state sequences for Raint can be viewed as paths through a graph
of the possible states at each time step. (States are shown as rectangles to avoid confusion
vation sequence [true, true, false, true, true]. For each t, we have shown the values of the
message m1:t, which gives the probability of the best sequence reaching each state at time t.
Also, for each state, the bold arrow leading into it indicates its best predecessor as measured
by the product of the preceding sequence probability and the transition probability. Following
butions over single time steps, whereas to ﬁnd the most likely sequence we must consider
joint probabilities over all the time steps. The results can in fact be quite different. (See
little more thought. It relies on the same Markov property that yielded efﬁcient algorithms for
ﬁltering and smoothing. The easiest way to think about the problem is to view each sequence
as a path through a graph whose nodes are the possible states at each time step. Such a
the most likely path through this graph, where the likelihood of any path is the product of
the transition probabilities along the path and the probabilities of the given observations at
each state. Let’s focus in particular on paths that reach the state Rain5 = true. Because of
the Markov property, it follows that the most likely path to the state Rain5 = true consists of
likelihood of that path. In other words, there is a recursive relationship between most likely
as an equation connecting the probabilities of the paths:
max
xt


.Chapter
Probabilistic Reasoning over Time
m1:t =
max
that is, the probabilities of the most likely path to each state xt; and
probability for the most likely sequence reaching each of the ﬁnal states. One can thus easily
select the most likely sequence overall (the states outlined in bold). In order to identify the
actual sequence, as opposed to just computing its probability, the algorithm will also need to
record, for each state, the best state that leads to it; these are indicated by the bold arrows in
from the best ﬁnal state.
The algorithm we have just described is called the Viterbi algorithm, after its inventor.
VITERBI ALGORITHM
Like the ﬁltering algorithm, its time complexity is linear in t, the length of the sequence.
Unlike ﬁltering, which uses constant space, its space requirement is also linear in t. This
is because the Viterbi algorithm needs to keep the pointers that identify the best sequence
leading to each state.
HIDDEN MARKOV MODELS
eral framework that was independent of the speciﬁc form of the transition and sensor models.
In this and the next two sections, we discuss more concrete models and applications that
illustrate the power of the basic algorithms and in some cases allow further improvements.
HIDDEN MARKOV
MODEL
able. The possible values of the variable are the possible states of the world. The umbrella
example described in the preceding section is therefore an HMM, since it has just one state
variable: Raint. What happens if you have a model with two or more state variables? You can
still ﬁt it into the HMM framework by combining the variables into a single “megavariable”
whose values are all possible tuples of values of the individual state variables. We will see
that the restricted structure of HMMs allows for a simple and elegant matrix implementation
proceeding with this section.Hidden Markov Models
Simpliﬁed matrix algorithms
With a single, discrete state variable Xt, we can give concrete form to the representations
of the transition model, the sensor model, and the forward and backward messages. Let the
That is, Tij is the probability of a transition from state i to state j. For example, the transition
matrix for the umbrella world is


.
We also put the sensor model in matrix form. In this case, because the value of the evidence
variable Et is known at time t (call it et), we need only specify, for each state, how likely it
is that the state causes et to appear: we need P(et | Xt = i) for each state i. For mathematical
convenience we place these values into an S × S diagonal matrix, Ot whose ith diagonal
O1 =


;
O3 =


.
cause the forward pass stores t vectors of size S.
Besides providing an elegant description of the ﬁltering and smoothing algorithms for
HMMs, the matrix formulation reveals opportunities for improved algorithms. The ﬁrst is
a simple variation on the forward–backward algorithm that allows smoothing to be carried
ing for any particular time slice k requires the simultaneous presence of both the forward and
gorithm achieves this by storing the fs computed on the forward pass so that they are available
during the backward pass. Another way to achieve this is with a single pass that propagates
both f and b in the same direction. For example, the “forward” message f can be propagated
pute ft:t (forgetting all the intermediate results) and then running the backward pass for bothChapter
Probabilistic Reasoning over Time
inputs: et, the current evidence for time step t
hmm, a hidden Markov model with S × S transition matrix T
d, the length of the lag for smoothing
f, the forward message P(Xt|e1:t), initially hmm.PRIOR
local variables: Ot−d, Ot, diagonal matrices containing the sensor model information
add et to the end of et−d:t
Ot ←diagonal matrix containing P(et|Xt)
if t > d then
f ←FORWARD(f, et)
Ot−d ←diagonal matrix containing P(et−d|Xt−d)
t−dT−1BTOt
else B ←BTOt
if t > d then return NORMALIZE(f × B1) else return null
An algorithm for smoothing with a ﬁxed time lag of d steps, implemented
as an online algorithm that outputs the new smoothed estimate given the observation for a
b and f together, using them to compute the smoothed estimate at each step. Since only one
copy of each message is needed, the storage requirements are constant (i.e., independent of
quires that the transition matrix be invertible and that the sensor model have no zeroes—that
is, that every observation be possible in every state.
A second area in which the matrix formulation reveals an improvement is in online
smoothing with a ﬁxed lag. The fact that smoothing can be done in constant space suggests
gorithm whose time complexity is independent of the length of the lag. Let us suppose that
the lag is d; that is, we are smoothing at time slice t −d, where the current time is t. By
for slice t −d. Then, when a new observation arrives, we need to computeHidden Markov Models
Computing the backward message incrementally is trickier, because there is no simple

t
TOi
 
thought of as a “transformation operator” that transforms a later backward message into an
tion arrives:


TOi
 
simple relationship: to get the second product, “divide” the ﬁrst product by the ﬁrst element
simple relationship between the old and new B matrices:
Hidden Markov model example: Localization
In that version, the robot had a single nondeterministic Move action and its sensors reported
perfectly whether or not obstacles lay immediately to the north, south, east, and west; the
robot’s belief state was the set of possible locations it could be in.
Here we make the problem slightly more realistic by including a simple probability
model for the robot’s motion and by allowing for noise in the sensors. The state variable Xt
represents the location of the robot on the discrete grid; the domain of this variable is the
set of empty squares {s1, . . . , sn}. Let NEIGHBORS(s) be the set of empty squares that are
adjacent to s and let N(s) be the size of that set. Then the transition model for Move action
says that the robot is equally likely to end up at any neighboring square:
We don’t know where the robot starts, so we will assume a uniform distribution over all the
ence or absence of an obstacle in a particular compass direction. We will use the notationChapter
Probabilistic Reasoning over Time
(a) Posterior distribution over robot location after E1 = NSW
(b) Posterior distribution over robot location after E1 = NSW, E2 = NS
Posterior distribution over robot location: (a) one observation E1 = NSW;
(b) after a second observation E2 = NS. The size of each disk corresponds to the probability
NS, for example, to mean that the north and south sensors report an obstacle and the east and
number of bits that are different—between the true values for square i and the actual reading
et, then the probability that a robot in square i would receive a sensor reading et is
For example, the probability that a square with obstacles to the north and south would produce
distributions P(X1 | E1 = NSW) and P(X2 | E1 = NSW, E2 = NS). This is the same maze
tions that were possible, assuming perfect sensing. Those same locations are still the most
likely with noisy sensing, but now every location has some nonzero probability.
In addition to ﬁltering to estimate its current location, the robot can use smoothingHidden Markov Models
Localization error
Number of observations
Path accuracy
Number of observations
(a)
(b)
(b) The Viterbi path accuracy, deﬁned as the fraction of correct states on the Viterbi path.
to integrate evidence over time and to take into account the probabilistic constraints imposed
the prior distribution P(X0) and in the transition model itself. Broadly speaking, high levels
of localization and path accuracy are maintained even in the face of substantial errors in the
models used.
The state variable for the example we have considered in this section is a physical
location in the world. Other problems can, of course, include other aspects of the world.
straight for as long as it can; only when it encounters an obstacle does it change to a new
(randomly selected) heading. To model this robot, each state in the model consists of a
number. If we add the possibility of dirt in the squares, the number of states is multiplied by
many state variables. If we allow the robot to move continuously rather than in a discrete
grid, the number of states becomes inﬁnite; the next section shows how to handle this case.Chapter
Probabilistic Reasoning over Time
KALMAN FILTERS
Imagine watching a small bird ﬂying through dense jungle foliage at dusk: you glimpse
brief, intermittent ﬂashes of motion; you try hard to guess where the bird is and where it will
appear next so that you don’t lose it. Or imagine that you are a World War II radar operator
back further still, imagine you are Kepler trying to reconstruct the motions of the planets
from a collection of highly inaccurate angular observations taken at irregular and imprecisely
measured intervals. In all these cases, you are doing ﬁltering: estimating state variables (here,
position and velocity) from noisy observations over time. If the variables were discrete, we
could model the system with a hidden Markov model. This section examines methods for
handling continuous variables, using an algorithm called Kalman ﬁltering, after one of its
KALMAN FILTERING
inventors, Rudolf E. Kalman.
The bird’s ﬂight might be speciﬁed by six continuous variables at each time point; three
for position (Xt, Yt, Zt) and three for velocity ( ˙Xt, ˙Yt, ˙Zt). We will need suitable conditional
current state Xt, plus some Gaussian noise, a condition that turns out to be quite reasonable in
for now. Let the time interval between observations be Δ, and assume constant velocity
noise (to account for wind variation, etc.), we obtain a linear Gaussian transition model:
The Bayesian network structure for a system with position vector Xt and velocity ˙Xt is shown
form will be described later in this section and covers a vast array of applications beyond the
simple motion examples of the ﬁrst paragraph. The reader might wish to consult Appendix A
poses, the most important is that a multivariate Gaussian distribution for d variables is
MULTIVARIATE
GAUSSIAN
Updating Gaussian distributions
tributions: it remains closed under the standard Bayesian network operations. Here, we make
this claim precise in the context of ﬁltering in a temporal probability model. The required

xt
is also a Gaussian distribution.Kalman Filters
t
Z
Z
t
X
X
t
X
X
Bayesian network structure for a linear dynamical system with position Xt,
velocity ˙Xt, and position measurement Zt.
Gaussian, then, after conditioning on the new evidence, the updated distribution
is also a Gaussian distribution.
Thus, the FORWARD operator for Kalman ﬁltering takes a Gaussian forward message f1:t,
produces a Gaussian state distribution for all time.
This seems to be a nice, elegant result, but why is it so important? The reason is that,
except for a few special cases such as this, ﬁltering with continuous or hybrid (discrete and
continuous) networks generates state distributions whose representation grows without bound
happens for a simple example.
We have said that the FORWARD operator for the Kalman ﬁlter maps a Gaussian into a new
ous mean and covariance matrix. Deriving the update rule in the general (multivariate) case
requires rather a lot of linear algebra, so we will stick to a very simple univariate case for now;
and later give the results for the general case. Even for the univariate case, the calculations
are somewhat tedious, but we feel that they are worth seeing because the usefulness of the
Kalman ﬁlter is tied so intimately to the mathematical properties of Gaussian distributions.
The temporal model we consider describes a random walk of a single continuous state
and is measured by a random consumer survey that also introduces Gaussian sampling noise.Chapter
Probabilistic Reasoning over Time
The prior distribution is assumed to be Gaussian with variance σ2
P(x0) = α e
„
σ2
«
.
(For simplicity, we use the same symbol α for all normalizing constants in this section.) The
transition model adds a Gaussian perturbation of constant variance σ2
x to the current state:
„
σ2x
«
.
The sensor model assumes Gaussian noise with variance σ2
z:
P(zt | xt) = α e
„
σ2z
«
.
P(x1) =
 ∞
−∞
P(x1 | x0)P(x0) dx0 = α
 ∞
−∞
e
„
σ2x
«
e
„
σ2
«
dx0
= α
 ∞
−∞
e
„
σ2
σ2
0σ2x
«
dx0 .
This integral looks rather complicated. The key to progress is to notice that the exponent is the
sum of two expressions that are quadratic in x0 and hence is itself a quadratic in x0. A simple
trick known as completing the square allows the rewriting of any quadratic ax2
COMPLETING THE
SQUARE
as the sum of a squared term a(x0 −−b
4a that is independent of
x0. The residual term can be taken outside the integral, giving us
“
c−b2
4a
”  ∞
−∞
we are left with only the residual term from the quadratic. Then, we notice that the residual
term is a quadratic in x1; in fact, after simpliﬁcation, we obtain
P(x1) = α e
„
σ2
«
.
equal to the sum of the original variance σ2
x.
To complete the update step, we need to condition on the observation at the ﬁrst time
P(x1 | z1) = α P(z1 | x1)P(x1)
= α e
„
σ2z
«
e
„
σ2
«
.
P(x1 | z1) = α e
B
B
@
(x1−(σ2
σ2
(σ2
C
C
A
.Kalman Filters
P(x)
x position
P(x0)
P(x1)
*z1
Stages in the Kalman ﬁlter update cycle for a random walk with a prior
P(x1) is ﬂattened out, relative to P(x0), by the transition noise. Notice also that the mean
of the posterior distribution P(x1 | z1) is slightly to the left of the observation z1 because the
mean is a weighted average of the prediction and the observation.
Thus, after one update cycle, we have a new Gaussian distribution for the state variable.
deviation can be calculated from the old mean and standard deviation as follows:
t + σ2
σ2
t + σ2x + σ2z
and
σ2
t + σ2
x)σ2
z
σ2
t + σ2x + σ2z
.
however, the equations have some interesting additional properties. First, we can interpret
z is large and we pay more
attention to the old mean; if the old mean is unreliable (σ2
t is large) or the process is highly
unpredictable (σ2
x is large), then we pay more attention to the observation. Second, notice
that the update for the variance σ2
compute in advance what the sequence of variance values will be. Third, the sequence of
variance values converges quickly to a ﬁxed value that depends only on σ2
x and σ2
z, thereby
The general case
The preceding derivation illustrates the key property of Gaussian distributions that allows
Kalman ﬁltering to work: the fact that the exponent is a quadratic form. This is true not just
for the univariate case; the full multivariate Gaussian distribution has the form
“
(x−μ)⊤Σ
”
.Chapter
Probabilistic Reasoning over Time
Multiplying out the terms in the exponent makes it clear that the exponent is also a quadratic
function of the values xi in x. As in the univariate case, the ﬁltering update preserves the
Gaussian nature of the state distribution.
sition model and the sensor model allow for a linear transformation with additive Gaussian
noise. Thus, we have
P(zt | xt) = N(Hxt, Σz)(zt) ,
variance, and H and Σz are the corresponding matrices for the sensor model. Now the update
equations for the mean and covariance, in their full, hairy horribleness, are
matrix. Believe it or not, these equations make some intuitive sense. For example, consider
KALMAN GAIN
MATRIX
observations. The sequence of values for Σt and Kt can therefore be computed ofﬂine, and
the actual calculations required during online tracking are quite modest.
To illustrate these equations at work, we have applied them to the problem of tracking
an object moving on the X–Y plane. The state variables are X = (X, Y, ˙X, ˙Y )⊤, so F, Σx,
observations, and the trajectory estimated by Kalman ﬁltering, along with the covariances
tracking the actual motion, and, as expected, the variance quickly reaches a ﬁxed point.
We can also derive equations for smoothing as well as ﬁltering with linear Gaussian
position estimate is sharply reduced, except at the ends of the trajectory (why?), and that the
estimated trajectory is much smoother.
Applicability of Kalman ﬁltering
The Kalman ﬁlter and its elaborations are used in a vast array of applications. The “classical”
application is in radar tracking of aircraft and missiles. Related applications include acoustic
tracking of submarines and ground vehicles and visual tracking of vehicles and people. In a
slightly more esoteric vein, Kalman ﬁlters are used to reconstruct particle trajectories from
range of application is much larger than just the tracking of motion: any system characterized
by continuous state variables and noisy measurements will do. Such systems include pulp
mills, chemical plants, nuclear reactors, plant ecosystems, and national economies.Kalman Filters
X
Y
X
Y
true
observed
smoothed
true
observed
smoothed
(a)
(b)
2D filtering
2D smoothing
(a) Results of Kalman ﬁltering for an object moving on the X–Y plane,
showing the true trajectory (left to right), a series of noisy observations, and the trajectory
estimated by Kalman ﬁltering. Variance in the position estimate is indicated by the ovals. (b)
The results of Kalman smoothing for the same observation sequence.
sults will be valid or useful. The assumptions made—a linear Gaussian transition and sensor
EXTENDED KALMAN
FILTER (EKF)
earities in the system being modeled. A system is nonlinear if the transition model cannot
NONLINEAR
works by modeling the system as locally linear in xt in the region of xt = μt, the mean of the
tracker to maintain and update a Gaussian state distribution that is a reasonable approximation
What does it mean for a system to be “unsmooth” or “poorly behaved”? Technically,
it means that there is signiﬁcant nonlinearity in system response within the region that is
in nontechnical terms, consider the example of trying to track a bird as it ﬂies through the
jungle. The bird appears to be heading at high speed straight for a tree trunk. The Kalman
ﬁlter, whether regular or extended, can make only a Gaussian prediction of the location of the
A reasonable model of the bird, on the other hand, would predict evasive action to one side or
decision varies sharply depending on its precise location relative to the trunk.
senting the behavior of the system being modeled. Within the control theory community, for
which problems such as evasive maneuvering by aircraft raise the same kinds of difﬁculties,
SWITCHING KALMAN
FILTERChapter
Probabilistic Reasoning over Time
(a)
(b)
A bird ﬂying toward a tree (top views). (a) A Kalman ﬁlter will predict the
location of the bird using a single Gaussian centered on the obstacle. (b) A more realistic
model allows for the bird’s evasive action, predicting that it will ﬂy to one side or the other.
ters run in parallel, each using a different model of the system—for example, one for straight
ﬂight, one for sharp left turns, and one for sharp right turns. A weighted sum of predictions
is used, where the weight depends on how well each ﬁlter ﬁts the current data. We will see
work model, obtained by adding a discrete “maneuver” state variable to the network shown
DYNAMIC BAYESIAN NETWORKS
A dynamic Bayesian network, or DBN, is a Bayesian network that represents a temporal
DYNAMIC BAYESIAN
NETWORK
general, each slice of a DBN can have any number of state variables Xt and evidence variables
Et. For simplicity, we assume that the variables and their links are exactly replicated from
can have parents only in its own slice or the immediately preceding slice.
It should be clear that every hidden Markov model can be represented as a DBN with
all the state variables in the DBN into a single state variable whose values are all possible
tuples of values of the individual state variables. Now, if every HMM is a DBN and everyDynamic Bayesian Networks
composing the state of a complex system into its constituent variables, the can take advantage
Boolean state variables, each of which has three parents in the preceding slice. Then the
is bad for at least three reasons: ﬁrst, the HMM itself requires much more space; second,
lem of learning such a huge number of parameters makes the pure HMM model unsuitable
for large problems. The relationship between DBNs and HMMs is roughly analogous to the
relationship between ordinary Bayesian networks and full tabulated joint distributions.
We have already explained that every Kalman ﬁlter model can be represented in a
It should be clear from the discussion at the end of the preceding section that not every DBN
can be represented by a Kalman ﬁlter model. In a Kalman ﬁlter, the current state distribution
is always a single multivariate Gaussian distribution—that is, a single “bump” in a particular
applications, this ﬂexibility is essential. Consider, for example, the current location of my
keys. They might be in my pocket, on the bedside table, on the kitchen counter, dangling
from the front door, or locked in the car. A single Gaussian bump that included all these
hall. Aspects of the real world such as purposive agents, obstacles, and pockets introduce
“nonlinearities” that require combinations of discrete and continuous variables in order to get
reasonable models.
Constructing DBNs
To construct a DBN, one must specify three kinds of information: the prior distribution over
nections between successive slices and between the state and evidence variables. Because
the transition and sensor models are assumed to be stationary—the same for all t—it is most
From this speciﬁcation, the complete DBN with an unbounded number of time slices can be
constructed as needed by copying the ﬁrst slice.
variables, which will include both Xt = (Xt, Yt) for position and ˙Xt = ( ˙Xt, ˙Yt) for velocity.
We assume some method of measuring position—perhaps a ﬁxed camera or onboard GPS
(Global Positioning System)—yielding measurements Zt. The position at the next time step
depends on the current position and velocity, as in the standard Kalman ﬁlter model. The
velocity at the next step depends on the current velocity and the state of the battery. We
add Batteryt to represent the actual battery charge level, which has as parents the previousChapter
Probabilistic Reasoning over Time
Rain0
Rain1
Umbrella1
t
f
P(U  )
R1
f
t
P(R )
R0
P(R )
Z1
X1
X1
tX
X0
X0
Battery
BMeter
(a)
(b)
(a) Speciﬁcation of the prior, transition model, and sensor model for the
for robot motion in the X–Y plane.
battery level and the velocity, and we add BMetert, which measures the battery charge level.
It is worth looking in more depth at the nature of the sensor model for BMetert. Let
always creeps into measurements. For continuous measurements, a Gaussian distribution
Gaussian using a distribution in which the probability of error drops off in the appropriate
way, so that the probability of a large error is very small. We use the term Gaussian error
model to cover both the continuous and discrete versions.
GAUSSIAN ERROR
MODEL
forms of automatic sensing will readily testify to the fact that small amounts of measurement
noise are often the least of one’s problems. Real sensors fail. When a sensor fails, it does
not necessarily send a signal saying, “Oh, by the way, the data I’m about to send you is a
load of nonsense.” Instead, it simply sends the nonsense. The simplest kind of failure is
called a transient failure, where the sensor occasionally decides to send some nonsense. For
TRANSIENT FAILURE
example, the battery level sensor might have a habit of sending a zero when someone bumps
the robot, even if the battery is fully charged.
Let’s see what happens when a transient failure occurs with a Gaussian error model that
doesn’t accommodate such failures. Suppose, for example, that the robot is sitting quietly and
tive charge levels. The beta distribution is sometimes a better choice for a variable whose range is restricted.Dynamic Bayesian Networks
believe about Battery21? According to Bayes’ rule, the answer depends on both the sensor
probability of a large sensor error is signiﬁcantly less likely than the probability of a transition
this conclusion almost certain. If the transient failure then disappears and the reading returns
the expected value of Batteryt over time, using a discrete Gaussian error model.
is empty; presumably, then, it should send out a mayday signal and shut down. Alas, its
oversimpliﬁed sensor model has led it astray. How can this be ﬁxed? Consider a familiar
example from everyday human driving: on sharp curves or steep hills, one’s “fuel tank empty”
warning light sometimes turns on. Rather than looking for the emergency phone, one simply
recalls that the fuel gauge sometimes gives a very large error when the fuel is sloshing around
in the tank. The moral of the story is the following: for the system to handle sensor failure
properly, the sensor model must include the possibility of failure.
The simplest kind of failure model for a sensor allows a certain probability that the
sensor will return some completely incorrect value, regardless of the true state of the world.
which is presumably much larger than the probability assigned by the simple Gaussian error
model. Let’s call this the transient failure model. How does it help when we are faced
TRANSIENT FAILURE
MODEL
failure model can handle transient failures without a catastrophic change in beliefs.
So much for temporary blips. What about a persistent sensor failure? Sadly, failures of
in the robot gradually coming to believe that its battery is empty when in fact it may be that
Obviously, we would prefer the robot to believe that its battery meter is broken—if indeed
this is the more likely event.
Unsurprisingly, to handle persistent failure, we need a persistent failure model that
PERSISTENT
FAILURE MODEL
describes how the sensor behaves under normal conditions and after failure. To do this, we
need to augment the state of the system with an additional variable, say, BMBroken, that
describes the status of the battery meter. The persistence of failure must be modeled by anChapter
Probabilistic Reasoning over Time
E(Batteryt)
Time step t
E(Batteryt)
Time step
(a)
(b)
ure is handled well, but the persistent failure results in excessive pessimism about the battery
charge.
Battery
Battery0
BMeter
BMBroken
BMBroken
f
t
B
P(B )
E(Batteryt)
Time step
(a)
(b)
eling persistent failure of the battery sensor. (b) Upper curves: trajectories of the expected
value of Batteryt for the “transient failure” and “permanent failure” observations sequences.
Lower curves: probability trajectories for BMBroken given the two observation sequences.
arc linking BMBroken0 to BMBroken1. This persistence arc has a CPT that gives a small
PERSISTENCE ARC
broken once it breaks. When the sensor is OK, the sensor model for BMeter is identical to
of the actual battery charge.Dynamic Bayesian Networks
f
t
P(R  )
R0
P(R0)
f
t
P(U  )
R1
Umbrella1
Rain0
Rain1
P(R0)
f
t
P(U  )
R4
f
t
P(R  )
R3
Umbrella4
Rain4
f
t
P(U  )
R3
f
t
R
P(R  )
Umbrella3
Rain3
f
t
P(U  )
R2
f
t
R
P(R  )
Umbrella2
Rain2
f
t
P(U  )
R1
f
t
R
P(R  )
Umbrella1
Rain0
Rain1
the observation period.
performance on the two data sequences (temporary blip and persistent failure) is shown in
of the temporary blip, the probability that the sensor is broken rises signiﬁcantly after the
and stays there. Finally, once the sensor is known to be broken, the robot can only assume
that its battery discharges at the “normal” rate, as shown by the gradually descending level of
E(Battery t | . . . ).
So far, we have merely scratched the surface of the problem of representing complex
processes. The variety of transition models is huge, encompassing topics as disparate as
modeling the human endocrine system and modeling multiple vehicles driving on a freeway.
Sensor modeling is also a vast subﬁeld in itself, but even subtle phenomena, such as sensor
drift, sudden decalibration, and the effects of exogenous conditions (such as weather) on
sensor readings, can be handled by explicit representation within dynamic Bayesian networks.
Exact inference in DBNs
Having sketched some ideas for representing complex processes as DBNs, we now turn to
the question of inference. In a sense, this question has already been answered: dynamic
Bayesian networks are Bayesian networks, and we already have algorithms for inference in
Bayesian networks. Given a sequence of observations, one can construct the full Bayesian
network representation of a DBN by replicating slices until the network is large enough to
in the context of relational probability models, is called unrolling. (Technically, the DBN is
the last observation have no effect on inferences within the observation period and can be
omitted.) Once the DBN is unrolled, one can use any of the inference algorithms—variable
Unfortunately, a naive application of unrolling would not be particularly efﬁcient. If
we want to perform ﬁltering or smoothing with a long sequence of observations e1:t, theChapter
Probabilistic Reasoning over Time
unrolled network would require O(t) space and would thus grow without bound as more
observations were added. Moreover, if we simply run the inference algorithm anew each
time an observation is added, the inference time per update will also increase as O(t).
can be achieved if the computation can be done recursively. Essentially, the ﬁltering update
the distribution for the new time step. Summing out variables is exactly what the variable
with the variables in temporal order exactly mimics the operation of the recursive ﬁltering
update. (The same performance can be achieved by suitable modiﬁcations to the clustering
So much for the good news; now for the bad news: It turns out that the “constant” for
state variables. What happens is that, as the variable elimination proceeds, the factors grow
to include all the state variables (or, more precisely, all those state variables that have parents
of parents of any state variable.
Of course, this is much less than the cost of HMM updating, which is O(d2n), but it
is still infeasible for large numbers of variables. This grim fact is somewhat hard to accept.
What it means is that even though we can use DBNs to represent very complex temporal
processes with many sparsely connected variables, we cannot reason efﬁciently and exactly
about those processes. The DBN model itself, which represents the prior joint distribution
tion conditioned on an observation sequence—that is, the forward message—is generally not
factorable. So far, no one has found a way around this problem, despite the fact that many
important areas of science and engineering would beneﬁt enormously from its solution. Thus,
we must fall back on approximate methods.
Approximate inference in DBNs
adapted to the DBN context. (An MCMC ﬁltering algorithm is described brieﬂy in the notes
at the end of the chapter.) We will see, however, that several improvements are required over
the standard likelihood weighting algorithm before a practical method emerges.
work in topological order, weighting each sample by the likelihood it accords to the observed
rectly to an unrolled DBN, but this would suffer from the same problems of increasing timeDynamic Bayesian Networks
and space requirements per update as the observation sequence grows. The problem is that
the standard algorithm runs each sample in turn, all the way through the network. Instead,
iﬁed algorithm ﬁts the general pattern of ﬁltering algorithms, with the set of N samples as
the forward message. The ﬁrst key innovation, then, is to use the samples themselves as an
approximate representation of the current state distribution. This meets the requirement of a
“constant” time per update, although the constant depends on the number of samples required
to maintain an accurate approximation. There is also no need to unroll the DBN, because we
need to have in memory only the current slice and the next slice.
gorithm’s accuracy suffers if the evidence variables are “downstream” from the variables
being sampled, because in that case the samples are generated without any inﬂuence from
the later evidence. In fact, looking more carefully, we see that none of the state variables has
any evidence variables among its ancestors! Hence, although the weight of each sample will
depend on the evidence, the actual set of samples generated will be completely independent
pling process could still hallucinate endless days of sunshine. What this means in practice is
that the fraction of samples that remain reasonably close to the actual series of events (and
tion sequence. In other words, to maintain a given level of accuracy, we need to increase the
number of samples exponentially with t. Given that a ﬁltering algorithm that works in real
time can use only a ﬁxed number of samples, what happens in practice is that the error blows
up after a very small number of update steps.
Clearly, we need a better solution. The second key innovation is to focus the set of
away samples that have very low weight, according to the observations, while replicating
those that have high weight. In that way, the population of samples will stay reasonably close
to reality. If we think of samples as a resource for modeling the posterior distribution, then it
makes sense to use more samples in regions of the state space where the posterior is higher.
A family of algorithms called particle ﬁltering is designed to do just that. Particle
PARTICLE FILTERING
from the prior distribution P(X0). Then the update cycle is repeated for each time step:
sample is selected from the current population; the probability that a particular sample
is selected is proportional to its weight. The new samples are unweighted.Chapter
Probabilistic Reasoning over Time
inputs: e, the new incoming evidence
N , the number of samples to be maintained
dbn, a DBN with prior P(X0), transition model P(X1|X0), sensor model P(E1|X1)
persistent: S, a vector of samples of size N , initially generated from P(X0)
local variables: W , a vector of weights of size N
S[i] ←sample from P(X1 | X0 = S[i])
W [i] ←P(e | X1 = S[i])
return S
eration with state (the set of samples).
expected time. The step numbers refer to the description in the text.
true
false
(a) Propagate
(c) Resample
Raint
(b) Weight
¬rain. Each is propagated forward by sampling the next state through the transition model.
We can show that this algorithm is consistent—gives the correct probabilities as N tends
to inﬁnity—by considering what happens during one update cycle. We assume that the sample
population starts with a correct representation of the forward message f1:t = P(Xt | e1:t) at
time t. Writing N(xt | e1:t) for the number of samples occupying state xt after observations
e1:t have been processed, we therefore have
N(xt | e1:t)/N = P(xt | e1:t)Keeping Track of Many Objects
xt is the transition probability times the population of xt; hence, the total number of samples


xt
therefore
Now for the resampling step. Since each sample is replicated with probability proportional


xt


xt


xt
Particle ﬁltering is consistent, therefore, but is it efﬁcient? In practice, it seems that the
answer is yes: particle ﬁltering seems to maintain a good approximation to the true posterior
possible to prove that the approximation maintains bounded error with high probability. On
the practical side, the range of applications has grown to include many ﬁelds of science and
engineering; some references are given at the end of the chapter.
KEEPING TRACK OF MANY OBJECTS
The preceding sections have considered—without mentioning it—state estimation problems
involving a single object. In this section, we see what happens when two or more objects
generate the observations. What makes this case different from plain old state estimation is
temporal context. In the control theory literature, this is the data association problem—that
DATA ASSOCIATION
is, the problem of associating observation data with the objects that generated them.Chapter
Probabilistic Reasoning over Time
(d)
(c)
(b)
(a)
track termination
false alarm
detection
failure
track
initiation
(a) Observations made of object locations in 2D space over ﬁve time steps.
Each observation is labeled with the time step but does not identify the object that produced
it. (b–c) Possible hypotheses about the underlying object tracks. (d) A hypothesis for the
The data association problem was studied originally in the context of radar tracking,
where reﬂected pulses are detected at ﬁxed time intervals by a rotating radar antenna. At each
time step, multiple blips may appear on the screen, but there is no direct observation of which
with two blips per time step for ﬁve steps. Let the two blip locations at time t be e1
t and e2
t .
information.) Let us assume, for the time being, that exactly two aircraft, A and B, generated
the blips; their true positions are XA
t and XB
t . Just to keep things simple, we’ll also assume
that the each aircraft moves independently according to a known transition model—e.g., a
Suppose we try to write down the overall probability model for this scenario, just as
distribution factors into contributions for each time step as follows:
P(xA
P(xA
t
P(xA
i | xA
i | xB
i , e2
i | xA
i , xB
i ) .
We would like to factor the observation term P(e1
i , e2
i | xA
i , xB
i ) into a product of two terms,
one for each object, but this would require knowing which observation was generated by
which object. Instead, we have to sum over all possible ways of associating the observationsKeeping Track of Many Objects
objects and T time steps, there are (n!)T ways of doing it—an awfully large number.
Mathematically speaking, the “way of associating the observations with the objects”
is a collection of unobserved random variable that identify the source of each observation.
pendent of the states of the objects, xA
t and xB
t ). So we can condition the observation term
P(e1
i , e2
i | xA
i , xB
i ) on ωt and then simplify:
P(e1
i , e2
i | xA
i , xB
i ) =


ωi
P(e1
i , e2
i | xA
i , xB
i , ωi)P(ωi | xA
i , xB
i )
=


ωi
P(eωi(A)
i
| xA
i )P(eωi(B)
i
| xB
i )P(ωi | xA
i , xB
i )


ωi
P(eωi(A)
i
| xA
i )P(eωi(B)
i
| xB
i ) .
and sensor models for individual objects and observations.
As for all probability models, inference means summing out the variables other than
the query and the evidence. For ﬁltering in HMMs and DBNs, we were able to sum out the
took advantage of special properties of Gaussians. For data association, we are less fortunate.
There is no (known) efﬁcient exact algorithm, for the same reason that there is none for the
t | e1
ends up as a mixture of exponentially many distributions, one for each way of picking a
sequence of observations to assign to A.
As a result of the complexity of exact inference, many different approximate methods
have been used. The simplest approach is to choose a single “best” assignment at each time
step, given the predicted positions of the objects at the current time step. This assignment
associates observations with objects and enables the track of each object to be updated and
a prediction made for the next time step. For choosing the “best” assignment, it is common
FILTER
neighbor ﬁlter works well when the objects are well separated in state space and the prediction
uncertainty and observation error are small—in other words, when there is no possibility of
confusion. When there is more uncertainty as to the correct assignment, a better approach
is to choose the assignment that maximizes the joint probability of the current observations
HUNGARIAN
ALGORITHM
Any method that commits to a single best assignment at each time step fails miserably
signment, the prediction at the next time step may be signiﬁcantly wrong, leading to moreChapter
Probabilistic Reasoning over Time
(a)
(b)
Images from (a) upstream and (b) downstream surveillance cameras roughly
identiﬁed at both cameras.
a large collection of possible current assignments. An MCMC algorithm explores the space
space—and can change its mind about previous assignment decisions. Current MCMC data
association methods can handle many hundreds of objects in real time while giving a good
approximation to the true posterior distributions.
The scenario described so far involved n known objects generating n observations at
each time step. Real application of data association are typically much more complicated.
Often, the reported observations include false alarms (also known as clutter), which are not
FALSE ALARM
CLUTTER
caused by real objects. Detection failures can occur, meaning that no observation is reported
DETECTION FAILURE
for a real object. Finally, new objects arrive and old ones disappear. These phenomena, which
In this application, we are interested in two goals: estimating the time it takes, under current
trafﬁc conditions, to go from one place to another in the freeway system; and measuring
demand, i.e., how many vehicles travel between any two points in the system at particular
times of the day and on particular days of the week. Both goals require solving the data
association problem over a wide area with many cameras and tens of thousands of vehicles
per hour. With visual surveillance, false alarms are caused by moving shadows, articulated
vehicles, reﬂections in puddles, etc.; detection failures are caused by occlusion, fog, darkness,
and lack of visual contrast; and vehicles are constantly entering and leaving the freeway
system. Furthermore, the appearance of any given vehicle can change dramatically between
cameras depending on lighting conditions and vehicle pose in the image, and the transition
model changes as trafﬁc jams come and go. Despite these problems, modern data associationSummary
cause without it there is no way to combine multiple observations of any given object. When
objects in the world interact with each other in complex activities, understanding the world
SUMMARY
bilistic temporal processes. The main points are as follows:
sent the state at each point in time.
• Representations can be designed to satisfy the Markov property, so that the future
is independent of the past given the present. Combined with the assumption that the
process is stationary—that is, the dynamics do not change over time—this greatly
simpliﬁes the representation.
scribing the state evolution and a sensor model describing the observation process.
ing, and computing the most likely explanation. Each of these can be achieved using
simple, recursive algorithms whose run time is linear in the length of the sequence.
els, Kalman ﬁlters, and dynamic Bayesian networks (which include the other two as
special cases).
• Unless special assumptions are made, as in Kalman ﬁlters, exact inference with many
state variables is intractable. In practice, the particle ﬁltering algorithm seems to be an
effective approximation algorithm.
• When trying to keep track of many objects, uncertainty arises as to which observations
belong to which objects—the data association problem. The number of association
hypotheses is typically intractably large, but MCMC and particle ﬁltering algorithms
for data association work well in practice.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
oped what was later called the Markov assumption in his analysis of stochastic processes;Chapter
Probabilistic Reasoning over Time
known as the Kalman ﬁlter for forward inference in linear systems with Gaussian noise;
Kalman’s results had, however, been obtained previously by the Danish statistician Thorvold
saw the applicability of the method to the tracking of rocket trajectories, and the ﬁlter was
later implemented for the Apollo missions. Important results on smoothing were derived by
control theory approach to time series analysis.
was one of the main precursors of the general formulation of the EM algorithm (Dempster
have been several extensions to the basic HMM model, for example the Hierarchical HMM
model, replacing the single state variable of HMMs.
Dynamic Bayesian networks (DBNs) can be viewed as a sparse encoding of a Markov
process and were ﬁrst used in AI by Dean and Kanazawa (1989b), Nicholson and Brady
popularize DBNs and the probabilistic approach to planning and control within AI. MurphyBibliographical and Historical Notes
backward algorithm and Bayesian network propagation, was made explicitly by Smyth et
history. The ﬁrst sampling algorithms for particle ﬁltering (also called sequential Monte Carlo
and the resampling idea that is the core of particle ﬁltering appeared in a Russian control
EVIDENCE
REVERSAL
This allows the evidence to inﬂuence sample generation directly and was proved by Doucet
applied in many areas, including tracking complex motion patterns in video (Isard and Blake,
BLACKWELLIZED
PARTICLE FILTER
of state variables and, for each particle, performs exact inference on the remaining variables
sands of state variables. An application of RBPF to localization and mapping in robotics is
sequential Monte Carlo (SMC) algorithms, of which particle ﬁltering is the most important
SEQUENTIAL MONTE
CARLO
instance. Pierre Del Moral and colleagues have performed extensive theoretical analyses of
ample, Gibbs sampling can be applied directly to an unrolled DBN. To avoid the problem of
increasing update times as the unrolled network grows, the decayed MCMC ﬁlter (Marthi
DECAYED MCMC
FILTER
to a particular ﬁnitely parameterized family; if the projection and update steps take it outside
FACTORED
FRONTIER
tion algorithm for the factorial HMM, a DBN in which two or more independently evolving
FACTORIAL HMMChapter
Probabilistic Reasoning over Time
of other applications.
Data association for multitarget tracking was ﬁrst described in a probabilistic setting
Egerv´ary. The basic theorem had been derived previously, however, in an unpublished Latin
EXERCISES
process with an augmented set of state variables. Can this always be done parsimoniously,
i.e., without increasing the number of parameters needed to specify the transition model?
In this exercise, we examine what happens to the probabilities in the umbrella world
in the limit of long time sequences.
a. Suppose we observe an unending sequence of days on which the umbrella appears.
tonically toward a ﬁxed point. Calculate this ﬁxed point.
b. Now consider forecasting further and further into the future, given just the ﬁrst two
and plot the results. You should see that the probability converges towards a ﬁxed point.
b. Show a similar result for the second half of the sequence.Chapter
Probabilistic Reasoning over Time
Xt
St
Zt
A Bayesian network representation of a switching Kalman ﬁlter.
The
switching variable St is a discrete state variable whose value determines the transition
model for the continuous state variables Xt. For any discrete state i, the transition model
Markov model.
robot. How accurately can we track the robot’s path?
dictably among a set of k distinct “modes.” For example, an aircraft trying to evade a missile
can execute a series of distinct maneuvers that the missile may attempt to track. A Bayesian
a. Suppose that the discrete state St has k possible values and that the prior continuous
state estimate P(X0) is a multivariate Gaussian distribution. Show that the prediction
P(X1) is a mixture of Gaussians—that is, a weighted sum of Gaussians such that the
ture of km Gaussians.
c. What aspect of the temporal process do the weights in the Gaussian mixture represent?
The results in (a) and (b) show that the representation of the posterior grows without limit even
for switching Kalman ﬁlters, which are among the simplest hybrid dynamic models.
a. Plot the value of σ2
t as a function of t, given various values for σ2
x and σ2
z.
b. Show that the update has a ﬁxed point σ2 such that σ2
the value of σ2.
c. Give a qualitative explanation for what happens as σ2MAKING SIMPLE
DECISIONS
In which we see how an agent should make decisions so that it gets what it wants—
on average, at least.
In this chapter, we ﬁll in the details of how utility theory combines with probability theory to
believes and what it wants. Such an agent can make decisions in contexts in which uncertainty
agent has a continuous measure of outcome quality.
utility functions in more detail, and in particular their relation to individual quantities such as
we introduce a formalism called a decision network (also known as an inﬂuence diagram)
that extends Bayesian networks by incorporating actions and utilities. The remainder of the
chapter discusses issues that arise in applications of decision theory to expert systems.
COMBINING BELIEFS AND DESIRES UNDER UNCERTAINTY
ability of their immediate outcomes; that is, the environment is assumed to be episodic in the
the notation RESULT(s0, a) for the state that is the deterministic outcome of taking action a
in state s0. In this chapter we deal with nondeterministic partially observable environments.
Since the agent may not know the current state, we omit it and deﬁne RESULT(a) as a random
variable whose values are the possible outcome states. The probability of outcome s′, given
evidence observations e, is written
P(RESULT(a) = s′ | a, e) ,The Basis of Utility Theory
The agent’s preferences are captured by a utility function, U(s), which assigns a single
UTILITY FUNCTION
EXPECTED UTILITY
dence, EU (a|e), is just the average utility value of the outcomes, weighted by the probability
that the outcome occurs:
EU (a|e) =


s′
P(RESULT(a) = s′ | a, e) U(s′) .
The principle of maximum expected utility (MEU) says that a rational agent should choose
MAXIMUM EXPECTED
UTILITY
the action that maximizes the agent’s expected utility:
action = argmax
a
EU (a|e)
In a sense, the MEU principle could be seen as deﬁning all of AI. All an intelligent agent has
to do is calculate the various quantities, maximize utility over its actions, and away it goes.
But this does not mean that the AI problem is solved by the deﬁnition!
The MEU principle formalizes the general notion that the agent should “do the right
timating the state of the world requires perception, learning, knowledge representation, and
inference. Computing P(RESULT(a) | a, e) requires a complete causal model of the world
ing the outcome utilities U(s′) often requires searching or planning, because an agent may
not know how good a state is until it knows where it can get to from that state. So, decision
theory is not a panacea that solves the AI problem—but it does provide a useful framework.
The MEU principle has a clear relation to the idea of performance measures introduced
agent having a given percept history, and consider the different agents that we could design.
If an agent acts so as to maximize a utility function that correctly reﬂects the performance
measure, then the agent will achieve the highest possible performance score (averaged over
all the possible environments). This is the central justiﬁcation for the MEU principle itself.
While the claim may seem tautological, it does in fact embody a very important transition
from a global, external criterion of rationality—the performance measure over environment
histories—to a local, internal criterion involving the maximization of a utility function applied
to the next state.
THE BASIS OF UTILITY THEORY
Intuitively, the principle of Maximum Expected Utility (MEU) seems like a reasonable way
to make decisions, but it is by no means obvious that it is the only rational way. After all,
why should maximizing the average utility be so special? What’s wrong with an agent that
P(RESULT(a) = s′ | a, e) = P
s P(RESULT(s, a) = s′ | a)P(S0 = s | e).Chapter
Making Simple Decisions
maximizes the weighted sum of the cubes of the possible utilities, or tries to minimize the
worst possible loss? Could an agent act rationally just by expressing preferences between
states, without giving them numeric values? Finally, why should a utility function with the
required properties exist at all? We shall see.
Constraints on rational preferences
These questions can be answered by writing down some constraints on the preferences that a
rational agent should have and then showing that the MEU principle can be derived from the
constraints. We use the following notation to describe an agent’s preferences:
A ≻B
the agent prefers A over B.
A ∼B
the agent is indifferent between A and B.
A ≻∼B
the agent prefers A over B or is indifferent between them.
Now the obvious question is, what sorts of things are A and B? They could be states of the
world, but more often than not there is uncertainty about what is really being offered. For
example, an airline passenger who is offered “the pasta dish or the chicken” does not know
juicy or overcooked beyond recognition. We can think of the set of outcomes for each action
as a lottery—think of each action as a ticket. A lottery L with possible outcomes S1, . . . , Sn
LOTTERY
that occur with probabilities p1, . . . , pn is written
L = [p1, S1; p2, S2; . . . pn, Sn] .
In general, each outcome Si of a lottery can be either an atomic state or another lottery. The
primary issue for utility theory is to understand how preferences between complex lotteries
are related to preferences between the underlying states in those lotteries. To address this
issue we list six constraints that we require any reasonable preference relation to obey:
• Orderability: Given any two lotteries, a rational agent must either prefer one to the
ORDERABILITY
other or else rate the two as equally preferable. That is, the agent cannot avoid deciding.
Exactly one of (A ≻B), (B ≻A), or (A ∼B) holds.
• Transitivity: Given any three lotteries, if an agent prefers A to B and prefers B to C,
TRANSITIVITY
then the agent must prefer A to C.
(A ≻B) ∧(B ≻C) ⇒(A ≻C) .
• Continuity: If some lottery B is between A and C in preference, then there is some
CONTINUITY
probability p for which the rational agent will be indifferent between getting B for sure
• Substitutability: If an agent is indifferent between two lotteries A and B, then the
SUBSTITUTABILITY
agent is indifferent between two more complex lotteries that are the same except that BThe Basis of Utility Theory
is substituted for A in one of them. This holds regardless of the probabilities and the
other outcome(s) in the lotteries.
This also holds if we substitute ≻for ∼in this axiom.
• Monotonicity: Suppose two lotteries have the same two possible outcomes, A and B.
MONOTONICITY
If an agent prefers A to B, then the agent must prefer the lottery that has a higher
probability for A (and vice versa).
• Decomposability: Compound lotteries can be reduced to simpler ones using the laws
DECOMPOSABILITY
of probability. This has been called the “no fun in gambling” rule because it says that
two consecutive lotteries can be compressed into a single equivalent lottery, as shown
These constraints are known as the axioms of utility theory. Each axiom can be motivated
by showing that an agent that violates it will exhibit patently irrational behavior in some
situations. For example, we can motivate transitivity by making an agent with nontransitive
preferences give us all its money. Suppose that the agent has the nontransitive preferences
A ≻B ≻C ≻A, where A, B, and C are goods that can be freely exchanged. If the agent
currently has A, then we could offer to trade C for A plus one cent. The agent prefers C,
and so would be willing to make this trade. We could then offer to trade B for C, extracting
another cent, and ﬁnally trade A for B. This brings us back where we started from, except
until the agent has no money at all. Clearly, the agent has acted irrationally in this case.
Preferences lead to utility
Notice that the axioms of utility theory are really axioms about preferences—they say nothing
about a utility function. But in fact from the axioms of utility we can derive the following
• Existence of Utility Function: If an agent’s preferences obey the axioms of utility, then
there exists a function U such that U(A) > U(B) if and only if A is preferred to B,
and U(A) = U(B) if and only if the agent is indifferent between A and B.
U(A) > U(B) ⇔A ≻B
U(A) = U(B) ⇔A ∼B
• Expected Utility of a Lottery: The utility of a lottery is the sum of the probability of
each outcome times the utility of that outcome.
U([p1, S1; . . . ; pn, Sn]) =


i
piU(Si) .Chapter
Making Simple Decisions
A
B
C
p
q
A
B
C
p
A
B
C
is equivalent to
(a)
(b)
(a) A cycle of exchanges showing that the nontransitive preferences A ≻
B ≻C ≻A result in irrational behavior. (b) The decomposability axiom.
In other words, once the probabilities and utilities of the possible outcome states are speciﬁed,
the utility of a compound lottery involving those states is completely determined. Because the
outcome of a nondeterministic action is a lottery, it follows that an agent can act rationally—
that is, consistently with its preferences—only by choosing an action that maximizes expected
The preceding theorems establish that a utility function exists for any rational agent, but
they do not establish that it is unique. It is easy to see, in fact, that an agent’s behavior would
not change if its utility function U(S) were transformed according to
U′(S) = aU(S) + b ,
ranking on states—the numbers don’t matter. This is called a value function or ordinal
VALUE FUNCTION
utility function.
ORDINAL UTILITY
FUNCTION
It is important to remember that the existence of a utility function that describes an
agent’s preference behavior does not necessarily mean that the agent is explicitly maximizing
be generated in any number of ways. By observing a rational agent’s preferences, however,
an observer can construct the utility function that represents what the agent is actually trying
to achieve (even if the agent doesn’t know it).Utility Functions
UTILITY FUNCTIONS
Utility is a function that maps from lotteries to real numbers. We know there are some axioms
on utilities that all rational agents must obey. Is that all we can say about utility functions?
Strictly speaking, that is it: an agent can have any preferences it likes. For example, an agent
example, the agent might prefer prime numbers of dollars only when it owns the Pinto, but
when it owns the Mercedes, it might prefer more dollars to fewer. Fortunately, the preferences
of real agents are usually more systematic, and thus easier to deal with.
Utility assessment and utility scales
on his or her behalf, we must ﬁrst work out what the agent’s utility function is. This process,
often called preference elicitation, involves presenting choices to the agent and using the
PREFERENCE
ELICITATION
observed preferences to pin down the underlying utility function.
lar problem. A scale can be established by ﬁxing the utilities of any two particular outcomes,
just as we ﬁx a temperature scale by ﬁxing the freezing point and boiling point of water.
Typically, we ﬁx the utility of a “best possible prize” at U(S) = u⊤and a “worst possible
NORMALIZED
UTILITIES
Given a utility scale between u⊤and u⊥, we can assess the utility of any particular
STANDARD LOTTERY
The probability p is adjusted until the agent is indifferent between S and the standard lottery.
Assuming normalized utilities, the utility of S is given by p. Once this is done for each prize,
the utilities for all lotteries involving those prizes are determined.
ple’s lives are at stake. In such cases, u⊥is the value assigned to immediate death (or perhaps
many deaths). Although nobody feels comfortable with putting a value on human life, it is a
fact that tradeoffs are made all the time. Aircraft are given a complete overhaul at intervals
determined by trips and miles ﬂown, rather than after every trip. Cars are manufactured in
a way that trades off costs against accident survival rates. Paradoxically, a refusal to “put a
perience with a government agency that commissioned a study on removing asbestos from
schools. The decision analysts performing the study assumed a particular dollar value for the
remove the asbestos. The agency, morally outraged at the idea of setting the value of a life,
rejected the report out of hand. It then decided against asbestos removal—implicitly asserting
a lower value for the life of a child than that assigned by the analysts.Chapter
Making Simple Decisions
Some attempts have been made to ﬁnd out the value that people place on their own
lives. One common “currency” used in medical and safety analysis is the micromort, a
MICROMORT
one in a million chance of death. If you ask people how much they would pay to avoid a
will respond with very large numbers, perhaps tens of thousands of dollars, but their actual
behavior reﬂects a much lower monetary value for a micromort. For example, driving in a car
of studies have conﬁrmed a ﬁgure in this range across many individuals and risk types. Of
course, this argument holds only for small risks. Most people won’t agree to kill themselves
QALY
are willing to accept a shorter life expectancy to be restored to full health. For example,
kidney patients on average are indifferent between living two years on a dialysis machine and
one year at full health.
The utility of money
Utility theory has its roots in economics, and economics provides one obvious candidate
for a utility measure: money (or more speciﬁcally, an agent’s total net assets). The almost
universal exchangeability of money for all kinds of goods and services suggests that money
plays a signiﬁcant role in human utility functions.
It will usually be the case that an agent prefers more money to less, all other things being
equal. We say that the agent exhibits a monotonic preference for more money. This does
MONOTONIC
PREFERENCE
not mean that money behaves as a utility function, because it says nothing about preferences
between lotteries involving money.
Suppose you have triumphed over the other competitors in a television game show. The
on the ﬂip of a coin. If the coin comes up heads, you end up with nothing, but if it comes
pocket the million. Are you being irrational?
EXPECTED
MONETARY VALUE
not necessarily mean that accepting the gamble is a better decision. Suppose we use Sn to
denote the state of possessing total wealth $n, and that your current wealth is $k. Then the
expected utilities of the two actions of accepting and declining the gamble are
EU (Accept) =
To determine what to do, we need to assign utilities to the outcome states. Utility is not
directly proportional to monetary value, because the utility for your ﬁrst million is very high
(or so they say), whereas the utility for an additional million is smaller. Suppose you assignUtility Functions
U
$
$
(a)
(b)
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
U
The utility of money. (a) Empirical data for Mr. Beard over a limited range.
(b) A typical curve for the full range.
most likely have a utility function that is locally linear over the range of a few million more,
and thus would accept the gamble.
money was almost exactly proportional to the logarithm of the amount. (This idea was ﬁrst
consistent with a utility function
We should not assume that this is the deﬁnitive utility function for monetary value, but
it is likely that most people have a utility function that is concave for positive wealth. Going
into debt is bad, but preferences between different levels of debt can display a reversal of
ing, then for any lottery L, the utility of being faced with that lottery is less than the utility of
being handed the expected monetary value of the lottery as a sure thing:
U(L) < U(SEMV (L)) .
payoff that is less than the expected monetary value of a gamble. On the other hand, in theChapter
Making Simple Decisions
The value an agent will accept in lieu of a lottery is called the certainty equivalent of the
CERTAINTY
EQUIVALENT
equivalent is called the insurance premium. Risk aversion is the basis for the insurance
INSURANCE
PREMIUM
industry, because it means that insurance premiums are positive. People would rather pay a
small insurance premium than gamble the price of their house against the chance of a ﬁre.
From the insurance company’s point of view, the price of the house is very small compared
with the ﬁrm’s total reserves. This means that the insurer’s utility curve is approximately
linear over such a small region, and the gamble costs the company almost nothing.
Notice that for small changes in wealth relative to the current wealth, almost any curve
gambles with small sums, therefore, we expect risk neutrality. In a sense, this justiﬁes the
simpliﬁed procedure that proposed small gambles to assess probabilities and to justify the
The rational way to choose the best action, a∗, is to maximize expected utility:
a∗= argmax
a
EU (a|e) .
If we have calculated the expected utility correctly according to our probability model, and if
the probability model correctly reﬂects the underlying stochastic processes that generate the
outcomes, then, on average, we will get the utility we expect if the whole process is repeated
many times.
In reality, however, our model usually oversimpliﬁes the real situation, either because
we don’t know enough (e.g., when making a complex investment decision) or because the
computation of the true expected utility is too difﬁcult (e.g., when estimating the utility of
successor states of the root node in backgammon). In that case, we are really working with
estimates !
EU (a|e) of the true expected utility. We will assume, kindly perhaps, that the
estimates are unbiased, that is, the expected value of the error, E(!
EU (a|e) −EU (a|e))), is
UNBIASED
zero. In that case, it still seems reasonable to choose the action with the highest estimated
utility and to expect to receive that utility, on average, when the action is executed.
Unfortunately, the real outcome will usually be signiﬁcantly worse than we estimated,
even though the estimate was unbiased! To see why, consider a decision problem in which
negative (pessimistic) and some will be positive (optimistic). Because we select the action
with the highest utility estimate, we are obviously favoring the overly optimistic estimates,
and that is the source of the bias. It is a straightforward matter to calculate the distributionUtility Functions
Error in utility estimate
Plot of the error in each of k utility estimates and of the distribution of the
disappointment will be around twice the standard deviation in the estimates.
This tendency for the estimated expected utility of the best choice to be too high is
OPTIMIZER’S CURSE
decision analysts and statisticians. Serious manifestations include believing that an exciting
average returns will continue to have them (it’s been chosen to appear in the advertisement
out of k = dozens of funds in the company’s overall portfolio). It can even be the case that
what appears to be the best choice may not be, if the variance in the utility estimate is high:
selection processes, so taking the utility estimates at face value is a bad idea. We can avoid the
curse by using an explicit probability model P(!
EU | EU ) of the error in the utility estimates.
Given this model and a prior P(EU ) on what we might reasonably expect the utilities to be,
we treat the utility estimate, once obtained, as evidence and compute the posterior distribution
for the true utility using Bayes’ rule.
Human judgment and irrationality
Decision theory is a normative theory: it describes how a rational agent should act. A
NORMATIVE THEORY
descriptive theory, on the other hand, describes how actual agents—for example, humans—
DESCRIPTIVE
THEORY
really do act. The application of economic theory would be greatly enhanced if the two
coincided, but there appears to be some experimental evidence to the contrary. The evidenceChapter
Making Simple Decisions
between lotteries A and B and then between C and D, which have the following prizes:
A :
Most people consistently prefer B over A (taking the sure thing), and C over D (taking the
higher EMV). The normative analysis disagrees! We can see this most easily if we use the
words, there is no utility function that is consistent with these choices. One explanation for
CERTAINTY EFFECT
people are strongly attracted to gains that are certain. There are several reasons why this may
be so. First, people may prefer to reduce their computational burden; by choosing certain
outcomes, they don’t have to compute with probabilities. But the effect persists even when
the computations involved are very easy ones. Second, people may distrust the legitimacy of
and the ﬂip, but I may distrust the result if the ﬂip is done by someone with a vested interest
people may be accounting for their emotional state as well as their ﬁnancial state. People
REGRET
no money and feeling like a complete idiot, which is worse than just getting no money. So
perhaps people who choose B over A and C over D are not being irrational; they are just
idiot.
A related problem is the Ellsberg paradox. Here the prizes are ﬁxed, but the probabilities
are underconstrained. Your payoff will depend on the color of a ball chosen from an urn. You
know how many black and how many yellow. Again, you are asked whether you prefer lottery
A or B; and then C or D:
It should be clear that if you think there are more red than black balls then you should prefer
A over B and C over D; if you think there are fewer red than black you should prefer the
opposite. But it turns out that most people prefer A over B and also prefer D over C, even
though there is no state of the world for which this is rational. It seems that people have
AMBIGUITY
AVERSION
from the Nigerian bank account of a previously unknown deceased relative.Utility Functions
Yet another problem is that the exact wording of a decision problem can have a big
impact on the agent’s choices; this is called the framing effect. Experiments show that people
FRAMING EFFECT
exactly the same thing. This discrepancy in judgment has been found in multiple experiments
and is about the same whether the subjects were patients in a clinic, statistically sophisticated
business school students, or experienced doctors.
People feel more comfortable making relative utility judgments rather than absolute
ones. I may have little idea how much I might enjoy the various wines offered by a restaurant.
but which serves to skew upward the customer’s estimate of the value of all wines and make
ANCHORING EFFECT
If human informants insist on contradictory preference judgments, there is nothing that
automated agents can do to be consistent with them. Fortunately, preference judgments made
by humans are often open to revision in the light of further consideration. Paradoxes like
ter. In work at the Harvard Business School on assessing the utility of money, Keeney and
exhibit unacceptably large risk premiums for lotteries with a large spread. . . . Most of the
subjects, however, can reconcile their inconsistencies and feel that they have learned an
important lesson about how they want to behave. As a consequence, some subjects cancel
their automobile collision insurance and take out more term insurance on their lives.
EVOLUTIONARY
PSYCHOLOGY
for computing with probabilities and utilities, or something functionally equivalent; if so, the
required inputs would be obtained through accumulated experience of outcomes and rewards
rather than through linguistic presentations of numerical values. It is far from obvious that we
lem elicit different choices suggests that the decision problem itself is not getting through.
soning and decision making in “evolutionarily appropriate” forms; for example, instead of
ing factor in these experiments!) With decision problems posed in this way, people seem to
be much closer to rational behavior than previously suspected.Chapter
Making Simple Decisions
MULTIATTRIBUTE UTILITY FUNCTIONS
Decision making in the ﬁeld of public policy involves high stakes, in both money and lives.
icy makers must weigh the prevention of death and disability against the beneﬁt of the power
eration of the disruption caused by construction; the cost of land; the distance from centers
of population; the noise of ﬂight operations; safety issues arising from local topography and
weather conditions; and so on. Problems like these, in which outcomes are characterized by
two or more attributes, are handled by multiattribute utility theory.
MULTIATTRIBUTE
UTILITY THEORY
We will call the attributes X = X1, . . . , Xn; a complete vector of assignments will be
x = ⟨x1, . . . , xn⟩, where each xi is either a numeric value or a discrete value with an assumed
ordering on values. We will assume that higher values of an attribute correspond to higher
utilities, all other things being equal. For example, if we choose AbsenceOfNoise as an
examining cases in which decisions can be made without combining the attribute values into
a single utility value. Then we look at cases in which the utilities of attribute combinations
can be speciﬁed very concisely.
Dominance
Suppose that airport site S1 costs less, generates less noise pollution, and is safer than site S2.
One would not hesitate to reject S2. We then say that there is strict dominance of S1 over
STRICT DOMINANCE
S2. In general, if an option is of lower value on all attributes than some other option, it need
not be considered further. Strict dominance is often very useful in narrowing down the ﬁeld
That is ﬁne for the deterministic case, in which the attribute values are known for sure.
What about the general case, where the outcomes are uncertain? A direct analog of strict
dominance can be constructed, where, despite the uncertainty, all possible concrete outcomes
will probably occur even less often than in the deterministic case.
Fortunately, there is a more useful generalization called stochastic dominance, which
STOCHASTIC
DOMINANCE
occurs very frequently in real problems. Stochastic dominance is easiest to understand in
the context of a single attribute. Suppose we believe that the cost of siting the airport at S1 is
cost plotted as a negative value. Then, given only the information that utility decreases with
attributes measuring the difference from the ideal, one colder and one hotter. Utility would then be monotonically
increasing in each attribute.Multiattribute Utility Functions
(a) 
A
B
C
D
A
B
C
(b) 
This region
dominates A 
X2
X2
X1
X1
Strict dominance. (a) Deterministic: Option A is strictly dominated by B but
not by C or D. (b) Uncertain: A is strictly dominated by B but not by C.
Probability
Negative cost
S1
S2
Probability
Negative cost
S1
S2
(a)
(b)
mulative distributions for the negative cost of S1 and S2.
cost, we can say that S1 stochastically dominates S2 (i.e., S2 can be discarded). It is important
to note that this does not follow from comparing the expected costs. For example, if we knew
additional information on the utility of money. (It might seem odd that more information on
the cost of S1 could make the agent less able to decide. The paradox is resolved by noting
that in the absence of exact cost information, the decision is easier to make but is more likely
to be wrong.)
The exact relationship between the attribute distributions needed to establish stochastic
(See also Appendix A.) The cumulative distribution measures the probability that the cost is
less than or equal to any given amount—that is, it integrates the original distribution. If the
cumulative distribution for S1 is always to the right of the cumulative distribution for S2,Chapter
Making Simple Decisions
then, stochastically speaking, S1 is cheaper than S2. Formally, if two actions A1 and A2 lead
to probability distributions p1(x) and p2(x) on attribute X, then A1 stochastically dominates
A2 on X if
∀x
x

−∞
p1(x′) dx′ ≤
x

−∞
p2(x′) dx′ .
The relevance of this deﬁnition to the selection of optimal decisions comes from the following
property: if A1 stochastically dominates A2, then for any monotonically nondecreasing utility
function U(x), the expected utility of A1 is at least as high as the expected utility of A2.
Hence, if an action is stochastically dominated by another action on all attributes, then it can
be discarded.
The stochastic dominance condition might seem rather technical and perhaps not so
easy to evaluate without extensive probability calculations. In fact, it can be decided very
easily in many cases. Suppose, for example, that the construction transportation cost depends
on the distance to the supplier. The cost itself is uncertain, but the greater the distance, the
greater the cost. If S1 is closer than S2, then S1 will dominate S2 on cost. Although we
will not present them here, there exist algorithms for propagating this kind of qualitative
information among uncertain variables in qualitative probabilistic networks, enabling a
QUALITATIVE
PROBABILISTIC
NETWORKS
system to make rational decisions based on stochastic dominance, without using any numeric
values.
Preference structure and multiattribute utility
Suppose we have n attributes, each of which has d distinct possible values. To specify the
complete utility function U(x1, . . . , xn), we need dn values in the worst case. Now, the worst
tiattribute utility theory is based on the supposition that the preferences of typical agents have
much more structure than that. The basic approach is to identify regularities in the preference
behavior we would expect to see and to use what are called representation theorems to show
REPRESENTATION
THEOREM
that an agent with a certain kind of preference structure has a utility function
U(x1, . . . , xn) = F[f1(x1), . . . , fn(xn)] ,
where F is, we hope, a simple function such as addition. Notice the similarity to the use of
Bayesian networks to decompose the joint probability of several random variables.
Preferences without uncertainty
Let us begin with the deterministic case. Remember that for deterministic environments the
agent has a value function V (x1, . . . , xn); the aim is to represent this function concisely.
The basic regularity that arises in deterministic preference structures is called preference
independence. Two attributes X1 and X2 are preferentially independent of a third attribute
PREFERENCE
INDEPENDENCE
X3 if the preference between outcomes ⟨x1, x2, x3⟩and ⟨x′
particular value x3 for attribute X3.
Going back to the airport example, where we have (among other attributes) Noise,Multiattribute Utility Functions
and the same independence would hold for preferences between any other pair of values for
Noise and Cost. It is also apparent that Cost and Deaths are preferentially independent of
Noise and that Noise and Deaths are preferentially independent of Cost. We say that the
set of attributes {Noise, Cost, Deaths} exhibits mutual preferential independence (MPI).
MUTUAL
PREFERENTIAL
INDEPENDENCE
MPI says that, whereas each attribute may be important, it does not affect the way in which
one trades off the other attributes against each other.
dependent, then the agent’s preference behavior can be described as maximizing the function
V (x1, . . . , xn) =


i
Vi(xi) ,
where each Vi is a value function referring only to the attribute Xi. For example, it might
well be the case that the airport decision can be made using a value function
A value function of this type is called an additive value function. Additive functions are an
ADDITIVE VALUE
FUNCTION
situations. For n attributes, assessing an additive value function requires assessing n separate
sents an exponential reduction in the number of preference experiments that are needed. Even
when MPI does not strictly hold, as might be the case at extreme values of the attributes, an
additive value function might still provide a good approximation to the agent’s preferences.
This is especially true when the violations of MPI occur in portions of the attribute ranges
that are unlikely to occur in practice.
To understand MPI better, it helps to look at cases where it doesn’t hold. Suppose you
are at a medieval market, considering the purchase of some hunting dogs, some chickens,
and some wicker cages for the chickens. The hunting dogs are very valuable, but if you
don’t have enough cages for the chickens, the dogs will eat the chickens; hence, the tradeoff
between dogs and chickens depends strongly on the number of cages, and MPI is violated.
The existence of these kinds of interactions among various attributes makes it much harder to
assess the overall value function.
Preferences with uncertainty
ences between lotteries and to understand the resulting properties of utility functions, rather
than just value functions. The mathematics of this problem can become quite complicated,
so we present just one of the main results to give a ﬂavor of what can be done. The reader isChapter
Making Simple Decisions
The basic notion of utility independence extends preference independence to cover
UTILITY
INDEPENDENCE
tween lotteries on the attributes in X are independent of the particular values of the attributes
in Y. A set of attributes is mutually utility independent (MUI) if each of its subsets is
MUTUALLY UTILITY
INDEPENDENT
the airport attributes are MUI.
MUI implies that the agent’s behavior can be described using a multiplicative utility
MULTIPLICATIVE
UTILITY FUNCTION
looking at the case for three attributes. For conciseness, we use Ui to mean Ui(xi):
U = k1U1 + k2U2 + k3U3 + k1k2U1U2 + k2k3U2U3 + k3k1U3U1
+ k1k2k3U1U2U3 .
be developed independently of the other attributes, and this combination will be guaranteed
to generate the correct overall preferences. Additional assumptions are required to obtain a
purely additive utility function.
DECISION NETWORKS
In this section, we look at a general mechanism for making rational decisions. The notation
INFLUENCE DIAGRAM
more descriptive term decision network. Decision networks combine Bayesian networks
DECISION NETWORK
with additional node types for actions and utilities. We use airport siting as an example.
Representing a decision problem with a decision network
In its most general form, a decision network represents information about the agent’s current
state, its possible actions, the state that will result from the agent’s action, and the utility of
problem. It illustrates the three types of nodes used:
• Chance nodes (ovals) represent random variables, just as they do in Bayesian networks.
CHANCE NODES
The agent could be uncertain about the construction cost, the level of air trafﬁc and the
potential for litigation, and the Deaths, Noise, and total Cost variables, each of which
also depends on the site chosen. Each chance node has associated with it a conditional
distribution that is indexed by the state of the parent nodes. In decision networks, the
parent nodes can include decision nodes as well as chance nodes. Note that each of
construction costs, air trafﬁc levels, or litigation potentials.
• Decision nodes (rectangles) represent points where the decision maker has a choice of
DECISION NODESDecision Networks
U
Airport Site
Deaths
Noise
Cost
Litigation
Construction
Air Traffic
actions. In this case, the AirportSite action can take on a different value for each site
under consideration. The choice inﬂuences the cost, safety, and noise that will result.
deals with cases in which more than one decision must be made.
UTILITY NODES
as parents all variables describing the outcome that directly affect utility. Associated
with the utility node is a description of the agent’s utility as a function of the parent
attributes. The description could be just a tabulation of the function, or it might be a
parameterized additive or linear function of the attribute values.
A simpliﬁed form is also used in many cases. The notation remains identical, but the
chance nodes describing the outcome state are omitted. Instead, the utility node is connected
a utility function on outcome states, the utility node represents the expected utility associated
FUNCTION
siting problem.
future states, they can never have their values set as evidence variables. Thus, the simpliﬁed
version that omits these nodes can be used whenever the more general form can be used.
Although the simpliﬁed form contains fewer nodes, the omission of an explicit description
of the outcome of the siting decision means that it is less ﬂexible with respect to changes in
by a change in the conditional probability table associated with the Noise node, whereas a
change in the weight accorded to noise pollution in the utility function can be reﬂected byChapter
Making Simple Decisions
U
Airport Site
Litigation
Construction
Air Traffic
responding to outcome states have been factored out.
Evaluating decision networks
sion node. Once the decision node is set, it behaves exactly like a chance node that has been
set as an evidence variable. The algorithm for evaluating decision networks is the following:
(a) Set the decision node to that value.
(b) Calculate the posterior probabilities for the parent nodes of the utility node, using
a standard probabilistic inference algorithm.
(c) Calculate the resulting utility for the action.
more interesting.
THE VALUE OF INFORMATION
able information, is provided to the agent before it makes its decision. In practice, this isThe Value of Information
hardly ever the case. One of the most important parts of decision making is knowing what
questions to ask. For example, a doctor cannot expect to be provided with the results of all
Tests are often expensive and sometimes hazardous (both directly and because of associated
delays). Their importance depends on two factors: whether the test results would lead to a
signiﬁcantly better treatment plan, and how likely the various test results are.
This section describes information value theory, which enables an agent to choose
INFORMATION VALUE
THEORY
what information to acquire. We assume that, prior to selecting a “real” action represented
by the decision node, the agent can acquire the value of any of the potentially observable
chance variables in the model. Thus, information value theory involves a simpliﬁed form
of sequential decision making—simpliﬁed because the observation actions affect only the
agent’s belief state, not the external physical state. The value of any particular observation
must derive from the potential to affect the agent’s eventual physical action; and this potential
can be estimated directly from the decision model itself.
A simple example
rights. Let us assume further that exactly one of the blocks contains oil worth C dollars, while
Now suppose that a seismologist offers the company the results of a survey of block
the company be willing to pay for the information? The way to answer this question is to
examine what the company would do if it had the information:
case the company will buy a different block. Now the probability of ﬁnding oil in one
Now we can calculate the expected proﬁt, given the survey information:
n
n
×
C
information: the information is worth as much as the block itself.
The value of information derives from the fact that with the information, one’s course
of action can be changed to suit the actual situation. One can discriminate according to the
situation, whereas without the information, one has to do what’s best on average over the
possible situations. In general, the value of a given piece of information is deﬁned to be the
difference in expected value between best actions before and after information is obtained.Chapter
Making Simple Decisions
A general formula for perfect information
It is simple to derive a general mathematical formula for the value of information. We assume
that exact evidence can be obtained about the value of some random variable Ej (that is, we
VALUE OF PERFECT
INFORMATION
Let the agent’s initial evidence be e. Then the value of the current best action α is
deﬁned by
EU (α|e) = max
a


s′
P(RESULT(a) = s′ | a, e) U(s′) ,
and the value of the new best action (after the new evidence Ej = ej is obtained) will be
EU (αej|e, ej) = max
a


s′
P(RESULT(a) = s′ | a, e, ej) U(s′) .
But Ej is a random variable whose value is currently unknown, so to determine the value of
discovering Ej, given current information e we must average over all possible values ejk that
we might discover for Ej, using our current beliefs about its value:
VPI e(Ej) =


k
P(Ej = ejk|e) EU (αejk|e, Ej = ejk)
 
−EU (α|e) .
To get some intuition for this formula, consider the simple case where there are only two
actions, a1 and a2, from which to choose. Their current expected utilities are U1 and U2. The
information Ej = ejk will yield some new expected utilities U′
before we obtain Ej, we will have some probability distributions over the possible values of
U′
Suppose that a1 and a2 represent two different routes through a mountain range in
winter. a1 is a nice, straight highway through a low pass, and a2 is a winding dirt road over
the top. Just given this information, a1 is clearly preferable, because it is quite possible that
a2 is blocked by avalanches, whereas it is unlikely that anything blocks a1. U1 is therefore
clearly higher than U2. It is possible to obtain satellite reports Ej on the actual state of each
road that would give new expectations, U′
expense of obtaining satellite reports, because it is unlikely that the information derived from
them will change the plan. With no change, information has no value.
Now suppose that we are choosing between two different winding dirt roads of slightly
different lengths and we are carrying a seriously injured passenger. Then, even when U1
and U2 are quite close, the distributions of U′
possibility that the second route will turn out to be clear while the ﬁrst is blocked, and in this
in which we become somewhat more certain about a variable. We can do that by introducing another variable
about which we learn perfect information. For example, suppose we initially have broad uncertainty about the
information about the true Temperature, and the uncertainty due to measurement error is encoded in the sensorThe Value of Information
(c)
P(U | Ej)
U1
U2
U
(b)
P(U | Ej)
U1
U2
U
(a)
P(U | Ej)
U1
U2
U
tainly remain superior to a2, so the information is not needed. In (b), the choice is unclear and
the information is crucial. In (c), the choice is unclear, but because it makes little difference,
the information is less valuable. (Note: The fact that U2 has a high peak in (c) means that its
expected value is known with higher certainty than U1.)
case the difference in utilities will be very high. The VPI formula indicates that it might be
Finally, suppose that we are choosing between the two dirt roads in summertime, when
blockage by avalanches is unlikely. In this case, satellite reports might show one route to be
more scenic than the other because of ﬂowering alpine meadows, or perhaps wetter because
of errant streams. It is therefore quite likely that we would change our plan if we had the
information. In this case, however, the difference in value between the two routes is still
likely to be very small, so we will not bother to obtain the reports. This situation is shown in
In sum, information has value to the extent that it is likely to cause a change of plan
and to the extent that the new plan will be signiﬁcantly better than the old plan.
Properties of the value of information
One might ask whether it is possible for information to be deleterious: can it actually have
negative expected value? Intuitively, one should expect this to be impossible. After all, one
could in the worst case just ignore the information and pretend that one has never received it.
The expected value of information is nonnegative:
The theorem follows directly from the deﬁnition of VPI, and we leave the proof as an exercise
information can easily lead to a plan that turns out to be worse than the original plan if the
information happens to be misleading. For example, a medical test that gives a false positive
result may lead to unnecessary surgery; but that does not mean that the test shouldn’t be done.Chapter
Making Simple Decisions
It is important to remember that VPI depends on the current state of information, which
is why it is subscripted. It can change as more information is acquired. For any given piece
of evidence Ej, the value of acquiring it can go down (e.g., if another variable strongly
constrains the posterior for Ej) or up (e.g., if another variable provides a clue on which Ej
builds, enabling a new and better plan to be devised). Thus, VPI is not additive. That is,
VPI e(Ej, Ek) ̸= VPI e(Ej) + VPI e(Ek)
(in general) .
VPI is, however, order independent. That is,
VPI e(Ej, Ek) = VPI e(Ej) + VPI e,ej(Ek) = VPI e(Ek) + VPI e,ek(Ej) .
Order independence distinguishes sensing actions from ordinary actions and simpliﬁes the
problem of calculating the value of a sequence of sensing actions.
A sensible agent should ask questions in a reasonable order, should avoid asking questions
that are irrelevant, should take into account the importance of each piece of information in
relation to its cost, and should stop asking questions when that is appropriate. All of these
capabilities can be achieved by using the value of information as a guide.
ligently before acting. For now, we assume that with each observable evidence variable
Ej, there is an associated cost, Cost(Ej), which reﬂects the cost of obtaining the evidence
through tests, consultants, questions, or whatever. The agent requests what appears to be the
most efﬁcient observation in terms of utility gain per unit cost. We assume that the result of
the action Request(Ej) is that the next percept provides the value of Ej. If no observation is
worth its cost, the agent selects a “real” action.
The agent algorithm we have described implements a form of information gathering
that is called myopic. This is because it uses the VPI formula shortsightedly, calculating the
MYOPIC
value of information as if only a single evidence variable will be acquired. Myopic control
is based on the same heuristic idea as greedy search and often works well in practice. (For
example, it has been shown to outperform expert physicians in selecting diagnostic tests.)
persistent: D, a decision network
integrate percept into D
j ←the value that maximizes VPI (Ej) / Cost(Ej)
if VPI (Ej) > Cost(Ej)
return REQUEST(Ej)
else return the best action from D
edly selecting the observation with the highest information value, until the cost of the next
observation is greater than its expected beneﬁt.However, if there is no single evidence variable that will help a lot, a myopic agent might
hastily take an action when it would have been better to request two or more variables ﬁrst
and then take action. A better approach in this situation would be to construct a conditional
steps depending on the answer.
dent. People may respond better to a series of questions if they “make sense,” so some expert
systems are built to take this into account, asking questions in an order that maximizes the
total utility of the system and human rather than an order that maximizes value of information.
The ﬁeld of decision analysis, which evolved in the 1950s and 1960s, studies the application
DECISION ANALYSIS
of decision theory to actual decision problems. It is used to help make rational decisions in
important domains where the stakes are high, such as business, government, law, military
strategy, medical diagnosis and public health, engineering design, and resource management.
The process involves a careful study of the possible actions and outcomes, as well as the
preferences placed on each outcome. It is traditional in decision analysis to talk about two
roles: the decision maker states preferences between outcomes, and the decision analyst
DECISION MAKER
DECISION ANALYST
enumerates the possible actions and outcomes and elicits preferences from the decision maker
to determine the best course of action. Until the early 1980s, the main purpose of decision
analysis was to help humans make decisions that actually reﬂect their own preferences. As
more and more decision processes become automated, decision analysis is increasingly used
to ensure that the automated processes are behaving as desired.
ing decisions. Those systems that did recommend actions rather than providing opinions on
resentations of outcomes and preferences. The emergence of Bayesian networks in the late
ences from evidence. The addition of decision networks means that expert systems can be
developed that recommend optimal decisions, reﬂecting the preferences of the agent as well
as the available evidence.
ated with the consultation process: confusing likelihood and importance. A common strategy
lihood and report the most likely. Unfortunately, this can be disastrous! For the majority of
patients in general practice, the two most likely diagnoses are usually “There’s nothing wrong
with you” and “You have a bad cold,” but if the third most likely diagnosis for a given patient
is lung cancer, that’s a serious matter. Obviously, a testing or treatment plan should depend
both on probabilities and utilities. Current medical expert systems can take into account the
value of information to recommend tests, and then describe a differential diagnosis.Chapter
Making Simple Decisions
tems. As an example we consider the problem of selecting a medical treatment for a kind of
AORTIC
COARCTATION
ing the aorta with a balloon placed inside the artery), or medication. The problem is to decide
what treatment to use and when to do it: the younger the infant, the greater the risks of certain
can be created by a team consisting of at least one domain expert (a pediatric cardiologist)
and one knowledge engineer. The process can be broken down into the following steps:
Create a causal model. Determine the possible symptoms, disorders, treatments, and
outcomes. Then draw arcs between them, indicating what disorders cause what symptoms,
and what treatments alleviate what disorders. Some of this will be well known to the domain
expert, and some will come from the literature. Often the model will match well with the
informal graphical descriptions given in medical textbooks.
Simplify to a qualitative decision model. Since we are using the model to make
treatment decisions and not for other purposes (such as determining the joint probability of
are not involved in treatment decisions. Sometimes variables will have to be split or joined
to match the expert’s intuitions. For example, the original aortic coarctation model had a
Treatment variable with values surgery, angioplasty, and medication, and a separate variable
for Timing of the treatment. But the expert had a hard time thinking of these separately, so
Assign probabilities. Probabilities can come from patient databases, literature studies,
toms and other observations to the disease or other cause of the problems. Thus, in the early
years of building these systems, experts were asked for the probability of a cause given an
effect. In general they found this difﬁcult to do, and were better able to assess the probability
of an effect given a cause. So modern systems usually assess causal knowledge and encode it
directly in the Bayesian network structure of the model, leaving the diagnostic reasoning to
Assign utilities. When there are a small number of possible outcomes, they can be
can be done by the expert, but it is better if the patient (or in the case of infants, the patient’s
ponentially many outcomes, we need some way to combine them using multiattribute utility
functions. For example, we may say that the costs of various complications are additive.
Verify and reﬁne the model. To evaluate the system we need a set of correct (input,
GOLD STANDARD
this usually means assembling the best available doctors, presenting them with a few cases,Tachypnea
Dyspnea
Heart
Failure
Age
Tachycardia
Failure
To Thrive
Intercostal
Recession
megaly
Pulmonary
Crepitations
Cardiomegaly
Treatment
Intermediate
Result
Late
Result
Paraplegia
Aortic
Aneurysm
Paradoxical
Hypertension
Postcoarctectomy
Syndrome
Sex
CVA
Aortic
Dissection
Myocardial
Infarction
U
Inﬂuence diagram for aortic coarctation (courtesy of Peter Lucas).
and asking them for their diagnosis and recommended treatment plan. We then see how
well the system matches their recommendations. If it does poorly, we try to isolate the parts
that are going wrong and ﬁx them. It can be useful to run the system “backward.” Instead
of presenting the system with symptoms and asking for a diagnosis, we can present it with
a diagnosis such as “heart failure,” examine the predicted probability of symptoms such as
tachycardia, and compare with the medical literature.
Perform sensitivity analysis. This important step checks whether the best decision is
SENSITIVITY
ANALYSIS
sensitive to small changes in the assigned probabilities and utilities by systematically varying
those parameters and running the evaluation again. If small changes lead to signiﬁcantly
different decisions, then it could be worthwhile to spend more resources to collect better
data. If all variations lead to the same decision, then the agent will have more conﬁdence that
it is the right decision. Sensitivity analysis is particularly important, because one of the mainChapter
Making Simple Decisions
criticisms of probabilistic approaches to expert systems is that it is too difﬁcult to assess the
numerical probabilities required. Sensitivity analysis often reveals that many of the numbers
need be speciﬁed only very approximately. For example, we might be uncertain about the
conditional probability P(tachycardia | dyspnea), but if the optimal decision is reasonably
robust to small variations in the probability, then our ignorance is less of a concern.
SUMMARY
This chapter shows how to combine utility theory with probability to enable an agent to select
actions that will maximize its expected performance.
• Probability theory describes what an agent should believe on the basis of evidence,
utility theory describes what an agent wants, and decision theory puts the two together
to describe what an agent should do.
• We can use decision theory to build a system that makes decisions by considering all
possible actions and choosing the one that leads to the best expected outcome. Such a
system is known as a rational agent.
• Utility theory shows that an agent whose preferences between lotteries are consistent
more, the agent selects actions as if maximizing its expected utility.
tributes of states. Stochastic dominance is a particularly useful technique for making
unambiguous decisions, even without precise utility values for attributes.
• Decision networks provide a simple formalism for expressing and solving decision
problems. They are a natural extension of Bayesian networks, containing decision and
utility nodes in addition to chance nodes.
cision. The value of information is deﬁned as the expected improvement in utility
compared with making a decision without the information.
pared with pure inference systems. In addition to being able to make decisions, they
ommend contingency plans; and they can calculate the sensitivity of their decisions to
small changes in probability and utility assessments.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
To judge what one must do to obtain a good or avoid an evil, it is necessary to consider
not only the good and the evil in itself, but also the probability that it happens or does not
happen; and to view geometrically the proportion that all these things have together.Bibliographical and Historical Notes
Modern texts talk of utility rather than good and evil, but this statement correctly notes that
one should multiply utility by probability (“view geometrically”) to give expected utility,
and maximize that over all outcomes (“all these things”) to “judge what one must do.” It
publication of Pascal’s wager.
was the ﬁrst to realize the importance of preference measurement for lotteries, writing “the
weighing “pleasures” and “pains,” arguing that all decisions (not just monetary ones) could
be reduced to utility comparisons.
A good presentation of these axioms, in the course of a discussion on risk preference, is given
analysis and its relationship to human preference structures. The micromort utility measure
ing effects on the price one is willing to pay to avoid a risk of death versus the price one is
people willing to pay for a QALY? When it comes down to a speciﬁc case of saving oneself
or a family member, the number is approximately “whatever I’ve got.” But we can ask at a
societal level: suppose there is a vaccine that would yield X QALYs but costs Y dollars; is it
argument for a major change in public health policy on grounds of increased expected utility
measured in QALYs.
The optimizer’s curse was brought to the attention of decision analysts in a forceful
projected by analysts for their proposed course of action almost never materialized. They
trace this directly to the bias introduced by selecting an optimal action and show that a more
complete Bayesian analysis eliminates the problem. The same underlying concept has been
DISAPPOINTMENT
WINNER’S CURSE
timated the value of the object in question. Capen et al. quote a petroleum engineer on theChapter
Making Simple Decisions
Finally, behind both curses is the general phenomenon of regression to the mean, whereby
REGRESSION TO THE
MEAN
individuals selected on the basis of exceptional characteristics previously exhibited will, with
high probability, become less exceptional in future.
become a military analyst at the RAND Corporation and to leak documents known as The
Pentagon Papers, which contributed to the end of the Vietnam war and the resignation of
from expected utility theory.
literature, arguing that humans are quite rational in evolutionarily appropriate contexts. Its
adherents point out that irrationality is penalized by deﬁnition in an evolutionary context and
There has been a recent resurgence of interest in Bayesian models of cognition, overturning
ory. They describe early computer implementations of methods for eliciting the necessary
which includes a system called URP (Utility Reasoning Package) that can use a collection
of statements about preference independence and conditional independence to analyze the
structure of decision problems. The use of stochastic dominance together with qualitative
lationships might be used to provide a structured model of a utility function, in much the
along these lines.
ence since the 1950s. Until the 1980s, decision trees were the main tool used for representingBibliographical and Historical Notes
derivation of a decision tree from a decision network, but in general the tree is of exponential
network, without the creation of an intermediate decision tree. This algorithm was also one
of the ﬁrst to provide complete inference for multiply connected Bayesian networks. Zhang
duce the size of trees in practice; they use the term decision network for networks that use this
approach (although others use it as a synonym for inﬂuence diagram). Nilsson and Lauritzen
used to solve games that involve gathering information by opposing players, and Detwarasiti
for a team that shares goals but is unable to share all information perfectly. The collection
also appear regularly in the journals Management Science and Decision Analysis.
which information has value by virtue of its ability to affect decisions. Stratonovich’s work
ends with the remark “If information value theory and associated decision theoretic structures
do not in the future occupy a large part of the education of engineers, then the engineering
profession will ﬁnd that its traditional role of managing scientiﬁc and economic resources for
the beneﬁt of man has been forfeited to another profession.” To date, the implied revolution
in managerial methods has not occurred.
myopic value of information is intractable even in polytree networks. There are other cases—
vide a provably good approximation to the optimal sequence of observations (Krause et al.,
experiments in order of success probability divided by cost gives an optimal solution (Kadane
design of the journal Artiﬁcial Intelligence has depicted a decision network, although some
artistic license appears to have been taken with the direction of the arrows.Chapter
Making Simple Decisions
EXERCISES
(Adapted from David Heckerman.) This exercise concerns the Almanac Game, which
is used by decision analysts to calibrate numeric estimation. For each of the questions that
follow, give your best guess of the answer, that is, a number that you think is as likely to be
too high as it is to be too low. Also give your guess at a 25th percentile estimate, that is, a
low. Do the same for the 75th percentile. (Thus, you should give three estimates in all—low,
median, and high—for each question.)
c. Year in which Coronado discovered the Mississippi River.
f. Height of the Hoover Dam in feet.
The correct answers appear after the last exercise of this chapter. From the point of view of
decision analysis, the interesting thing is not how close your median guesses came to the real
was about half the time, then your bounds are accurate. But if you’re like most people, you
will be more sure of yourself than you should be, and fewer than half the answers will fall
within the bounds. With practice, you can calibrate yourself to give realistic bounds, and thus
be more useful in supplying information for decision making. Try this second set of questions
and see if there is any improvement:
a. Year of birth of Zsa Zsa Gabor.
b. Maximum distance from Mars to the sun in miles.
g. Year in which Roger Williams founded Providence, Rhode Island.
h. Height of Mt. Kilimanjaro in feet.
i. Length of the Brooklyn Bridge in feet.Chapter
Making Simple Decisions
c. Does network (i) assert that P(Wrapper|Shape) = P(Wrapper)?
d. What is the probability that your candy has a red wrapper?
e. In the box is a round candy with a red wrapper. What is the probability that its ﬂavor is
strawberry?
chovy candy is worth a. Write an expression for the value of an unopened candy box.
g. A new law prohibits trading of unwrapped candies, but it is still legal to trade wrapped
candies (out of the box). Is an unopened candy box now worth more than less than, or
the same as before?
the axiom of substitutability.
A (taking the sure thing), and C over D (taking the higher EMV) is not acting rationally,
according to utility theory. Do you think this indicates a problem for the agent, a problem for
the theory, or no problem at all? Explain.
tary value of a lottery ticket? When (if ever) is it rational to buy a ticket? Be precise—show an
proportionate number of lottery tickets. Do you think this is because they are worse decision
makers or because they have a different utility function? Consider the value of contemplating
the possibility of winning the lottery versus the value of contemplating becoming an action
hero while watching an adventure movie.
Assess your own utility for different incremental amounts of money by running a series
different values of M1 and M2, and vary p until you are indifferent between the two choices.
Plot the resulting utility function.
How much is a micromort worth to you? Devise a protocol to determine this. Ask
questions based both on paying to avoid risk and being paid to accept risk.
Let continuous variables X1, . . . , Xk be independently distributed according to the
same probability density function f(x). Prove that the density function for max{X1, . . . , Xk}
Economists often make use of an exponential utility function for money: U(x) =
ance reﬂects how likely an individual is to accept a lottery with a particular expected monetary
value (EMV) versus some certain payoff. As R (which is measured in the same units as x)Chapter
Making Simple Decisions
ing on the outcome of the tests, decide which car to buy. We will assume that the buyer is
deciding whether to buy car c1, that there is time to carry out at most one test, and that t1 is
a. Draw the decision network that represents this problem.
b. Calculate the expected net gain from buying c1, given no test.
c. Tests can be described by the probability that the car will pass or fail the test given that
the car is in good or bad shape. We have the following information:
Use Bayes’ theorem to calculate the probability that the car will pass (or fail) its test and
hence the probability that it is in good (or bad) shape given each possible test outcome.
d. Calculate the optimal decisions given either a pass or a fail, and their expected utilities.
e. Calculate the value of information of the test, and derive an optimal conditional plan
for the buyer.
a. Prove that the value of information is nonnegative and order independent.
ample, not wanting to know the sex of their baby when an ultrasound is done.
c. A function f on sets is submodular if, for any element x and any sets A and B such
SUBMODULARITY
formation, viewed as a function f on sets of possible observations, submodular? Prove
this or ﬁnd a counterexample.MAKING COMPLEX
DECISIONS
In which we examine methods for deciding what to do today, given that we may
decide again tomorrow.
problems, in which the utility of each action’s outcome was well known, we are concerned
here with sequential decision problems, in which the agent’s utility depends on a sequence
SEQUENTIAL
DECISION PROBLEM
of decisions. Sequential decision problems incorporate utilities, uncertainty, and sensing,
be solved to produce optimal behavior that balances the risks and rewards of acting in an
vironments, the notion of optimal behavior is complicated by the interactions among the
can be designed so that multiple agents can achieve a common goal.
SEQUENTIAL DECISION PROBLEMS
search problems, the actions available to the agent in each state are given by ACTIONS(s),
Down, Left, and Right. We assume for now that the environment is fully observable, so that
the agent always knows where it is.Chapter
Making Complex Decisions
START
(a)
(b)
decision problem. (b) Illustration of the transition model of the environment: the “intended”
to the intended direction. A collision with a wall results in no movement. The two terminal
If the environment were deterministic, a solution would be easy: [Up, Up, Right, Right,
Right]. Unfortunately, the environment won’t always go along with this solution, because the
actions are unreliable. The particular model of stochastic motion that we adopt is illustrated
of the time, the action moves the agent at right angles to the intended direction. Furthermore,
if the agent bumps into a wall, it stays in the same square. For example, from the start square
small chance of accidentally reaching the goal by going the other way around with probability
arise) describes the outcome of each action in each state. Here, the outcome is stochastic,
so we write P(s′ | s, a) to denote the probability of reaching state s′ if action a is done in
probability of reaching s′ from s depends only on s and not on the history of earlier states. For
To complete the deﬁnition of the task environment, we must specify the utility function
for the agent. Because the decision problem is sequential, the utility function will depend
on a sequence of states—an environment history—rather than on a single state. Later in
this section, we investigate how such utility functions can be speciﬁed in general; for now,
we simply stipulate that in each state s, the agent receives a reward R(s), which may be
REWARDSequential Decision Problems
environment history is just (for now) the sum of the rewards received. For example, if the
agent does not enjoy living in this environment and so wants to leave as soon as possible.
To sum up: a sequential decision problem for a fully observable, stochastic environment
with a Markovian transition model and additive rewards is called a Markov decision process,
MARKOV DECISION
PROCESS
or MDP, and consists of a set of states (with an initial state s0); a set ACTIONS(s) of actions
The next question is, what does a solution to the problem look like? We have seen that
any ﬁxed action sequence won’t solve the problem, because the agent might end up in a state
other than the goal. Therefore, a solution must specify what the agent should do for any state
that the agent might reach. A solution of this kind is called a policy. It is traditional to denote
POLICY
a policy by π, and π(s) is the action recommended by the policy π for state s. If the agent
has a complete policy, then no matter what the outcome of any action, the agent will always
know what to do next.
Each time a given policy is executed starting from the initial state, the stochastic nature
of the environment may lead to a different environment history. The quality of a policy is
therefore measured by the expected utility of the possible environment histories generated
by that policy. An optimal policy is a policy that yields the highest expected utility. We
OPTIMAL POLICY
use π∗to denote an optimal policy. Given π∗, the agent decides what to do by consulting
its current percept, which tells it the current state s, and then executing the action π∗(s). A
policy represents the agent function explicitly and is therefore a description of a simple reﬂex
that, because the cost of taking a step is fairly small compared with the penalty for ending
recommends taking the long way round, rather than taking the shortcut and thereby risking
is R(s, a, s′). This simpliﬁes the description of some environments but does not change the problem in anyChapter
Making Complex Decisions
(a)
(b)
the nonterminal states. (b) Optimal policies for four different ranges of R(s).
cause it never enters a terminal state. Surprisingly, it turns out that there are six other optimal
The careful balancing of risk and reward is a characteristic of MDPs that does not
decision problems. For this reason, MDPs have been studied in several ﬁelds, including
AI, operations research, economics, and control theory. Dozens of algorithms have been
most important algorithm families. First, however, we must complete our investigation of
utilities and policies for sequential decision problems.
Utilities over time
rewards for the states visited. This choice of performance measure is not arbitrary, but it is
not the only possibility for the utility function on environment histories, which we write as
is somewhat technical; the impatient reader may wish to skip to the next section.
The ﬁrst question to answer is whether there is a ﬁnite horizon or an inﬁnite horizon
FINITE HORIZON
INFINITE HORIZON
for decision making. A ﬁnite horizon means that there is a ﬁxed time N after which nothing
then there is plenty of time to take the safe route by going Left. So, with a ﬁnite horizon,Sequential Decision Problems
the optimal action in a given state could change over time. We say that the optimal policy
for a ﬁnite horizon is nonstationary. With no ﬁxed time limit, on the other hand, there is
NONSTATIONARY
POLICY
tion depends only on the current state, and the optimal policy is stationary. Policies for the
STATIONARY POLICY
does not necessarily mean that all state sequences are inﬁnite; it just means that there is no
containing a terminal state.
The next question we must decide is how to calculate the utility of state sequences. In
the terminology of multiattribute utility theory, each state si can be viewed as an attribute of
the state sequence [s0, s1, s2 . . .]. To obtain a simple expression in terms of the attributes, we
sumption is that the agent’s preferences between state sequences are stationary. Stationarity
STATIONARY
PREFERENCE
for preferences means the following: if two state sequences [s0, s1, s2, . . .] and [s′
begin with the same state (i.e., s0 = s′
the same way as the sequences [s1, s2, . . .] and [s′
prefer one future to another starting tomorrow, then you should still prefer that future if it
strong consequences: it turns out that under stationarity there are just two coherent ways to
assign utilities to sequences:
ADDITIVE REWARD
Uh([s0, s1, s2, . . .]) = R(s0) + R(s1) + R(s2) + · · · .
DISCOUNTED
REWARD
Uh([s0, s1, s2, . . .]) = R(s0) + γR(s1) + γ2R(s2) + · · · ,
DISCOUNT FACTOR
the preference of an agent for current rewards over future rewards. When γ is close
rewards are exactly equivalent to additive rewards, so additive rewards are a special
case of discounted rewards. Discounting appears to be a good model of both animal
and human preferences over time. A discount factor of γ is equivalent to an interest rate
For reasons that will shortly become clear, we assume discounted rewards in the remainder
Lurking beneath our choice of inﬁnite horizons is a problem: if the environment does
not contain a terminal state, or if the agent never reaches one, then all environment histories
will be inﬁnitely long, and utilities with additive, undiscounted rewards will generally beChapter
Making Complex Decisions
+∞utility is more difﬁcult. There are three solutions, two of which we have seen already:
and rewards are bounded by ±Rmax, we have
Uh([s0, s1, s2, . . .]) =
∞


γtR(st) ≤
∞


using the standard formula for the sum of an inﬁnite geometric series.
eventually, then we will never need to compare inﬁnite sequences. A policy that is
guaranteed to reach a terminal state is called a proper policy. With proper policies, we
PROPER POLICY
are proper, but the fourth is improper. It gains inﬁnite total reward by staying away from
the terminal states when the reward for the nonterminal states is positive. The existence
of improper policies can cause the standard algorithms for solving MDPs to fail with
additive rewards, and so provides a good reason for using discounted rewards.
AVERAGE REWARD
beyond the scope of this book.
In sum, discounted rewards present the fewest difﬁculties in evaluating state sequences.
Optimal policies and the utilities of states
Having decided that the utility of a given state sequence is the sum of discounted rewards
obtained during the sequence, we can compare policies by comparing the expected utilities
obtained when executing them. We assume the agent is in some initial state s and deﬁne St
(a random variable) to be the state the agent reaches at time t when executing a particular
policy π. (Obviously, S0 = s, the state the agent is in now.) The probability distribution over
state sequences S1, S2, . . . , is determined by the initial state s, the policy π, and the transition
model for the environment.
The expected utility obtained by executing π starting in s is given by
Uπ(s) = E
" ∞


γtR(St)
#
,
termined by s and π. Now, out of all the policies the agent could choose to execute starting in
s, one (or more) will have higher expected utilities than all the others. We’ll use π∗
s to denote
one of these policies:
π∗
s = argmax
π
Uπ(s) .Sequential Decision Problems
Remember that π∗
s is a policy, so it recommends an action for every state; its connection
with s in particular is that it’s an optimal policy when s is the starting state. A remarkable
consequence of using discounted utilities with inﬁnite horizons is that the optimal policy is
independent of the starting state. (Of course, the action sequence won’t be independent;
remember that a policy is a function specifying an action for each state.) This fact seems
intuitively obvious: if policy π∗
a is optimal starting in a and policy π∗
b is optimal starting in b,
then, when they reach a third state c, there’s no good reason for them to disagree with each
other, or with π∗
Given this deﬁnition, the true utility of a state is just U π∗(s)—that is, the expected
sum of discounted rewards if the agent executes an optimal policy. We write this as U(s),
R(s) are quite different quantities; R(s) is the “short term” reward for being in s, whereas
steps are required to reach the exit.
The utility function U(s) allows the agent to select actions by using the principle of
expected utility of the subsequent state:
π∗(s) = argmax


s′
P(s′ | s, a)U(s′) .
The next two sections describe algorithms for ﬁnding optimal policies.
rewards over time. The proof follows directly from the uniqueness of the utility function on states, as shown inChapter
Making Complex Decisions
VALUE ITERATION
In this section, we present an algorithm, called value iteration, for calculating an optimal
VALUE ITERATION
policy. The basic idea is to calculate the utility of each state and then use the state utilities to
select an optimal action in each state.
The Bellman equation for utilities
from that point onwards. From this, it follows that there is a direct relationship between the
utility of a state and the utility of its neighbors: the utility of a state is the immediate reward
for that state plus the expected discounted utility of the next state, assuming that the agent
chooses the optimal action. That is, the utility of a state is given by
U(s) = R(s) + γ max


s′
P(s′ | s, a)U(s′) .
BELLMAN EQUATION
solutions of the set of Bellman equations. In fact, they are the unique solutions, as we show
(Up)
(Left)
(Down)
(Right)
The value iteration algorithm
The Bellman equation is the basis of the value iteration algorithm for solving MDPs. If there
are n possible states, then there are n Bellman equations, one for each state. The n equations
contain n unknowns—the utilities of the states. So we would like to solve these simultaneous
equations to ﬁnd the utilities. There is one problem: the equations are nonlinear, because the
“max” operator is not a linear operator. Whereas systems of linear equations can be solved
quickly using linear algebra techniques, systems of nonlinear equations are more problematic.
One thing to try is an iterative approach. We start with arbitrary initial values for the utilities,
updating the utility of each state from the utilities of its neighbors. We repeat this until we
reach an equilibrium. Let Ui(s) be the utility value for state s at the ith iteration. The iteration
step, called a Bellman update, looks like this:
BELLMAN UPDATE


s′
P(s′ | s, a)Ui(s′) ,Value Iteration
inputs: mdp, an MDP with states S, actions A(s), transition model P(s′ | s, a),
rewards R(s), discount γ
ϵ, the maximum error allowed in the utility of any state
local variables: U , U ′, vectors of utilities for states in S, initially zero
δ, the maximum change in the utility of any state in an iteration
repeat
for each state s in S do
U ′[s] ←R(s) + γ
max


s′
P(s′ | s, a) U [s′]
return U
Utility estimates
Number of iterations
Iterations required
Discount factor γ
(a)
(b)
(a) Graph showing the evolution of the utilities of selected states using value
iteration. (b) The number of value iterations k required to guarantee an error of at most
ϵ = c · Rmax, for different values of c, as a function of the discount factor γ.
where the update is assumed to be applied simultaneously to all the states at each iteration.
If we apply the Bellman update inﬁnitely often, we are guaranteed to reach an equilibrium
equations. In fact, they are also the unique solutions, and the corresponding policy (obtainedChapter
Making Complex Decisions
the utilities start to increase. We can think of the value iteration algorithm as propagating
information through the state space by means of local updates.
Convergence of value iteration
We said that value iteration eventually converges to a unique set of solutions of the Bellman
matical ideas along the way, and we obtain some methods for assessing the error in the utility
function returned when the algorithm is terminated early; this is useful because it means that
we don’t have to run forever. This section is quite technical.
traction. Roughly speaking, a contraction is a function of one argument that, when applied to
CONTRACTION
two different inputs in turn, produces two output values that are “closer together,” by at least
some constant factor, than the original inputs. For example, the function “divide by two” is
a contraction, because, after we divide any two numbers by two, their difference is halved.
Notice that the “divide by two” function has a ﬁxed point, namely zero, that is unchanged by
the application of the function. From this example, we can discern two important properties
of contractions:
• A contraction has only one ﬁxed point; if there were two ﬁxed points they would not
get closer together when the function was applied, so it would not be a contraction.
• When the function is applied to any argument, the value must get closer to the ﬁxed
point (because the ﬁxed point does not move), so repeated application of a contraction
always reaches the ﬁxed point in the limit.
simultaneously to update the utility of every state. Let Ui denote the vector of utilities for all
the states at the ith iteration. Then the Bellman update equation can be written as
Next, we need a way to measure distances between utility vectors. We will use the max norm,
MAX NORM
which measures the “length” of a vector by the absolute value of its biggest component:
||U|| = max
s
|U(s)| .
ference between any two corresponding elements. The main result of this section is the
following: Let Ui and U′
i be any two utility vectors. Then we have
||B Ui −B U′
i|| ≤γ ||Ui −U′
i|| .
That is, the Bellman update is a contraction by a factor of γ on the space of utility vectors.
contractions in general, it follows that value iteration always converges to a unique solutionValue Iteration
tion. In particular, we can replace U′
B U = U. Then we obtain the inequality
||B Ui −U|| ≤γ ||Ui −U|| .
So, if we view ||Ui −U|| as the error in the estimate Ui, we see that the error is reduced by a
factor of at least γ on each iteration. This means that value iteration converges exponentially
ϵ/Rmax. The good news is that, because of the exponentially fast convergence, N does not
The error bound in the preceding paragraph gives some idea of the factors inﬂuencing
the run time of the algorithm, but is sometimes overly conservative as a method of deciding
when to stop the iteration. For the latter purpose, we can use a bound relating the error
to the size of the Bellman update on any given iteration. From the contraction property
much), then the error, compared with the true utility function, also is small. More precisely,
if
then
So far, we have analyzed the error in the utility function returned by the value iteration
algorithm. What the agent really cares about, however, is how well it will do if it makes its
decisions on the basis of this utility function. Suppose that after i iterations of value iteration,
the agent has an estimate Ui of the true utility U and obtains the MEU policy πi based on
as good as the optimal behavior? This is a crucial question for any real agent, and it turns out
that the answer is yes. Uπi(s) is the utility obtained if πi is executed starting in s, and the
policy loss ||Uπi −U|| is the most the agent can lose by executing πi instead of the optimal
POLICY LOSS
policy π∗. The policy loss of πi is connected to the error in Ui by the following inequality:
if
||Ui −U|| < ϵ
then
shows how the maximum error in Ui and the policy loss approach zero as the value iteration
Now we have everything we need to use value iteration in practice. We know that
it converges to the correct utilities, we can bound the error in the utility estimates if weChapter
Making Complex Decisions
stop after a ﬁnite number of iterations, and we can bound the policy loss that results from
executing the corresponding MEU policy. As a ﬁnal note, all of the results in this section
then a similar set of convergence results and error bounds can be derived whenever certain
technical conditions are satisﬁed.
POLICY ITERATION
In the previous section, we observed that it is possible to get an optimal policy even when
the utility function estimate is inaccurate. If one action is clearly better than all others, then
the exact magnitude of the utilities on the states involved need not be precise. This insight
suggests an alternative way to ﬁnd optimal policies. The policy iteration algorithm alternates
POLICY ITERATION
the following two steps, beginning from some initial policy π0:
• Policy evaluation: given a policy πi, calculate Ui = Uπi, the utility of each state if πi
POLICY EVALUATION
were to be executed.
POLICY
IMPROVEMENT
The algorithm terminates when the policy improvement step yields no change in the utilities.
At this point, we know that the utility function Ui is a ﬁxed point of the Bellman update, so
it is a solution to the Bellman equations, and πi must be an optimal policy. Because there are
only ﬁnitely many policies for a ﬁnite state space, and each iteration can be shown to yield a
The policy improvement step is obviously straightforward, but how do we implement
the standard Bellman equations (which is what value iteration does), because the action in
each state is ﬁxed by the policy. At the ith iteration, the policy πi speciﬁes the action πi(s) in
Number of iterations
Max error
Policy loss
The maximum error ||Ui −U|| of the utility estimates and the policy loss
||U πi −U||, as a function of the number of iterations of value iteration.Policy Iteration
the utility of s (under πi) to the utilities of its neighbors:
Ui(s) = R(s) + γ


s′
P(s′ | s, πi(s))Ui(s′) .
...
The important point is that these equations are linear, because the “max” operator has been
removed. For n states, we have n linear equations with n unknowns, which can be solved
exactly in time O(n3) by standard linear algebra methods.
For small state spaces, policy evaluation using exact solution methods is often the most
efﬁcient approach. For large state spaces, O(n3) time might be prohibitive. Fortunately, it
is not necessary to do exact policy evaluation. Instead, we can perform some number of
simpliﬁed value iteration steps (simpliﬁed because the policy is ﬁxed) to give a reasonably
good approximation of the utilities. The simpliﬁed Bellman update for this process is


s′
P(s′ | s, πi(s))Ui(s′) ,
and this is repeated k times to produce the next utility estimate. The resulting algorithm is
called modiﬁed policy iteration. It is often much more efﬁcient than standard policy iteration
MODIFIED POLICY
ITERATION
or value iteration.
inputs: mdp, an MDP with states S, actions A(s), transition model P(s′ | s, a)
local variables: U , a vector of utilities for states in S, initially zero
π, a policy vector indexed by state, initially random
repeat
unchanged? ←true
for each state s in S do
if
max


s′
P(s′ | s, a) U [s′] >


s′
P(s′ | s, π[s]) U [s′] then do
π[s] ←argmax


s′
P(s′ | s, a) U [s′]
unchanged? ←false
until unchanged?
return π
The policy iteration algorithm for calculating an optimal policy.Chapter
Making Complex Decisions
The algorithms we have described so far require updating the utility or policy for all
states at once. It turns out that this is not strictly necessary. In fact, on each iteration, we
pliﬁed value iteration) to that subset. This very general algorithm is called asynchronous
policy iteration. Given certain conditions on the initial policy and initial utility function,
ASYNCHRONOUS
POLICY ITERATION
asynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom
to choose any states to work on means that we can design much more efﬁcient heuristic
algorithms—for example, algorithms that concentrate on updating the values of states that
are likely to be reached by a good policy. This makes a lot of sense in real life: if one has no
intention of throwing oneself off a cliff, one should not spend time worrying about the exact
value of the resulting states.
PARTIALLY OBSERVABLE MDPS
was fully observable. With this assumption, the agent always knows which state it is in.
This, combined with the Markov assumption for the transition model, means that the optimal
policy depends only on the current state. When the environment is only partially observable,
the situation is, one might say, much less clear. The agent does not necessarily know which
state it is in, so it cannot execute the action π(s) recommended for that state. Furthermore, the
utility of a state s and the optimal action in s depend not just on s, but also on how much the
agent knows when it is in s. For these reasons, partially observable MDPs (or POMDPs—
PARTIALLY
OBSERVABLE MDP
We cannot avoid POMDPs, however, because the real world is one.
Deﬁnition of POMDPs
To get a handle on POMDPs, we must ﬁrst deﬁne them properly. A POMDP has the same
elements as an MDP—the transition model P(s′ | s, a), actions A(s), and reward function
a POMDP by adding a noisy or partial sensor instead of assuming that the agent knows its
location exactly. Such a sensor might measure the number of adjacent walls, which happens
problems and identiﬁed the belief state—the set of actual states the agent might be in—as a
key concept for describing and calculating solutions. In POMDPs, the belief state b becomes a
again this change is not fundamental.Partially Observable MDPs
actual state s by belief state b. The agent can calculate its current belief state as the conditional
probability distribution over the actual states given the sequence of percepts and actions so
belief state and the new evidence. For POMDPs, we also have an action to consider, but the
result is essentially the same. If b(s) was the previous belief state, and the agent does action
a and then perceives evidence e, then the new belief state is given by
b′(s′) = α P(e | s′)


s
P(s′ | s, a)b(s) ,
b′ = FORWARD(b, a, e) .
it’s quite likely (although not guaranteed, because both the motion and the sensor are noisy)
for the new belief state.
The fundamental insight required to understand POMDPs is this: the optimal action
depends only on the agent’s current belief state. That is, the optimal policy can be described
by a mapping π∗(b) from belief states to actions. It does not depend on the actual state the
agent is in. This is a good thing, because the agent does not know its actual state; all it knows
is the belief state. Hence, the decision cycle of a POMDP agent can be broken down into the
following three steps:
continuous space. An action changes the belief state, not just the physical state. Hence, the
action is evaluated at least in part according to the information the agent acquires as a result.
decision problem.
Let’s look more carefully at the outcome of actions. In particular, let’s calculate the
probability that an agent in belief state b reaches belief state b′ after executing action a. Now,
deterministic update to the belief state: b′ = FORWARD(b, a, e). Of course, the subsequent
percept is not yet known, so the agent might arrive in one of several possible belief states b′,
depending on the percept that is received. The probability of perceiving e, given that a wasChapter
Making Complex Decisions
performed starting in belief state b, is given by summing over all the actual states s′ that the
agent might reach:
P(e|a, b) =


s′
P(e|a, s′, b)P(s′|a, b)
=


s′
P(e | s′)P(s′|a, b)
=


s′
P(e | s′)


s
P(s′ | s, a)b(s) .
Let us write the probability of reaching b′ from b, given action a, as P(b′ | b, a)). Then that
gives us
P(b′ | b, a) = P(b′|a, b) =


e
P(b′|e, a, b)P(e|a, b)
=


e
P(b′|e, a, b)


s′
P(e | s′)


s
P(s′ | s, a)b(s) ,
We can also deﬁne a reward function for belief states (i.e., the expected reward for the actual
states the agent might be in):
ρ(b) =


s
b(s)R(s) .
thermore, it can be shown that an optimal policy for this MDP, π∗(b), is also an optimal policy
for the original POMDP. In other words, solving a POMDP on a physical state space can be
surprising if we remember that the belief state is always observable to the agent, by deﬁnition.
Notice that, although we have reduced POMDPs to MDPs, the MDP we obtain has a
tions describe a value iteration algorithm designed speciﬁcally for POMDPs and an online
Value iteration for POMDPs
state. With inﬁnitely many belief states, we need to be more creative. Consider an optimal
policy π∗and its application in a speciﬁc belief state b: the policy generates an action, then,
for each subsequent percept, the belief state is updated and a new action is generated, and so
about policies, let us think about conditional plans and how the expected utility of executing
a ﬁxed conditional plan varies with the initial belief state. We make two observations:Partially Observable MDPs
Then the expected utility of executing p in belief state b is just 
s b(s)αp(s), or b · αp
if we think of them both as vectors. Hence, the expected utility of a ﬁxed conditional
plan varies linearly with b; that is, it corresponds to a hyperplane in belief space.
plan with highest expected utility; and the expected utility of b under the optimal policy
is just the utility of that conditional plan:
U(b) = Uπ∗(b) = max
p
b · αp .
If the optimal policy π∗chooses to execute p starting at b, then it is reasonable to expect
that it might choose to execute p in belief states that are very close to b; in fact, if we
bound the depth of the conditional plans, then there are only ﬁnitely many such plans
and the continuous space of belief states will generally be divided into regions, each
corresponding to a particular conditional plan that is optimal in that region.
From these two observations, we see that the utility function U(b) on belief states, being the
maximum of a collection of hyperplanes, will be piecewise linear and convex.
state followed by the (discounted) reward for the state reached after the action:
their maximum is shown in bold. The bold line therefore represents the utility function for
linear utility function the optimal action is the ﬁrst action of the corresponding conditional
possible ﬁrst action, each possible subsequent percept, and then each way of choosing aChapter
Making Complex Decisions
Utility
[Stay]
[Go]
Utility
(a)
(b)
Utility
Utility
(c)
(d)
Notice that four of the plans, shown as dashed lines, are suboptimal across the entire belief
space—we say these plans are dominated, and they need not be considered further. There
DOMINATED PLAN
αp(s) = R(s) + γ


s′
P(s′ | s, a)


e
P(e | s′)αp.e(s′)
 
.Partially Observable MDPs
inputs: pomdp, a POMDP with states S, actions A(s), transition model P(s′ | s, a),
sensor model P(e | s), rewards R(s), discount γ
ϵ, the maximum error allowed in the utility of any state
local variables: U , U ′, sets of plans p with associated utility vectors αp
U ′ ←a set containing just the empty plan [ ], with α[ ](s) = R(s)
repeat
U ←U ′
U ′ ←the set of all plans consisting of an action and, for each possible next percept,
return U
The
as linear programs.
marily on how many plans get generated. Given |A| actions and |E| possible observations, it
states have even lower utility because the agent lacks the information needed to choose a
Given such a utility function, an executable policy can be extracted by looking at which
before eliminating the dominated ones. Since the 1970s, when this algorithm was developed,
there have been several advances including more efﬁcient forms of value iteration and various
kinds of policy iteration algorithms. Some of these are discussed in the notes at the end of the
hard, in fact—i.e., very hard indeed). Problems with a few dozen states are often infeasible.
The next section describes a different, approximate method for solving POMDPs, one basedChapter
Making Complex Decisions
At
Rt
Xt
Et
The generic structure of a dynamic decision network. Variables with known
values are shaded. The current time is t and the agent must decide what to do—that is, choose
a value for At. The network has been unrolled into the future for three steps and represents
Online agents for POMDPs
tic environments. The basic elements of the design are already familiar:
• The transition and sensor models are represented by a dynamic Bayesian network
• The dynamic Bayesian network is extended with decision and utility nodes, as used in
network, or DDN.
DYNAMIC DECISION
NETWORK
• A ﬁltering algorithm is used to incorporate each new percept and action and to update
the belief state representation.
• Decisions are made by projecting forward possible action sequences and choosing the
best one.
In the DBN, the single state St becomes a set of state variables Xt, and there may be
multiple evidence variables Et. We will use At to refer to the action at time t, so the transition
refer to the reward received at time t and Ut to refer to the utility of the state at time t. (Both
of these are random variables.) With this notation, a dynamic decision network looks like the
Dynamic decision networks can be used as inputs for any POMDP algorithm, including
that project action sequences forward from the current belief state in much the same way as do
three steps into the future; the current and future decisions A and the future observationsPartially Observable MDPs
. . .
...
...
...
...
...
...
. . .
...
...
...
...
...
...
. . .
...
. . .
...
...
. . .
...
. . .
At in P(Xt | E1:t)
will be taken in the belief state indicated.
E and rewards R are all unknown. Notice that the network includes nodes for the rewards
deterministic regardless of the actual outcome.
gorithm to the sequence of percepts and actions leading to it. In this way, the algorithm
A decision can be extracted from the search tree by backing up the utility values from
the leaves, taking an average at the chance nodes and taking the maximum at the decision
nodes. This is similar to the EXPECTIMINIMAX algorithm for game trees with chance nodes,
spond to belief states rather than actual states. The time complexity of an exhaustive searchChapter
Making Complex Decisions
plans generated by value iteration.) For problems in which the discount factor γ is not too
possible to approximate the averaging step at the chance nodes, by sampling from the set of
possible percepts instead of summing over all possible percepts. There are various other ways
tages compared with other, simpler agent designs presented in earlier chapters. In particular,
they handle partially observable, uncertain environments and can easily revise their “plans” to
handle unexpected evidence. With appropriate sensor models, they can handle sensor failure
and can plan to gather information. They exhibit “graceful degradation” under time pressure
and in complex environments, using various approximation techniques. So what is missing?
tional nature of the DDN language. We would like to be able to extend some of the ideas for
shown that this extension is possible and has signiﬁcant beneﬁts, as discussed in the notes at
the end of the chapter.
DECISIONS WITH MULTIPLE AGENTS: GAME THEORY
This chapter has concentrated on making decisions in uncertain environments. But what if
the uncertainty is due to other agents and the decisions they make? And what if the decisions
of those agents are in turn inﬂuenced by our decisions? We addressed this question once
used to ﬁnd optimal moves. In this section we study the aspects of game theory that analyze
GAME THEORY
games with simultaneous moves and other sources of partial observability. (Game theorists
use the terms perfect information and imperfect information rather than fully and partially
observable.) Game theory can be used in at least two ways:
utility for each decision (under the assumption that other agents are acting optimally
O and E, simultaneously display one or two ﬁngers. Let the total number of ﬁngers
be f. If f is odd, O collects f dollars from E; and if f is even, E collects f dollars
from O. Game theory can determine the best strategy against a rational player and the
facility (such as a restaurant or a biological weapons plant), and the facility operator chooses a day to hide all the
nasty stuff. The inspector wins if the days are different, and the facility operator wins if they are the same.Decisions with Multiple Agents: Game Theory
possible to deﬁne the rules of the environment (i.e., the game that the agents must
play) so that the collective good of all agents is maximized when each agent adopts the
help design the protocols for a collection of Internet trafﬁc routers so that each router
has an incentive to act in such a way that global throughput is maximized. Mechanism
design can also be used to construct intelligent multiagent systems that solve complex
problems in a distributed fashion.
neously and the result of the game is based on this single set of actions. (Actually, it is not
crucial that the actions take place at exactly the same time; what matters is that no player has
knowledge of the other players’ choices.) The restriction to a single move (and the very use
and wireless frequency spectrum rights, bankruptcy proceedings, product development and
pricing decisions, and national defense—situations involving billions of dollars and hundreds
PLAYER
capitalized names, like Alice and Bob or O and E.
• Actions that the players can choose. We will give actions lowercase names, like one or
ACTION
testify. The players may or may not have the same set of actions available.
• A payoff function that gives the utility to each player for each combination of actions
PAYOFF FUNCTION
matrix, a representation known as the strategic form (also called normal form). The
STRATEGIC FORM
O: one
O: two
E: one
E: two
Each player in a game must adopt and then execute a strategy (which is the name used in
STRATEGY
PURE STRATEGY
a pure strategy is just a single action. For many games an agent can do better with a mixed
MIXED STRATEGY
bution. The mixed strategy that chooses action a with probability p and action b otherwise
STRATEGY PROFILE
the strategy proﬁle, the game’s outcome is a numeric value for each player.
OUTCOMEChapter
Making Complex Decisions
A solution to a game is a strategy proﬁle in which each player adopts a rational strategy.
SOLUTION
We will see that the most important issue in game theory is to deﬁne what “rational” means
when each agent chooses only part of the strategy proﬁle that determines the outcome. It is
important to realize that outcomes are actual results of playing a game, while solutions are
theoretical constructs used to analyze a game. We will see that some games have a solution
only in mixed strategies. But that does not mean that a player must literally be adopting a
mixed strategy to be rational.
handed near the scene of a burglary and are interrogated separately. A prosecutor offers each
a deal: if you testify against your partner as the leader of a burglary ring, you’ll go free for
PRISONER’S
DILEMMA
or refuse? Being rational agents, Alice and Bob each want to maximize their own expected
utility. Let’s assume that Alice is callously unconcerned about her partner’s fate, so her utility
decreases in proportion to the number of years she will spend in prison, regardless of what
happens to Bob. Bob feels exactly the same way. To help reach a rational decision, they both
construct the following payoff matrix:
Alice:testify
Alice:refuse
Bob:testify
Bob:refuse
better. So in either case, it’s better for me to testify, so that’s what I must do.”
Alice has discovered that testify is a dominant strategy for the game. We say that a
DOMINANT
STRATEGY
strategy s for player p strongly dominates strategy s′ if the outcome for s is better for p than
STRONG
DOMINATION
the outcome for s′, for every choice of strategies by the other player(s). Strategy s weakly
dominates s′ if s is better than s′ on at least one strategy proﬁle and no worse on any other.
WEAK DOMINATION
A dominant strategy is a strategy that dominates all others. It is irrational to play a dominated
strategy, and irrational not to play a dominant strategy if one exists. Being rational, Alice
chooses the dominant strategy. We need just a bit more terminology: we say that an outcome
is Pareto optimal5 if there is no other outcome that all players would prefer. An outcome is
PARETO OPTIMAL
Pareto dominated by another outcome if all players would prefer the other outcome.
PARETO DOMINATED
If Alice is clever as well as rational, she will continue to reason as follows: Bob’s
dominant strategy is also to testify. Therefore, he will testify and we will both get ﬁve years.
When each player has a dominant strategy, the combination of those strategies is called a
dominant strategy equilibrium. In general, a strategy proﬁle forms an equilibrium if no
DOMINANT
STRATEGY
EQUILIBRIUM
EQUILIBRIUM
player can beneﬁt by switching strategies, given that every other player sticks with the sameDecisions with Multiple Agents: Game Theory
strategy. An equilibrium is essentially a local optimum in the space of policies; it is the top
of a peak that slopes downward along every dimension, where a dimension corresponds to a
player’s strategy choices.
librium. The general concept of equilibrium is now called Nash equilibrium in his honor.
NASH EQUILIBRIUM
games have Nash equilibria but no dominant strategies.
The dilemma in the prisoner’s dilemma is that the equilibrium outcome is worse for
both players than the outcome they would get if they both refused to testify. In other words,
for both of them to refuse to testify, but is is hard to see how rational agents can get there,
given the deﬁnition of the game. Either player contemplating playing refuse will realize that
he or she would do better by playing testify. That is the attractive power of an equilibrium
point. Game theorists agree that being a Nash equilibrium is a necessary condition for being
a solution—although they disagree whether it is a sufﬁcient condition.
It is easy enough to get to the (refuse, refuse) solution if we modify the game. For
example, we could change to a repeated game in which the players know that they will meet
again. Or the agents might have moral beliefs that encourage cooperation and fairness. That
means they have a different utility function, necessitating a different payoff matrix, making
it a different game. We will see later that agents with limited computational powers, rather
agent that knows that the other agent has limited rationality. In each case, we are considering
a different game than the one described by the payoff matrix above.
Now let’s look at a game that has no dominant strategy. Acme, a video game console
Meanwhile, the video game software producer Best needs to decide whether to produce its
if they disagree, as shown in the following payoff matrix:
Acme:bluray
Acme:dvd
Best:bluray
Best:dvd
There is no dominant strategy equilibrium for this game, but there are two Nash equilibria:
(bluray, bluray) and (dvd, dvd). We know these are Nash equilibria because if either player
unilaterally moves to a different strategy, that player will be worse off. Now the agents have
a problem: there are multiple acceptable solutions, but if each agent aims for a different
solution, then both agents will suffer. How can they agree on a solution? One answer isChapter
Making Complex Decisions
them the agents can either guess or communicate, which can be done either by establishing
a convention that orders the solutions before the game begins or by negotiating to reach a
mutually beneﬁcial solution during the game (which would mean including communicative
actions as part of a sequential game). Communication thus arises in game theory for exactly
need to communicate like this are called coordination games.
COORDINATION
GAME
A game can have more than one Nash equilibrium; how do we know that every game
ﬁngers is even, then O will want to switch; on the other hand (so to speak), if the total is odd,
then E will want to switch. Therefore, no pure strategy proﬁle can be an equilibrium and we
must look to mixed strategies instead.
know that the payoffs are equal and opposite, so we need consider the payoffs of only one
E to be the maximizer, so we can deﬁne the payoff matrix by the values UE(e, o)—the payoff
to E if E does e and O does o. (For convenience we call player E “her” and O “him.”) Von
Neumann’s method is called the the maximin technique, and it works as follows:
MAXIMIN
• Suppose we change the rules as follows: ﬁrst E picks her strategy and reveals it to
O. Then O picks his strategy, with knowledge of E’s strategy. Finally, we evaluate
Let’s suppose this gives an outcome UE,O. Clearly, this game favors O, so the true
utility U of the original game (from E’s point of view) is at least UE,O. For example,
• Now suppose we change the rules to force O to reveal his strategy ﬁrst, followed by E.
Then the minimax value of this game is UO,E, and because this game favors E we know
Combining these two arguments, we see that the true utility U of the solution to the original
game must satisfy
or in this case,
To pinpoint the value of U, we need to turn our analysis to mixed strategies. First, observe the
following: once the ﬁrst player has revealed his or her strategy, the second player might as
well choose a pure strategy. The reason is simple: if the second player plays a mixed strategy,Decisions with Multiple Agents: Game Theory
one
one
one
two
two
two
E
O
one
one
one
two
two
two
O
E
one
two
E
O
one
two
O
E
two
one
U
p
two
one
U
q
(a)
(b)
(c)
(d)
(e)
(f)
turns playing pure strategies. (c) and (d): Parameterized game trees where the ﬁrst player
plays a mixed strategy. The payoffs depend on the probability parameter (p or q) in the
mixed strategy. (e) and (f): For any particular value of the probability parameter, the second
player will choose the “better” of the two actions, so the value of the ﬁrst player’s mixed
strategy is given by the heavy lines. The ﬁrst player will choose the probability parameter for
the mixed strategy at the intersection point.
the utilities of the pure strategies, uone and utwo. This linear combination can never be better
than the better of uone and utwo, so the second player can just choose the better one.
With this observation in mind, the minimax trees can be thought of as having inﬁnitely
many branches at the root, corresponding to the inﬁnitely many mixed strategies the ﬁrstChapter
Making Complex Decisions
player can choose. Each of these leads to a node with two branches corresponding to the
pure strategies for the second player. We can depict these inﬁnite trees ﬁnitely by having one
“parameterized” choice at the root:
lines, as shown by the heavy lines in the ﬁgure. Therefore, the best that E can do at the
root is to choose p to be at the intersection point, which is where
⇒
shows that the best O can do at the root is to choose the intersection point:
⇒
which should be played by both players. This strategy is called the maximin equilibrium of
MAXIMIN
EQUILIBRIUM
the game, and is a Nash equilibrium. Note that each component strategy in an equilibrium
mixed strategy has the same expected utility. In this case, both one and two have the same
player who adopts the maximin strategy has two guarantees: First, no other strategy can do
better against an opponent who plays well (although some other strategies might be better at
exploiting an opponent who makes irrational mistakes). Second, the player continues to do
just as well even if the strategy is revealed to the opponent.
also possible for some pure strategies for the second player to be dominated by others, so
that they are not optimal against any strategy for the ﬁrst player. After removing all such
strategies (which might have to be done repeatedly), the optimal choice at the root is theDecisions with Multiple Agents: Game Theory
highest (or lowest) intersection point of the remaining hyperplanes. Finding this choice is
an example of a linear programming problem: maximizing an objective function subject to
linear constraints. Such problems can be solved by standard techniques in time polynomial
in the number of actions (and in the number of bits used to specify the reward function, if you
want to get technical).
The question remains, what should a rational agent actually do in playing a single game
maximin equilibrium strategy, and will assume that this is mutual knowledge with a rational
the agent could just decide to play one, or two. In either case, the expected payoff remains
payoff, but allowing the other agent to know that one has made such a unilateral decision does
affect the expected payoff, because then the opponent can adjust his strategy accordingly.
strategies. For example, ﬁrst try all strategy proﬁles where each player uses a single action,
then those where each player uses either one or two actions, and so on. This is exponential
ers these equations are linear and can be solved with basic linear programming techniques,
but for three or more players they are nonlinear and may be very difﬁcult to solve.
Repeated games
move game is the repeated game, in which players face the same choice repeatedly, but each
REPEATED GAME
time with knowledge of the history of all players’ previous choices. A strategy proﬁle for a
repeated game speciﬁes an action choice for each player at each time step for every possible
history of previous choices. As with MDPs, payoffs are additive over time.
Let’s consider the repeated version of the prisoner’s dilemma. Will Alice and Bob work
together and refuse to testify, knowing they will meet again? The answer depends on the
details of the engagement. For example, suppose Alice and Bob know that they must play
be a repeated game—that is, its outcome can have no effect on future rounds—and therefore
they will both choose the dominant strategy, testify, in that round. But once the 100th round
is determined, the 99th round can have no effect on subsequent rounds, so it too will have
a dominant strategy equilibrium at (testify, testify). By induction, both players will choose
We can get different solutions by changing the rules of the interaction. For example,Chapter
Making Complex Decisions
will be the last. Under these conditions, more cooperative behavior is possible. For example,
one equilibrium strategy is for each player to refuse unless the other player has ever played
testify. This strategy could be called perpetual punishment. Suppose both players have
PERPETUAL
PUNISHMENT
adopted this strategy, and this is mutual knowledge. Then as long as neither player has played
testify, then at any point in time the expected future total payoff for each player is
∞


total expected future payoff becomes
∞


Therefore, at every step, there is no incentive to deviate from (refuse, refuse). Perpetual
punishment is the “mutually assured destruction” strategy of the prisoner’s dilemma: once
either player decides to testify, it ensures that both players suffer a great deal. But it works
as a deterrent only if the other player believes you have adopted this strategy—or at least that
you might have adopted it.
ing with refuse and then echoing the other player’s previous move on all subsequent moves.
So Alice would refuse as long as Bob refuses and would testify the move after Bob testiﬁed,
but would go back to refusing if Bob did. Although very simple, this strategy has proven to
be highly robust and effective against a wide variety of strategies.
We can also get different solutions by changing the agents, rather than changing the
are playing a game with m > n total steps. The agents are thus incapable of representing
the number of remaining steps, and must treat it as an unknown. Therefore, they cannot do
the induction, and are free to arrive at the more favorable (refuse, refuse) equilibrium. In
this case, ignorance is bliss—or rather, having your opponent believe that you are ignorant is
bliss. Your success in these repeated games depends on the other player’s perception of you
as a bully or a simpleton, and not on your actual characteristics.
Sequential games
In the general case, a game consists of a sequence of turns that need not be all the same. Such
games are best represented by a game tree, which game theorists call the extensive form. The
EXTENSIVE FORM
PLAYER(s) that tells which player has the move, a function ACTIONS(s) enumerating the
possible actions, a function RESULT(s, a) that deﬁnes the transition to a new state, and a
partial function UTILITY(s, p), which is deﬁned only on terminal states, to give the payoff
for each player.
To represent stochastic games, such as backgammon, we add a distinguished player,
chance, that can take random actions. Chance’s “strategy” is part of the deﬁnition of theDecisions with Multiple Agents: Game Theory
game, speciﬁed as a probability distribution over actions (the other players get to choose
their own strategy). To represent games with nondeterministic actions, such as billiards, we
break the action into two pieces: the player’s action itself has a deterministic result, and then
chance has a turn to react to the action in its own capricious way. To represent simultaneous
players, but we have the option of asserting that the earlier player’s actions are not observable
to the subsequent players: e.g., Alice must choose refuse or testify ﬁrst, then Bob chooses,
but Bob does not know what choice Alice made at that time (we can also represent the fact
that the move is revealed later). However, we assume the players always remember all their
own previous actions; this assumption is called perfect recall.
observable game such as Kriegspiel can create a game tree over the space of belief states.
With that tree, we saw that in some cases a player can ﬁnd a sequence of moves (a strategy)
that leads to a forced checkmate regardless of what actual state we started in, and regardless of
what to do when there is no guaranteed checkmate. If the player’s best strategy depends
on the opponent’s strategy and vice versa, then minimax (or alpha–beta) by itself cannot
ﬁnd a solution. The extensive form does allow us to ﬁnd solutions because it represents the
belief states (game theorists call them information sets) of all players at once. From that
INFORMATION SETS
the payoff for that square. If we specify that no movement occurs when the two agents try
tions), then certain pure strategies can get stuck forever. Thus, agents need a mixed strategy
to perform well in this game: randomly choose between moving ahead and staying put. This
is exactly what is done to resolve packet collisions in Ethernet networks.
Next we’ll consider a very simple variant of poker. The deck has only four cards, two
aces and two kings. One card is dealt to each player. The ﬁrst player then has the option
depends on the cards: it is zero for both players if they have the same card; otherwise the
player with the king pays the stakes to the player with the ace.
depicted as an arrow with a label, corresponding to a raise, check, call, or fold, or, for chance,Chapter
Making Complex Decisions
r 
k 
r 
k 
r 
k 
r 
k 
c 
f 
c 
f 
c 
f 
c 
f 
Extensive form of a simpliﬁed version of poker.
information set is shown as two boxes rather than one.)
to normal form and then ﬁnding a solution (usually a mixed strategy) using standard linear
programming methods. That works in theory. But if a player has I information sets and
a actions per set, then that player will have aI pure strategies. In other words, the size ofDecisions with Multiple Agents: Game Theory
approach works only for very small game trees, on the order of a dozen states. A game like
games of perfect information with huge game trees by generating the tree incrementally, by
pruning some branches, and by heuristically evaluating nonterminal nodes. But that approach
does not work well for games with imperfect information, for two reasons: ﬁrst, it is harder
to prune, because we need to consider mixed strategies that combine multiple branches, not a
pure strategy that always chooses the best branch. Second, it is harder to heuristically evaluate
a nonterminal node, because we are dealing with information sets, not individual states.
SEQUENCE FORM
ponential. Rather than represent strategies, it represents paths through the tree; the number
of paths is equal to the number of terminal nodes. Standard linear programming methods
can again be applied to this representation. The resulting system can solve poker variants
game to a simpler form. For example, if I hold an ace and am considering the possibility that
the next card will give me a pair of aces, then I don’t care about the suit of the next card; any
suit will do equally well. This suggests forming an abstraction of the game, one in which
ABSTRACTION
can solve this smaller game; how will the solution to that game relate to the original game?
If no player is going for a ﬂush (or blufﬁng so), then the suits don’t matter to any player, and
the solution for the abstraction will also be a solution for the original game. However, if any
player is contemplating a ﬂush, then the abstraction will be only an approximate solution (but
it is possible to compute bounds on the error).
There are many opportunities for abstraction. For example, at the point in a game where
each player has two cards, if I hold a pair of queens, then the other players’ hands could be
abstracted into three classes: better (only a pair of kings or a pair of aces), same (pair of
queens) or worse (everything else). However, this abstraction might be too coarse. A better
abstraction would divide worse into, say, medium pair (nines through jacks), low pair, and
no pair. These examples are abstractions of states; it is also possible to abstract actions. For
We can also abstract over chance nodes, by considering only a subset of the possible deals.
This is equivalent to the rollout technique used in Go programs. Putting all these abstractions
current techniques.
Poker programs based on this approach can easily defeat novice and some experienced
human players, but are not yet at the level of master players. Part of the problem is that
the solution these programs approximate—the equilibrium solution—is optimal only against
an opponent who also plays the equilibrium strategy. Against fallible human players it is
important to be able to exploit an opponent’s deviation from the equilibrium strategy. AsChapter
Making Complex Decisions
Gautam Rao (aka “The Count”), the world’s leading online poker player, said (Billings et al.,
everyone.” However, good models of human fallability remain elusive.
In a sense, extensive game form is the one of the most complete representations we have
seen so far: it can handle partially observable, multiagent, stochastic, sequential, dynamic
However, there are two limitations of game theory. First, it does not deal well with continuous
states and actions (although there have been some extensions to the continuous case; for
example, the theory of Cournot competition uses game theory to solve problems where two
COURNOT
COMPETITION
companies choose prices for their products from a continuous space). Second, game theory
assumes the game is known. Parts of the game may be speciﬁed as unobservable to some of
the players, but it must be known what parts are unobservable. In cases in which the players
learn the unknown structure of the game over time, the model begins to break down. Let’s
examine each source of uncertainty, and whether each can be represented in game theory.
Actions: There is no easy way to represent a game where the players have to discover
what actions are available. Consider the game between computer virus writers and security
experts. Part of the problem is anticipating what action the virus writers will try next.
Strategies: Game theory is very good at representing the idea that the other players’
strategies are initially unknown—as long as we assume all agents are rational. The theory
itself does not say what to do when the other players are less than fully rational. The notion
of a Bayes–Nash equilibrium partially addresses this point: it is an equilibrium with respect
BAYES–NASH
EQUILIBRIUM
to a player’s prior probability distribution over the other players’ strategies—in other words,
it expresses a player’s beliefs about the other players’ likely strategies.
Chance: If a game depends on the roll of a die, it is easy enough to model a chance node
with uniform distribution over the outcomes. But what if it is possible that the die is unfair?
We can represent that with another chance node, higher up in the tree, with two branches for
“die is fair” and “die is unfair,” such that the corresponding nodes in each branch are in the
same information set (that is, the players don’t know if the die is fair or not). And what if we
suspect the other opponent does know? Then we add another chance node, with one branch
representing the case where the opponent does know, and one where he doesn’t.
Utilities: What if we don’t know our opponent’s utilities? Again, that can be modeled
with a chance node, such that the other agent knows its own utilities in each branch, but we
don’t. But what if we don’t know our own utilities? For example, how do I know if it is
rational to order the Chef’s salad if I don’t know how much I will like it? We can model that
with yet another chance node specifying an unobservable “intrinsic quality” of the salad.
Thus, we see that game theory is good at representing most sources of uncertainty—but
at the cost of doubling the size of the tree every time we add another node; a habit which
quickly leads to intractably large trees. Because of these and other problems, game theory
has been used primarily to analyze environments that are at equilibrium, rather than to control
agents within an environment. Next we shall see how it can help design environments.Mechanism Design
MECHANISM DESIGN
tion, we ask, “Given that agents pick rational strategies, what game should we design?” More
ing its own rational strategy, result in the maximization of some global utility function. This
MECHANISM DESIGN
to get rich, the total wealth of society will increase. But the examples we will discuss show
that proper mechanism design is necessary to keep the invisible hand on track. For collections
of agents, mechanism design allows us to construct smart systems out of a collection of more
limited systems—even uncooperative systems—in much the same way that teams of humans
can achieve goals beyond the reach of any individual.
Examples of mechanism design include auctioning off cheap airline tickets, routing
TCP packets between computers, deciding how medical interns will be assigned to hospitals,
and deciding how robotic soccer players will cooperate with their teammates. Mechanism
design became more than an academic subject in the 1990s when several nations, faced with
the problem of auctioning off licenses to broadcast in various frequency bands, lost hundreds
of millions of dollars in potential revenue as a result of poor mechanism design. Formally,
MECHANISM
CENTER
center uses to determine the payoffs to each agent, given their strategy choices.
Auctions
Let’s consider auctions ﬁrst. An auction is a mechanism for selling some goods to members
AUCTION
of a pool of bidders. For simplicity, we concentrate on auctions with a single item for sale.
Each bidder i has a utility value vi for having the item. In some cases, each bidder has a
private value for the item. For example, the ﬁrst item sold on eBay was a broken laser
such as auctioning drilling rights for an oil tract, the item has a common value—the tract
will produce some amount of money, X, and all bidders value a dollar equally—but there is
uncertainty as to what the actual value of X is. Different bidders have different information,
and hence different estimates of the item’s true value. In either case, bidders end up with their
own vi. Given vi, each bidder gets a chance, at the appropriate time or times in the auction,
to make a bid bi. The highest bid, bmax wins the item, but the price paid need not be bmax;
that’s part of the mechanism design.
ENGLISH AUCTION
which the center starts by asking for a minimum (or reserve) bid bmin. If some bidder isChapter
Making Complex Decisions
willing to pay that amount, the center then asks for bmin + d, for some increment d, and
continues up from there. The auction ends when nobody is willing to bid anymore; then the
last bidder wins the item, paying the price he bid.
How do we know if this is a good mechanism? One goal is to maximize expected
revenue for the seller. Another goal is to maximize a notion of global utility. These goals
overlap to some extent, because one aspect of maximizing global utility is to ensure that the
winner of the auction is the agent who values the item the most (and thus is willing to pay
the most). We say an auction is efﬁcient if the goods go to the agent who values them most.
EFFICIENT
price is set too high, the bidder who values it most may not bid, and if the reserve is set too
low, the seller loses net revenue.
Probably the most important things that an auction mechanism can do is encourage a
sion. Collusion is an unfair or illegal agreement by two or more bidders to manipulate prices.
COLLUSION
It can happen in secret backroom deals or tacitly, within the rules of the mechanism.
simultaneous auction (bids were taken on all ten blocks at the same time), using the rule that
said they “interpreted Mannesman’s ﬁrst bid as an offer.” Both parties could compute that
can each get half the blocks for 20M; let’s not spoil it by bidding the prices up higher.”
German government got less than they expected, because the two competitors were able to
use the bidding mechanism to come to a tacit agreement on how not to compete. From
the government’s point of view, a better result could have been obtained by any of these
the competitors could not communicate through their bids; or incentives to bring in a third
ders, although global utility can suffer if you count the cost of wasted time of bidders that
have no chance of winning. One way to encourage more bidders is to make the mechanism
easier for them. After all, if it requires too much research or computation on the part of the
bidders, they may decide to take their money elsewhere. So it is desirable that the bidders
have a dominant strategy. Recall that “dominant” means that the strategy works against all
other strategies, which in turn means that an agent can adopt it without regard for the other
ing other agents’ possible strategies. A mechanism where agents have a dominant strategy
REVELATION
PRINCIPLEMechanism Design
design is ﬁnding these equivalent mechanisms.
bidder with the highest value vi gets the goods at a price of bo + d, where bo is the highest
dominant strategy: keep bidding as long as the current cost is below your vi. The mechanism
have a lower bound on vi but not an exact amount.
one advantaged company that everyone agrees would be able to leverage existing customers
and infrastructure, and thus can make a larger proﬁt than anyone else. Potential competitors
pany can always bid higher. Thus, the competitors may not enter at all, and the advantaged
company ends up winning at the reserve price.
Another negative property of the English auction is its high communication costs. Either
cation lines; in either case they have to have the time available to go through several rounds of
bid auction. Each bidder makes a single bid and communicates it to the auctioneer, without
AUCTION
egy. If your value is vi and you believe that the maximum of all the other agents’ bids will
depends on your estimation of the other agents’ bids, requiring you to do more work. Also,
note that the agent with the highest vi might not win the auction. This is offset by the fact
that the auction is more competitive, reducing the bias toward an advantaged bidder.
AUCTION
VICKREY AUCTION
bid among the other agents, bo, is
ui =
 (vi −bo)
if bi > bo
otherwise.
To see that bi = vi is a dominant strategy, note that when (vi −bo) is positive, any bid
that wins the auction is optimal, and bidding vi in particular wins the auction. On the other
hand, when (vi −bo) is negative, any bid that loses the auction is optimal, and bidding vi in
bo < vi < bo + d. The chance of this can be made arbitrarily small by decreasing the increment d.
died of a heart attack three days later.Chapter
Making Complex Decisions
particular loses the auction. So bidding vi is optimal for all possible values of bo, and in fact,
vi is the only bid that has this property. Because of its simplicity and the minimal computation
requirements for both seller and bidders, the Vickrey auction is widely used in constructing
distributed AI systems. Also, Internet search engines conduct over a billion auctions a day
billion a year in goods, all using variants of the Vickrey auction. Note that the expected value
to the seller is bo, which is the same expected return as the limit of the English auction as
the increment d goes to zero. This is actually a very general result: the revenue equivalence
REVENUE
EQUIVALENCE
THEOREM
bidders have values vi known only to themselves (but know a probability distribution from
which those values are sampled), will yield the same expected revenue. This principle means
that the various mechanisms are not competing on the basis of revenue generation, but rather
on other qualities.
engines use a mechanism where they auction k slots for ads on a page. The highest bidder
wins the top spot, the second highest gets the second spot, and so on. Each winner pays the
searcher actually clicks on the ad. The top slots are considered more valuable because they
are more likely to be noticed and clicked on. Imagine that three bidders, b1, b2 and b3, have
bidding less than her true value in this case. In general, bidders in this multislot auction must
spend a lot of energy analyzing the bids of others to determine their best strategy; there is no
mechanism for this multislot problem, in which the winner of slot j pays the full price for
winner pays the price for the lower slot for the remaining clicks. In our example, b1 would
Another example of where auctions can come into play within AI is when a collection
show that this can be accomplished efﬁciently with an auction in which the agents bid for
roles in the joint plan.Mechanism Design
Common goods
Now let’s consider another type of game, in which countries set their policy for controlling
implementing the necessary changes, or they can continue to pollute, which gives them a net
(because the air is shared across countries). Clearly, the dominant strategy for each country
TRAGEDY OF THE
COMMONS
has to pay for using a common resource, then it tends to be exploited in a way that leads to
a lower total utility for all agents. It is similar to the prisoner’s dilemma: there is another
solution to the game that is better for all parties, but there appears to be no way for rational
agents to arrive at that solution.
The standard approach for dealing with the tragedy of the commons is to change the
mechanism to one that charges each agent for using the commons. More generally, we need
EXTERNALITIES
dividual agents’ transactions—are made explicit. Setting the prices correctly is the difﬁcult
part. In the limit, this approach amounts to creating a mechanism in which each agent is
effectively required to maximize global utility, but can do so by making a local decision. For
this example, a carbon tax would be an example of a mechanism that charges for use of the
commons in a way that, if implemented well, maximizes global utility.
As a ﬁnal example, consider the problem of allocating some common goods. Suppose a
city decides it wants to install some free wireless Internet transceivers. However, the number
of transceivers they can afford is less than the number of neighborhoods that want them. The
city wants to allocate the goods efﬁciently, to the neighborhoods that would value them the
most. That is, they want to maximize the global utility V = 
i vi. The problem is that if
they just ask each neighborhood council “how much do you value this free gift?” they would
all have an incentive to lie, and report a high value. It turns out there is a mechanism, known
GROVES
VCG
each agent to report its true utility and that achieves an efﬁcient allocation of the goods. The
trick is that each agent pays a tax equivalent to the loss in global utility that occurs because
of the agent’s presence in the game. The mechanism works like this:
the notation bi(A) to mean the result to i under this allocation: bi if i is in A (that is, i
B = 
i bi(A).
except i. We use the notation B−i = 
j̸=i bj(A). The center also computes (for each
i) the allocation that would maximize total global utility if i were not in the game; call
that sum W−i.Chapter
Making Complex Decisions
In this example, the VCG rule means that each winner would pay a tax equal to the highest
because they pay a tax that is less than their value, and all losers are as happy as they can be,
because they value the goods less than the required tax.
which is the value of getting an item, minus the tax:
vi(A) −(W−i −B−i) .
Here we distinguish the agent’s true utility, vi, from his reported utility bi (but we are trying
to show that a dominant strategy is bi = vi). Agent i knows that the center will maximize
global utility using the reported values,


j
bj(A) = bi(A) +


j̸=i
bj(A)
vi(A) +


j̸=i
bj(A) −W−i .
Since agent i cannot affect the value of W−i (it depends only on the other agents), the only
way i can make the center optimize what i wants is to report the true utility, bi = vi.
SUMMARY
This chapter shows how to use knowledge about the world to make decisions even when the
outcomes of an action are uncertain and the rewards for acting might not be reaped until many
actions have passed. The main points are as follows:
• Sequential decision problems in uncertain environments, also called Markov decision
processes, or MDPs, are deﬁned by a transition model specifying the probabilistic
outcomes of actions and a reward function specifying the reward in each state.
• The utility of a state sequence is the sum of all the rewards over the sequence, possibly
discounted over time. The solution of an MDP is a policy that associates a decision
with every state that the agent might reach. An optimal policy maximizes the utility of
the state sequences encountered when it is executed.
• The utility of a state is the expected utility of the state sequences encountered when
an optimal policy is executed, starting in that state. The value iteration algorithm for
solving MDPs works by iteratively solving the equations relating the utility of each state
to those of its neighbors.
• Policy iteration alternates between calculating the utilities of states under the current
policy and improving the current policy with respect to the current utilities.
• Partially observable MDPs, or POMDPs, are much more difﬁcult to solve than are
MDPs. They can be solved by conversion to an MDP in the continuous space of beliefBibliographical and Historical Notes
states; both value iteration and policy iteration algorithms have been devised. Optimal
fore make better decisions in the future.
uses a dynamic decision network to represent the transition and sensor models, to
update its belief state, and to project forward possible action sequences.
• Game theory describes rational behavior for agents in situations in which multiple
ﬁles in which no agent has an incentive to deviate from the speciﬁed strategy.
• Mechanism design can be used to set the rules by which agents will interact, in order
to maximize some global utility through the operation of individually rational agents.
Sometimes, mechanisms exist that achieve this goal without requiring each agent to
consider the choices made by other agents.
forcement learning methods that allow an agent to improve its behavior from experience in
sequential, uncertain environments.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Richard Bellman developed the ideas underlying the modern approach to sequential decision
describe results on the computational complexity of MDPs.
for solving MDPs played a signiﬁcant role in introducing MDPs into the AI community, as
similar ideas, but was not taken up to the same extent.) The connection between MDPs and
STRIPS operators provide a compact representation for transition models (see also Wellman,Chapter
Making Complex Decisions
the combinatorics of large state spaces by using a limited search horizon and abstract states.
Heuristics based on the value of information can be used to select areas of the state space
ity. Agents using this approach can tailor their effort to handle time pressure and generate
some interesting behaviors such as using familiar “beaten paths” to ﬁnd their way around the
state space quickly without having to recompute optimal decisions at each point.
pressive representations that can accommodate much larger problems than the traditional
atomic representations based on transition matrices. The use of a dynamic Bayesian network
to represent transition models was an obvious idea, but work on factored MDPs (Boutilier
FACTORED MDP
representations of the value function with provable improvements in complexity. Relational
RELATIONAL MDP
representations to handle domains with many related objects.
The observation that a partially observable MDP can be transformed into a regular MDP
the exact solution of POMDPs—essentially the value iteration algorithm presented in this
simistic conclusions about the feasibility of solving large problems. The ﬁrst signiﬁcant
value iteration methods that, at each iteration, generate conditional plans and α-vectors for
need generate plans for only a small, randomly selected subset of points to improve on the
within a factored representation.
ideas for an agent architecture using dynamic decision networks were proposed by DeanBibliographical and Historical Notes
such agents can be scaled up and identiﬁes a number of open research issues.
The roots of game theory can be traced back to proposals made in the 17th century
by Christiaan Huygens and Gottfried Leibniz to study competitive and cooperative human
interactions scientiﬁcally and mathematically. Throughout the 19th century, several leading
had, the year before, suggested a form of minimax search for games, albeit an incorrect one).
deﬁning book for game theory. Publication of the book was delayed by the wartime paper
shortage until a member of the Rockefeller family personally subsidized its publication.
The prisoner’s dilemma was invented as a classroom exercise by Albert W. Tucker in
to the ﬁeld and describe a working system for representing and solving sequential games.
The use of abstraction to reduce a game tree to a size that can be solved with Koller’s
show that the abstraction approach is vulnerable to making systematic errors in approximating
the equilibrium solution, meaning that the whole approach is on shaky ground: it works for
Game theory and MDPs are combined in the theory of Markov games, also calledChapter
Making Complex Decisions
opponent’s strategy is changing, how should you react? Textbooks on game theory from
they were involved in.
Trading Agents Competition (TAC), in which agents try to make the best proﬁt on a series
issues in auctions often appear in the ACM Conferences on Electronic Commerce.
EXERCISES
Markov model.
tained in any state in the sequence. Show that this utility function does not result in stationary
preferences between state sequences. Is it still possible to deﬁne a utility function on states
such that MEU decision making gives optimal behavior?
Sometimes MDPs are formulated with a reward function R(s, a) that depends on the
action taken or with a reward function R(s, a, s′) that also depends on the outcome state.
a. Write the Bellman equations for these formulations.Chapter
Making Complex Decisions
· · ·
· · ·
· · ·
Start
r
(a)
(b)
a choice of two deterministic actions, Up or Down, but in the other states the agent has one
deterministic action, Right. Assuming a discounted reward function, for what values of the
discount γ should the agent choose Up and for which Down? Compute the utility of each
situations in which one must weigh the value of an immediate action versus the potential
and b. The transition model is as follows:
Answer the following questions:
b. Apply policy iteration, showing each step in full, to determine the optimal policy and
c. What happens to policy iteration if the initial policy has action a in both states? Does
discounting help? Does the optimal policy depend on the discount factor?
raphy of the environment is easily altered. Some code for doing this is already in the
online code repository.Chapter
Making Complex Decisions
A Dutch auction is similar in an English auction, but rather than starting the bidding
at a low price and increasing, in a Dutch auction the seller starts at a high price and gradually
lowers the price until some buyer is willing to accept that price. (If multiple bidders accept
the price, one is arbitrarily chosen as the winner.) More formally, the seller begins with a
price p and gradually lowers p by increments of d until at least one buyer accepts the price.
Assuming all bidders act rationally, is it true that for arbitrarily small d, a Dutch auction will
always result in the bidder with the highest value for the item obtaining the item? If so, show
mathematically why. If not, explain how it may be possible for the bidder with highest value
for the item not to obtain it.
Assuming all agents are rational, what is the expected revenue to the auctioneer for this
were playing too conservatively in overtime (to avoid a loss), and it would be more exciting
b. Suppose that at a certain time t in a game, the home team has probability p of winning
Give equations for the expected value for the home and visiting teams.
c. Imagine that it were legal and ethical for the two teams to enter into a pact where they
agree that they will skate to a tie in regulation time, and then both try in earnest to win
in overtime. Under what conditions, in terms of p and q, would it be rational for both
teams to agree to this pact?
vative play after the rule change?LEARNING FROM
EXAMPLES
In which we describe agents that can improve their behavior through diligent
study of their own experiences.
An agent is learning if it improves its performance on future tasks after making observations
LEARNING
about the world. Learning can range from the trivial, as exhibited by jotting down a phone
number, to the profound, as exhibited by Albert Einstein, who inferred a new theory of the
universe. In this chapter we will concentrate on one class of learning problem, which seems
restricted but actually has vast applicability: from a collection of input–output pairs, learn a
function that predicts the output for new inputs.
Why would we want an agent to learn? If the design of the agent can be improved,
why wouldn’t the designers just program in that improvement to begin with? There are three
main reasons. First, the designers cannot anticipate all possible situations that the agent
might ﬁnd itself in. For example, a robot designed to navigate mazes must learn the layout
of each new maze it encounters. Second, the designers cannot anticipate all changes over
time; a program designed to predict tomorrow’s stock market prices must learn to adapt when
conditions change from boom to bust. Third, sometimes human programmers have no idea
how to program a solution themselves. For example, most people are good at recognizing the
faces of family members, but even the best programmers are unable to program a computer
to accomplish that task, except by using learning algorithms. This chapter ﬁrst gives an
models (in particular, neural networks), nonparametric models, and support vector machines.
Finally we show how ensembles of models can outperform a single model.
FORMS OF LEARNING
Any component of an agent can be improved by learning from data. The improvements, and
the techniques used to make them, depend on four major factors:
• Which component is to be improved.Chapter
Learning from Examples
• What prior knowledge the agent already has.
• What representation is used for the data and the component.
• What feedback is available to learn from.
Components to be learned
the agent can take.
Each of these components can be learned. Consider, for example, an agent training to become
a taxi driver. Every time the instructor shouts “Brake!” the agent might learn a condition–
does not shout. By seeing many camera images that it is told contain buses, it can learn
from passengers who have been thoroughly shaken up during the trip, it can learn a useful
Representation and prior knowledge
We have seen several examples of representations for agent components: propositional and
rithms have been devised for all of these representations. This chapter (and most of current
machine learning research) covers inputs that form a factored representation—a vector of
attribute values—and outputs that can be either a continuous numerical value or a discrete
There is another way to look at the various types of learning. We say that learning
INDUCTIVE
LEARNING
learning: going from a known general rule to a new rule that is logically entailed, but is
DEDUCTIVE
LEARNING
useful because it allows more efﬁcient processing.
Feedback to learn from
There are three types of feedback that determine the three main types of learning:
In unsupervised learning the agent learns patterns in the input even though no explicit
UNSUPERVISED
LEARNING
feedback is supplied. The most common unsupervised learning task is clustering: detecting
CLUSTERINGSupervised Learning
potentially useful clusters of input examples. For example, a taxi agent might gradually
develop a concept of “good trafﬁc days” and “bad trafﬁc days” without ever being given
labeled examples of each by a teacher.
In reinforcement learning the agent learns from a series of reinforcements—rewards
REINFORCEMENT
LEARNING
or punishments. For example, the lack of a tip at the end of the journey gives the taxi agent an
indication that it did something wrong. The two points for a win at the end of a chess game
tells the agent it did something right. It is up to the agent to decide which of the actions prior
to the reinforcement were most responsible for it.
In supervised learning the agent observes some example input–output pairs and learns
SUPERVISED
LEARNING
inputs are camera images and the outputs again come from a teacher who says “that’s a bus.”
in feet. In this case the output value is available directly from the agent’s percepts (after the
fact); the environment is the teacher.
LEARNING
labeled examples. Even the labels themselves may not be the oracular truths that we hope
for. Imagine that you are trying to build a system to guess a person’s age from a photo. You
gather some labeled examples by snapping pictures of people and asking their age. That’s
supervised learning. But in reality some of the people lied about their age. It’s not just
that there is random noise in the data; rather the inaccuracies are systematic, and to uncover
known) ages. Thus, both noise and lack of labels create a continuum between supervised and
unsupervised learning.
SUPERVISED LEARNING
The task of supervised learning is this:
Given a training set of N example input–output pairs
TRAINING SET
(x1, y1), (x2, y2), . . . (xN, yN) ,
where each yj was generated by an unknown function y = f(x),
discover a function h that approximates the true function f.
HYPOTHESIS
Learning is a search through the space of possible hypotheses for one that will perform well,
even on new examples beyond the training set. To measure the accuracy of a hypothesis we
give it a test set of examples that are distinct from the training set. We say a hypothesis
TEST SET
we will use xj for the jth example and we will use i to index the n attributes of each example. The elements ofChapter
Learning from Examples
(c)
(a)
(b)
(d)
x
x
x
x
f(x)
f(x)
f(x)
f(x)
sinusoidal ﬁt to the same data set.
generalizes well if it correctly predicts the value of y for novel examples. Sometimes the
GENERALIZATION
function f is stochastic—it is not strictly a function of x, and what we have to learn is a
conditional probability distribution, P(Y | x).
When the output y is one of a ﬁnite set of values (such as sunny, cloudy or rainy),
the learning problem is called classiﬁcation, and is called Boolean or binary classiﬁcation
CLASSIFICATION
if there are only two values. When y is a number (such as tomorrow’s temperature), the
learning problem is called regression. (Technically, solving a regression problem is ﬁnding
REGRESSION
a conditional expectation or average value of y, because the probability that we have found
points. The examples are points in the (x, y) plane, where y = f(x). We don’t know what f
is, but we will approximate it with a function h selected from a hypothesis space, H, which
HYPOTHESIS SPACE
CONSISTENT
degree polynomial that is also consistent with the same data. This illustrates a fundamental
problem in inductive learning: how do we choose from among multiple consistent hypotheses?
One answer is to prefer the simplest hypothesis consistent with the data. This principle is
OCKHAM’S RAZOR
used it to argue sharply against all sorts of complications. Deﬁning simplicity is not easy, but
data and we do not expect it to generalize well. A straight line that is not consistent with
any of the data points, but might generalize fairly well for unseen values of x, is also shown
in (c). In general, there is a tradeoff between complex hypotheses that ﬁt the training dataLearning Decision Trees
hypothesis space H to allow polynomials over both x and sin(x), and ﬁnd that the data in
(c) can be ﬁtted exactly by a simple function of the form ax + b + c sin(x). This shows the
importance of the choice of hypothesis space. We say that a learning problem is realizable if
REALIZABLE
the hypothesis space contains the true function. Unfortunately, we cannot always tell whether
a given learning problem is realizable, because the true function is not known.
distinctions about the hypothesis space, to say—even before seeing any data—not just that a
hypothesis is possible or impossible, but rather how probable it is. Supervised learning can
be done by choosing the hypothesis h∗that is most probable given the data:
h∗= argmax
P(h|data) .
By Bayes’ rule this is equivalent to
h∗= argmax
P(data|h) P(h) .
really need them, but we discourage them by giving them a low prior probability.
Why not let H be the class of all Java programs, or Turing machines? After all, every
computable function can be represented by some Turing machine, and that is the best we
can do. One problem with this idea is that it does not take into account the computational
complexity of learning. There is a tradeoff between the expressiveness of a hypothesis space
and the complexity of ﬁnding a good hypothesis within that space. For example, ﬁtting a
harder; and ﬁtting Turing machines is in general undecidable. A second reason to prefer
simple hypothesis spaces is that presumably we will want to use h after we have learned it,
and computing h(x) when h is a linear function is guaranteed to be fast, while computing
an arbitrary Turing machine program is not even guaranteed to terminate. For these reasons,
most work on learning has focused on simple representations.
We will see that the expressiveness–complexity tradeoff is not as simple as it ﬁrst seems:
makes it possible for a simple hypothesis to ﬁt the data, whereas restricting the expressiveness
of the language means that any consistent hypothesis must be very complex. For example,
pages when written in propositional logic.
LEARNING DECISION TREES
Decision tree induction is one of the simplest and yet most successful forms of machine
learning. We ﬁrst describe the representation—the hypothesis space—and then show how to
learn a good hypothesis.Chapter
Learning from Examples
The decision tree representation
A decision tree represents a function that takes as input a vector of attribute values and
DECISION TREE
returns a “decision”—a single output value. The input and output values can be discrete or
continuous. For now we will concentrate on problems where the inputs have discrete values
and the output has exactly two possible values; this is Boolean classiﬁcation, where each
example input will be classiﬁed as true (a positive example) or false (a negative example).
POSITIVE
NEGATIVE
A decision tree reaches its decision by performing a sequence of tests. Each internal
node in the tree corresponds to a test of the value of one of the input attributes, Ai, and
the branches from the node are labeled with the possible values of the attribute, Ai = vik.
Each leaf node in the tree speciﬁes a value to be returned by the function. The decision tree
representation is natural for humans; indeed, many “How To” manuals (e.g., for car repair)
are written entirely as a single decision tree stretching over hundreds of pages.
As an example, we will build a decision tree to decide whether to wait for a table at a
restaurant. The aim here is to learn a deﬁnition for the goal predicate WillWait. First we
GOAL PREDICATE
list the attributes that we will consider as part of the input:
Note that every variable has a small set of possible values; the value of WaitEstimate, for
Notice that the tree ignores the Price and Type attributes. Examples are processed by the tree
starting at the root and following the appropriate branch until a leaf is reached. For instance,
(i.e., yes, we will wait for a table).
Expressiveness of decision trees
A Boolean decision tree is logically equivalent to the assertion that the goal attribute is true
if and only if the input attributes satisfy one of the paths leading to a leaf with value true.
Writing this out in propositional logic, we have
Goal ⇔(Path1 ∨Path2 ∨· · ·) ,Learning Decision Trees
that any function in propositional logic can be expressed as a decision tree. As an example,
For a wide variety of problems, the decision tree format yields a nice, concise result. But
some functions cannot be represented concisely. For example, the majority function, which
returns true if and only if more than half of the inputs are true, requires an exponentially
large decision tree. In other words, decision trees are good for some kinds of functions and
bad for others. Is there any kind of representation that is efﬁcient for all kinds of functions?
Unfortunately, the answer is no. We can show this in a general way. Consider the set of all
Boolean functions on n attributes. How many different functions are in this set? This is just
the number of different truth tables that we can write down, because the function is deﬁned
by its truth table. A truth table over n attributes has 2n rows, one for each combination of
that deﬁnes the function. That means there are 22n different functions (and there will be more
than that number of trees, since more than one tree can compute the same function). This is
a scary number. For example, with just the ten Boolean attributes of our restaurant problem
a large space.
Inducing decision trees from examples
An example for a Boolean decision tree consists of an (x, y) pair, where x is a vector of values
No
 Yes
No
 Yes
No
 Yes
No
 Yes
None
Some
Full
No
 Yes
Alternate?
Hungry?
Reservation?
Bar?
Raining?
Alternate?
Patrons?
No
Yes
No
Yes
Yes
Yes
No
 Yes
No
Yes
Yes
No
Yes
No
 Yes
Yes
No
WaitEstimate?
A decision tree for deciding whether to wait for a table.Chapter
Learning from Examples
Example
Input Attributes
Goal
Alt
Bar
Fri
Hun
Pat
Price Rain
Res
Type
Est
WillWait
x1
Yes
No
No
Yes
Some
$$$
No
Yes
French
y1 = Yes
x2
Yes
No
No
Yes
Full
$
No
No
Thai
y2 = No
x3
No
Yes
No
No
Some
$
No
No
Burger
y3 = Yes
x4
Yes
No
Yes
Yes
Full
$
Yes
No
Thai
y4 = Yes
x5
Yes
No
Yes
No
Full
$$$
No
Yes
French
y5 = No
x6
No
Yes
No
Yes
Some
$$
Yes
Yes
Italian
y6 = Yes
x7
No
Yes
No
No
None
$
Yes
No
Burger
y7 = No
x8
No
No
No
Yes
Some
$$
Yes
Yes
Thai
y8 = Yes
x9
No
Yes
Yes
No
Full
$
Yes
No
Burger
y9 = No
x10
Yes
Yes
Yes
Yes
Full
$$$
No
Yes
Italian
y10 = No
x11
No
No
No
No
None
$
No
No
Thai
y11 = No
x12
Yes
Yes
Yes
Yes
Full
$
No
No
Burger
y12 = Yes
Examples for the restaurant domain.
true (x1, x3, . . .); the negative examples are the ones in which it is false (x2, x5, . . .).
fortunately, no matter how we measure size, it is an intractable problem to ﬁnd the smallest
consistent tree; there is no way to efﬁciently search through the 22n trees. With some simple
strategy: always test the most important attribute ﬁrst. This test divides the problem up into
smaller subproblems that can then be solved recursively. By “most important attribute,” we
mean the one that makes the most difference to the classiﬁcation of an example. That way, we
hope to get to the correct classiﬁcation with a small number of tests, meaning that all paths in
the tree will be short and the tree as a whole will be shallow.
outcomes, each of which has the same number of positive as negative examples. On the other
hand, in (b) we see that Patrons is a fairly important attribute, because if the value is None or
Some, then we are left with example sets for which we can answer deﬁnitively (No and Yes,
respectively). If the value is Full, we are left with a mixed set of examples. In general, after
the ﬁrst attribute test splits up the examples, each outcome is a new decision tree learning
problem in itself, with fewer examples and one less attribute. There are four cases to consider
for these recursive problems:
Some branches.Learning Decision Trees
(a)
None
Some
Full
Patrons?
Yes
No
Hungry?
(b)
No
Yes
French
Italian
Thai
Burger
Type?
Splitting the examples by testing on attributes. At each node we show the
positive (light boxes) and negative (dark boxes) examples remaining. (a) Splitting on Type
brings us no nearer to distinguishing between positive and negative examples. (b) Splitting
on Patrons does a good job of separating positive and negative examples. After splitting on
Patrons, Hungry is a fairly good second test.
bination of attribute values, and we return a default value calculated from the plurality
classiﬁcation of all the examples that were used in constructing the node’s parent. These
are passed along in the variable parent examples.
these examples have exactly the same description, but different classiﬁcations. This can
NOISE
ministic; or because we can’t observe an attribute that would distinguish the examples.
The best we can do is return the plurality classiﬁcation of the remaining examples.
examples is crucial for constructing the tree, but nowhere do the examples appear in the tree
itself. A tree consists of just tests on attributes in the interior nodes, values of attributes on
the branches, and output values on the leaf nodes. The details of the IMPORTANCE function
of learning the correct function. This would be the wrong conclusion to draw, however. The
learning algorithm looks at the examples, not at the correct function, and in fact, its hypothesis
than the original tree! The learning algorithm has no reason to include tests for Raining and
Reservation, because it can classify all the examples without them. It has also detected an
interesting and previously unsuspected pattern: the ﬁrst author will wait for Thai food on
weekends. It is also bound to make some mistakes for cases where it has seen no examples.Chapter
Learning from Examples
a tree
else if all examples have the same classiﬁcation then return the classiﬁcation
else
tree ←a new decision tree with root test A
for each value vk of A do
add a branch to tree with label (A = vk) and subtree subtree
return tree
value among a set of examples, breaking ties randomly.
None
Some
Full
Patrons?
No
Yes
No
 Yes
Hungry?
No
No
 Yes
Yes
No
Yes
Type?
French
Italian
Thai
Burger
Yes
No
In that case it says not to wait when Hungry is false, but I (SR) would certainly wait. With
more training examples the learning program could correct this mistake.
trary: with slightly different input examples, a different variable would be chosen to split on
ﬁrst, and the whole tree would look completely different. The function computed by the tree
would still be similar, but the structure of the tree can vary widely.
We can evaluate the accuracy of a learning algorithm with a learning curve, as shown
LEARNING CURVELearning Decision Trees
Proportion correct on test set
Training set size
a test set. We learn a hypothesis h with the training set and measure its accuracy with the test
increases. (For this reason, learning curves are also called happy graphs.) In this graph we
Choosing attribute tests
The greedy search used in decision tree learning is designed to approximately minimize the
depth of the ﬁnal tree. The idea is to pick the attribute that goes as far as possible toward
providing an exact classiﬁcation of the examples. A perfect attribute divides the examples
into sets, each of which are all positive or all negative and thus will be leaves of the tree. The
Patrons attribute is not perfect, but it is fairly good. A really useless attribute, such as Type,
leaves the example sets with roughly the same proportion of positive and negative examples
as the original set.
All we need, then, is a formal measure of “fairly good” and “really useless” and we can
gain, which is deﬁned in terms of entropy, the fundamental quantity in information theory
ENTROPY
Entropy is a measure of the uncertainty of a random variable; acquisition of information
corresponds to a reduction in entropy. A random variable with only one value—a coin that
always comes up heads—has no uncertainty and thus its entropy is deﬁned as zero; thus, we
gain no information by observing its value. A ﬂip of a fair coin is equally likely to come up
Intuitively, this coin has less uncertainty than the fair coin—if we guess heads we’ll be wrongChapter
Learning from Examples
positive. In general, the entropy of a random variable V with values vk, each with probability
P(vk), is deﬁned as
Entropy:
H(V ) =


k
P(vk) log2
P(vk) = −


k
P(vk) log2 P(vk) .
It will help to deﬁne B(q) as the entropy of a Boolean random variable that is true with
probability q:
training set contains p positive examples and n negative examples, then the entropy of the
goal attribute on the whole set is
H(Goal) = B

p
p + n

.
can measure exactly how much by looking at the entropy remaining after the attribute test.
An attribute A with d distinct values divides the training set E into subsets E1, . . . , Ed.
Each subset Ek has pk positive examples and nk negative examples, so if we go along that
tion. A randomly chosen example from the training set has the kth value for the attribute with
probability (pk + nk)/(p + n), so the expected entropy remaining after testing attribute A is
Remainder(A) =
d


pk
The information gain from the attribute test on A is the expected reduction in entropy:
INFORMATION GAIN
Gain(A) = B(
p
In fact Gain(A) is just what we need to implement the IMPORTANCE function. Returning to
%
%
conﬁrming our intuition that Patrons is a better attribute to split on. In fact, Patrons has
algorithm as the root.Learning Decision Trees
Generalization and overﬁtting
when there is actually no pattern to be found. Consider the problem of trying to predict
with various dice and that the attributes describing each training example include the color
of the die, its weight, the time when the roll was done, and whether the experimenters had
their ﬁngers crossed. If the dice are fair, the right thing to learn is a tree with a single node
that case. This problem is called overﬁtting. A general phenomenon, overﬁtting occurs with
OVERFITTING
(c), we saw polynomial functions overﬁtting the data. Overﬁtting becomes more likely as the
hypothesis space and the number of input attributes grows, and less likely as we increase the
number of training examples.
DECISION TREE
PRUNING
ing works by eliminating nodes that are not clearly relevant. We start with a full tree, as
nodes as descendants. If the test appears to be irrelevant—detecting only noise in the data—
then we eliminate the test, replacing it with a leaf node. We repeat this process, considering
each test with only leaf descendants, until each one has either been pruned or accepted as is.
The question is, how do we detect that a node is testing an irrelevant attribute? Suppose
we are at a node consisting of p positive and n negative examples. If the attribute is irrelevant,
we would expect that it would split the examples into subsets that each have roughly the same
is, how large a gain should we require in order to split on a particular attribute?
We can answer this question by using a statistical signiﬁcance test. Such a test begins
SIGNIFICANCE TEST
NULL HYPOTHESIS
tual data are analyzed to calculate the extent to which they deviate from a perfect absence of
ability or less), then that is considered to be good evidence for the presence of a signiﬁcant
pattern in the data. The probabilities are calculated from standard distributions of the amount
of deviation one would expect to see in random sampling.
In this case, the null hypothesis is that the attribute is irrelevant and, hence, that the
information gain for an inﬁnitely large sample would be zero. We need to calculate the
probability that, under the null hypothesis, a sample of size v = n + p would exhibit the
observed deviation from the expected distribution of positive and negative examples. We can
measure the deviation by comparing the actual numbers of positive and negative examples inChapter
Learning from Examples
each subset, pk and nk, with the expected numbers, ˆpk and ˆnk, assuming true irrelevance:
ˆpk = p × pk + nk
p + n
ˆnk = n × pk + nk
p + n
.
A convenient measure of the total deviation is given by
Δ =
d


ˆpk
ˆnk
.
library routine to see if a particular Δ value conﬁrms or rejects the null hypothesis. For
example, consider the restaurant type attribute, with four values and thus three degrees of
as χ2 pruning.
χ2 PRUNING
With pruning, noise in the examples can be tolerated. Errors in the example’s label (e.g.,
an example (x, Yes) that should be (x, No)) give a linear increase in prediction error, whereas
errors in the descriptions of examples (e.g., Price = $ when it was actually Price = $$) have
an asymptotic effect that gets worse as the tree shrinks down to smaller sets. Pruned trees
perform signiﬁcantly better than unpruned trees when the data contain a large amount of
noise. Also, the pruned trees are often much smaller and hence easier to understand.
One ﬁnal warning: You might think that χ2 pruning and information gain look similar,
so why not combine them using an approach called early stopping—have the decision tree
EARLY STOPPING
algorithm stop generating nodes when there is no good attribute to split on, rather than going
to all the trouble of generating nodes and then pruning them away. The problem with early
stopping is that it stops us from recognizing situations where there is no one good attribute,
but there are combinations of attributes that are informative. For example, consider the XOR
function of two binary attributes. If there are roughly equal number of examples for all four
combinations of input values, then neither attribute will be informative, yet the correct thing
to do is to split on one of the attributes (it doesn’t matter which one), and then at the second
Broadening the applicability of decision trees
In order to extend decision tree induction to a wider variety of problems, a number of issues
must be addressed. We will brieﬂy mention several, suggesting that a full understanding is
best obtained by doing the associated exercises:
• Missing data: In many domains, not all the attribute values will be known for every
example. The values might have gone unrecorded, or they might be too expensive to
obtain. This gives rise to two problems: First, given a complete decision tree, how
should one classify an example that is missing one of the test attributes? Second, howLearning Decision Trees
• Multivalued attributes: When an attribute has many possible values, the information
treme case, an attribute such as ExactTime has a different value for every example,
which means each subset of examples is a singleton with a unique classiﬁcation, and
ing this split ﬁrst is unlikely to yield the best tree. One solution is to use the gain ratio
GAIN RATIO
is, picking out just one of the possible values for an attribute, leaving the remaining
values to possibly be tested later in the tree.
tributes such as Height and Weight, have an inﬁnite set of possible values. Rather than
split point that gives the highest information gain. For example, at a given node in
SPLIT POINT
tion. Efﬁcient methods exist for ﬁnding good split points: start by sorting the values
of the attribute, and then consider only split points that are between two examples in
sorted order that have different classiﬁcations, while keeping track of the running totals
of positive and negative examples on each side of the split point. Splitting is the most
value, such as the price of an apartment, then we need a regression tree rather than a
REGRESSION TREE
classiﬁcation tree. A regression tree has at each leaf a linear function of some subset
bedroom apartments might end with a linear function of square footage, number of
bathrooms, and average income for the neighborhood. The learning algorithm must
over the attributes.
physical and ﬁnancial processes provide numerical data. Several commercial packages have
been built that meet these criteria, and they have been used to develop thousands of ﬁelded
systems. In many areas of industry and commerce, decision trees are usually the ﬁrst method
tried when a classiﬁcation method is to be extracted from a data set. One important property
of decision trees is that it is possible for a human to understand the reason for the output of the
learning algorithm. (Indeed, this is a legal requirement for ﬁnancial decisions that are subject
such as neural networks.Chapter
Learning from Examples
EVALUATING AND CHOOSING THE BEST HYPOTHESIS
We want to learn a hypothesis that ﬁts the future data best. To make that precise we need
to deﬁne “future data” and “best.” We make the stationarity assumption: that there is a
STATIONARITY
ASSUMPTION
probability distribution over examples that remains stationary over time. Each example data
point (before we see it) is a random variable Ej whose observed value ej = (xj, yj) is sampled
from that distribution, and is independent of the previous examples:
and each example has an identical prior probability distribution:
Examples that satisfy these assumptions are called independent and identically distributed or
i.i.d.. An i.i.d. assumption connects the past to the future; without some such connection, all
I.I.D.
bets are off—the future could be anything. (We will see later that learning can still occur if
there are slow changes in the distribution.)
The next step is to deﬁne “best ﬁt.” We deﬁne the error rate of a hypothesis as the
ERROR RATE
proportion of mistakes it makes—the proportion of times that h(x) ̸= y for an (x, y) example.
Now, just because a hypothesis h has a low error rate on the training set does not mean that
it will generalize well. A professor knows that an exam will not accurately evaluate students
if they have already seen the exam questions. Similarly, to get an accurate evaluation of a
hypothesis, we need to test it on a set of examples it has not seen yet. The simplest approach is
the one we have seen already: randomly split the available data into a training set from which
the learning algorithm produces h and a test set on which the accuracy of h is evaluated. This
HOLDOUT
all the available data; if we use half the data for the test set, then we are only training on half
the data for the test set, then we may, by statistical chance, get a poor estimate of the actual
accuracy.
We can squeeze more out of the data and still get an accurate estimate using a technique
data and test data. First we split the data into k equal subsets. We then perform k rounds of
are used as training data. The average test set score of the k rounds should then be a better
LOOCV
Despite the best efforts of statistical methodologists, users frequently invalidate their
results by inadvertently peeking at the test data. Peeking can happen like this: A learning
PEEKING
algorithm has various “knobs” that can be twiddled to tune its behavior—for example, various
different criteria for choosing the next attribute in decision tree learning. The researcher
generates hypotheses for various different settings of the knobs, measures their error rates on
the test set, and reports the error rate of the best hypothesis. Alas, peeking has occurred! TheEvaluating and Choosing the Best Hypothesis
reason is that the hypothesis was selected on the basis of its test set error rate, so information
about the test set has leaked into the learning algorithm.
evaluate it. The way to avoid this is to really hold the test set out—lock it away until you
are completely done with learning and simply wish to obtain an independent evaluation of
the ﬁnal hypothesis. (And then, if you don’t like the results . . . you have to obtain, and lock
away, a completely new test set if you want to go back and ﬁnd a better hypothesis.) If the
test set is locked away, but you still want to measure performance on unseen data as a way of
selecting a good hypothesis, then divide the available data (without the test set) into a training
set and a validation set. The next section shows how to use validation sets to ﬁnd a good
VALIDATION SET
tradeoff between hypothesis complexity and goodness of ﬁt.
Model selection: Complexity versus goodness of ﬁt
better, but when the degree is too high they will overﬁt, and perform poorly on validation data.
Choosing the degree of the polynomial is an instance of the problem of model selection. You
MODEL SELECTION
can think of the task of ﬁnding the best hypothesis as two tasks: model selection deﬁnes the
hypothesis space and then optimization ﬁnds the best hypothesis within that space.
OPTIMIZATION
In this section we explain how to select among models that are parameterized by size.
and so on. For decision trees, the size could be the number of nodes in the tree. In all cases
we want to ﬁnd the value of the size parameter that best balances underﬁtting and overﬁtting
to give the best test set accuracy.
WRAPPER
for example). The wrapper enumerates models according to a parameter, size. For each size,
it uses cross validation on Learner to compute the average error rate on the training and
test sets. We start with the smallest, simplest models (which probably underﬁt the data), and
iterate, considering more complex models at each step, until the models start to overﬁt. In
there may in general be slight random variation), while the validation set error decreases at
We then generate a hypothesis of that size, using all the data (without holding out any of it).
Finally, of course, we should evaluate the returned hypothesis on a separate test set.
This approach requires that the learning algorithm accept a parameter, size, and deliver
a hypothesis of that size. As we said, for decision tree learning, the size can be the number of
the highest gain attribute ﬁrst), and stops when it reaches the desired number of nodes.Chapter
Learning from Examples
if errT has converged then do
best size ←the value of size with minimum errV [size]
return Learner(best size,examples)
average training set error rate, average validation set error rate
training set,validation set ←PARTITION(examples,fold,k)
h ←Learner(size,training set)
An algorithm to select the model that has the lowest error rate on validation
ical error rate on validation data. Here errT means error rate on the training data, and
esis whose complexity is set by the parameter size, and which is trained on the examples.
and a training set with all the other examples. The split is different for each value of fold.
From error rates to loss
So far, we have been trying to minimize error rate. This is clearly better than maximizing
error rate, but it is not the full story. Consider the problem of classifying email messages
should maximize expected utility, and utility is what learners should maximize as well. In
machine learning it is traditional to express utilities by means of a loss function. The loss
LOSS FUNCTION
function L(x, y, ˆy) is deﬁned as the amount of utility lost by predicting h(x) = ˆy when the
correct answer is f(x) = y:
L(x, y, ˆy) = Utility(result of using y given an input x)
−Utility(result of using ˆy given an input x)Evaluating and Choosing the Best Hypothesis
Error rate
Tree size
Validation Set Error
Training Set Error
Error rates on training data (lower, dashed line) and validation data (upper,
totes, and then choose the tree with minimal error on the validation set; in this case the tree
This is the most general formulation of the loss function. Often a simpliﬁed version is used,
L(y, ˆy), that is independent of x. We will use the simpliﬁed version for the rest of this
chapter, which means we can’t say that it is worse to misclassify a letter from Mom than it
Note that L(y, y) is always zero; by deﬁnition there is no loss when you guess exactly right.
be? In general small errors are better than large ones; two functions that implement that idea
are the absolute value of the difference (called the L1 loss), and the square of the difference
(called the L2 loss). If we are content with the idea of minimizing error rate, we can use
Absolute value loss: L1(y, ˆy) = |y −ˆy|
Squared error loss:
esis that minimizes expected loss over all input–output pairs it will see. It is meaningless
to talk about this expectation without deﬁning a prior probability distribution, P(X, Y ) over
alization loss for a hypothesis h (with respect to loss function L) is
GENERALIZATION
LOSSChapter
Learning from Examples
GenLossL(h) =


(x,y)∈E
L(y, h(x)) P(x, y) ,
and the best hypothesis, h∗, is the one with the minimum expected generalization loss:
h∗= argmin
GenLossL(h) .
Because P(x, y) is not known, the learning agent can only estimate generalization loss with
empirical loss on a set of examples, E:
EMPIRICAL LOSS
N


(x,y)∈E
L(y, h(x)) .
The estimated best hypothesis ˆh∗is then the one with minimum empirical loss:
ˆh∗= argmin
EmpLoss L,E(h) .
There are four reasons why ˆh∗may differ from the true function, f: unrealizability, variance,
noise, and computational complexity. First, f may not be realizable—may not be in H—or
rithm will return different hypotheses for different sets of examples, even if those sets are
drawn from the same true function f, and those hypotheses will make different predictions
on new examples. The higher the variance among the predictions, the higher the probability
of signiﬁcant error. Note that even when the problem is realizable, there will still be random
creases. Third, f may be nondeterministic or noisy—it may return different values for f(x)
NOISE
each time x occurs. By deﬁnition, noise cannot be predicted; in many cases, it arises because
the observed labels y are the result of attributes of the environment not listed in x. And ﬁnally,
when H is complex, it can be computationally intractable to systematically search the whole
hypothesis space. The best we can do is a local search (hill climbing or greedy search) that
explores only part of the space. That gives us an approximation error. Combining the sources
of error, we’re left with an estimation of an approximation of the true function f.
Traditional methods in statistics and the early years of machine learning concentrated
LEARNING
low thousands. Here the generalization error mostly comes from the approximation error of
not having the true f in the hypothesis space, and from estimation error of not having enough
scale learning, often with millions of examples. Here the generalization error is dominated
LEARNING
by limits of computation: there is enough data and a rich enough model that we could ﬁnd an
h that is very close to the true f, but the computation to ﬁnd it is too complex, so we settle
Regularization
alternative approach is to search for a hypothesis that directly minimizes the weighted sum ofThe Theory of Learning
empirical loss and the complexity of the hypothesis, which we will call the total cost:
Cost(h) = EmpLoss(h) + λ Complexity(h)
ˆh∗= argmin
Cost(h) .
hypothesis complexity (which after all are not measured on the same scale). This approach
combines loss and complexity into one metric, allowing us to ﬁnd the best hypothesis all at
REGULARIZATION
cause it looks for a function that is more regular, or less complex). Note that the cost function
requires us to make two choices: the loss function and the complexity measure, which is
pothesis space. For example, a good regularization function for polynomials is the sum of
the squares of the coefﬁcients—keeping the sum small would guide us away from the wiggly
Another way to simplify models is to reduce the dimensions that the models work with.
FEATURE SELECTION
evant. χ2 pruning is a kind of feature selection.
It is in fact possible to have the empirical loss and the complexity measured on the
the hypothesis as a Turing machine program, and count the number of bits. Then count
the number of bits required to encode the data, where a correctly predicted example costs
zero bits and the cost of an incorrectly predicted example depends on how large the error is.
The minimum description length or MDL hypothesis minimizes the total number of bits
MINIMUM
DESCRIPTION
LENGTH
required. This works well in the limit, but for smaller problems there is a difﬁculty in that
the choice of encoding for the program—for example, how best to encode a decision tree
interpretation of the MDL approach.
THE THEORY OF LEARNING
The main unanswered question in learning is this: How can we be sure that our learning
algorithm has produced a hypothesis that will predict the correct value for previously unseen
inputs? In formal terms, how do we know that the hypothesis h is close to the target function
f if we don’t know what f is? These questions have been pondered for several centuries.
In more recent decades, other questions have emerged: how many examples do we need
to get a good h? What hypothesis space should we use? If the hypothesis space is very
complex, can we even ﬁnd the best h, or do we have to settle for a local maximum in theChapter
Learning from Examples
space of hypotheses? How complex should h be? How do we avoid overﬁtting? This section
examines these questions.
We’ll start with the question of how many examples are needed for learning. We saw
eral principles governing the number of examples needed in general? Questions like this are
addressed by computational learning theory, which lies at the intersection of AI, statistics,
COMPUTATIONAL
LEARNING THEORY
ously wrong will almost certainly be “found out” with high probability after a small number
tent with a sufﬁciently large set of training examples is unlikely to be seriously wrong: that is,
it must be probably approximately correct. Any learning algorithm that returns hypotheses
PROBABLY
APPROXIMATELY
CORRECT
that are probably approximately correct is called a PAC learning algorithm; we can use this
PAC LEARNING
approach to provide bounds on the performance of various learning algorithms.
a theorem (as opposed to, say, a political pundit) states something about the future based on
the past, the axioms have to provide the “juice” to make that connection. For PAC learning,
future examples are going to be drawn from the same ﬁxed distribution P(E) = P(X, Y )
as past examples. (Note that we do not have to know what distribution that is, just that it
doesn’t change.) In addition, to keep things simple, we will assume that the true function f
is deterministic and is a member of the hypothesis class H that is being considered.
propriate. The error rate of a hypothesis h, deﬁned informally earlier, is deﬁned formally
here as the expected generalization error for examples drawn from the stationary distribution:


x,y
In other words, error(h) is the probability that h misclassiﬁes a new example. This is the
same quantity being measured experimentally by the learning curves shown earlier.
constant. We will show that we can ﬁnd an N such that, after seeing N examples, with high
probability, all consistent hypotheses will be approximately correct. One can think of an
approximately correct hypothesis as being “close” to the true function in hypothesis space: it
ϵ-BALL
this ball is called Hbad.
consistent with the ﬁrst N examples as follows. We know that error(hb) > ϵ. Thus, the
independent, the bound for N examples isThe Theory of Learning
The probability that Hbad contains at least one consistent hypothesis is bounded by the sum
of the individual probabilities:
where we have used the fact that |Hbad| ≤|H|. We would like to reduce the probability of
ϵ

δ + ln |H|

examples. Thus, if a learning algorithm returns a hypothesis that is consistent with this many
is called the sample complexity of the hypothesis space.
SAMPLE
COMPLEXITY
As we saw earlier, if H is the set of all Boolean functions on n attributes, then |H| =
22n. Thus, the sample complexity of the space grows as 2n. Because the number of possible
requires seeing all, or nearly all, of the possible examples. A moment’s thought reveals the
reason for this: H contains enough hypotheses to classify any given set of examples in all
possible ways. In particular, for any set of N examples, the set of hypotheses consistent with
To obtain real generalization to unseen examples, then, it seems we need to restrict
the hypothesis space H in some way; but of course, if we do restrict the space, we might
eliminate the true function altogether. There are three ways to escape this dilemma. The ﬁrst,
any consistent hypothesis, but preferably a simple one (as is done in decision tree learning). In
cases where ﬁnding simple consistent hypotheses is tractable, the sample complexity results
are generally better than for analyses based only on consistency. The third escape, which
we pursue next, is to focus on learnable subsets of the entire hypothesis space of Boolean
functions. This approach relies on the assumption that the restricted language contains a
hypothesis h that is close enough to the true function f; the beneﬁts are that the restricted
hypothesis space allows for effective generalization and is typically easier to search. We now
examine one such restricted language in more detail.
PAC learning example: Learning decision lists
We now show how to apply PAC learning to a new hypothesis space: decision lists. A
DECISION LISTS
decision list consists of a series of tests, each of which is a conjunction of literals. If a
test succeeds when applied to an example description, the decision list speciﬁes the value
to be returned. If the test fails, processing continues with the next test in the list. Decision
lists resemble decision trees, but their overall structure is simpler: they branch only in oneChapter
Learning from Examples
Patrons(x, Some)
No
Yes
Yes
No
Patrons(x, Full)
Yes
No
Yes
^
A decision list for the restaurant problem.
list that represents the following hypothesis:
If we allow tests of arbitrary size, then decision lists can represent any Boolean function
then it is possible for the learning algorithm to generalize successfully from a small number
trees of depth at most k. It is important to remember that the particular language referred to
be approximated accurately after training on a reasonable number of examples. To do this,
we need to calculate the number of hypotheses in the language. Let the language of tests—
conjunctions of at most k literals using n attributes—be Conj (n, k). Because a decision list
is constructed of tests, and because each test can be attached to either a Yes or a No outcome
tests. Each of these sets of tests can be in any order, so
The number of conjunctions of k literals from n attributes is given by
|Conj (n, k)| =
k



2n
i

= O(nk) .
Hence, after some work, we obtain
ϵ

δ + O(nk log2(nk))

.
in a reasonable number of examples, for small k.
The next task is to ﬁnd an efﬁcient algorithm that returns a consistent decision list.Regression and Classiﬁcation with Linear Models
if examples is empty then return the trivial decision list No
t ←a test that matches a nonempty subset examplest of examples
such that the members of examplest are all positive or all negative
if there is no such t then return failure
if the examples in examplest are positive then o ←Yes else o ←No
return a decision list with initial test t and outcome o and remaining tests given by
An algorithm for learning decision lists.
Proportion correct on test set
Training set size
Decision tree
Decision list
test that agrees exactly with some subset of the training set. Once it ﬁnds such a test, it
adds it to the decision list under construction and removes the corresponding examples. It
then constructs the remainder of the decision list, using just the remaining examples. This is
This algorithm does not specify the method for selecting the next test to add to the
decision list. Although the formal results given earlier do not depend on the selection method,
it would seem reasonable to prefer small tests that match large sets of uniformly classiﬁed
examples, so that the overall decision list will be as compact as possible. The simplest strategy
is to ﬁnd the smallest test t that matches any uniformly classiﬁed subset, regardless of the size
REGRESSION AND CLASSIFICATION WITH LINEAR MODELS
Now it is time to move on from decision trees and lists to a different hypothesis space, one
LINEAR FUNCTIONChapter
Learning from Examples
House size in square feet
w0
w1
Loss
(a)
(b)
(a) Data points of price versus ﬂoor space of houses for sale in Berkeley,
values of w0, w1. Note that the loss function is convex, with a single global minimum.
and soft thresholds.
Univariate linear regression
we think of the coefﬁcients as weights; the value of y is changed by changing the relative
WEIGHT
weight of one term or another. We’ll deﬁne w to be the vector [w0, w1], and deﬁne
hw(x) = w1x + w0 .
representing the size in square feet and the price of a house offered for sale. The task of
ﬁnding the hw that best ﬁts these data is called linear regression. To ﬁt a line to the data, all
LINEAR REGRESSION
we have to do is ﬁnd the values of the weights [w0, w1] that minimize the empirical loss. It is
traditional (going back to Gauss3) to use the squared loss function, L2, summed over all the
training examples:
Loss(hw) =
N


L2(yj, hw(xj)) =
N


N


are obtained by minimizing the sum of the squares of the errors.Regression and Classiﬁcation with Linear Models
We would like to ﬁnd w∗= argminw Loss(hw). The sum N
minimized when its partial derivatives with respect to w0 and w1 are zero:
∂
∂w0
N


∂
∂w1
N


These equations have a unique solution:
w1 = N( xjyj) −( xj)( yj)
N( x2
; w0 = (


yj −w1(


xj))/N .
those weights is shown as a dashed line in the ﬁgure.
Many forms of learning involve adjusting weights to minimize a loss, so it helps to
have a mental picture of what’s going on in weight space—the space deﬁned by all possible
WEIGHT SPACE
settings of the weights. For univariate linear regression, the weight space deﬁned by w0 and
for every linear regression problem with an L2 loss function, and implies that there are no
local minima. In some sense that’s the end of the story for linear models; if we need to ﬁt
To go beyond linear models, we will need to face the fact that the equations deﬁning
will face a general optimization search problem in a continuous weight space. As indicated
follows the gradient of the function to be optimized. In this case, because we are trying to
minimize the loss, we will use gradient descent. We choose any starting point in weight
GRADIENT DESCENT
space—here, a point in the (w0, w1) plane—and then move to a neighboring point that is
downhill, repeating until we converge on the minimum possible loss:
w ←any point in the parameter space
loop until convergence do
for each wi in w do
wi ←wi −α ∂
∂wi
Loss(w)
rate when we are trying to minimize loss in a learning problem. It can be a ﬁxed constant, or
LEARNING RATE
it can decay over time as the learning process proceeds.
tive will be a linear function. (The only calculus you need to know is that
∂
∂xx2 = 2x and
∂
pendent of x; all results rely on the stationarity assumption; etc.Chapter
Learning from Examples
only one training example, (x, y):
∂
∂wi
Loss(w) =
∂
∂wi
∂
∂wi
(y −hw(x))
∂
∂wi
(y −(w1x + w0)) ,
applying this to both w0 and w1 we get:
∂
∂w0
∂
∂w1
rate α, we get the following learning rule for the weights:
w0 ←w0 + α (y −hw(x)) ;
w1 ←w1 + α (y −hw(x)) × x
These updates make intuitive sense: if hw(x) > y, i.e., the output of the hypothesis is too
large, reduce w0 a bit, and reduce w1 if x was a positive input but increase w1 if x was a
negative input.
The preceding equations cover one training example. For N training examples, we want
to minimize the sum of the individual losses for each example. The derivative of a sum is the
sum of the derivatives, so we have:
w0 ←w0 + α


j
(yj −hw(xj)) ;
w1 ←w1 + α


j
(yj −hw(xj)) × xj .
BATCH GRADIENT
DESCENT
gression. Convergence to the unique global minimum is guaranteed (as long as we pick α
small enough) but may be very slow: we have to cycle through all the training data for every
step, and there may be many steps.
There is another possibility, called stochastic gradient descent, where we consider
STOCHASTIC
GRADIENT DESCENT
Stochastic gradient descent can be used in an online setting, where new data are coming in
sary, taking a step after considering each single example. It is often faster than batch gradient
cillate around the minimum without settling down. In some cases, as we see later, a schedule
of decreasing learning rates (as in simulated annealing) does guarantee convergence.
Multivariate linear regression
We can easily extend to multivariate linear regression problems, in which each example xj
MULTIVARIATE
LINEAR REGRESSION


i
wixj,i .Regression and Classiﬁcation with Linear Models
The w0 term, the intercept, stands out as different from the others. We can ﬁx that by inventing
dot product of the weights and the input vector (or equivalently, the matrix product of the
transpose of the weights and the input vector):
hsw(xj) = w · xj = w⊤xj =


i
wixj,i .
w∗= argmin
w


j
L2(yj, w · xj) .
Multivariate linear regression is actually not much more complicated than the univariate case
we just covered. Gradient descent will reach the (unique) minimum of the loss function; the
update equation for each weight wi is
wi ←wi + α


j
xj,i(yj −hw(xj)) .
It is also possible to solve analytically for the w that minimizes loss. Let y be the vector of
outputs for the training examples, and X be the data matrix, i.e., the matrix of inputs with
DATA MATRIX
w∗= (X⊤X)−1X⊤y
minimizes the squared error.
With univariate linear regression we didn’t have to worry about overﬁtting. But with
that is actually irrelevant appears by chance to be useful, resulting in overﬁtting.
ﬁtting. Recall that with regularization we minimize the total cost of a hypothesis, counting
both the empirical loss and the complexity of the hypothesis:
Cost(h) = EmpLoss(h) + λ Complexity(h) .
For linear functions the complexity can be speciﬁed as a function of the weights. We can
consider a family of regularization functions:
Complexity(hw) = Lq(w) =


i
|wi|q .
ization has an important advantage: it tends to produce a sparse model. That is, it often sets
SPARSE MODEL
many weights to zero, effectively declaring the corresponding attributes to be irrelevant—just
discard attributes can be easier for a human to understand, and may be less likely to overﬁt.
not be used in pairs: you could use L2 loss with L1 regularization, or vice versa.Chapter
Learning from Examples
w1
w2
w1
w2
larization (box), the minimal achievable loss (concentric contours) often occurs on an axis,
meaning a weight of zero. (b) With L2 regularization (circle), the minimal loss is likely to
occur anywhere on the circle, giving no preference to zero weights.
zero, while L2 regularization does not. Note that minimizing Loss(w) + λComplexity(w)
is equivalent to minimizing Loss(w) subject to the constraint that Complexity(w) ≤c, for
c; our solution will have to be somewhere inside this box. The concentric ovals represent
contours of the loss function, with the minimum loss at the center. We want to ﬁnd the point
in the box that is closest to the minimum; you can see from the diagram that, for an arbitrary
position of the minimum and its contours, it will be common for the corner of the box to ﬁnd
its way closest to the minimum, just because the corners are pointy. And of course the corners
the same for the L2 complexity measure, which represents a circle rather than a diamond.
Here you can see that, in general, there is no reason for the intersection to appear on one of
the axes; thus L2 regularization does not tend to produce zero weights. The result is that the
number of examples required to ﬁnd a good h is linear in the number of irrelevant features for
L2 regularization, but only logarithmic with L1 regularization. Empirical evidence on many
problems supports this analysis.
Another way to look at it is that L1 regularization takes the dimensional axes seriously,
while L2 treats them as arbitrary. The L2 function is spherical, which makes it rotationally
invariant: Imagine a set of points in a plane, measured by their x and y coordinates. Now
imagine rotating the axes by 45o. You’d get a different set of (x′, y′) values representing
the same points. If you apply L2 regularization before and after rotating, you get exactly
the same point as the answer (although the point would be described with the new (x′, y′)
coordinates). That is appropriate when the choice of axes really is arbitrary—when it doesn’tRegression and Classiﬁcation with Linear Models
rotationally invariant. That is appropriate when the axes are not interchangeable; it doesn’t
make sense to rotate “number of bathrooms” 45o towards “lot size.”
Linear classiﬁers with a hard threshold
gists) and underground explosions (which are of interest to arms control experts). Each point
is deﬁned by two input values, x1 and x2, that refer to body and surface wave magnitudes
computed from the seismic signal. Given these training data, the task of classiﬁcation is to
x2
x1
x2
x1
(a)
(b)
Also shown is a decision boundary between the classes. (b) The same domain with more data
points. The earthquakes and explosions are no longer linearly separable.
A decision boundary is a line (or a surface, in higher dimensions) that separates the
DECISION
BOUNDARY
boundary is called a linear separator and data that admit such a separator are called linearly
LINEAR SEPARATOR
separable. The linear separator in this case is deﬁned by
LINEAR
SEPARABILITY
orChapter
Learning from Examples
Alternatively, we can think of h as the result of passing the linear function w · x through a
threshold function:
THRESHOLD
FUNCTION
this both in closed form (by setting the gradient to zero and solving for the weights) and
by gradient descent in weight space. Here, we cannot do either of those things because the
and at those points the gradient is undeﬁned.
There is, however, a simple weight update rule that converges to a solution—that is, a
linear separator that classiﬁes the data perfectly–provided the data are linearly separable. For
a single example (x, y), we have
wi ←wi + α (y −hw(x)) × xi
PERCEPTRON
LEARNING RULE
three possibilities:
• If the output is correct, i.e., y = hw(x), then the weights are not changed.
and decreased when xi is negative. This makes sense, because we want to make w · x
and increased when xi is negative. This makes sense, because we want to make w · x
Typically the learning rule is applied one example at a time, choosing examples at random
TRAINING CURVE
measures the classiﬁer performance on a ﬁxed training set as the learning process proceeds
separator. The “convergence” process isn’t exactly pretty, but it always works. This particular
We have said that the perceptron learning rule converges to a perfect linear separator
when the data points are linearly separable, but what if they are not? This situation is all
rithm keeps changing the weights. In general, the perceptron rule may not converge to aRegression and Classiﬁcation with Linear Models
Proportion correct
Number of weight updates
Proportion correct
Number of weight updates
Proportion correct
Number of weight updates
(a)
(b)
(c)
Linear classiﬁcation with logistic regression
We have seen that passing the output of a linear function through the threshold function
creates a linear classiﬁer; yet the hard nature of the threshold causes some problems: the
hypothesis hw(x) is not differentiable and is in fact a discontinuous function of its inputs and
even for examples that are very close to the boundary; in many situations, we really need
more gradated predictions.
All of these issues can be resolved to a large extent by softening the threshold function—
normal distribution (used for the probit model) and the logistic function (used for the logit
model). Although the two functions are very similar in shape, the logistic function
Logistic(z) =
these conditions.Chapter
Learning from Examples
x1
x2
(a)
(b)
(c)
Note
With the logistic function replacing the threshold function, we now have
hw(x) = Logistic(w · x) =
The process of ﬁtting the weights of this model to minimize loss on a data set is called
LOGISTIC
REGRESSION
this model, but the gradient descent computation is straightforward. Because our hypotheses
readable, we’ll use g to stand for the logistic function, with g′ its derivative.
For a single example (x, y), the derivation of the gradient is the same as for linear
derivation, we will need the chain rule: ∂g(f(x))/∂x = g′(f(x)) ∂f(x)/∂x.) We have
CHAIN RULE
∂
∂wi
Loss(w) =
∂
∂wi
∂
∂wi
(y −hw(x))
∂wi
w · xArtiﬁcial Neural Networks
Squared error per example
Number of weight updates
Squared error per example
Number of weight updates
Squared error per example
Number of weight updates
(a)
(b)
(c)
the same scale.
so the weight update for minimizing the loss is
arable case, logistic regression is somewhat slower to converge, but behaves much more
predictably. In (b) and (c), where the data are noisy and nonseparable, logistic regression
niques for problems in medicine, marketing and survey analysis, credit scoring, public health,
and other applications.
ARTIFICIAL NEURAL NETWORKS
We turn now to what seems to be a somewhat unrelated topic: the brain. In fact, as we
will see, the technical ideas we have discussed so far in this chapter turn out to be useful in
building mathematical models of the brain’s activity; conversely, thinking about the brain has
helped in extending the scope of the technical ideas.
pothesis that mental activity consists primarily of electrochemical activity in networks of
neuron.) Inspired by this hypothesis, some of the earliest AI work aimed to create artiﬁcial
neural networks. (Other names for the ﬁeld include connectionism, parallel distributed
NEURAL NETWORK
linear combination of its inputs exceeds some (hard or soft) threshold—that is, it implementsChapter
Learning from Examples
Output
Σ
Input
Links
Activation
Function
Input
Function
Output
Links
aj = g(inj)
aj
g
inj
wi,j
w0,j
Bias Weight
ai
A simple mathematical model for a neuron. The unit’s output activation is
aj = g(n
link from unit i to this unit.
a linear classiﬁer of the kind described in the preceding section. A neural network is just a
collection of units connected together; the properties of the network are determined by its
topology and the properties of the “neurons.”
neurons and for larger systems in the brain, leading to the modern ﬁeld of computational
neuroscience. On the other hand, researchers in AI and statistics became interested in the
COMPUTATIONAL
NEUROSCIENCE
putation, to tolerate noisy inputs, and to learn. Although we understand now that other kinds
of systems—including Bayesian networks—have these properties, neural networks remain
one of the most popular and effective forms of learning system and are worthy of study in
their own right.
Neural network structures
UNIT
LINK
ACTIVATION
also has a numeric weight wi,j associated with it, which determines the strength and sign of
WEIGHT
an associated weight w0,j. Each unit j ﬁrst computes a weighted sum of its inputs:
inj =
n


wi,jai .
Then it applies an activation function g to this sum to derive the output:
ACTIVATION
FUNCTION
aj = g(inj) = g
 n


wi,jai
 
.
indexed by i , so that an “external” activation ai is given by input xi; but index j will refer to internal units
rather than examples. Throughout this section, the mathematical derivations concern a single generic example x,
omitting the usual summations over examples to obtain results for the whole data set.Artiﬁcial Neural Networks
PERCEPTRON
sigmoid perceptron is sometimes used. Both of these nonlinear activation function ensure
SIGMOID
PERCEPTRON
the important property that the entire network of units can represent a nonlinear function (see
activation function has the added advantage of being differentiable.
Having decided on the mathematical model for individual “neurons,” the next task is
to connect them together to form a network. There are two fundamentally distinct ways to
NETWORK
directed acyclic graph. Every node receives input from “upstream” nodes and delivers output
its current input; thus, it has no internal state other than the weights themselves. A recurrent
network, on the other hand, feeds its outputs back into its own inputs. This means that
RECURRENT
NETWORK
the activation levels of the network form a dynamical system that may reach a stable state or
exhibit oscillations or even chaotic behavior. Moreover, the response of the network to a given
input depends on its initial state, which may depend on previous inputs. Hence, recurrent
more interesting as models of the brain, but also more difﬁcult to understand. This section
networks are given at the end of the chapter.
LAYERS
only from units in the immediately preceding layer. In the next two subsections, we will look
its outputs, and multilayer networks, which have one or more layers of hidden units that are
HIDDEN UNIT
not connected to the outputs of the network. So far in this chapter, we have considered only
learning problems with a single output variable y, but neural networks are often used in cases
where multiple outputs are appropriate. For example, if we want to train a network to add
bit. Also, when the learning problem involves classiﬁcation into more than two classes—for
example, when learning to categorize images of handwritten digits—it is common to use one
output unit for each class.
PERCEPTRON
NETWORK
for example. Here are all the training data we will need:
x1
x2
y3 (carry)
y4 (sum)Chapter
Learning from Examples
The ﬁrst thing to notice is that a perceptron network with m outputs is really m separate
rate training processes. Furthermore, depending on the type of activation function used, the
illustrates, this function is not linearly separable so the perceptron cannot learn it.
even such simple functions as XOR was a signiﬁcant setback to the nascent neural network
w
w
w
w
w
w
w
w
w
(b)
(a)
(a) A perceptron network with two inputs and two output units. (b) A neural
network with two inputs, one hidden layer of two units, and one output unit. Not shown are
the dummy inputs and their associated weights.
(a) x1 and x2
x1
x2
(b) x1 or x2
x1
x2
(c) x1 xor x2
?
x1
x2
Linear separability in threshold perceptrons. Black dots indicate a point in
no such line exists that correctly classiﬁes the inputs.Artiﬁcial Neural Networks
Proportion correct on test set
Training set size
Perceptron
Decision tree
Proportion correct on test set
Training set size
Perceptron
Decision tree
(a)
(b)
learning the WillWait predicate in the restaurant example.
that logistic regression (i.e., training a sigmoid perceptron) is even today a very popular and
the function quite quickly, because the majority function is linearly separable. On the other
(although not impossible) to represent as a decision tree. On the right, we have the restaurant
example. The solution problem is easily represented as a decision tree, but is not linearly
their problems. In fact, their paper proves that such a unit can represent the basic Boolean
functions AND, OR, and NOT and then goes on to argue that any desired functionality can be
obtained by connecting large numbers of units into (possibly recurrent) networks of arbitrary
depth. The problem was that nobody knew how to train such networks.
This turns out to be an easy problem if we think of a network the right way: as aChapter
Learning from Examples
x1
x2
hW(x1, x2)
x1
x2
hW(x1, x2)
(a)
(b)
produce a ridge. (b) The result of combining two ridges to produce a bump.
Thus, we have the output expressed as a function of the inputs and the weights. A similar
represented by a network can be highly nonlinear—composed, as it is, of nested nonlinear soft
threshold functions—we can see neural networks as a tool for doing nonlinear regression.
NONLINEAR
REGRESSION
Before delving into learning rules, let us look at the ways in which networks generate
complicated functions. First, remember that each unit in a sigmoid network represents a soft
soft threshold functions and thresholding the result, we can obtain a “ridge” function as shown
With more hidden units, we can produce more bumps of different sizes in more places.
In fact, with a single, sufﬁciently large hidden layer, it is possible to represent any continuous
function of the inputs with arbitrary accuracy; with two layers, even discontinuous functions
acterize exactly which functions can be represented and which ones cannot.Artiﬁcial Neural Networks
Learning in multilayer networks
First, let us dispense with one minor complication arising in multilayer networks: interactions
among the learning problems when the network has multiple outputs. In such cases, we
should think of the network as implementing a vector function hw rather than a scalar function
target output will be a vector y. Whereas a perceptron network decomposes into m separate
updates to those weights will depend on errors in both a5 and a6. Fortunately, this dependency
is very simple in the case of any loss function that is additive across the components of the
error vector y −hw(x). For the L2 loss, we have, for any weight w,
∂
∂wLoss(w) = ∂
∂w


k


k
∂
where the index k ranges over nodes in the output layer. Each term in the ﬁnal summation
is just the gradient of the loss for the kth output, computed as if the other outputs did not
provided we remember to add up the gradient contributions from each of them when updating
the weights.
The major complication comes from the addition of hidden layers to the network.
Whereas the error y −hw at the output layer is clear, the error at the hidden layers seems
mysterious because the training data do not say what value the hidden nodes should have.
error gradient. First, we will describe the process with an intuitive justiﬁcation; then, we will
show the derivation.
multiple output units, so let Errk be the kth component of the error vector y −hw. We will
update rule becomes
wj,k ←wj,k + α × aj × Δk .
To update the connections between the input units and the hidden units, we need to deﬁne a
propagation. The idea is that hidden node j is “responsible” for some fraction of the error Δk
in each of the output nodes to which it connects. Thus, the Δk values are divided according
agated back to provide the Δj values for the hidden layer. The propagation rule for the Δ
values is the following:
Δj = g′(inj)


k
wj,kΔk .Chapter
Learning from Examples
inputs: examples, a set of examples, each with input vector x and output vector y
network, a multilayer network with L layers, weights wi,j, activation function g
local variables: Δ, a vector of errors, indexed by network node
repeat
for each weight wi,j in network do
wi,j ←a small random number
for each example (x, y) in examples do
/* Propagate the inputs forward to compute the outputs */
for each node i in the input layer do
ai ←xi
for each node j in layer ℓdo
inj ←
i wi,j ai
aj ←g(inj)
/* Propagate deltas backward from output layer to input layer */
for each node j in the output layer do
Δ[j] ←g′(inj) × (yj −aj)
for each node i in layer ℓdo
Δ[i] ←g′(ini) 
j wi,j Δ[j]
/* Update every weight in network using deltas */
for each weight wi,j in network do
wi,j ←wi,j + α × ai × Δ[j]
until some stopping criterion is satisﬁed
return network
tially identical to the update rule for the output layer:
wi,j ←wi,j + α × ai × Δj .
• Compute the Δ values for the output units, using the observed error.
• Starting with output layer, repeat the following for each layer in the network, until the
earliest hidden layer is reached:
– Propagate the Δ values back to the previous layer.
– Update the weights between the two layers.
from ﬁrst principles. The derivation is quite similar to the gradient calculation for logisticArtiﬁcial Neural Networks
rule more than once.
the kth output. The gradient of this loss with respect to weights connecting the hidden layer
to the output layer will be zero except for weights wj,k that connect to the kth output unit.
For those weights, we have
∂Lossk
∂wj,k
∂wj,k
∂wj,k
∂wj,k
∂
∂wj,k
⎛
⎝

j
wj,kaj
⎞
⎠
with Δk deﬁned as before. To obtain the gradient with respect to the wi,j weights connecting
the input layer to the hidden ¡layer, we have to expand out the activations aj and reapply the
chain rule. We will show the derivation in gory detail because it is interesting to see how the
derivative operator propagates back through the network:
∂Lossk
∂wi,j
∂wi,j
∂wi,j
∂wi,j
= −2Δk
∂
∂wi,j
⎛
⎝

j
wj,kaj
⎞
⎠
= −2Δkwj,k
∂aj
∂wi,j
= −2Δkwj,k
∂g(inj)
∂wi,j
= −2Δkwj,kg′(inj) ∂inj
∂wi,j
= −2Δkwj,kg′(inj)
∂
∂wi,j


i
wi,jai
 
= −2Δkwj,kg′(inj)ai = −aiΔj ,
where Δj is deﬁned as before. Thus, we obtain the update rules obtained earlier from intuitive
considerations. It is also clear that the process can be continued for networks with more than
Should they be fully connected? There is no good theory that will tell us the answer. (See the
which one works best. It turns out that a network with one hidden layer containing four nodesChapter
Learning from Examples
Total error on training set
Number of epochs
Proportion correct on test set
Training set size
Decision tree
Multilayer network
(a)
(b)
(a) Training curve showing the gradual reduction in error as weights are
modiﬁed over several epochs, for a given set of examples in the restaurant domain. (b)
to a perfect ﬁt to the training data. The second curve is the standard learning curve for the
tree learning; this is perhaps not surprising, because the data were generated from a simple
decision tree in the ﬁrst place.
Neural networks are capable of far more complex learning tasks of course, although it
must be said that a certain amount of twiddling is needed to get the network structure right
and to achieve convergence to something close to the global optimum in weight space. There
looks at one such application in more depth.
Learning neural network structures
So far, we have considered the problem of learning weights, given a ﬁxed network structure;
just as with Bayesian networks, we also need to understand how to ﬁnd the best network
structure. If we choose a network that is too big, it will be able to memorize all the examples
by forming a large lookup table, but will not necessarily generalize well to inputs that have
If we stick to fully connected networks, the only choices to be made concern the number
restriction keeps the activation values in the linear region of the sigmoid function g(x) where x is close to zero.Nonparametric Models
of hidden layers and their sizes. The usual approach is to try several and keep the best. The
set. That is, we choose the network architecture that gives the highest prediction accuracy on
the validation sets.
If we want to consider networks that are not fully connected, then we need to ﬁnd
some effective search method through the very large space of possible connection topologies.
The optimal brain damage algorithm begins with a fully connected network and removes
OPTIMAL BRAIN
DAMAGE
approach identiﬁes an optimal selection of connections that can be dropped. The network
is then retrained, and if its performance has not decreased then the process is repeated. In
addition to removing connections, it is also possible to remove units that are not contributing
much to the result.
Several algorithms have been proposed for growing a larger network from a smaller one.
TILING
unit that does its best to produce the correct output on as many of the training examples as
possible. Subsequent units are added to take care of the examples that the ﬁrst unit got wrong.
The algorithm adds only as many units as are needed to cover all the examples.
NONPARAMETRIC MODELS
eters w. That deﬁnes our hypothesis hw(x), and at that point we can throw away the training
data, because they are all summarized by w. A learning model that summarizes data with a
set of parameters of ﬁxed size (independent of the number of training examples) is called a
parametric model.
PARAMETRIC MODEL
No matter how much data you throw at a parametric model, it won’t change its mind
about how many parameters it needs. When data sets are small, it makes sense to have a strong
restriction on the allowable hypotheses, to avoid overﬁtting. But when there are thousands or
millions or billions of examples to learn from, it seems like a better idea to let the data speak
for themselves rather than forcing them to speak through a tiny vector of parameters. If the
data say that the correct answer is a very wiggly function, we shouldn’t restrict ourselves to
linear or slightly wiggly functions.
NONPARAMETRIC
MODEL
eters. For example, suppose that each hypothesis we generate simply retains within itself all
of the training examples and uses all of them to predict the next example. Such a hypothesis
family would be nonparametric because the effective number of parameters is unbounded—
LEARNING
TABLE LOOKUP
all the training examples, put them in a lookup table, and then when asked for h(x), see if x is
in the table; if it is, return the corresponding y. The problem with this method is that it does
not generalize well: when x is not in the table all it can do is return some default value.Chapter
Learning from Examples
x1
x2
x1
x2
problem goes away for this data set.
Nearest neighbor models
We can improve on table lookup with a slight variation: given a query xq, ﬁnd the k examples
NEAREST
NEIGHBORS
NN (k, xq) to denote the set of k nearest neighbors.
To do classiﬁcation, ﬁrst ﬁnd NN (k, xq), then take the plurality vote of the neighbors
(which is the majority vote in the case of binary classiﬁcation). To avoid ties, k is always
chosen to be an odd number. To do regression, we can take the mean or median of the k
neighbors, or we can solve a linear regression problem on the neighbors.
neighbors is overﬁtting; it reacts too much to the black outlier in the upper right and the white
The very word “nearest” implies a distance metric. How do we measure the distance
from a query point xq to an example point xj? Typically, distances are measured with a
Minkowski distance or Lp norm, deﬁned as
MINKOWSKI
DISTANCE
Lp(xj, xq) = (


i
HAMMING DISTANCE
as the width, height and depth of parts on a conveyor belt, and Manhattan distance is used if
they are dissimilar, such as age, weight, and gender of a patient. Note that if we use the raw
numbers from each dimension then the total distance will be affected by a change in scale
in any dimension. That is, if we change dimension i from measurements in centimeters toNonparametric Models
miles while keeping the other dimensions the same, we’ll get different nearest neighbors. To
avoid this, it is common to apply normalization to the measurements in each dimension. One
NORMALIZATION
dimension, and rescale them so that xj,i becomes (xj,i −μi)/σi. A more complex metric
known as the Mahalanobis distance takes into account the covariance between dimensions.
MAHALANOBIS
DISTANCE
are likely to have enough nearby data points to get a good answer. But as the number of
ℓbe the average side length of a neighborhood. Then the volume of the neighborhood (which
This problem has been called the curse of dimensionality.
CURSE OF
DIMENSIONALITY
Another way to look at it: consider the points that fall within a thin shell making up the
value for them because we will be extrapolating rather than interpolating. In one dimension,
The NN (k, xq) function is conceptually trivial: given a set of N examples and a query
xq, iterate through the examples, measure the distance to xq from each one, and keep the best
k. If we are satisﬁed with an implementation that takes O(N) execution time, then that is the
like an algorithm with sublinear run time. Elementary analysis of algorithms tells us that
with a hash table. We will now see that binary trees and hash tables are also applicable for
ﬁnding nearest neighbors.
balanced binary tree. We start with a set of examples and at the root node we split them along
examples along the ith dimension; thus half the examples will be in the left branch of the treeChapter
Learning from Examples
Edge length of neighborhood
Number of dimensions
Proportion of points in exterior shell
Number of dimensions
(a)
(b)
The curse of dimensionality: (a) The length of the average neighborhood for
of dimensions. (b) The proportion of points that fall within a thin shell consisting of the
randomly distributed points.
and half in the right. We then recursively make a tree for the left and right sets of examples,
stopping when there are fewer than two examples left. To choose a dimension to split on at
each node of the tree, one can simply select dimension i mod n at level i of the tree. (Note
that we may need to split on any given dimension several times as we proceed down the tree.)
Another strategy is to split on the dimension that has the widest spread of values.
complication that you need to pay attention to which dimension you are testing at each node).
But nearest neighbor lookup is more complicated. As we go down the branches, splitting
the examples in half, in some cases we can discard the other half of the examples. But not
always. Sometimes the point we are querying for falls very close to the dividing boundary.
The query point itself might be on the left hand side of the boundary, but one or more of
possibility by computing the distance of the query point to the dividing boundary, and then
searching both sides if we can’t ﬁnd k examples on the left that are closer than this distance.
we don’t have enough examples, lookup is no faster than a linear scan of the entire data set.
Hash tables have the potential to provide even faster lookup than binary trees. But how can
we ﬁnd nearest neighbors using a hash table, when hash codes rely on an exact match? Hash
codes randomly distribute values among the bins, but we want to have near points grouped
HASHNonparametric Models
We can’t use hashes to solve NN (k, xq) exactly, but with a clever use of randomized
neighbors problem: given a data set of example points and a query point xq, ﬁnd, with high
APPROXIMATE
probability, an example point (or points) that is near xq. To be more precise, we require that
if there is a point xj that is within a radius r of xq, then with high probability the algorithm
will ﬁnd a point xj′ that is within distance c r of q. If there is no point within radius r then the
algorithm is allowed to report failure. The values of c and “high probability” are parameters
of the algorithm.
To solve approximate near neighbors, we will need a hash function g(x) that has the
property that, for any two points xj and xj′, the probability that they have the same hash code
is small if their distance is more than c r, and is high if their distance is less than r. For
simplicity we will treat each point as a bit string. (Any features that are not Boolean can be
encoded into a set of Boolean features.)
ability, near points project down to exactly the same bin. Points that are far away from each
other will tend to project down into different bins for most projections, but there will always
bin for point xq contains many (but not all) points that are near to xq, as well as some points
that are far away.
The trick of LSH is to create multiple random projections and combine them. A random
random projections and create ℓhash tables, g1(x), . . . , gℓ(x). We then enter all the examples
into each hash table. Then when given a query point xq, we fetch the set of points in bin gk(q)
for each k, and union these sets together into a set of candidate points, C. Then we compute
the actual distance to xq for each of the points in C and return the k closest points. With high
probability, each of the points that are near to xq will show up in at least one of the bins, and
Nonparametric regression
linear nonparametric regression.” This model creates a function h(x) that, when given a
query xq, solves the ordinary linear regression problem with just two points: the training
examples immediately to the left and right of xq. When noise is low, this trivial method is
actually not too bad, which is why it is a standard feature of charting software in spreadsheets.Chapter
Learning from Examples
(a)
(b)
(c)
(d)
But when the data are noisy, the resulting function is spiky, and does not generalize well.
NEIGHBORS
REGRESSION
stead of using just the two examples to the left and right of a query point xq, we use the
which ﬁnds the best line through the k examples. This does a better job of capturing trends at
the outliers, but is still discontinuous. In both (b) and (c), we’re left with the question of how
LOCALLY WEIGHTED
REGRESSIONNonparametric Models
nuities in the set of examples we use to estimate h(x). The idea of locally weighted regression
is that at each query point xq, the examples that are close to xq are weighted heavily, and the
examples that are farther away are weighted less heavily or not at all. The decrease in weight
over distance is always gradual, not sudden.
We decide how much to weight each example with a function known as a kernel. A
KERNEL
note that we invoke a kernel function K with K(Distance(xj, xq)), where xq is a query point
that is a given distance from xj, and we want to know how much to weight that distance.
must remain bounded as we go to ±∞. Other shapes, such as Gaussians, have been used for
kernels, but the latest research suggests that the choice of shape doesn’t matter much. We
do have to be careful about the width of the kernel. Again, this is a parameter of the model
kernels are too wide we’ll get underﬁtting and if they are too narrow we’ll get overﬁtting. In
more responsive to individual points.
Doing locally weighted regression with kernels is now straightforward. For a given
query point xq we solve the following weighted regression problem using gradient descent:
w∗= argmin
w


j
where Distance is any of the distance metrics discussed for nearest neighbors. Then the
answer is h(xq) = w∗· xq.
Note that we need to solve a new regression problem for every query point—that’s what
it means to be local. (In ordinary linear regression, we solved the regression problem once,
globally, and then used the same hw for any query point.) Mitigating against this extra workChapter
Learning from Examples
is the fact that each regression problem will be easier to solve, because it involves only the
examples with nonzero weight—the examples whose kernels overlap the query point. When
kernel widths are small, this may be just a few points.
instance, when given a test example (x, y) we retrieve the k nearest neighbors once, compute
and record distinct results for leaving out each of the k neighbors. With N examples the
whole process is O(k), not O(kN).
SUPPORT VECTOR MACHINES
The support vector machine or SVM framework is currently the most popular approach for
SUPPORT VECTOR
MACHINE
a domain, then the SVM is an excellent method to try ﬁrst. There are three properties that
make SVMs attractive:
possible distance to example points. This helps them generalize well.
original space. This means the hypothesis space is greatly expanded over methods that
use strictly linear representations.
to store them all. On the other hand, in practice they often end up retaining only a
small fraction of the number of examples—sometimes as few as a small constant times
the number of dimensions. Thus SVMs combine the advantages of nonparametric and
parametric models: they have the ﬂexibility to represent complex functions, but they
are resistant to overﬁtting.
You could say that SVMs are successful because of one key insight and one neat trick. We
candidate decision boundaries, each a linear separator. Each of them is consistent with all
regression would ﬁnd some separating line; the exact location of the line depends on all the
example points. The key insight of SVMs is that some examples are more important than
others, and that paying attention to them can lead to better generalization.
black examples. Although it classiﬁes all the examples correctly, and thus minimizes loss, itSupport Vector Machines
(a)
(b)
Support vector machine classiﬁcation: (a) Two classes of points (black and
white circles) and three candidate linear separators. (b) The maximum margin separator
(heavy line), is at the midpoint of the margin (area between dashed lines). The support
vectors (points with large circles) are the examples closest to the separator.
should make you nervous that so many examples are close to the line; it may be that other
black examples will turn out to fall on the other side of the line.
SVMs address this issue: Instead of minimizing expected empirical loss on the training
data, SVMs attempt to minimize expected generalization loss. We don’t know where the
from the same distribution as the previously seen examples, there are some arguments from
by choosing the separator that is farthest away from the examples we have seen so far. We
MAXIMUM MARGIN
SEPARATOR
MARGIN
is the width of the area bounded by dashed lines in the ﬁgure—twice the distance from the
separator to the nearest example point.
Now, how do we ﬁnd this separator? Before showing the equations, some notation:
as a separate parameter, b. With that in mind, the separator is deﬁned as the set of points
parameters that maximize the margin while correctly classifying all the examples.
However, it turns out there is another approach to solving this problem. We won’t
show the details, but will just say that there is an alternative representation called the dualChapter
Learning from Examples
representation, in which the optimal solution is found by solving
argmax
α


j


j,k
αjαkyjyk(xj · xk)
QUADRATIC
PROGRAMMING
optimization problem, for which there are good software packages. Once we have found the
vector α we can get back to w with the equation w = 
j αjxj, or we can stay in the dual
is convex; it has a single global maximum that can be found efﬁciently. Second, the data enter
the expression only in the form of dot products of pairs of points. This second property is also
true of the equation for the separator itself; once the optimal αj have been calculated, it is
h(x) = sign
⎛
⎝

j
αjyj(x · xj) −b
⎞
⎠.
cept for the support vectors—the points closest to the separator. (They are called “support”
SUPPORT VECTOR
vectors because they “hold up” the separating plane.) Because there are usually many fewer
support vectors than examples, SVMs gain some of the advantages of parametric models.
of feature values, F(x). In particular, let us use the three features
f1 = x2
f2 = x2
f3 =
√
2x1x2 .
the data are linearly separable in this space! This phenomenon is actually fairly general: if
data are mapped into a space of sufﬁciently high dimension, then they will almost always be
linearly separable—if you look at a set of points from enough directions, you’ll ﬁnd a way to
that four dimensions sufﬁce for linearly separating a circle anywhere in the plane (not just at
the origin), and ﬁve dimensions sufﬁce to linearly separate any ellipse. In general (with some
special cases excepted) if we have N data points then they will always be separable in spaces
Now, we would not usually expect to ﬁnd a linear separator in the input space x, but
F(x) in any learning algorithm has the required effect—but the dot product has some special
properties. It turns out that F(xj) · F(xk) can often be computed without ﬁrst computing FSupport Vector Machines
x2
x1
x1
x2
√2x1x2
(a)
(b)
cles and negative examples as white circles. The true decision boundary, x2
is also shown.
(x2
√
2x1x2). The circular decision boundary in (a) becomes a linear decision boundary
of algebra shows that
(That’s why the
√
KERNEL FUNCTION
is usually written as K(xj, xk). The kernel function can be applied to pairs of input data to
evaluate dot products in some corresponding feature space. So, we can ﬁnd linear separators
compute only kernel functions rather than the full list of features for each data point.
correspond to other feature spaces.
MERCER’S THEOREM
POLYNOMIAL
KERNEL
whose dimension is exponential in d.
SVM kernels are distance metrics, but not all are.Chapter
Learning from Examples
KERNEL TRICK
optimal linear separators can be found efﬁciently in feature spaces with billions of (or, in
some cases, inﬁnitely many) dimensions. The resulting linear separators, when mapped back
aries between the positive and negative examples.
sible with the soft margin classiﬁer, which allows examples to fall on the wrong side of the
SOFT MARGIN
decision boundary, but assigns them a penalty proportional to the distance required to move
them back on the correct side.
The kernel method can be applied not only with learning algorithms that ﬁnd optimal
linear separators, but also with any other algorithm that can be reformulated to work only
done, the dot product is replaced by a kernel function and we have a kernelized version
KERNELIZATION
ENSEMBLE LEARNING
So far we have looked at learning methods in which a single hypothesis, chosen from a
hypothesis space, is used to make predictions. The idea of ensemble learning methods is
ENSEMBLE
LEARNING
to select a collection, or ensemble, of hypotheses from the hypothesis space and combine
decision trees, and have them vote on the best classiﬁcation for a new example.
potheses and suppose that we combine their predictions using simple majority voting. For the
sify it. The hope is that this is much less likely than a misclassiﬁcation by a single hypothesis.
Suppose we assume that each hypothesis hk in the ensemble has an error of p—that is, the
probability that a randomly chosen example is misclassiﬁed by hk is p. Furthermore, suppose
we assume that the errors made by each hypothesis are independent. In that case, if p is small,
the assumption of independence is unreasonable, because hypotheses are likely to be misled
in the same way by any misleading aspects of the training data. But if the hypotheses are at
least a little bit different, thereby reducing the correlation between their errors, then ensemble
learning can be very useful.
Another way to think about the ensemble idea is as a generic way of enlarging the
hypothesis space. That is, think of the ensemble itself as a hypothesis and the new hypothesisEnsemble Learning
+
+ +
+
+
+
+
+
+
+
+ +
+
+
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
– –
–
–
–
–
–
–
ing. We take three linear threshold hypotheses, each of which classiﬁes positively on the
unshaded side, and classify as positive any example classiﬁed positively by all three. The
resulting triangular region is a hypothesis not expressible in the original hypothesis space.
space as the set of all possible ensembles constructable from hypotheses in the original space.
hypothesis space allows for a simple and efﬁcient learning algorithm, then the ensemble
method provides a way to learn a much more expressive class of hypotheses without incurring
much additional computational or algorithmic complexity.
The most widely used ensemble method is called boosting. To understand how it works,
BOOSTING
we need ﬁrst to explain the idea of a weighted training set. In such a training set, each
WEIGHTED TRAINING
SET
is the importance attached to it during the learning of a hypothesis. It is straightforward to
amples correctly and some incorrectly. We would like the next hypothesis to do better on the
rectly classiﬁed examples. From this new weighted training set, we generate hypothesis h2.
The process continues in this way until we have generated K hypotheses, where K is an input
of all the K hypotheses, each weighted according to how well it performed on the training set.
sic boosting idea, with different ways of adjusting the weights and combining the hypotheses.
important property: if the input learning algorithm L is a weak learning algorithm—which
WEAK LEARNING
the jth example appears wj times, using randomization to handle fractional weights.Chapter
Learning from Examples
h
h1 =
h2 =
h3 =
h4 =
How the boosting algorithm works. Each shaded rectangle corresponds to
an example; the height of the rectangle corresponds to the weight. The checks and crosses
indicate whether the example was classiﬁed correctly by the current hypothesis. The size of
the decision tree indicates the weight of that hypothesis in the ﬁnal ensemble.
means that L always returns a hypothesis with accuracy on the training set that is slightly
return a hypothesis that classiﬁes the training data perfectly for large enough K. Thus, the
algorithm boosts the accuracy of the original learning algorithm on the training data. This
result holds no matter how inexpressive the original hypothesis space and no matter how
complex the function being learned.
Let us see how well boosting does on the restaurant data. We will choose as our original
hypothesis space the class of decision stumps, which are decision trees with just one test, at
DECISION STUMP
remains at zero. The graph also shows that the test set performance continues to increase
This ﬁnding, which is quite robust across data sets and hypothesis spaces, came as quite
a surprise when it was ﬁrst noticed. Ockham’s razor tells us not to make hypotheses moreEnsemble Learning
inputs: examples, set of N labeled examples (x1, y1), . . . , (xN, yN)
L, a learning algorithm
K, the number of hypotheses in the ensemble
h, a vector of K hypotheses
z, a vector of K hypothesis weights
h[k] ←L(examples,w)
if h[k](xj) ̸= yj then error ←error + w[j]
w ←NORMALIZE(w)
The ADABOOST variant of the boosting method for ensemble learning. The
highest vote from the hypotheses in h, with votes weighted by z.
Proportion correct on test set
Training set size
Boosted decision stumps
Decision stump
Number of hypotheses K
Training error
Test error
(a)
(b)
versus unboosted decision stumps on the restaurant data. (b) The proportion correct on the
training set and the test set as a function of K, the number of hypotheses in the ensemble.
i.e., after the ensemble ﬁts the data exactly.Chapter
Learning from Examples
complex than necessary, but the graph tells us that the predictions improve as the ensemble
hypothesis gets more complex! Various explanations have been proposed for this. One view
be an optimal learning algorithm, and the approximation improves as more hypotheses are
added. Another possible explanation is that the addition of further hypotheses enables the
ensemble to be more deﬁnite in its distinction between positive and negative examples, which
helps it when it comes to classifying new examples.
Online Learning
So far, everything we have done in this chapter has relied on the assumption that the data are
i.i.d. (independent and identically distributed). On the one hand, that is a sensible assumption:
if the future bears no resemblance to the past, then how can we predict anything? On the other
hand, it is too strong an assumption: it is rare that our inputs have captured all the information
that would make the future truly independent of the past.
In this section we examine what to do when the data are not i.i.d.; when they can change
over time. In this case, it matters when we make a prediction, so we will adopt the perspective
called online learning: an agent receives an input xj from nature, predicts the corresponding
ONLINE LEARNING
might think this task is hopeless—if nature is adversarial, all the predictions may be wrong.
It turns out that there are some guarantees we can make.
Let us consider the situation where our input consists of predictions from a panel of
experts. For example, each day a set of K pundits predicts whether the stock market will go
up or down, and our task is to pool those predictions and make our own. One way to do this
is to keep track of how well each expert performs, and choose to believe them in proportion
to their past performance. This is called the randomized weighted majority algorithm. We
RANDOMIZED
WEIGHTED
MAJORITY
ALGORITHM
can described it more formally:
k′ wk′).
We measure the success of this algorithm in terms of regret, which is deﬁned as the
REGRET
number of additional mistakes we make compared to the expert who, in hindsight, had the
best prediction record. Let M∗be the number of mistakes made by the best expert. Then the
number of mistakes, M, made by the random weighted majority algorithm, is bounded by15
.Practical Machine Learning
This bound holds for any sequence of examples, even ones chosen by adversaries trying to
changes, we will pick up on it before too long. However, we pay a penalty at the beginning,
when we start with all experts trusted equally; we may accept the advice of the bad experts
LEARNING
Online learning is helpful when the data may be changing rapidly over time. It is also
useful for applications that involve a large collection of data that is constantly growing, even
if changes are gradual. For example, with a database of millions of Web images, you wouldn’t
want to train, say, a linear regression model on all the data, and then retrain from scratch every
time a new image is added. It would be more practical to have an online algorithm that allows
images to be added incrementally. For most learning algorithms based on minimizing loss,
there is an online version based on minimizing regret. It is a bonus that many of these online
algorithms come with guaranteed bounds on regret.
To some observers, it is surprising that there are such tight bounds on how well we can
do compared to a panel of experts. To others, the really surprising thing is that when panels
of human experts congregate—predicting stock market prices, sports outcomes, or political
contests—the viewing public is so willing to listen to them pontiﬁcate and so unwilling to
quantify their error rates.
PRACTICAL MACHINE LEARNING
We have introduced a wide range of machine learning techniques, each illustrated with simple
learning tasks. In this section, we consider two aspects of practical machine learning. The ﬁrst
involves ﬁnding algorithms capable of learning to recognize handwritten digits and squeezing
every last drop of predictive performance out of them. The second involves anything but—
pointing out that obtaining, cleaning, and representing the data can be at least as important as
algorithm engineering.
Case study: Handwritten digit recognition
Recognizing handwritten digits is an important problem with many applications, including
automated sorting of mail by postal code, automated reading of checks and tax returns, and
because of better learning algorithms and in part because of the availability of better training
sets. The United States National Institute of Science and Technology (NIST) has archived a
has become one of the standard benchmark problems for comparing new learning algorithms.Chapter
Learning from Examples
Examples from the NIST database of handwritten digits. Top row: examples
Many different learning approaches have been tried. One of the ﬁrst, and probably the
A series of specialized neural networks called LeNet were devised to take advantage
of the structure of the problem—that the input consists of pixels in a two–dimensional array,
and that small changes in the position or slant of an image are unimportant. Each network
input unit is presented with a local neighborhood of pixels. This was followed by three layers
of hidden units. Each layer consisted of several planes of n × n arrays, where n is smaller
of every unit in a plane are constrained to be identical, so that the plane is acting as a feature
by applying afﬁne transformations to the actual inputs: shifting, slightly rotating, and scaling
A boosted neural network combined three copies of the LeNet architecture, with the
trained on patterns for which the ﬁrst two disagreed. During testing, the three nets voted with
neighbor approach, required almost no thought or iterated experimentation on the part of the
opment. Indeed, the support vector machine makes no use of the structure of the problem,
and would perform just as well if the pixels were presented in a permuted order.Practical Machine Learning
A virtual support vector machine starts with a regular SVM and then improves it
VIRTUAL SUPPORT
VECTOR MACHINE
with a technique that is designed to take advantage of the structure of the problem. Instead of
allowing products of all pixel pairs, this approach concentrates on kernels formed from pairs
of nearby pixels. It also augments the training set with transformations of the examples, just
Shape matching is a technique from computer vision used to align corresponding parts
of points in each of the two images, and then compute, for each point in the ﬁrst image,
which point in the second image it corresponds to. From this alignment, we then compute a
transformation between the images. The transformation gives us a measure of the distance
between the images. This distance measure is better motivated than just counting the number
sample points per image extracted from a Canny edge detector, a shape matching classiﬁer
is somewhat suspect because humans have not been tested as extensively as have machine
learning algorithms. On a similar data set of digits from the United States Postal Service,
quirements, and amount of training time for the seven algorithms we have discussed. It also
The following table summarizes the error rate and some of the other characteristics of
the seven techniques we have discussed.
Boosted
Virtual Shape
NN
Hidden LeNet
LeNet
SVM
SVM
Match
Error rate (pct.)
Memory requirements (Mbyte)
Training time (days)
Case study: Word senses and house prices
In a textbook we need to deal with simple, toy data to get the ideas across: a small data set,
usually in two dimensions. But in practical applications of machine learning, the data set
is usually large, multidimensional, and messy. The data are not handed to the analyst in a
prepackaged set of (x, y) values; rather the analyst needs to go out and acquire the right data.
There is a task to be accomplished, and most of the engineering problem is deciding what
data are necessary to accomplish the task; a smaller part is choosing and implementing anChapter
Learning from Examples
Proportion correct on test set
Training set size (millions of words)
Learning curves for ﬁve learning algorithms on a common task. Note that
there appears to be more room for improvement in the horizontal direction (more training
data) than in the vertical direction (different machine learning algorithm). Adapted from
vertical direction: Can I invent a new learning algorithm that performs better than previously
there is more room for improvement in the horizontal direction: instead of inventing a new
even more data, the curves continue to rise, dwarﬁng the differences between algorithms.
Consider another problem: the task of estimating the true value of houses that are for
house size to asking price. You probably noticed many limitations of this model. First, it is
measuring the wrong thing: we want to estimate the selling price of a house, not the asking
price. To solve this task we’ll need data on actual sales. But that doesn’t mean we should
throw away the data about asking price—we can use it as one of the input features. Besides
the size of the house, we’ll need more information: the number of rooms, bedrooms and
bathrooms; whether the kitchen and bathrooms have been recently remodeled; the age of
the house; we’ll also need information about the lot, and the neighborhood. But how do
we deﬁne neighborhood? By zip code? What if part of one zip code is on the “wrong”
side of the highway or train tracks, and the other part is desirable? What about the school
district? Should the name of the school district be a feature, or the average test scores? In
addition to deciding what features to include, we will have to deal with missing data; different
areas have different customs on what data are reported, and individual cases will always be
missing some data. If the data you want are not available, perhaps you can set up a social
networking site to encourage people to share and correct data. In the end, this process ofSummary
deciding what features to use, and how to use them, is just as important as choosing between
linear regression, decision trees, or some other form of learning.
That said, one does have to pick a method (or methods) for a problem. There is no
guaranteed way to pick the best method, but there are some rough guidelines. Decision
trees are good when there are a lot of discrete features and you believe that many of them
may be irrelevant. Nonparametric methods are good when you have a lot of data and no prior
knowledge, and when you don’t want to worry too much about choosing just the right features
a function h that is more expensive to run. Support vector machines are often considered the
best method to try ﬁrst, provided the data set is not too large.
SUMMARY
This chapter has concentrated on inductive learning of functions from examples. The main
points were as follows:
• Learning takes many forms, depending on the nature of the agent, the component to be
improved, and the available feedback.
ing problem is called supervised learning. The task is to learn a function y = h(x).
tion is called regression.
• Inductive learning involves ﬁnding a hypothesis that agrees well with the examples.
Ockham’s razor suggests choosing the simplest consistent hypothesis. The difﬁculty
of this task depends on the chosen representation.
provides an efﬁcient method for ﬁnding a simple, consistent decision tree.
• The performance of a learning algorithm is measured by the learning curve, which
a model that will generalize well.
• Sometimes not all errors are equal. A loss function tells us how bad each error is; the
goal is then to minimize loss over a validation set.
• Computational learning theory analyzes the sample complexity and computational
complexity of inductive learning. There is a tradeoff between the expressiveness of the
hypothesis language and the ease of learning.
sion model can be found by gradient descent search, or computed exactly.
• A linear classiﬁer with a hard threshold—also known as a perceptron—can be trained
by a simple weight update rule to ﬁt data that are linearly separable. In other cases,
the rule fails to converge.Chapter
Learning from Examples
ﬁned by a logistic function. Gradient descent works well even for noisy data that are
not linearly separable.
scent in parameter space to minimize the output error.
• Nonparametric models use all the data to make each prediction, rather than trying to
summarize the data ﬁrst with a few parameters. Examples include nearest neighbors
and locally weighted regression.
• Support vector machines ﬁnd linear separators with maximum margin to improve
the generalization performance of the classiﬁer. Kernel methods implicitly transform
• Ensemble methods such as boosting often perform better than individual methods. In
online learning we can aggregate the opinions of experts to come arbitrarily close to the
best expert’s performance, even when the distribution of the data is constantly shifting.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
tributor to medieval epistemology, logic, and metaphysics, is credited with a statement called
glish, “Entities are not to be multiplied beyond necessity.” Unfortunately, this laudable piece
of advice is nowhere to be found in his writings in precisely these words (although he did
say “Pluralitas non est ponenda sine necessitate,” or “plurality shouldn’t be posited without
chapter VI: “For the more limited, if adequate, is always preferable.”
The ﬁrst notable use of decision trees was in EPAM, the “Elementary Perceiver And
the basis for the decision tree algorithm in this chapter. Information theory was developed by
non also contributed one of the earliest examples of machine learning, a mechanical mouse
named Theseus that learned to navigate through a maze by trial and error.) The χ2 method
known as the “CART book,” is the principal reference.Bibliographical and Historical Notes
tages of using large amounts of data. It was Robert Mercer, a speech researcher who said
identiﬁcation in the limit. This approach was motivated in part by models of scientiﬁc
the study of Kolmogorov complexity or algorithmic complexity, developed independently
KOLMOGOROV
COMPLEXITY
for the notion of simplicity used in Ockham’s razor. To escape the problem that simplicity
depends on the way in which information is represented, it is proposed that simplicity be
measured by the length of the shortest program for a universal Turing machine that correctly
reproduces the observed data. Although there are many possible universal Turing machines,
and hence many possible “shortest” programs, these programs differ in length by at most a
constant that is independent of the amount of data. This beautiful insight, which essentially
shows that any initial representation bias will eventually be overcome by the data itself, is
MINIMUM
DESCRIPTION
LENGTH
information is available in the examples. Some positive results were obtained for classes such
UNIFORM
CONVERGENCE
THEORY
VC DIMENSION
ln |H| measure obtained from PAC analysis. The VC dimension can be applied to continuous
VC theory were ﬁrst connected by the “four Germans” (none of whom actually is German):Chapter
Learning from Examples
tic model than the unconstrained geometric growth proposed by Thomas Malthus. Verhulst
gression is due to Francis Galton, nineteenth century statistician, cousin of Charles Darwin,
and initiator of the ﬁelds of meteorology, ﬁngerprint analysis, and statistical correlation, who
used it in the sense of regression to the mean. The term curse of dimensionality comes from
that it avoids creating the full matrices all at once, and instead creates parts of them on the
standard tool in statistics and pattern recognition ever since. Within AI, they were popularized
recent survey of LSH and related methods.
troduced the kernel trick), but the full development of the theory is due to Vapnik and his
(SMO) algorithm for efﬁciently solving SVM problems using quadratic programming (Platt,
As part of this process, many new kernels have been designed that work with strings, trees,
plicitly represent an exponential feature space is the voted perceptron (Freund and Schapire,
limitations of SVMs and other local, nonparametric methods for learning functions that have
a global structure but do not have local smoothness.
Ensemble learning is an increasingly popular technique for improving the performance
BAGGING
inal data set. The boosting method described in this chapter originated with theoretical workBibliographical and Historical Notes
each parameter, they also maintain a measure of conﬁdence, so that a new example can have
a large effect on features that were rarely seen before (and thus had low conﬁdence) and a
date) to cover in detail. Cowan and Sharp (1988b, 1988a) survey the early history, beginning
of neural learning.) Norbert Wiener, a pioneer of cybernetics and control theory (Wiener,
cluding Marvin Minsky, who may have been the ﬁrst to develop a working neural network in
report titled Intelligent Machinery that begins with the sentence “I propose to investigate the
question as to whether it is possible for machinery to show intelligent behaviour” and goes on
was all but ignored until recently.
research efforts was hastened (or, the authors later claimed, merely explained) by the book
rable concepts and noted the lack of effective learning algorithms for multilayer networks.
(Parallel Distributed Processing) anthology (Rumelhart et al., 1986a) and a short article in
Nature (Rumelhart et al., 1986b) attracted a great deal of attention—indeed, the number of
The analysis of neural networks using the physical theory of magnetic spin glasses (Amit
The probabilistic interpretation of neural networks has several sources, including BaumChapter
Learning from Examples
enough to represent any function and a single layer is enough to represent any continuous
function. The “optimal brain damage” method for removing useless connections is by LeCun
showed that the general problem of ﬁnding a set of weights consistent with a set of examples
ples required for effective learning grows as roughly W log W, where W is the number of
depends on the size of the weights as well as on their number, a result that should not be
surprising in the light of our discussion of regularization.
The most popular kind of neural network that we did not cover is the radial basis
function, or RBF, network. A radial basis function combines a weighted collection of kernels
RADIAL BASIS
FUNCTION
(usually Gaussians, of course) to do function approximation. RBF networks can be trained in
two phases: ﬁrst, an unsupervised clustering approach is used to train the parameters of the
the relative weights of the Gaussians are determined. This is a system of linear equations,
which we know how to solve directly. Thus, both phases of RBF training have a nice beneﬁt:
the ﬁrst phase is unsupervised, and thus does not require labeled training data, and the second
HOPFIELD NETWORK
understood class of recurrent networks. They use bidirectional connections with symmetric
weights (i.e., wi,j = wj,i), all of the units are both input and output units, the activation
functions as an associative memory: after the network trains on a set of examples, a new
ASSOCIATIVE
MEMORY
stimulus will cause it to settle into an activation pattern corresponding to the example in the
sists of a set of photographs, and the new stimulus is a small piece of one of the photographs,
then the network activation levels will reproduce the photograph from which the piece was
taken. Notice that the original photographs are not stored separately in the network; each
mended using roughly ten times as many examples as weights.Chapter
Learning from Examples
classiﬁcation, minimizes the absolute error over the set of examples at the leaf.
CLASS PROBABILITY
Suppose that an attribute splits the set of examples E into subsets Ek and that each
subset has pk positive examples and nk negative examples. Show that the attribute has strictly
Consider the following data set comprised of three binary input attributes (A1, A2, and
A3) and one binary output:
Example A1 A2 A3 Output y
x1
x2
x3
x4
x5
computations made to determine the attribute to split at each node.
A decision graph is a generalization of a decision tree that allows nodes (i.e., attributes
used for splits) to have multiple parents, rather than just a single parent. The resulting graph
must still be acyclic. Now, consider the XOR function of three binary input attributes, which
a. Create a data set with two input attributes, such that the information gain at the root of
with all the data. What would χ2 pruning do on this data set if applied bottom up? If
applied top down?
not handle cases in which some examples have missing attribute values.
a. First, we need to ﬁnd a way to classify such examples, given a decision tree that includes
tests on the attributes for which values can be missing. Suppose that an example x has
a missing value for attribute A and that the decision tree tests for A at a node that x
reaches. One way to handle this case is to pretend that the example has all possible
values for the attribute, but to weight each value according to its frequency among all
of the examples that reach that node in the decision tree. The classiﬁcation algorithm
should follow all branches at any node for which a value is missing and should multiplyChapter
Learning from Examples
a. Show that decision lists can represent any Boolean function, if the size of the tests is
not limited.
b. Show that if the tests can contain at most k literals each, then decision lists can represent
any function that can be represented by a decision tree of depth k.
function on this data? There is a common name in statistics for this value as a function of the
y values; what is it? Answer the same two questions for the L2 loss function.
from the features (x1, x2) to the two dimensions (x2
at the origin? What if it is an ellipse, not a circle? The general equation for a circle (and
a. Expand out the equation for the circle and show what the weights wi would be for the
this means that any circle is linearly separable in this space.
Construct a support vector machine that computes the XOR function. Use values of
Draw the four input points in this space, and the maximal margin separator. What is the
margin? Now draw the separating line back in the original Euclidean input space.
Consider an ensemble learning algorithm that uses simple majority voting among
by each hypothesis are independent of the others’. Calculate a formula for the error of the
Construct by hand a neural network that computes the XOR function of two inputs.
Make sure to specify what sort of units you are using.
many of these are representable by a threshold perceptron?
and calculate the derivative of log p with respect to each weight wi. Repeat the process forKNOWLEDGE IN
LEARNING
In which we examine the problem of learning when you know something already.
In all of the approaches to learning described in the previous chapter, the idea is to construct
a function that has the input–output behavior observed in the data. In each case, the learning
methods can be understood as searching a hypothesis space to ﬁnd a suitable function, starting
polynomial” or “decision tree” and perhaps a preference for simpler hypotheses. Doing this
amounts to saying that before you can learn something new, you must ﬁrst forget (almost)
everything you know. In this chapter, we study learning methods that can take advantage
of prior knowledge about the world. In most cases, the prior knowledge is represented
PRIOR KNOWLEDGE
knowledge representation and learning.
A LOGICAL FORMULATION OF LEARNING
esis is represented by a set of logical sentences. Example descriptions and classiﬁcations will
also be logical sentences, and a new example can be classiﬁed by inferring a classiﬁcation
mental construction of hypotheses, one sentence at a time. It also allows for prior knowledge,
because sentences that are already known can assist in the classiﬁcation of new examples.
The logical formulation of learning may seem like a lot of extra work at ﬁrst, but it turns out
to clarify many of the issues in learning. It enables us to go well beyond the simple learning
Examples and hypothesesA Logical Formulation of Learning
and so on. In a logical setting, an example is described by a logical sentence; the attributes
become unary predicates. Let us generically call the ith example Xi. For instance, the ﬁrst
We will use the notation Di(Xi) to refer to the description of Xi, where Di can be any logical
expression taking a single argument. The classiﬁcation of the example is given by a literal
using the goal predicate, in this case
WillWait(X1)
or
¬WillWait(X1) .
tions and goal literals.
amples well and generalizes well to new examples. Here we are concerned with hypotheses
expressed in logic; each hypothesis hj will have the form
∀x Goal(x) ⇔Cj(x) ,
where Cj(x) is a candidate deﬁnition—some expression involving the attribute predicates.
For example, a decision tree can be interpreted as a logical expression of this form. Thus, the
hr for future reference):
∀r WillWait(r) ⇔Patrons(r, Some)
∨Patrons(r, Full) ∧Hungry(r) ∧Type(r, French)
∨Patrons(r, Full) ∧Hungry(r) ∧Type(r, Thai)
∨Patrons(r, Full) ∧Hungry(r) ∧Type(r, Burger) .
date deﬁnition—will be examples of the goal predicate. This set is called the extension of
EXTENSION
the predicate. Two hypotheses with different extensions are therefore logically inconsistent
with each other, because they disagree on their predictions for at least one example. If they
have the same extension, they are logically equivalent.
esis space therefore consists of all these decision trees. Presumably, the learning algorithm
believes that one of the hypotheses is correct; that is, it believes the sentence
h1 ∨h2 ∨h3 ∨. . . ∨hn .
As the examples arrive, hypotheses that are not consistent with the examples can be ruled
out. Let us examine this notion of consistency more carefully. Obviously, if hypothesis hj is
consistent with the entire training set, it has to be consistent with each example in the training
set. What would it mean for it to be inconsistent with an example? There are two possible
ways that this can happen:Chapter
Knowledge in Learning
• An example can be a false negative for the hypothesis, if the hypothesis says it should
FALSE NEGATIVE
be negative but in fact it is positive. For instance, the new example X13 described by
Patrons(X13, Full) ∧¬Hungry(X13) ∧. . . ∧WillWait(X13)
would be a false negative for the hypothesis hr given earlier. From hr and the example
description, we can deduce both WillWait(X13), which is what the example says,
and ¬WillWait(X13), which is what the hypothesis predicts. The hypothesis and the
example are therefore logically inconsistent.
• An example can be a false positive for the hypothesis, if the hypothesis says it should
FALSE POSITIVE
If an example is a false positive or false negative for a hypothesis, then the example and the
hypothesis are logically inconsistent with each other. Assuming that the example is a correct
observation of fact, then the hypothesis can be ruled out. Logically, this is exactly analogous
responds to a clause and the example corresponds to a literal that resolves against one of the
literals in the clause. An ordinary logical inference system therefore could, in principle, learn
from the example by eliminating one or more hypotheses. Suppose, for example, that the
example is denoted by the sentence I1, and the hypothesis space is h1 ∨h2 ∨h3 ∨h4. Then if
I1 is inconsistent with h2 and h3, the logical inference system can deduce the new hypothesis
space h1 ∨h4.
We therefore can characterize inductive learning in a logical setting as a process of
gradually eliminating hypotheses that are inconsistent with the examples, narrowing down
the possibilities. Because the hypothesis space is usually vast (or even inﬁnite in the case of
based theorem proving and a complete enumeration of the hypothesis space. Instead, we will
describe two approaches that ﬁnd logically consistent hypotheses with much less effort.
HYPOTHESIS
adjust it as new examples arrive in order to maintain consistency. The basic algorithm was
Suppose we have some hypothesis such as hr, of which we have grown quite fond.
As long as each new example is consistent, we need do nothing. Then along comes a false
everything inside the rectangle is part of the extension of hr. The examples that have actually
been seen so far are shown as “+” or “–”, and we see that hr correctly categorizes all the
(circled) is a false negative: the hypothesis says it should be negative but it is actually positive.
The extension of the hypothesis must be increased to include it. This is called generalization;
GENERALIZATION
positive: the hypothesis says the new example (circled) should be positive, but it actually is
tests. A result is a false positive if it indicates that the patient has the disease when in fact no disease is present.A Logical Formulation of Learning
(a)
(b)
(c)
(d)
(e)
+
+
+
+
+
++
–
–
–
–
–
–
–
– –
–
+
+
+
+
+
++
–
–
–
–
–
–
–
– –
–
+
+
+
+
+
+
++
–
–
–
–
–
–
–
– –
–
+
+
+
+
+
+
++
–
–
–
–
–
–
–
–
–
+
–
+
+
+
+
+
++
–
–
–
–
–
–
–
–
–
+
–
–
eralized. (d) A false positive. (e) The hypothesis is specialized.
if examples is empty then
return h
e ←FIRST(examples)
if e is consistent with h then
else if e is a false positive for h then
for each h′ in specializations of h consistent with examples seen so far do
if h′′ ̸= fail then return h′′
else if e is a false negative for h then
for each h′ in generalizations of h consistent with examples seen so far do
if h′′ ̸= fail then return h′′
return fail
it will be specialized or gneralized as needed.
negative. The extension of the hypothesis must be decreased to exclude the example. This is
SPECIALIZATION
The “more general than” and “more speciﬁc than” relations between hypotheses provide the
logical structure on the hypothesis space that makes efﬁcient search possible.
Notice that each time we consider generalizing or specializing the hypothesis, we must checkChapter
Knowledge in Learning
sion of a hypothesis. Now we need to determine exactly how they can be implemented as
syntactic operations that change the candidate deﬁnition associated with the hypothesis, so
ization are also logical relationships between hypotheses. If hypothesis h1, with deﬁnition
C1, is a generalization of hypothesis h2 with deﬁnition C2, then we must have
∀x C2(x) ⇒C1(x) .
tion C1 that is logically implied by C2.
This is easily done.
For example, if C2(x) is
Alternate(x) ∧Patrons(x, Some), then one possible generalization is given by C1(x) ≡
Patrons(x, Some). This is called dropping conditions. Intuitively, it generates a weaker
DROPPING
CONDITIONS
deﬁnition and therefore allows a larger set of positive examples. There are a number of other
generalization operations, depending on the language being operated on. Similarly, we can
specialize a hypothesis by adding extra conditions to its candidate deﬁnition or by removing
disjuncts from a disjunctive deﬁnition. Let us see how this works on the restaurant example,
• The ﬁrst example, X1, is positive. The attribute Alternate(X1) is true, so let the initial
hypothesis be
h1 :
∀x WillWait(x) ⇔Alternate(x) .
• The second example, X2, is negative. h1 predicts it to be positive, so it is a false positive.
Therefore, we need to specialize h1. This can be done by adding an extra condition that
will rule out X2, while continuing to classify X1 as positive. One possibility is
h2 :
∀x WillWait(x) ⇔Alternate(x) ∧Patrons(x, Some) .
• The third example, X3, is positive. h2 predicts it to be negative, so it is a false negative.
Therefore, we need to generalize h2. We drop the Alternate condition, yielding
h3 :
∀x WillWait(x) ⇔Patrons(x, Some) .
• The fourth example, X4, is positive. h3 predicts it to be negative, so it is a false negative.
We therefore need to generalize h3. We cannot drop the Patrons condition, because
possibility is to add a disjunct:
h4 :
∀x WillWait(x) ⇔Patrons(x, Some)
Already, the hypothesis is starting to look reasonable. Obviously, there are other possibilities
consistent with the ﬁrst four examples; here are two of them:
h′
h′′
∀x WillWait(x) ⇔Patrons(x, Some)
point, there may be several possible specializations or generalizations that can be applied. TheA Logical Formulation of Learning
local variables: V , the version space: the set of all hypotheses
V ←the set of all hypotheses
for each example e in examples do
return V
The version space learning algorithm. It ﬁnds a subset of V that is consistent
with all the examples.
choices that are made will not necessarily lead to the simplest hypothesis, and may lead to an
unrecoverable situation where no simple modiﬁcation of the hypothesis is consistent with all
of the data. In such cases, the program must backtrack to a previous choice point.
With a large number of examples and a large space, however, some difﬁculties arise:
hypothesis space can be a doubly exponentially large place.
hypothesis as its best guess even though it does not have enough data yet to be sure of the
choice. What we can do instead is to keep around all and only those hypotheses that are
consistent with all the data so far. Each new example will either have no effect or will get
rid of some of the hypotheses. Recall that the original hypothesis space can be viewed as a
disjunctive sentence
h1 ∨h2 ∨h3 . . . ∨hn .
As various hypotheses are found to be inconsistent with the examples, this disjunction shrinks,
retaining only those hypotheses not ruled out. Assuming that the original hypothesis space
swer because only incorrect hypotheses have been removed. The set of hypotheses remaining
VERSION SPACE
version space learning algorithm (also the candidate elimination algorithm).
CANDIDATE
ELIMINATION
One important property of this approach is that it is incremental: one never has to
go back and reexamine the old examples. All remaining hypotheses are guaranteed to be
consistent with them already. But there is an obvious problem. We already said that theChapter
Knowledge in Learning
This region all inconsistent
This region all inconsistent
More general
More specific
S1
G1
S2
G2
G3
 . . . 
Gm
 . . .
Sn
The version space contains all hypotheses consistent with the examples.
hypothesis space is enormous, so how can we possibly write down this enormous disjunction?
we have an ordering on the real numbers.
This is a partial ordering, which means that each boundary will not be a point but rather a
set of hypotheses called a boundary set. The great thing is that we can represent the entire
BOUNDARY SET
examples. Before we prove this, let us recap:
• The current version space is the set of hypotheses consistent with all the examples so
consistent hypotheses that are more speciﬁc.
consistent hypotheses that are more general.
space. To show that the representation is sufﬁcient, we need the following two properties:A Logical Formulation of Learning
there are no “stragglers” left outside.) This follows directly from the deﬁnitions of S
and G. If there were a straggler h, then it would have to be no more speciﬁc than any
member of G, in which case it belongs in G; or no more general than any member of
S, in which case it belongs in S.
tween the boundaries.) Any h between S and G must reject all the negative examples
itive examples accepted by any member of S (because it is more general). Thus, h must
the situation: there are no known examples outside S but inside G, so any hypothesis
in the gap must be consistent.
We have therefore shown that if S and G are maintained according to their deﬁnitions, then
they provide a satisfactory representation of the version space. The only remaining problem
function). This may appear rather complicated at ﬁrst, but from the deﬁnitions and with the
+
+
+
+
+
+
+
+
+
+
–
–
–
–
–
–
–
–
–
–
–
–
–
–
S1
G1
G2
The extensions of the members of G and S. No known examples lie in
between the two sets of boundaries.
new example may be a false positive or a false negative.
generalizations, provided they are more speciﬁc than some member of G.
specializations, provided they are more general than some member of S.Chapter
Knowledge in Learning
We continue these operations for each new example until one of three things happens:
the unique hypothesis.
no consistent hypotheses for the training set. This is the same case as the failure of the
simple version of the decision tree algorithm.
This means the version space represents a disjunction of hypotheses. For any new
example, if all the disjuncts agree, then we can return their classiﬁcation of the example.
If they disagree, one possibility is to take the majority vote.
restaurant data.
• If the domain contains noise or insufﬁcient attributes for exact classiﬁcation, the version
space will always collapse.
disjunction of the descriptions of the negative examples.
exponentially in the number of attributes, even though efﬁcient learning algorithms exist
for those hypothesis spaces.
To date, no completely successful solution has been found for the problem of noise. The
problem of disjunction can be addressed by allowing only limited forms of disjunction or by
including a generalization hierarchy of more general predicates. For example, instead of
GENERALIZATION
HIERARCHY
single literal LongWait(x). The set of generalization and specialization operations can be
easily extended to handle this.
which was designed to learn rules for predicting how molecules would break into pieces in
rules that were sufﬁciently novel to warrant publication in a journal of analytical chemistry—
the ﬁrst real scientiﬁc knowledge generated by a computer program. It was also used in the
tion problems by studying its own successes and failures. Although version space methods
they provide a good deal of insight into the logical structure of hypothesis space.Knowledge in Learning
Observations
Predictions
Hypotheses
Prior 
knowledge
inductive learning
A cumulative learning process uses, and adds to, its stock of background
knowledge over time.
KNOWLEDGE IN LEARNING
The preceding section described the simplest setting for inductive learning. To understand the
role of prior knowledge, we need to talk about the logical relationships among hypotheses,
example descriptions, and classiﬁcations. Let Descriptions denote the conjunction of all the
example descriptions in the training set, and let Classiﬁcations denote the conjunction of all
the example classiﬁcations. Then a Hypothesis that “explains the observations” must satisfy
the following property (recall that |= means “logically entails”):
Hypothesis ∧Descriptions |= Classiﬁcations .
ENTAILMENT
CONSTRAINT
known.” Pure inductive learning means solving this constraint, where Hypothesis is drawn
from some predeﬁned hypothesis space. For example, if we consider a decision tree as a
of the hypothesis, of course, then Hypothesis = Classiﬁcations also satisﬁes the constraint.
Ockham’s razor tells us to prefer small, consistent hypotheses, so we try to do better than
simply memorizing the examples.
The modern approach is to design agents that already know something and are trying to learn
some more. This may not sound like a terriﬁcally deep insight, but it makes quite a difference
to the way we design agents. It might also have some relevance to our theories about how
An autonomous learning agent that uses background knowledge must somehow obtain
the background knowledge in the ﬁrst place, in order for it to be used in the new learning
fore be characterized by cumulative, or incremental, development. Presumably, the agent
tion program. But once it has eaten from the Tree of Knowledge, it can no longer pursue
such naive speculations and should use its background knowledge to learn more and more
effectively. The question is then how to actually do this.Chapter
Knowledge in Learning
Some simple examples
Let us consider some commonsense examples of learning with background knowledge. Many
apparently rational cases of inferential behavior in the face of observations clearly do not
follow the simple principles of pure induction.
• Sometimes one leaps to general conclusions after only one observation. Gary Larson
once drew a cartoon in which a bespectacled caveman, Zog, is roasting his lizard on
the end of a pointed stick. He is watched by an amazed crowd of his less intellectual
contemporaries, who have been using their bare hands to hold their victuals over the ﬁre.
This enlightening experience is enough to convince the watchers of a general principle
of painless cooking.
• Or consider the case of the traveler to Brazil meeting her ﬁrst Brazilian. On hearing him
speak Portuguese, she immediately concludes that Brazilians speak Portuguese, yet on
discovering that his name is Fernando, she does not conclude that all Brazilians are
called Fernando. Similar examples appear in science. For example, when a freshman
ticular temperature, she is quite conﬁdent in generalizing those values to all pieces of
copper. Yet when she measures its mass, she does not even consider the hypothesis that
all pieces of copper have that mass. On the other hand, it would be quite reasonable to
make such a generalization over all pennies.
cated medical student observing a consulting session between a patient and an expert
internist. After a series of questions and answers, the expert tells the patient to take a
course of a particular antibiotic. The medical student infers the general rule that that
particular antibiotic is effective for a particular type of infection.
These are all cases in which the use of background knowledge allows much faster learning
than one might expect from a pure induction program.
Some general schemes
In each of the preceding examples, one can appeal to prior knowledge to try to justify the
ing in each case. The constraints will involve the Background knowledge, in addition to the
Hypothesis and the observed Descriptions and Classiﬁcations.
In the case of lizard toasting, the cavemen generalize by explaining the success of the
pointed stick: it supports the lizard while keeping the hand away from the ﬁre. From this
explanation, they can infer a general rule: that any long, rigid, sharp object can be used to toast
based learning, or EBL. Notice that the general rule follows logically from the background
BASED
LEARNING
knowledge possessed by the cavemen. Hence, the entailment constraints satisﬁed by EBL are
the following:
Hypothesis ∧Descriptions |= Classiﬁcations
Background |= Hypothesis .Knowledge in Learning
amples. But because it requires that the background knowledge be sufﬁcient to explain the
thing factually new from the example. The agent could have derived the example from what
it already knew, although that might have required an unreasonable amount of computation.
plain why Fernando speaks the way he does, unless she knows her papal bulls. Moreover,
the same generalization would be forthcoming from a traveler entirely ignorant of colonial
history. The relevant prior knowledge in this case is that, within any given country, most
people tend to speak the same language; on the other hand, Fernando is not assumed to be
the name of all Brazilians because this kind of regularity does not hold for names. Similarly,
the freshman physics student also would be hard put to explain the particular values that she
rial of which an object is composed and its temperature together determine its conductance.
In each case, the prior knowledge Background concerns the relevance of a set of features to
RELEVANCE
the goal predicate. This knowledge, together with the observations, allows the agent to infer
a new, general rule that explains the observations:
Hypothesis ∧Descriptions |= Classiﬁcations ,
Background ∧Descriptions ∧Classiﬁcations |= Hypothesis .
LEARNING
not standard). Notice that whereas RBL does make use of the content of the observations, it
does not produce hypotheses that go beyond the logical content of the background knowledge
and the observations. It is a deductive form of learning and cannot by itself account for the
creation of new knowledge starting from scratch.
In the case of the medical student watching the expert, we assume that the student’s
prior knowledge is sufﬁcient to infer the patient’s disease D from the symptoms. This is
not, however, enough to explain the fact that the doctor prescribes a particular medicine M.
The student needs to propose another rule, namely, that M generally is effective against D.
Given this rule and the student’s prior knowledge, the student can now explain why the expert
prescribes M in this particular case. We can generalize this example to come up with the
entailment constraint
Background ∧Hypothesis ∧Descriptions |= Classiﬁcations .
That is, the background knowledge and the new hypothesis combine to explain the examples.
As with pure inductive learning, the learning algorithm should propose hypotheses that are as
INDUCTIVE
LEARNING
INDUCTIVE LOGIC
PROGRAMMING
edge plays two key roles in reducing the complexity of learning:Chapter
Knowledge in Learning
as with the new observations, the effective hypothesis space size is reduced to include
only those theories that are consistent with what is already known.
explanation for the observations can be much reduced, because the prior knowledge
will be available to help out the new rules in explaining the observations. The smaller
the hypothesis, the easier it is to ﬁnd.
In addition to allowing the use of prior knowledge in induction, ILP systems can formulate
simpler systems.
vations. As an example, consider the problem of differentiating and simplifying algebraic
X, we obtain 2X. (We use a capital letter for the arithmetic unknown X, to distinguish it
from the logical variable x.) In a logical reasoning system, the goal might be expressed as
ASK(Derivative(X2, X) = d, KB), with solution d = 2X.
Anyone who knows differential calculus can see this solution “by inspection” as a result
of practice in solving such problems. A student encountering such problems for the ﬁrst time,
or a program with no experience, will have a much more difﬁcult job. Application of the
eventually this simpliﬁes to 2X. In the authors’ logic programming implementation, this
experience, we would like the program to solve the same problem much more quickly the
next time it arises.
The technique of memoization has long been used in computer science to speed up
MEMOIZATION
programs by saving the results of computation.
The basic idea of memo functions is to
accumulate a database of input–output pairs; when the function is called, it ﬁrst checks the
learning takes this a good deal further, by creating general rules that cover an entire class
of cases. In the case of differentiation, memoization would remember that the derivative of
X2 with respect to X is 2X, but would leave the agent to calculate the derivative of Z2 with
respect to Z from scratch. We would like to be able to extract the general rule that for any
arithmetic unknown u, the derivative of u2 with respect to u is 2u. (An even more general
rule for un can also be produced, but the current example sufﬁces to make the point.) In
logical terms, this is expressed by the rule
ArithmeticUnknown(u) ⇒Derivative(u2, u) = 2u .If the knowledge base contains such a rule, then any new case that is an instance of this rule
can be solved immediately.
thing is understood, it can be generalized and reused in other circumstances. It becomes an
matica, wrote “Civilization advances by extending the number of important operations that
we can do without thinking about them,” perhaps himself applying EBL to his understanding
ation example, then your brain is already busily trying to extract the general principles of
saw the example. Like the cavemen watching Zog, you (and we) needed an example before
we could generate the basic principles. This is because explaining why something is a good
idea is much easier than coming up with the idea in the ﬁrst place.
Extracting general rules from examples
The basic idea behind EBL is ﬁrst to construct an explanation of the observation using prior
nation structure can be used. This deﬁnition provides the basis for a rule covering all of the
cases in the class. The “explanation” can be a logical proof, but more generally it can be any
identify the necessary conditions for those same steps to apply to another case.
example, so we will use a simpler problem to illustrate the generalization method. Suppose
Rewrite(u, v) ∧Simplify(v, w) ⇒Simplify(u, w) .
Primitive(u) ⇒Simplify(u, u) .
ArithmeticUnknown(u) ⇒Primitive(u) .
Number(u) ⇒Primitive(u) .
...
actually constructs two proof trees simultaneously. The second proof tree uses a variabilized
goal in which the constants from the original goal are replaced by variables. As the original
tions. This could cause some of the variables to become instantiated. For example, in orderChapter
Knowledge in Learning
Primitive(X)
ArithmeticUnknown(X)
Primitive(z)
ArithmeticUnknown(z)
Simplify(X,w)
Yes, {  }
{w / X}
Yes, {  }
Yes, {v' / X}
Simplify(z,w)
{w / z}
Proof trees for the simpliﬁcation problem. The ﬁrst tree shows the proof for
the original problem instance, from which we can derive
ables, from which we can derive a variety of other rules.
(with the necessary bindings) and form a general rule for the goal predicate:
We can therefore drop them from the rule, yielding
In general, conditions can be dropped from the ﬁnal rule if they impose no constraints on the
will be more efﬁcient. Notice that we cannot drop the condition ArithmeticUnknown(z),
because not all possible values of z are arithmetic unknowns. Values other than arithmetic
To recap, the basic EBL process works as follows:
the available background knowledge.inference steps as in the original proof.
from the generalized proof).
variables in the goal.
Improving efﬁciency
when it reaches the Primitive step, we get the rule
This rule is as valid as, but more general than, the rule using ArithmeticUnknown, because
it covers cases where z is a number. We can extract a still more general rule by pruning after
the step Simplify(y + z, w), yielding the rule
In general, a rule can be extracted from any partial subtree of the generalized proof tree. Now
we have a problem: which of these rules do we choose?
The choice of which rule to generate comes down to the question of efﬁciency. There
are three factors involved in the analysis of efﬁciency gains from EBL:
ference mechanism must still check those rules even in cases where they do not yield a
solution. In other words, it increases the branching factor in the search space.
increases in speed for the cases that they do cover. These increases come about mainly
cause they shorten the proof itself.
set of cases.
ity of each subgoal in the rule. A subgoal is operational if it is “easy” to solve. For example,
OPERATIONALITY
the subgoal Primitive(z) is easy to solve, requiring at most two steps, whereas the subgoal
Simplify(y + z, w) could lead to an arbitrary amount of inference, depending on the values
of y and z. If a test for operationality is carried out at each step in the construction of the
generalized proof, then we can prune the rest of a branch as soon as an operational subgoal is
found, keeping just the operational subgoal as a conjunct of the new rule.
Unfortunately, there is usually a tradeoff between operationality and generality. More
speciﬁc subgoals are generally easier to solve but cover fewer cases. Also, operationalityChapter
Knowledge in Learning
Finally, the cost of solving a given subgoal depends on what other rules are available in the
knowledge base. It can go up or down as more rules are added. Thus, EBL systems really
face a very complex optimization problem in trying to maximize the efﬁciency of a given
initial knowledge base. It is sometimes possible to derive a mathematical model of the effect
on overall efﬁciency of adding a given rule and to use this model to select the best rule to
add. The analysis can become very complicated, however, especially when recursive rules
are involved. One promising approach is to address the problem of efﬁciency empirically,
simply by adding several rules and seeing which ones are useful and actually speed things up.
Empirical analysis of efﬁciency is actually at the heart of EBL. What we have been
plexity on a distribution of problems. By generalizing from past example problems, EBL
makes the knowledge base more efﬁcient for the kind of problems that it is reasonable to
expect. This works as long as the distribution of past examples is roughly the same as for
system is carefully engineered, it is possible to obtain signiﬁcant speedups. For example, a
LEARNING USING RELEVANCE INFORMATION
guage spoken by other Brazilians. The inference is sanctioned by her background knowledge,
namely, that people in a given country (usually) speak the same language. We can express
(Literal translation: “If x and y have the same nationality n and x speaks language l, then y
also speaks it.”) It is not difﬁcult to show that, from this sentence and the observation that
Nationality(Fernando, Brazil) ∧Language(Fernando, Portuguese) ,
Nationality(x, Brazil) ⇒Language(x, Portuguese) .
is fully determined. (Put another way: language is a function of nationality.) These sentences
are called functional dependencies or determinations. They occur so commonly in certain
FUNCTIONAL
DEPENDENCY
DETERMINATION
kinds of applications (e.g., deﬁning database designs) that a special syntax is used to write
Nationality(x, n) ≻Language(x, l) .
be amended for countries such as Switzerland and India.Learning Using Relevance Information
As usual, this is simply a syntactic sugaring, but it makes it clear that the determination is
really a relationship between the predicates: nationality determines language. The relevant
properties determining conductance and density can be expressed similarly:
Material(x, m) ∧Temperature(x, t) ≻Conductance(x, ρ) ;
Material(x, m) ∧Temperature(x, t) ≻Density(x, d) .
The corresponding generalizations follow logically from the determinations and observations.
Determining the hypothesis space
Although the determinations sanction general conclusions concerning all Brazilians, or all
pieces of copper at a given temperature, they cannot, of course, yield a general predictive
theory for all nationalities, or for all temperatures and materials, from a single example.
Their main effect can be seen as limiting the space of hypotheses that the learning agent need
perature and can ignore mass, ownership, day of the week, the current president, and so on.
specify a sufﬁcient basis vocabulary from which to construct hypotheses concerning the target
predicate. This statement can be proven by showing that a given determination is logically
equivalent to a statement that the correct deﬁnition of the target predicate is one of the set of
ier to learn the target predicate. Using the basic results of computational learning theory
log(|H|) examples are required to converge to a reasonable hypothesis, where |H| is the
size of the hypothesis space. If the learner has n Boolean features with which to construct
will require only O(2d) examples, a reduction of O(2n−d).
Learning and using relevance information
As we stated in the introduction to this chapter, prior knowledge is useful in learning; but
we must therefore provide a learning algorithm for determinations. The learning algorithm
sistent with the observations. A determination P ≻Q says that if any examples match on
P, then they must also match on Q. A determination is therefore consistent with a set of
the goal predicate. For example, suppose we have the following examples of conductance
measurements on material samples:Chapter
Knowledge in Learning
inputs: E, a set of examples
A, a set of attributes, of size n
for each subset Ai of A of size i do
inputs: A, a set of attributes
E, a set of examples
local variables: H , a hash table
for each example e in E do
if some example in H has the same values as e for the attributes A
but a different classiﬁcation then return false
store the class of e in H , indexed by the values for attributes A of the example e
return true
An algorithm for ﬁnding a minimal consistent determination.
Sample Mass Temperature Material Size
Conductance
S1
Copper
S1
Copper
S2
Copper
S3
Lead
S3
Lead
S4
Lead
The minimal consistent determination is Material ∧Temperature ≻Conductance. There
is a nonminimal but consistent determination, namely, Mass ∧Size ∧Temperature ≻
Conductance. This is consistent with the examples because mass and size determine density
and, in our data set, we do not have two different materials with the same density. As usual,
we would need a larger sample set in order to eliminate a nearly correct hypothesis.
There are several possible algorithms for ﬁnding minimal consistent determinations.
The time complexity of this algorithm depends on the size of the smallest consistent
determination. Suppose this determination has p attributes out of the n total attributes. Then
the algorithm will not ﬁnd it until searching the subsets of A of size p. There are
n
p

= O(np)Learning Using Relevance Information
Proportion correct on test set
Training set size
RBDTL
DTL
attributes.
such subsets; hence the algorithm is exponential in the size of the minimal determination. It
deﬁnition of locally structured domains) that p will be small.
Given an algorithm for learning determinations, a learning agent has a way to construct
a minimal hypothesis within which to learn the target predicate. For example, we can combine
set of relevant attributes and then passes this set to the decision tree algorithm for learning.
formation in order to minimize its hypothesis space. We expect that RBDTL will learn faster
performance for the two algorithms on randomly generated data for a function that depends
RBDTL will show no advantage.
This section has only scratched the surface of the ﬁeld of declarative bias, which aims
DECLARATIVE BIAS
to understand how prior knowledge can be used to identify the appropriate hypothesis space
within which to search for the correct target deﬁnition. There are many unanswered questions:
• How can the algorithms be extended to handle noise?
• How can other kinds of prior knowledge be used, besides determinations?
Some of these questions are addressed in the next section.Chapter
Knowledge in Learning
INDUCTIVE LOGIC PROGRAMMING
of a protein molecule cannot be represented reasonably by a set of attributes, because the
conﬁguration inherently refers to relationships between objects, not to attributes of a single
inductive logic programming produces hypotheses that are (relatively) easy for humans to
by working biologists. This means that inductive logic programming systems can participate
in the scientiﬁc cycle of experimentation, hypothesis generation, debate, and refutation. Such
neural networks.
An example
the entailment constraint
Background ∧Hypothesis ∧Descriptions |= Classiﬁcations
for the unknown Hypothesis, given the Background knowledge and examples described by
Descriptions and Classiﬁcations. To illustrate this, we will use the problem of learning
family relationships from examples. The descriptions will consist of an extended family
tree, described in terms of Mother, Father, and Married relations and Male and Female
Father(Philip, Charles)
Father(Philip, Anne)
. . .
Mother(Mum, Margaret) Mother(Mum, Elizabeth)
. . .
Married(Diana, Charles) Married(Elizabeth, Philip) . . .
Male(Philip)
Male(Charles)
. . .
Female(Beatrice)
Female(Margaret)
. . .
The sentences in Classiﬁcations depend on the target concept being learned. We might want
to learn Grandparent, BrotherInLaw, or Ancestor, for example. For Grandparent, the
including Horn clauses, conjunctive normal form, uniﬁcation, and resolution.Inductive Logic Programming
Grandparent(Mum, Charles) Grandparent(Elizabeth, Beatrice) . . .
¬Grandparent(Mum, Harry) ¬Grandparent(Spencer, Peter)
. . .
We could of course learn from a subset of this complete set.
The object of an inductive learning program is to come up with a set of sentences for
the Hypothesis such that the entailment constraint is satisﬁed. Suppose, for the moment, that
the agent has no background knowledge: Background is empty. Then one possible solution
(a)
(b)
(a) and (b) show positive and negative examples, respectively, of the
Each
Helix(p, h1) ∧Length(h1, HIGH) ∧Position(p, h1, n)
such as we saw in previous chapters. The rule can be translated into English as “ Protein pChapter
Knowledge in Learning
for Hypothesis is the following:
Grandparent(x, y)
⇔
[∃z Mother(x, z) ∧Mother(z, y)]
∨
[∃z Mother(x, z) ∧Father(z, y)]
∨
[∃z Father(x, z) ∧Mother(z, y)]
∨
[∃z Father(x, z) ∧Father(z, y)] .
get nowhere in solving this problem. In order to express Grandparent as an attribute (i.e., a
unary predicate), we would need to make pairs of people into objects:
Grandparent(⟨Mum, Charles⟩) . . .
Then we get stuck in trying to represent the example descriptions. The only possible attributes
are horrible things such as
FirstElementIsMotherOfElizabeth(⟨Mum, Charles⟩) .
tages of ILP algorithms is their applicability to a much wider range of problems, including
relational problems.
The reader will certainly have noticed that a little bit of background knowledge would
cluded the sentence
Parent(x, y) ⇔[Mother(x, y) ∨Father(x, y)] ,
then the deﬁnition of Grandparent would be reduced to
Grandparent(x, y) ⇔[∃z Parent(x, z) ∧Parent(z, y)] .
quired to explain the observations.
It is also possible for ILP algorithms to create new predicates in order to facilitate the
expression of explanatory hypotheses. Given the example data shown earlier, it is entirely
reasonable for the ILP program to propose an additional predicate, which we would call
Beatrice
Andrew
Eugenie
William Harry
Charles
Diana
Mum
George
Philip
Elizabeth
Margaret
Kydd
Spencer
Peter
Mark
Zara
Anne
Sarah
Edward
Sophie
Louise
James
A typical family tree.Inductive Logic Programming
“Parent,” in order to simplify the deﬁnitions of the target predicates. Algorithms that can
generate new predicates are called constructive induction algorithms. Clearly, constructive
CONSTRUCTIVE
INDUCTION
induction is a necessary part of the picture of cumulative learning. It has been one of the
hardest problems in machine learning, but some ILP techniques provide effective mechanisms
for achieving it.
In the rest of this chapter, we will study the two principal approaches to ILP. The ﬁrst
uses a generalization of decision tree methods, and the second uses techniques based on
inverting a resolution proof.
The ﬁrst approach to ILP works by starting with a very general rule and gradually specializing
decision tree is gradually grown until it is consistent with the observations. To do ILP we
Suppose we are trying to learn a deﬁnition of the Grandfather(x, y) predicate, using
into positive and negative examples. Positive examples are
⟨George, Anne⟩, ⟨Philip, Peter⟩, ⟨Spencer, Harry⟩, . . .
and negative examples are
⟨George, Elizabeth⟩, ⟨Harry, Zara⟩, ⟨Charles, Philip⟩, . . .
Notice that each example is a pair of objects, because Grandfather is a binary predicate. In
pairs of people).
FOIL constructs a set of clauses, each with Grandfather(x, y) as the head. The clauses
that negated literals are allowed in the body of a clause and are interpreted using negation as
failure, as in Prolog. The initial clause has an empty body:
⇒Grandfather(x, y) .
This clause classiﬁes every example as positive, so it needs to be specialized. We do this by
Father(x, y) ⇒Grandfather(x, y) .
Parent(x, z) ⇒Grandfather(x, y) .
Father(x, z) ⇒Grandfather(x, y) .
(Notice that we are assuming that a clause deﬁning Parent is already part of the background
ples as negative and can thus be ignored. The second and third agree with all of the positive
examples, but the second is incorrect on a larger fraction of the negative examples—twice as
many, because it allows mothers as well as fathers. Hence, we prefer the third clause.Chapter
Knowledge in Learning
Now we need to specialize this clause further, to rule out the cases in which x is the
father of some z, but z is not a parent of y. Adding the single literal Parent(z, y) gives
Father(x, z) ∧Parent(z, y) ⇒Grandfather(x, y) ,
which correctly classiﬁes all the examples. FOIL will ﬁnd and choose this literal, thereby
solving the learning task. In general, the solution is a set of Horn clauses, each of which
implies the target predicate. For example, if we didn’t have the Parent predicate in our
vocabulary, then the solution might be
Father(x, z) ∧Father(z, y) ⇒Grandfather(x, y)
Father(x, z) ∧Mother(z, y) ⇒Grandfather(x, y) .
Note that each of these clauses covers some of the positive examples, that together they cover
will incorrectly cover a negative example. In general FOIL will have to search through many
unsuccessful clauses before ﬁnding a correct solution.
clause, literal by literal, until it agrees with some subset of the positive examples and none of
the negative examples. Then the positive examples covered by the clause are removed from
the training set, and the process continues until no positive examples remain. The two main
be added to the clause. Let us use as an example the clause
Father(x, z) ⇒Grandfather(x, y) .
There are three kinds of literals that can be added:
(including the goal predicate) can be used, and the arguments must all be variables. Any
variable can be used for any argument of the predicate, with one restriction: each literal
must include at least one variable from an earlier literal or from the head of the clause.
Literals such as Mother(z, u), Married(z, z), ¬Male(y), and Grandfather(v, x) are
allowed, whereas Married(u, v) is not. Notice that the use of the predicate from the
head of the clause allows FOIL to learn recursive deﬁnitions.
might use the empty list [ ].
threshold value can be chosen to maximize the discriminatory power of the test.
can also use type information to reduce it. For example, if the domain included numbers asInductive Logic Programming
function FOIL(examples,target) returns a set of Horn clauses
inputs: examples, set of examples
target, a literal for the goal predicate
local variables: clauses, set of clauses, initially empty
while examples contains positive examples do
remove positive examples covered by clause from examples
add clause to clauses
return clauses
local variables: clause, a clause with target as head and an empty body
l, a literal to be added to the clause
extended examples, a set of examples with values for new variables
extended examples ←examples
while extended examples contains negative examples do
append l to the body of clause
to each example in extended examples
return clause
if example satisﬁes literal
then return the set of examples created by extending example with
each possible constant value for each new variable in literal
else return the empty set
as Parent(x, n), where x is a person and n is a number.
to decide which literal to add. The exact details are not important here, and a number of
different variations have been tried. One interesting additional feature of FOIL is the use of
Ockham’s razor to eliminate some hypotheses. If a clause becomes longer (according to some
metric) than the total length of the positive examples that the clause explains, that clause is
not considered as a potential hypothesis. This technique provides a way to avoid overcomplex
clauses that ﬁt noise in the data.
FOIL and its relatives have been used to learn a wide variety of deﬁnitions. One of theChapter
Knowledge in Learning
each case, the program was able to learn a correct deﬁnition of the function from a small set
of examples, using the previously learned functions as background knowledge.
Inductive learning with inverse deduction
The second major approach to ILP involves inverting the normal deductive proof process.
Inverse resolution is based on the observation that if the example Classiﬁcations follow
INVERSE
RESOLUTION
from Background ∧Hypothesis ∧Descriptions, then one must be able to prove this fact by
resolution (because resolution is complete). If we can “run the proof backward,” then we can
ﬁnd a Hypothesis such that the proof goes through. The key, then, is to ﬁnd a way to invert
the resolution process.
We will show a backward proof process for inverse resolution that consists of individual
backward steps. Recall that an ordinary resolution step takes two clauses C1 and C2 and
resolves them to produce the resolvent C. An inverse resolution step takes a resolvent C
and produces two clauses C1 and C2, such that C is the result of resolving C1 and C2.
Alternatively, it may take a resolvent C and clause C1 and produce a clause C2 such that C
is the result of resolving C1 and C2.
focus on the positive example Grandparent(George, Anne). The process begins at the end
of the proof (shown at the bottom of the ﬁgure). We take the resolvent C to be empty
tion of the goal example. The ﬁrst inverse step takes C and C2 and generates the clause
Grandparent(George, Anne) for C1. The next step takes this clause as C and the clause
Parent(Elizabeth, Anne) as C2, and generates the clause
¬Parent(Elizabeth, y) ∨Grandparent(George, y)
as C1. The ﬁnal step treats this clause as the resolvent. With Parent(George, Elizabeth) as
C2, one possible clause C1 is the hypothesis
Parent(x, z) ∧Parent(z, y) ⇒Grandparent(x, y) .
Now we have a resolution proof that the hypothesis, descriptions, and background knowledge
entail the classiﬁcation Grandparent(George, Anne).
terministic, because for any C, there can be many or even an inﬁnite number of clauses
C1 and C2 that resolve to C. For example, instead of choosing ¬Parent(Elizabeth, y) ∨
might have chosen any of the following sentences:
¬Parent(Elizabeth, Anne) ∨Grandparent(George, Anne) .
¬Parent(z, Anne) ∨Grandparent(George, Anne) .
¬Parent(z, y) ∨Grandparent(George, y) .
...
chosen from the Background knowledge, from the example Descriptions, from the negatedInductive Logic Programming
Classiﬁcations, or from hypothesized clauses that have already been generated in the inverse
fore an inefﬁcient search) without additional controls. A number of approaches to taming the
search have been tried in implemented ILP systems:
ciﬁc hypotheses possible and by requiring that all the hypothesized clauses be consistent
with each other, and with the observations. This last criterion would rule out the clause
¬Parent(z, y) ∨Grandparent(George, y), listed before.
resolution is a complete, restricted strategy. Linear resolution produces proof trees that
have a linear branching structure—the whole tree follows one line, with only single
bols or by allowing only Horn clauses. For instance, PROGOL operates with Horn
clauses using inverse entailment. The idea is to change the entailment constraint
INVERSE
ENTAILMENT
Background ∧Hypothesis ∧Descriptions |= Classiﬁcations
to the logically equivalent form
Background ∧Descriptions ∧¬Classiﬁcations |= ¬Hypothesis.
this is an incomplete method, but it can be more efﬁcient than full resolution. It is also
Parent(Elizabeth,Anne)
Grandparent(George,Anne)
Grandparent(George,Anne)
Grandparent(George,y)
Parent(Elizabeth,y)
>
Parent(George,Elizabeth)
>
Parent(z,y)
Grandparent(x,y)
>
Parent(x,z)
¬
¬
¬
¬
Early steps in an inverse resolution process.
The shaded clauses are
generated by inverse resolution steps from the clause to the right and the clause below.
The unshaded clauses are from the Descriptions and Classiﬁcations (including negated
Classiﬁcations).Chapter
Knowledge in Learning
is, like answer set programming, it generates possible values for logical variables, and
checks for consistency.
ories into propositional logic, solving them with a propositional learning system, and
then translating back. Working with propositional formulas can be more efﬁcient on
Making discoveries with inductive logic programming
An inverse resolution procedure that inverts a complete resolution strategy is, in principle, a
generates a set of examples, then an inverse resolution procedure can generate Hypothesis
from the examples. This observation suggests an interesting possibility: Suppose that the
lution program be theoretically capable of inferring the law of gravity? The answer is clearly
yes, because the law of gravity allows one to explain the examples, given suitable background
mathematics. Similarly, one can imagine that electromagnetism, quantum mechanics, and the
theory of relativity are also within the scope of ILP programs. Of course, they are also within
the scope of a monkey with a typewriter; we still need better heuristics and new ways to
structure the search space.
One thing that inverse resolution systems will do for you is invent new predicates. This
ability is often seen as somewhat magical, because computers are often thought of as “merely
working with what they are given.” In fact, new predicates fall directly out of the inverse
resolution step. The simplest case arises in hypothesizing two new clauses C1 and C2, given
a clause C. The resolution of C1 and C2 eliminates a literal that the two clauses share; hence,
it is quite possible that the eliminated literal contained a predicate that does not appear in C.
Thus, when working backward, one possibility is to generate a new predicate from which to
reconstruct the missing literal.
tion steps. For example, a later step might hypothesize that Mother(x, y) ⇒P(x, y). Thus,
the new predicate P has its meaning constrained by the generation of hypotheses that involve
it. Another example might lead to the constraint Father(x, y) ⇒P(x, y). In other words,
the predicate P is what we usually think of as the Parent relationship. As we mentioned
earlier, the invention of new predicates can signiﬁcantly reduce the size of the deﬁnition of
the goal predicate. Hence, by including the ability to invent new predicates, inverse resolution
systems can often solve learning problems that are infeasible with other techniques.
Some of the deepest revolutions in science come from the invention of new predicates
mal energy. Once these terms are available, the discovery of new laws becomes (relatively)
easy. The difﬁcult part lies in realizing that some new entity, with a speciﬁc relationship
to existing entities, will allow an entire body of observations to be explained with a muchSummary
Father(x,y)
P(x,y)
>
Father(George,y)
Ancestor(George,y)
>
P(George,y)
Ancestor(George,y)
>
¬
¬
An inverse resolution step that generates a new predicate P.
simpler and more elegant theory than previously existed.
As yet, ILP systems have not made discoveries on the level of Galileo or Joule, but their
discoveries have been deemed publishable in the scientiﬁc literature. For example, in the
for protein folding by the ILP program PROGOL. Many of the rules discovered by PROGOL
could have been derived from known principles, but most had not been previously published
based rules for the mutagenicity of nitroaromatic compounds. These compounds are found in
identify four important features, and linear regression on these features outperforms ILP. For
allow it to outperform linear regression, neural nets, and decision trees. Most impressively,
and extended ILP techniques to include experiment design, thereby creating an autonomous
scientist that actually discovered new knowledge about the functional genomics of yeast. For
all these examples it appears that the ability both to represent relations and to use background
knowledge contribute to ILP’s high performance. The fact that the rules found by ILP can be
interpreted by humans contributes to the acceptance of these techniques in biology journals
rather than just computer science journals.
tant is natural language processing, where ILP has been used to extract complex relational
SUMMARY
This chapter has investigated various ways in which prior knowledge can help an agent to
learning of relational models. The important points are:
• The use of prior knowledge in learning leads to a picture of cumulative learning, in
which learning agents improve their learning ability as they acquire more knowledge.
• Prior knowledge helps learning by eliminating otherwise consistent hypotheses and byChapter
Knowledge in Learning
“ﬁlling in” the explanation of examples, thereby allowing for shorter hypotheses. These
contributions often result in faster learning from fewer examples.
• Understanding the different logical roles played by prior knowledge, as expressed by
entailment constraints, helps to deﬁne a variety of learning techniques.
plaining the examples and generalizing the explanation. It provides a deductive method
to identify the relevant attributes, thereby generating a reduced hypothesis space and
speeding up learning. RBL also allows deductive generalizations from single examples.
sets of observations with the help of background knowledge.
• Inductive logic programming (ILP) techniques perform KBIL on knowledge that is
• ILP methods naturally generate new predicates with which concise new theories can be
BIBLIOGRAPHICAL AND HISTORICAL NOTES
phers of science, little formal work was done until quite recently. Fact, Fiction, and Forecast,
was simply a matter of seeing enough examples of some universally quantiﬁed proposition
and then adopting it as a hypothesis. Consider, for example, the hypothesis “All emeralds are
grue,” where grue means “green if observed before time t, but blue if observed thereafter.”
At any time up to t, we might have observed millions of instances conﬁrming the rule that
emeralds are grue, and no disconﬁrming instances, and yet we are unwilling to adopt the rule.
This can be explained only by appeal to the role of relevant prior knowledge in the induction
ful, including a version of determinations called overhypotheses. Unfortunately, Goodman’s
ideas were never pursued in machine learning.
work in cognitive psychology also suggested that it is a natural form of concept learning in
a different approach, maintaining the set of all consistent hypotheses and eliminating thoseBibliographical and Historical Notes
LEX system, which learns to solve calculus problems. A third inﬂuential thread was formed
by the work of Michalski and colleagues on the AQ series of algorithms, which learned sets
When a plan was constructed, a generalized version of it was saved in a plan library and
the EBL algorithm described in the text, showing how it could be incorporated directly into a
tensive extra work, EBL could easily slow down a program signiﬁcantly. Formal probabilistic
Instead of using examples as foci for generalization, one can use them directly to solve
new problems, in a process known as analogical reasoning. This form of reasoning ranges
ANALOGICAL
REASONING
a form of deductive inference based on determinations but requiring the participation of the
generalization of the old example to ﬁt the needs of the new problem. This latter form of
Relevance information in the form of functional dependencies was ﬁrst developed in
able subsets.
Functional dependencies were used for analogical reasoning by Carbonell
for determinations and the improved performance obtained by RBDTL were ﬁrst shown in
very ingenious algorithm for learning with determinations that shows large improvements in
learning speed.
The idea that inductive learning can be performed by inverse deduction can be traced
Probabilities has led me to adopt the opinion that there is no such thing as a distinct method
ment of deduction.” Computational investigations began with the remarkable Ph.D. thesis byChapter
Knowledge in Learning
methods that are in current use in ILP, he was discouraged by some undecidability results for
tion of relational rules. The ﬁeld of relational learning was reinvigorated by Muggleton and
resolution and was capable of generating new predicates. The inverse resolution method also
and has been applied to a number of practical problems, particularly in biology and natural
tainty in the form of stochastic logic programs.
ing of the importance of syntactic restrictions on clauses, positive results have been obtained
Although ILP now seems to be the dominant approach to constructive induction, it has
DISCOVERY SYSTEM
of scientiﬁc discovery of new concepts, usually by a direct search in the space of concept
discovery heuristics expressed as expert system rules to guide its search for concepts and
conjectures in elementary number theory. Unlike most systems designed for mathematical
reasoning, AM lacked a concept of proof and could only make conjectures. It rediscovered
Goldbach’s conjecture and the Unique Prime Factorization theorem. AM’s architecture was
ing the system’s own discovery heuristics. EURISKO was applied in a number of areas other
than mathematical discovery, although with less success than AM. The methodology of AM
Another class of discovery systems aims to operate with real scientiﬁc data to ﬁnd new
systems that look for quantitative relationships in experimental data from physical systems;
tory of science. Discovery systems based on probabilistic techniques—especially clusteringLEARNING
PROBABILISTIC MODELS
In which we view learning as a form of uncertain reasoning from observations.
uncertainty by using the methods of probability and decision theory, but ﬁrst they must learn
their probabilistic theories of the world from experience. This chapter explains how they
can do that, by formulating the learning task itself as a process of probabilistic inference
general solutions to the problems of noise, overﬁtting, and optimal prediction. It also takes
of the world is correct, yet must still make decisions by using some theory of the world.
We describe methods for learning probability models—primarily Bayesian networks—
though the general lessons can be understood without plunging into the details. It may beneﬁt
STATISTICAL LEARNING
data are evidence—that is, instantiations of some or all of the random variables describing the
domain. The hypotheses in this chapter are probabilistic theories of how the domain works,
including logical theories as a special case.
Consider a simple example. Our favorite Surprise candy comes in two ﬂavors: cherry
(yum) and lime (ugh). The manufacturer has a peculiar sense of humor and wraps each piece
of candy in the same opaque wrapper, regardless of ﬂavor. The candy is sold in very large
bags, of which there are known to be ﬁve kinds—again, indistinguishable from the outside:Statistical Learning
Given a new bag of candy, the random variable H (for hypothesis) denotes the type of the
bag, with possible values h1 through h5. H is not directly observable, of course. As the
pieces of candy are opened and inspected, data are revealed—D1, D2, . . ., DN, where each
Di is a random variable with possible values cherry and lime. The basic task faced by the
scenario serves to introduce many of the major issues. The agent really does need to infer a
theory of its world, albeit a very simple one.
Bayesian learning simply calculates the probability of each hypothesis, given the data,
BAYESIAN LEARNING
potheses, weighted by their probabilities, rather than by using just a single “best” hypothesis.
In this way, learning is reduced to probabilistic inference. Let D represent all the data, with
observed value d; then the probability of each hypothesis is obtained by Bayes’ rule:
P(hi | d) = αP(d | hi)P(hi) .
Now, suppose we want to make a prediction about an unknown quantity X. Then we have
P(X | d) =


i
P(X | d, hi)P(hi | d) =


i
P(X | hi)P(hi | d) ,
where we have assumed that each hypothesis determines a probability distribution over X.
vidual hypotheses. The hypotheses themselves are essentially “intermediaries” between the
raw data and the predictions. The key quantities in the Bayesian approach are the hypothesis
prior, P(hi), and the likelihood of the data under each hypothesis, P(d | hi).
HYPOTHESIS PRIOR
LIKELIHOOD
For our candy example, we will assume for the time being that the prior distribution
likelihood of the data is calculated under the assumption that the observations are i.i.d. (see
P(d | hi) =

j
P(dj | hi) .
lime candies is observed. Notice that the probabilities start out at their prior values, so h3
urns and balls less compelling than candy; furthermore, candy lends itself to other tasks, such as deciding whether
it is more correct (but less hygienic) to rewrap each candy after inspection and return it to the bag.Chapter
Learning Probabilistic Models
Posterior probability of hypothesis
Number of observations in d
P(h1 | d)
P(h2 | d)
P(h3 | d)
P(h4 | d)
P(h5 | d)
Probability that next candy is lime
Number of observations in d
(a)
(b)
The
pothesis. This is characteristic of Bayesian learning. For any ﬁxed prior that does not rule
out the true hypothesis, the posterior probability of any false hypothesis will, under certain
erating “uncharacteristic” data indeﬁnitely is vanishingly small. (This point is analogous to
prediction is optimal, whether the data set be small or large. Given the hypothesis prior, any
other prediction is expected to be correct less often.
The optimality of Bayesian learning comes at a price, of course. For real learning
carried out tractably, but in most cases we must resort to approximate or simpliﬁed methods.
dictions based on a single most probable hypothesis—that is, an hi that maximizes P(hi | d).
MAXIMUM A
POSTERIORI
Predictions made according to an MAP hypothesis hMAP are approximately Bayesian to the
dies in a row, so the MAP learner then predicts that the fourth candy is lime with probability
the competitors to the MAP hypothesis become less and less probable.
Although our example doesn’t show it, ﬁnding MAP hypotheses is often much easier
than Bayesian learning, because it requires solving an optimization problem instead of a large
summation (or integration) problem. We will see examples of this later in the chapter.Statistical Learning
is too expressive, so that it contains many hypotheses that ﬁt the data set well. Rather than
placing an arbitrary limit on the hypotheses to be considered, Bayesian and MAP learning
methods use the prior to penalize complexity. Typically, more complex hypotheses have a
lower prior probability—in part because there are usually many more complex hypotheses
ity to ﬁt the data. (In the extreme case, a lookup table can reproduce the data exactly with
hypothesis and its degree of ﬁt to the data.
We can see the effect of this tradeoff most clearly in the logical case, where H contains
is consistent with the data. Therefore, maximum a posteriori learning provides a natural
embodiment of Ockham’s razor.
Another insight into the tradeoff between complexity and degree of ﬁt is obtained by
equivalent to minimizing
−log2 P(d | hi) −log2 P(hi) .
Using the connection between information encoding and probability that we introduced in
ify the hypothesis hi. Furthermore, −log2 P(d | hi) is the additional number of bits required
to specify the data, given the hypothesis. (To see this, consider that no bits are required
if the hypothesis predicts the data exactly—as with h5 and the string of lime candies—and
pression of the data. The same task is addressed more directly by the minimum description
length, or MDL, learning method. Whereas MAP learning expresses simplicity by assigning
higher probabilities to simpler hypotheses, MDL expresses it directly by counting the bits in
a binary encoding of the hypotheses and data.
potheses. In that case, MAP learning reduces to choosing an hi that maximizes P(d | hi).
LIKELIHOOD
is very common in statistics, a discipline in which many researchers distrust the subjective
nature of hypothesis priors. It is a reasonable approach when there is no reason to prefer one
hypothesis over another a priori—for example, when all hypotheses are equally complex. It
provides a good approximation to Bayesian and MAP learning when the data set is large,
because the data swamps the prior distribution over hypotheses, but it has problems (as we
shall see) with small data sets.Chapter
Learning Probabilistic Models
LEARNING WITH COMPLETE DATA
The general task of learning a probability model, given data that are assumed to be generated
from that model, is called density estimation. (The term applied originally to probability
DENSITY ESTIMATION
density functions for continuous variables, but is used now for discrete distributions too.)
COMPLETE DATA
plete when each data point contains values for every variable in the probability model being
PARAMETER
LEARNING
bility model whose structure is ﬁxed. For example, we might be interested in learning the
conditional probabilities in a Bayesian network with a given structure. We will also look
brieﬂy at the problem of learning structure and at nonparametric density estimation.
Suppose we buy a bag of lime and cherry candy from a new manufacturer whose lime–cherry
that case, we have a continuum of hypotheses. The parameter in this case, which we call
likelihood approach is reasonable. If we model the situation with a Bayesian network, we
need just one random variable, Flavor (the ﬂavor of a randomly chosen candy from the bag).
suppose we unwrap N candies, of which c are cherries and ℓ= N −c are limes. According
N

sion. The same value is obtained by maximizing the log likelihood,
LOG LIKELIHOOD
N


(By taking logarithms, we reduce the product to a sum over the data, which is usually easier
θ and set the resulting expression to zero:
= c
θ −
ℓ
⇒
θ =
c
c + ℓ= c
N .
of cherries in the bag is equal to the observed proportion in the candies unwrapped so far!
It appears that we have done a lot of work to discover the obvious. In fact, though,
with broad applicability:Learning with Complete Data
Flavor
(a)
θ
Flavor
Wrapper
(b)
θ
F
cherry
lime
θ1
θ2
bilistically) on the candy ﬂavor.
The trickiest step is usually the last. In our example, it was trivial, but we will see that in
many cases we need to resort to iterative solution algorithms or other numerical optimization
hypothesis assigns zero probability to those events. Various tricks are used to avoid this
Let us look at another example. Suppose this new candy manufacturer wants to give a
little hint to the consumer and uses candy wrappers colored red and green. The Wrapper for
each candy is selected probabilistically, according to some unknown conditional distribution,
seeing, say, a cherry candy in a green wrapper can be obtained from the standard semantics
Now we unwrap N candies, of which c are cherries and ℓare limes. The wrapper counts are
as follows: rc of the cherries have red wrappers and gc have green, while rℓof the limes have
red and gℓhave green. The likelihood of the data is given byChapter
Learning Probabilistic Models
This looks pretty horrible, but taking logarithms helps:
The beneﬁt of taking logs is clear: the log likelihood is the sum of three terms, each of which
contains a single parameter. When we take derivatives with respect to each parameter and set
them to zero, we get three independent equations, each containing just one parameter:
∂L
∂θ =
c
θ −
ℓ
⇒
θ =
c
∂L
∂θ1 =
rc
θ1 −
gc
⇒
θ1 =
rc
∂L
∂θ2 =
rℓ
θ2 −
gℓ
⇒
θ2 =
rℓ
rℓ+gℓ.
candy has a red wrapper, is the observed fraction of cherry candies with red wrappers, and
These results are very comforting, and it is easy to see that they can be extended to any
for a Bayesian network decomposes into separate learning problems, one for each parameter.
probabilities.) The second point is that the parameter values for a variable, given its parents,
are just the observed frequencies of the variable values for each setting of the parent values.
As before, we must be careful to avoid zeroes when the data set is small.
Naive Bayes models
Probably the most common Bayesian network model used in machine learning is the naive
be predicted) is the root and the “attribute” variables Xi are the leaves. The model is “naive”
because it assumes that the attributes are conditionally independent of each other, given the
attribute, Wrapper.) Assuming Boolean variables, the parameters are
θ = P(C = true), θi1 = P(Xi = true | C = true), θi2 = P(Xi = true | C = false).
ples for which the class variable C is unobserved. With observed attribute values x1, . . . , xn,
the probability of each class is given by
P(C | x1, . . . , xn) = α P(C)

i
P(xi | C) .
shows the learning curve for this method when it is applied to the restaurant problem from
actly using a naive Bayes model. Naive Bayes learning turns out to do surprisingly well in aLearning with Complete Data
Proportion correct on test set
Training set size
Decision tree
Naive Bayes
The learning curve for naive Bayes learning applied to the restaurant problem
systems have no difﬁculty with noisy or missing data and can give probabilistic predictions
when appropriate.
tant to know how to learn the parameters of continuous models from data. The principles for
Let us begin with a very simple case: learning the parameters of a Gaussian density
function on a single variable. That is, the data are generated as follows:
P(x) =
√
2σ2
.
normalizing “constant” depends on σ, so we cannot ignore it.) Let the observed values be
x1, . . . , xN. Then the log likelihood is
L =
N


log
√
2πσ
e−
2σ2
= N(−log
√
2π −log σ) −
N


2σ2
.
Setting the derivatives to zero as usual, we obtain
∂L
σ2
N
⇒
μ =
P
j xj
N
∂L
∂σ = −N
σ3
N
⇒
σ =
	 P
N
.
likelihood value of the standard deviation is the square root of the sample variance. Again,
these are comforting results that conﬁrm “commonsense” practice.Chapter
Learning Probabilistic Models
x
y
P(y |x)
y
x
(a)
(b)
(a) A linear Gaussian model described as y = θ1x + θ2 plus Gaussian noise
Now consider a linear Gaussian model with one continuous parent X and a continuous
linearly on the value of X and whose standard deviation is ﬁxed. To learn the conditional
distribution P(Y | X), we can maximize the conditional likelihood
P(y | x) =
√
2σ2
.
the log likelihood with respect to these parameters is the same as minimizing the numerator
model, provided that the data are generated with Gaussian noise of ﬁxed variance.
Bayesian parameter learning
serious deﬁciencies with small data sets. For example, after seeing one cherry candy, the
hypothesis prior is that bags must be either all cherry or all lime, this is not a reasonable
conclusion. It is more likely that the bag is a mixture of lime and cherry. The Bayesian
approach to parameter learning starts by deﬁning a prior probability distribution over the
possible hypotheses. We call this the hypothesis prior. Then, as data arrives, the posterior
HYPOTHESIS PRIOR
probability distribution is updated.Learning with Complete Data
P(Θ = θ)
P(Θ = θ)
(a)
(b)
Examples of the beta[a, b] distribution for different values of [a, b].
value of a random variable Θ that deﬁnes the hypothesis space; the hypothesis prior is just
the prior distribution P(Θ). Thus, P(Θ = θ) is the prior probability that the bag has a fraction
θ of cherry candies.
density is a member of the family of beta distributions. Each beta distribution is deﬁned by
BETA DISTRIBUTION
two hyperparameters3 a and b such that
HYPERPARAMETER
distribution more peaked, suggesting greater certainty about the value of Θ. Thus, the beta
family provides a useful range of possibilities for the hypothesis prior.
Besides its ﬂexibility, the beta family has another wonderful property: if Θ has a prior
beta[a, b], then, after a data point is observed, the posterior distribution for Θ is also a beta
distribution. In other words, beta is closed under update. The beta family is called the
CONJUGATE PRIOR
works. Suppose we observe a cherry candy; then we haveChapter
Learning Probabilistic Models
Flavor1
Wrapper1
Flavor2
Wrapper2
Flavor3
Wrapper3
Θ
Θ1
Θ2
rior distributions for the parameter variables Θ, Θ1, and Θ2 can be inferred from their prior
distributions and the evidence in the Flavor i and Wrapper i variables.
P(θ | D1 = cherry) = α P(D1 = cherry | θ)P(θ)
Thus, after seeing a cherry candy, we simply increment the a parameter to get the posterior;
similarly, after seeing a lime candy, we increment the b parameter. Thus, we can view the a
and b hyperparameters as virtual counts, in the sense that a prior beta[a, b] behaves exactly
VIRTUAL COUNTS
By examining a sequence of beta distributions for increasing values of a and b, keeping
the proportions ﬁxed, we can see vividly how the posterior distribution over the parameter
is converging to a narrow peak around the true value of Θ. For large data sets, then, Bayesian
θ2 is the probability of a red wrapper on a lime candy. The Bayesian hypothesis prior must
cover all three parameters—that is, we need to specify P(Θ, Θ1, Θ2). Usually, we assume
parameter independence:
PARAMETER
INDEPENDENCE
P(Θ, Θ1, Θ2) = P(Θ)P(Θ1)P(Θ2) .Learning with Complete Data
any data into one Bayesian network. The nodes Θ, Θ1, Θ2 have no parents. But each time
we make an observation of a wrapper and corresponding ﬂavor of a piece of candy, we add a
node Flavor i, which is dependent on the ﬂavor parameter Θ:
P(Flavor i = cherry | Θ = θ) = θ .
We also add a node Wrapper i, which is dependent on Θ1 and Θ2:
P(Wrapper i = red | Flavor i = cherry, Θ1 = θ1) = θ1
P(Wrapper i = red | Flavor i = lime, Θ2 = θ2) = θ2 .
Now, the entire Bayesian learning process can be formulated as an inference problem. We
mulation of learning and prediction makes it clear that Bayesian learning requires no extra
“principles of learning.” Furthermore, there is, in essence, just one learning algorithm —the
variables.
Learning Bayes net structures
So far, we have assumed that the structure of the Bayes net is given and we are just trying to
learn the parameters. The structure of the network represents basic causal knowledge about
the domain that is often easy for an expert, or even a naive user, to supply. In some cases,
however, the causal model may be unavailable or subject to dispute—for example, certain
corporations have long claimed that smoking does not cause cancer—so it is important to
understand how the structure of a Bayes net can be learned from data. This section gives a
brief sketch of the main ideas.
The most obvious approach is to search for a good model. We can start with a model
containing no links and begin adding parents for each node, ﬁtting the parameters with the
annealing search to make modiﬁcations, retuning the parameters after each change in the
troduce cycles in the process, so many algorithms assume that an ordering is given for the
variables, and that a node can have parents only among those nodes that come earlier in the
to search over possible orderings.
There are two alternative methods for deciding when a good structure has been found.
The ﬁrst is to test whether the conditional independence assertions implicit in the structure are
actually satisﬁed in the data. For example, the use of a naive Bayes model for the restaurant
problem assumes thatChapter
Learning Probabilistic Models
tional frequencies. But even if the structure describes the true causal nature of the domain,
statistical ﬂuctuations in the data set mean that the equation will never be satisﬁed exactly,
so we need to perform a suitable statistical test to see if there is sufﬁcient evidence that the
independence hypothesis is violated. The complexity of the resulting network will depend
on the threshold used for this test—the stricter the independence test, the more links will be
added and the greater the danger of overﬁtting.
An approach more consistent with the ideas in this chapter is to assess the degree to
which the proposed model explains the data (in a probabilistic sense). We must be careful
some way. The MAP (or MDL) approach simply subtracts a penalty from the likelihood of
each structure (after parameter tuning) before comparing different structures. The Bayesian
approach places a joint prior over structures and parameters. There are usually far too many
structures to sum over (superexponential in the number of variables), so most practitioners
use MCMC to sample over structures.
Penalizing complexity (whether by MAP or Bayesian methods) introduces an important
tional distributions in the network. With tabular distributions, the complexity penalty for a
pactly parameterized) models tends to produce learned structures with more parents than does
learning with tabular distributions.
Density estimation with nonparametric models
It is possible to learn a probability model without making any assumptions about its structure
nonparametric density estimation is typically done in continuous domains, such as that
NONPARAMETRIC
DENSITY ESTIMATION
density function. The question is, can we recover the model from the samples?
neighbor models for classiﬁcation and regression; here we see them for density estimation.)
Given a sample of data points, to estimate the unknown probability density at a query point x
shows two query points (small squares). For each query point we have drawn the smallest
circle is large, meaning there is a low density there, and the circle on the right is small,
while (a) is too spiky (k is too small) and (c) is too smooth (k is too big).Learning with Complete Data
Density
(a)
(b)
point sample of points from the mixture, together with two query points (small squares) and
Density
Density
Density
(a)
(b)
(c)
Density
Density
Density
(a)
(b)
(c)Chapter
Learning Probabilistic Models
sion. To apply a kernel model to density estimation, assume that each data point generates its
own little density function, using a Gaussian kernel. The estimated density at a query point x
is then the average density as given by each kernel function:
N
N


K(x, xj) .
We will assume spherical Gaussians with standard deviation w along each axis:
K(x, xj) =
(w2√
2π)d e−
2w2
,
where d is the number of dimensions in x and D is the Euclidean distance function. We
values that are too small, just right, and too large. A good value of w can be chosen by using
LEARNING WITH HIDDEN VARIABLES: THE EM ALGORITHM
hidden variables (sometimes called latent variables), which are not observable in the data
LATENT VARIABLE
that are available for learning. For example, medical records often include the observed
symptoms, the physician’s diagnosis, the treatment applied, and perhaps the outcome of the
treatment, but they seldom contain a direct observation of the disease itself! (Note that the
diagnosis is not the disease; it is a causal consequence of the observed symptoms, which are in
turn caused by the disease.) One might ask, “If the disease is not observed, why not construct
diagnostic model for heart disease. There are three observable predisposing factors and three
observable symptoms (which are too depressing to name). Assume that each variable has
three possible values (e.g., none, moderate, and severe). Removing the hidden variable
from the network in (a) yields the network in (b); the total number of parameters increases
required to specify a Bayesian network. This, in turn, can dramatically reduce the amount of
data needed to learn the parameters.
HeartDisease, given its parents, because we do not know the value of HeartDisease in each
case; the same problem arises in learning the distributions for the symptoms. This section
describes an algorithm called expectation–maximization, or EM, that solves this problem
EXPECTATION–
MAXIMIZATION
in a very general way. We will show three examples and then provide a general description.
The algorithm seems like magic at ﬁrst, but once the intuition has been developed, one can
ﬁnd applications for EM in a huge range of learning problems.Learning with Hidden Variables: The EM Algorithm
Smoking
Diet
Exercise
Symptom1
Symptom2
Symptom3
(a)
(b)
HeartDisease
Smoking
Diet
Exercise
Symptom1
Symptom2
Symptom3
(a) A simple diagnostic network for heart disease, which is assumed to be
a hidden variable. Each variable has three possible values and is labeled with the number
equivalent network with HeartDisease removed. Note that the symptom variables are no
Unsupervised clustering: Learning mixtures of Gaussians
Unsupervised clustering is the problem of discerning multiple categories in a collection of
UNSUPERVISED
CLUSTERING
objects. The problem is unsupervised because the category labels are not given. For example,
suppose we record the spectra of a hundred thousand stars; are there different types of stars
revealed by the spectra, and, if so, how many types and what are their characteristics? We
are all familiar with terms such as “red giant” and “white dwarf,” but the stars do not carry
these labels on their hats—astronomers had to perform unsupervised clustering to identify
these categories. Other examples include the identiﬁcation of species, genera, orders, and
so on in the Linnæan taxonomy and the creation of natural kinds for ordinary objects (see
of which speciﬁes the values of two continuous attributes. The data points might correspond
cies. Next, we need to understand what kind of probability distribution might have generated
the data. Clustering presumes that the data are generated from a mixture distribution, P.
MIXTURE
DISTRIBUTION
Such a distribution has k components, each of which is a distribution in its own right. A
COMPONENT
data point is generated by ﬁrst choosing a component and then generating a sample from that
mixture distribution is given by
P(x) =
k


P(C = i) P(x | C = i) ,
where x refers to the values of the attributes for a data point. For continuous data, a natural
mixture of Gaussians family of distributions. The parameters of a mixture of Gaussians are
MIXTURE OF
GAUSSIANSChapter
Learning Probabilistic Models
(a)
(b)
(c)
reconstructed by EM from the data in (b).
wi = P(C = i) (the weight of each component), μi (the mean of each component), and Σi
this mixture is in fact the source of the data in (b) as well as being the model shown in
The unsupervised clustering problem, then, is to recover a mixture model like the one
ponent generated each data point, then it would be easy to recover the component Gaussians:
we could just select all the data points from a given component and then apply (a multivariate
On the other hand, if we knew the parameters of each component, then we could, at least in
a probabilistic sense, assign each data point to a component. The problem is that we know
neither the assignments nor the parameters.
The basic idea of EM in this context is to pretend that we know the parameters of the
model and then to infer the probability that each data point belongs to each component. After
that, we reﬁt the components to the data, where each component is ﬁtted to the entire data set
with each point weighted by the probability that it belongs to that component. The process
iterates until convergence. Essentially, we are “completing” the data by inferring probability
distributions over the hidden variables—which component each data point belongs to—based
ters arbitrarily and then iterate the following two steps:
was generated by component i. By Bayes’ rule, we have pij = αP(xj | C = i)P(C = i).
The term P(xj | C = i) is just the probability at xj of the ith Gaussian, and the term
P(C = i) is just the weight parameter for the ith Gaussian. Deﬁne ni = 
j pij, the
effective number of data points currently assigned to component i.
ing steps in sequence:Learning with Hidden Variables: The EM Algorithm
μi ←


j
Σi ←


j
pij(xj −μi)(xj −μi)⊤/ni
INDICATOR VARIABLE
step, ﬁnds the new values of the parameters that maximize the log likelihood of the data,
given the expected values of the hidden indicator variables.
current model as EM progresses.
There are two points to notice. First, the log likelihood for the ﬁnal learned model
slightly exceeds that of the original model, from which the data were generated. This might
seem surprising, but it simply reﬂects the fact that the data were generated randomly and
might not provide an exact reﬂection of the underlying model. The second point is that EM
increases the log likelihood of the data at every iteration. This fact can be proved in general.
Furthermore, under certain conditions (that hold in ost cases), EM can be proven to reach
a local maximum in likelihood. (In rare cases, it could reach a saddle point or even a local
that it has no “step size” parameter.
Iteration number
Iteration number
(a)
(b)
Graphs showing the log likelihood of the data, L, as a function of the EM
iteration. The horizontal line shows the log likelihood according to the true model. (a) GraphChapter
Learning Probabilistic Models
(a)
(b)
C
X
Hole
Bag
θ
Wrapper
Flavor
Bag
θF2
θF1
pers, presence of holes depend on the bag, which is not observed. (b) Bayesian network for
a Gaussian mixture. The mean and covariance of the observable variables X depend on the
component C.
example, that one Gaussian component shrinks so that it covers just a single data point. Then
its variance will go to zero and its likelihood will go to inﬁnity! Another problem is that
two components can “merge,” acquiring identical means and variances and sharing their data
points. These kinds of degenerate local maxima are serious problems, especially in high
dimensions. One solution is to place priors on the model parameters and to apply the MAP
version of EM. Another is to restart a component with new random parameters if it gets too
small or too close to another component. Sensible initialization also helps.
Learning Bayesian networks with hidden variables
To learn a Bayesian network with hidden variables, we apply the same insights that worked
candies that have been mixed together. Candies are described by three features: in addition
to the Flavor and the Wrapper, some candies have a Hole in the middle and some do not.
The distribution of candies in each bag is described by a naive Bayes model: the features
are independent, given the bag, but the conditional probability distribution for each feature
depends on the bag. The parameters are as follows: θ is the prior probability that a candy
the overall model is a mixture model. (In fact, we can also model the mixture of Gaussians
variable because, once the candies have been mixed together, we no longer know which bag
each candy came from. In such a case, can we recover the descriptions of the two bags byLearning with Hidden Variables: The EM Algorithm
observing candies from the mixture? Let us work through an iteration of EM for this problem.
are as follows:
That is, the candies are equally likely to come from either bag; the ﬁrst is mostly cherries
with red wrappers and holes; the second is mostly limes with green wrappers and no holes.
The counts for the eight possible kinds of candy are as follows:
W = red
W = green
F = cherry
F = lime
We start by initializing the parameters. For numerical simplicity, we arbitrarily choose5
N


These probabilities can be computed by any inference algorithm for Bayesian networks. For
a naive Bayes model such as the one in our example, we can do the inference “by hand,”
using Bayes’ rule and applying conditional independence:
N
N



i P(ﬂavor j | Bag = i)P(wrapper j | Bag = i)P(holesj | Bag = i)P(Bag = i) .
tribution of


j:Flavor j = cherryChapter
Learning Probabilistic Models
Again, these probabilities can be calculated by any Bayes net algorithm. Completing this
process, we obtain the new values of all the parameters:
work learning with hidden variables are directly available from the results of inference on
ter. Here, “local” means that the CPT for each variable Xi can be learned from posterior
P(Xi = xij | Ui = uik), the update is given by the normalized expected counts as follows:
θijk ←ˆN(Xi = xij, Ui = uik)/ ˆN(Ui = uik) .
The expected counts are obtained by summing over the examples, computing the probabilities
P(Xi = xij, Ui = uik) for each by using any Bayes net inference algorithm. For the exact
algorithms—including variable elimination—all these probabilities are obtainable directly as
Moreover, the information needed for learning is available locally for each parameter.
Learning hidden Markov models
Our ﬁnal application of EM involves learning the transition probabilities in hidden Markov
Each data point consists of an observation sequence of ﬁnite length, so the problem is to
learn the transition probabilities from a set of observation sequences (or from just one long
sequence).
We have already worked out how to learn Bayes nets, but there is one complication:
in Bayes nets, each parameter is distinct; in a hidden Markov model, on the other hand, the
are repeated across time—that is, θijt = θij for all t. To estimate the transition probability
from state i to state j, we simply calculate the expected proportion of times that the system
undergoes a transition to state j when in state i:
θij ←


t


t
ˆN(Xt = i) .
The expected counts are computed by an HMM inference algorithm. The forward–backward
abilities. One important point is that the probabilities required are obtained by smoothingLearning with Hidden Variables: The EM Algorithm
f
t
P(R )
R0
P(R0)
f
t
P(U )
R1
Umbrella1
Rain0
Rain1
P(R0)
f
t
P(U )
R4
f
t
P(R  )
R3
Umbrella4
Rain4
f
t
P(U )
R3
f
t
R
P(R  )
Umbrella3
Rain3
f
t
P(U )
R2
f
t
R
P(R )
Umbrella2
Rain2
f
t
P(U )
R1
f
t
R
P(R )
Umbrella1
Rain0
Rain1
An unrolled dynamic Bayesian network that represents a hidden Markov
rather than ﬁltering; that is, we need to pay attention to subsequent evidence in estimating
the probability that a particular transition occurred. The evidence in a murder case is usually
obtained after the crime (i.e., the transition from state i to state j) has taken place.
The general form of the EM algorithm
We have seen several instances of the EM algorithm. Each involves computing expected
values of hidden variables for each example and then recomputing the parameters, using the
expected values as if they were observed values. Let x be all the observed values in all the
parameters for the probability model. Then the EM algorithm is
θ


z
P(Z = z | x, θ(i))L(x, Z = z | θ) .
tion, which is the expectation of the log likelihood of the “completed” data with respect to the
distribution P(Z = z | x, θ(i)), which is the posterior over the hidden variables, given the data.
was generated by component i. For Bayes nets, Zij is the value of unobserved variable Xi in
example j. For HMMs, Zjt is the state of the sequence in example j at time t. Starting from
the general form, it is possible to derive an EM algorithm for a speciﬁc application once the
appropriate hidden variables have been identiﬁed.
As soon as we understand the general idea of EM, it becomes easy to derive all sorts
posteriors over the hidden variables—is intractable, as in large Bayes nets. It turns out that
actly as if it were a complete observation. Thus, the parameters can be updated directly after
each MCMC transition. Other forms of approximate inference, such as variational and loopy
methods, have also proved effective for learning very large networks.Chapter
Learning Probabilistic Models
Learning Bayes net structures with hidden variables
data. When unobserved variables may be inﬂuencing the data that are observed, things get
tain hidden variables exist, leaving it to the algorithm to ﬁnd a place for them in the network
searches over structures and an inner loop that ﬁts the network parameters given the structure.
If the learning algorithm is not told which hidden variables exist, then there are two
choices: either pretend that the data is really complete—which may force the algorithm to
variables in order to simplify the model. The latter approach can be implemented by including
new modiﬁcation choices in the structure search: in addition to modifying links, the algorithm
can add or delete a hidden variable or change its arity. Of course, the algorithm will not know
that the new variable it has invented is called HeartDisease; nor will it have meaningful
names for the values. Fortunately, newly invented hidden variables will usually be connected
to preexisting variables, so a human expert can often inspect the local conditional distributions
involving the new variable and ascertain its meaning.
a completely connected network (moreover, one with no hidden variables), so some form of
complexity penalty is required. We can also apply MCMC to sample many possible network
structures, thereby approximating Bayesian learning. For example, we can learn mixtures of
imate posterior distribution for the number of Gaussians is given by the sampling frequencies
of the MCMC process.
problem. To date, this approach has proved impractical for learning complex models. One
STRUCTURAL EM
same way as ordinary (parametric) EM except that the algorithm can update the structure
as well as the parameters. Just as ordinary EM uses the current parameters to compute the
each potential structure.) In this way, structural EM may make several structural alterations
trivial Bayes net structures. Nonetheless, much work remains to be done before we can saySummary
SUMMARY
Statistical learning methods range from simple calculation of averages to the construction of
complex models such as Bayesian networks. They have applications throughout computer
science, engineering, computational biology, neuroscience, psychology, and physics. This
pinnings. The main points are as follows:
• Bayesian learning methods formulate learning as a form of probabilistic inference,
using the observations to update a prior distribution over hypotheses. This approach
provides a good way to implement Ockham’s razor, but quickly becomes intractable for
complex hypothesis spaces.
• Maximum a posteriori (MAP) learning selects a single most likely hypothesis given
the data. The hypothesis prior is still used and the method is often more tractable than
full Bayesian learning.
hood of the data; it is equivalent to MAP learning with a uniform prior. In simple cases
solutions can be found easily in closed form. Naive Bayes learning is a particularly
effective technique that scales well.
• When some variables are hidden, local maximum likelihood solutions can be found
using the EM algorithm. Applications include clustering using mixtures of Gaussians,
learning Bayesian networks, and learning hidden Markov models.
• Learning the structure of Bayesian networks is an example of model selection. This
usually involves a discrete search in the space of structures. Some method is required
for trading off model complexity against degree of ﬁt.
• Nonparametric models represent a distribution using the collection of data points.
look at the examples nearest to the point in question, whereas kernel methods form a
Statistical learning continues to be a very active area of research. Enormous strides have been
made in both theory and practice, to the point where it is possible to learn almost any model
for which exact or approximate inference is feasible.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The application of statistical learning techniques in AI was an active area of research in the
latter ﬁeld concentrated on symbolic methods. A resurgence of interest occurred shortly after
the introduction of Bayesian network models in the late 1980s; at roughly the same time,Chapter
Learning Probabilistic Models
a statistical view of neural network learning began to emerge. In the late 1990s, there was
a noticeable convergence of interests in machine learning, statistics, and neural networks,
centered on methods for creating large probabilistic models from data.
The naive Bayes model is one of the oldest and simplest forms of Bayesian network,
ter learning with Dirichlet priors for Bayesian networks was discussed by Spiegelhalter et al.
provides a very powerful tool for formulating and learning complex probability models. The
ﬁrst algorithms for learning Bayes net structures used conditional independence tests (Pearl,
embodied in the TETRAD package for Bayes net learning. Algorithmic improvements since
Several algorithmic advances since that time have led to quite respectable performance in
learned structure.
and analyzed its convergence, is one of the most cited papers in both computer science and
statistics. (Dempster himself views EM as a schema rather than an algorithm, since a goodChapter
Learning Probabilistic Models
Suppose that Ann’s utilities for cherry and lime candies are cA and ℓA, whereas Bob’s
utilities are cB and ℓB. (But once Ann has unwrapped a piece of candy, Bob won’t buy
it.) Presumably, if Bob likes lime candies much more than Ann, it would be wise for Ann
to sell her bag of candies once she is sufﬁciently sure of its lime content. On the other hand,
if Ann unwraps too many candies in the process, the bag will be worth less. Discuss the
problem of determining the optimal point at which to sell the bag. Determine the expected
will the ﬁrst statistician (an avid Bayesian) do? How about the second statistician, who always
uses the maximum likelihood hypothesis?
The doctor does some research and discovers that disease B actually comes in two
drug. Now that there are three hypotheses, what will the two statisticians do?
the performance of the resulting algorithm on the restaurant learning problem.
Consider N data points (xj, yj), where the yjs are generated from the xjs according to
the conditional log likelihood of the data.
data. (Hint: use the chain rule for partial derivatives.)
tribution beta[a, b] is given by α = Γ(a + b)/Γ(a)Γ(b) where Γ(x) is the Gamma
GAMMA FUNCTION
is updated?
Consider an arbitrary Bayesian network, a complete data set for that network, and the
likelihood for the data set according to the network. Give a simple proof that the likelihood
likelihood parameter values.
Consider a single Boolean random variable Y (the “classiﬁcation”). Let the prior
probability P(Y = true) be π. Let’s try to ﬁnd π, given a training set D = (y1, . . . , yN) with
N independent samples of Y . Furthermore, suppose p of the N are positive and n of the N
are negative.REINFORCEMENT
LEARNING
ward and punishment.
INTRODUCTION
models from examples. In this chapter, we will study how agents can learn what to do in the
absence of labeled examples of what to do.
Consider, for example, the problem of learning to play chess. A supervised learning
agent needs to be told the correct move for each position it encounters, but such feedback is
seldom available. In the absence of feedback from a teacher, an agent can learn a transition
model for its own moves and can perhaps learn to predict the opponent’s moves, but without
ing which move to make. The agent needs to know that something good has happened when
it (accidentally) checkmates the opponent, and that something bad has happened when it is
checkmated—or vice versa, if the game is suicide chess. This kind of feedback is called a
reward, or reinforcement. In games like chess, the reinforcement is received only at the end
REINFORCEMENT
point scored can be considered a reward; when learning to crawl, any forward motion is an
achievement. Our framework for agents regards the reward as part of the input percept, but
the agent must be “hardwired” to recognize that part as a reward rather than as just another
sensory input. Thus, animals seem to be hardwired to recognize pain and hunger as negative
rewards and pleasure and food intake as positive rewards. Reinforcement has been carefully
in Markov decision processes (MDPs). An optimal policy is a policy that maximizes the
expected total reward. The task of reinforcement learning is to use observed rewards to learn
has a complete model of the environment and knows the reward function, here we assume noIntroduction
prior knowledge of either. Imagine playing a new game whose rules you don’t know; after a
hundred or so moves, your opponent announces, “You lose.” This is reinforcement learning
in a nutshell.
In many complex domains, reinforcement learning is the only feasible way to train a
program to perform at high levels. For example, in game playing, it is very hard for a human
to provide accurate and consistent evaluations of large numbers of positions, which would be
needed to train an evaluation function directly from examples. Instead, the program can be
told when it has won or lost, and it can use this information to learn an evaluation function
that gives reasonably accurate estimates of the probability of winning from any given position.
Similarly, it is extremely difﬁcult to program an agent to ﬂy a helicopter; yet given appropriate
negative rewards for crashing, wobbling, or deviating from a set course, an agent can learn to
ﬂy by itself.
Reinforcement learning might be considered to encompass all of AI: an agent is placed
ageable, we will concentrate on simple environments and simple agent designs. For the most
part, we will assume a fully observable environment, so that the current state is supplied by
vironment works or what its actions do, and we will allow for probabilistic action outcomes.
Thus, the agent faces an unknown Markov decision process. We will consider three of the
maximize the expected outcome utility.
pected utility of taking a given action in a given state.
• A reﬂex agent learns a policy that maps directly from states to actions.
because it must know the states to which its actions will lead. For example, in order to make
use of a backgammon evaluation function, a backgammon program must know what its legal
moves are and how they affect the board position. Only in this way can it apply the utility
expected utilities for its available choices without needing to know their outcomes, so it does
not need a model of the environment. On the other hand, because they do not know where
to learn, as we shall see.
PASSIVE LEARNING
the task is to learn the utilities of states (or state–action pairs); this could also involve learning
ACTIVE LEARNING
learn what to do. The principal issue is exploration: an agent must experience as much as
EXPLORATION
covers methods for learning direct policy representations in reﬂex agents. An understandingChapter
Reinforcement Learning
PASSIVE REINFORCEMENT LEARNING
representation in a fully observable environment. In passive learning, the agent’s policy π
is ﬁxed: in state s, it always executes the action π(s). Its goal is simply to learn how good
the policy is—that is, to learn the utility function Uπ(s). We will use as our example the
corresponding utilities. Clearly, the passive learning task is similar to the policy evaluation
is that the passive learning agent does not know the transition model P(s′ | s, a), which
speciﬁes the probability of reaching state s′ from state s after doing action a; nor does it
know the reward function R(s), which speciﬁes the reward for each state.
(a)
(b)
The agent executes a set of trials in the environment using its policy π. In each trial, the
TRIAL
received in that state. Typical trials might look like this:
Note that each state percept is subscripted with the reward received. The object is to use the
nal state s. The utility is deﬁned to be the expected sum of (discounted) rewards obtained ifPassive Reinforcement Learning
Uπ(s) = E
" ∞


γtR(St)
#
where R(s) is the reward for a state, St (a random variable) is the state reached at time t when
executing policy π, and S0 = s. We will include a discount factor γ in all of our equations,
Direct utility estimation
A simple method for direct utility estimation was invented in the late 1950s in the area of
DIRECT UTILITY
ESTIMATION
ADAPTIVE CONTROL
THEORY
each trial provides a sample of this quantity for each state visited. For example, the ﬁrst trial
and updates the estimated utility for that state accordingly, just by keeping a running average
for each state in a table. In the limit of inﬁnitely many trials, the sample average will converge
It is clear that direct utility estimation is just an instance of supervised learning where
that we have reduced reinforcement learning to a standard inductive learning problem, as
tations for the utility function. Learning techniques for those representations can be applied
directly to the observed data.
Direct utility estimation succeeds in reducing the reinforcement learning problem to
an inductive learning problem, about which much is known. Unfortunately, it misses a very
dent! The utility of each state equals its own reward plus the expected utility of its successor
states. That is, the utility values obey the Bellman equations for a ﬁxed policy (see also
Uπ(s) = R(s) + γ


s′
P(s′ | s, π(s))Uπ(s′) .
By ignoring the connections between states, direct utility estimation misses opportunities for
from the ﬁrst trial to have a high utility. The Bellman equation suggests immediately that
learns nothing until the end of the trial. More broadly, we can view direct utility estimation
as searching for U in a hypothesis space that is much larger than it needs to be, in that it
includes many functions that violate the Bellman equations. For this reason, the algorithm
often converges very slowly.Chapter
Reinforcement Learning
inputs: percept, a percept indicating the current state s′ and reward signal r ′
persistent: π, a ﬁxed policy
mdp, an MDP with model P, rewards R, discount γ
U , a table of utilities, initially empty
Nsa, a table of frequencies for state–action pairs, initially zero
Ns′|sa, a table of outcome frequencies given state–action pairs, initially zero
s, a, the previous state and action, initially null
if s′ is new then U [s′] ←r ′; R[s′] ←r ′
if s is not null then
increment Nsa[s,a] and Ns′|sa[s′,s,a]
for each t such that Ns′|sa[t,s,a] is nonzero do
P(t | s, a) ←Ns′|sa[t,s,a] / Nsa[s,a]
if s′.TERMINAL? then s,a ←null else s,a ←s′,π[s′]
return a
Adaptive dynamic programming
An adaptive dynamic programming (or ADP) agent takes advantage of the constraints
ADAPTIVE DYNAMIC
PROGRAMMING
ing the corresponding Markov decision process using a dynamic programming method. For
a passive learning agent, this means plugging the learned transition model P(s′ | s, π(s)) and
using a simpliﬁed value iteration process to update the utility estimates after each change to
the learned model. Because the model usually changes only slightly with each observation,
the value iteration process can use the previous utility estimates as initial values and should
converge quite quickly.
servable. This means that we have a supervised learning task where the input is a state–action
sition model as a table of probabilities. We keep track of how often each action outcome
occurs and estimate the transition probability P(s′ | s, a) from the frequency with which s′Passive Reinforcement Learning
Utility estimates
Number of trials
RMS error in utility
Number of trials
(a)
(b)
of the number of trials. Notice the large changes occurring around the 78th trial—this is the
timates improve, the ADP agent is limited only by its ability to learn the transition model.
In this sense, it provides a standard against which to measure other reinforcement learning
algorithms. It is, however, intractable for large state spaces. In backgammon, for example, it
model; moreover, by choosing a policy based solely on the estimated model it is acting as
if the model were correct. This is not necessarily a good idea! For example, a taxi agent
that didn’t know about how trafﬁc lights might ignore a red light once or twice without no
ill effects and then formulate a policy to ignore red lights from then on. Instead, it might
mum likelihood, works reasonably well for the whole range of models that have a reasonable
chance of being the true model. There are two mathematical approaches that have this ﬂavor.
The ﬁrst approach, Bayesian reinforcement learning, assumes a prior probability
BAYESIAN
REINFORCEMENT
LEARNING
P(h) for each hypothesis h about what the true model is; the posterior probability P(h | e) is
obtained in the usual way by Bayes’ rule given the observations to date. Then, if the agent has
decided to stop learning, the optimal policy is the one that gives the highest expected utility.
Let uπ
h be the expected utility, averaged over all possible start states, obtained by executing
policy π in model h. Then we have
π∗= argmax
π


h
P(h | e)uπ
h .Chapter
Reinforcement Learning
In some special cases, this policy can even be computed! If the agent will continue learning
in the future, however, then ﬁnding an optimal policy becomes considerably more difﬁcult,
because the agent must consider the effects of future observations on its beliefs about the
transition model. The problem becomes a POMDP whose belief states are distributions over
models. This concept provides an analytical foundation for understanding the exploration
The second approach, derived from robust control theory, allows for a set of possible
ROBUST CONTROL
THEORY
models H and deﬁnes an optimal robust policy as one that gives the best outcome in the worst
case over H:
π∗= argmax
π
min
h uπ
h .
Often, the set H will be the set of models that exceed some likelihood threshold on P(h | e),
so the robust and Bayesian approaches are related. Sometimes, the robust solution can be
computed efﬁciently. There are, moreover, reinforcement learning algorithms that tend to
produce robust solutions, although we do not cover them here.
Solving the underlying MDP as in the preceding section is not the only way to bring the
Bellman equations to bear on the learning problem. Another way is to use the observed
transitions to adjust the utilities of the observed states so that they agree with the constraint
ties to obey the equation
be increased. More generally, when a transition occurs from state s to state s′, we apply the
following update to Uπ(s):
Uπ(s) ←Uπ(s) + α(R(s) + γ Uπ(s′) −Uπ(s)) .
Here, α is the learning rate parameter. Because this update rule uses the difference in utilities
DIFFERENCE
involved. First, notice that the update involves only the observed successor s′, whereas the
actual equilibrium conditions involve all possible next states. One might think that this causes
an improperly large change in Uπ(s) when a very rare transition occurs; but, in fact, because
rare transitions occur only rarely, the average value of U π(s) will converge to the correct
value. Furthermore, if we change α from a ﬁxed parameter to a function that decreases as
the number of times a state has been visited increases, then U π(s) itself will converge to thePassive Reinforcement Learning
inputs: percept, a percept indicating the current state s′ and reward signal r ′
persistent: π, a ﬁxed policy
U , a table of utilities, initially empty
Ns, a table of frequencies for states, initially zero
s, a, r, the previous state, action, and reward, initially null
if s′ is new then U [s′] ←r ′
if s is not null then
increment N s[s]
U [s] ←U [s] + α(Ns[s])(r + γ U [s′] −U [s])
if s′.TERMINAL? then s,a,r ←null else s,a,r ←s′,π[s′],r ′
return a
in the text.
the ADP agent and shows much higher variability, but it is much simpler and requires much
less computation per observation. Notice that TD does not need a transition model to perform
its updates. The environment supplies the connection between neighboring states in the form
of observed transitions.
The ADP approach and the TD approach are actually closely related. Both try to make
the effects of TD adjustments are averaged over a large number of transitions, because the
served transition, ADP makes as many as it needs to restore consistency between the utility
estimates U and the environment model P. Although the observed transition makes only a
local change in P, its effects might need to be propagated throughout U . Thus, TD can be
viewed as a crude but efﬁcient ﬁrst approximation to ADP.
sult of a “pseudoexperience” generated by simulating the current environment model. It
is possible to extend the TD approach to use an environment model to generate several
pseudoexperiences—transitions that the TD agent can imagine might happen, given its current
model. For each observed transition, the TD agent can generate a large number of imaginary
satisﬁes the conditions.Chapter
Reinforcement Learning
Utility estimates
Number of trials
RMS error in utility
Number of trials
(a)
(b)
transitions. In this way, the resulting utility estimates will approximate more and more closely
those of ADP—of course, at the expense of increased computation time.
mating the algorithms for value iteration or policy iteration. Even though the value iteration
sible approach to generating reasonably good answers quickly is to bound the number of
sible adjustments so as to carry out only the most signiﬁcant ones. The prioritized sweeping
PRIORITIZED
SWEEPING
heuristic prefers to make adjustments to states whose likely successors have just undergone a
large adjustment in their own utility estimates. Using heuristics like this, approximate ADP
quences, but can be several orders of magnitude more efﬁcient in terms of computation. (See
Approximate ADP algorithms have an additional advantage: in the early stages of learning a
new environment, the environment model P often will be far from correct, so there is little
point in calculating an exact utility function to match it. An approximation algorithm can use
a minimum adjustment size that decreases as the environment model becomes more accurate.
This eliminates the very long value iterations that can occur early in learning due to large
changes in the model.Active Reinforcement Learning
ACTIVE REINFORCEMENT LEARNING
A passive learning agent has a ﬁxed policy that determines its behavior. An active agent must
decide what actions to take. Let us begin with the adaptive dynamic programming agent and
consider how it must be modiﬁed to handle this new freedom.
First, the agent will need to learn a complete model with outcome probabilities for all
actions, rather than just the model for the ﬁxed policy. The simple learning mechanism used
fact that the agent has a choice of actions. The utilities it needs to learn are those deﬁned by
for convenience:
U(s) = R(s) + γ max
a


s′
P(s′ | s, a)U(s′) .
These equations can be solved to obtain the utility function U using the value iteration or
obtained a utility function U that is optimal for the learned model, the agent can extract an
uses policy iteration, the optimal policy is already available, so it should simply execute the
action the optimal policy recommends. Or should it?
Exploration
recommendation of the optimal policy for the learned model at each step. The agent does
not learn the true utilities or the true optimal policy! What happens instead is that, in the
276th trial onward it sticks to that policy, never learning the utilities of the other states and
GREEDY AGENT
Repeated experiments show that the greedy agent very seldom converges to the optimal policy
for this environment and sometimes converges to really horrendous policies.
How can it be that choosing the optimal action leads to suboptimal results? The answer
is that the learned model is not the same as the true environment; what is optimal in the
learned model can therefore be suboptimal in the true environment. Unfortunately, the agent
does not know what the true environment is, so it cannot compute the optimal action for the
true environment. What, then, is to be done?
What the greedy agent has overlooked is that actions do more than provide rewards
fecting the percepts that are received. By improving the model, the agent will receive greater
EXPLOITATION
EXPLORATIONChapter
Reinforcement Learning
RMS error, policy loss
Number of trials
RMS error
Policy loss
(a)
(b)
Performance of a greedy ADP agent that executes the action recommended
by the optimal policy for the learned model. (a) RMS error in the utility estimates averaged
over the nine nonterminal squares. (b) The suboptimal policy to which the greedy agent
converges in this particular sequence of trials.
to improve one’s knowledge is of no use if one never puts that knowledge into practice. In the
real world, one constantly has to decide between continuing in a comfortable existence and
striking out into the unknown in the hopes of discovering a new and better life. With greater
understanding, less exploration is necessary.
Can we be a little more precise than this? Is there an optimal exploration policy? This
question has been studied in depth in the subﬁeld of statistical decision theory that deals with
BANDIT PROBLEM
Although bandit problems are extremely difﬁcult to solve exactly to obtain an optimal
exploration method, it is nonetheless possible to come up with a reasonable scheme that
will eventually lead to optimal behavior by the agent. Technically, any such scheme needs
to be greedy in the limit of inﬁnite exploration, or GLIE. A GLIE scheme must try each
GLIE
action in each state an unbounded number of times to avoid having a ﬁnite probability that
an optimal action is missed because of an unusually bad series of outcomes. An ADP agent
using such a scheme will eventually learn the true environment model. A GLIE scheme must
also eventually become greedy, so that the agent’s actions become optimal with respect to the
learned (and hence the true) model.
does eventually converge to an optimal policy, it can be extremely slow. A more sensible
approach would give some weight to actions that the agent has not tried very often, while
tending to avoid actions that are believed to be of low utility. This can be implemented byActive Reinforcement Learning
EXPLORATION AND BANDITS
The gambler must choose which lever to play on each successive coin—the one
that has paid off best, or maybe one that has not been tried?
tally important areas, such as deciding on the annual budget for AI research and
responds to the beneﬁts obtained from taking the action (immense). Exploration,
whether it is exploration of a new research ﬁeld or exploration of a new shopping
mall, is risky, is expensive, and has uncertain payoffs; on the other hand, failure to
explore at all means that one never discovers any actions that are worthwhile.
To formulate a bandit problem properly, one must deﬁne exactly what is meant
by optimal behavior. Most deﬁnitions in the literature assume that the aim is to
maximize the expected total reward obtained over the agent’s lifetime. These deﬁ-
nitions require that the expectation be taken over the possible worlds that the agent
could be in, as well as over the possible results of each action sequence in any given
der to act optimally, the agent needs a prior distribution over the possible models.
The resulting optimization problems are usually wildly intractable.
In some cases—for example, when the payoff of each machine is independent
and discounted rewards are used—it is possible to calculate a Gittins index for
times the slot machine has been played and how much it has paid off. The index for
each machine indicates how worthwhile it is to invest more; generally speaking, the
higher the expected return and the higher the uncertainty in the utility of a given
choice, the better. Choosing the machine with the highest index value gives an
optimal exploration policy. Unfortunately, no way has been found to extend Gittins
indices to sequential decision problems.
investment of a coin in one arm to be the reproduction of those genes, then it can
be proven that genetic algorithms allocate coins optimally, given an appropriate set
of independence assumptions.Chapter
Reinforcement Learning
ble environments and causes the agent to behave initially as if there were wonderful rewards
a has been tried in state s. Suppose we are using value iteration in an ADP learning agent;
optimistic estimate. The following equation does this:
a
f



.
Here, f(u, n) is called the exploration function. It determines how greed (preference for
EXPLORATION
FUNCTION
high values of u) is traded off against curiosity (preference for actions that have not been
tried often and have low n). The function f(u, n) should be increasing in u and decreasing
in n. Obviously, there are many possible functions that ﬁt these conditions. One particularly
simple deﬁnition is
f(u, n) =
if n < Ne
u
otherwise
is a ﬁxed parameter. This will have the effect of making the agent try each action–state pair
at least Ne times.
very important. As exploration proceeds, the states and actions near the start state might well
be tried a large number of times. If we used U, the more pessimistic utility estimate, then
the agent would soon become disinclined to explore further aﬁeld. The use of U + means
that the beneﬁts of exploration are propagated back from the edges of unexplored regions,
so that actions that lead toward unexplored regions are weighted more highly, rather than
just actions that are themselves unfamiliar. The effect of this exploration policy can be seen
that the utility estimates themselves do not converge as quickly. This is because the agent
stops exploring the unrewarding parts of the state space fairly soon, visiting them only “by
accident” thereafter. However, it makes perfect sense for the agent not to care about the exact
utilities of states that it knows are undesirable and can be avoided.
difference learning agent. The most obvious change from the passive case is that the agent
is no longer equipped with a ﬁxed policy, so, if it learns a utility function U, it will need to
The model acquisition problem for the TD agent is identical to that for the ADP agent. What
This might seem odd, for the following reason: Suppose the agent takes a step that normallyActive Reinforcement Learning
Utility estimates
Number of trials
RMS error, policy loss
Number of trials
RMS error
Policy loss
(a)
(b)
Utility estimates for selected states over time. (b) The RMS error in utility values and the
associated policy loss.
leads to a good destination, but because of nondeterminism in the environment the agent ends
up in a catastrophic state. The TD update rule will take this as seriously as if the outcome had
been the normal result of the action, whereas one might suppose that, because the outcome
was a ﬂuke, the agent should not worry about it too much. In fact, of course, the unlikely
outcome will occur only infrequently in a large set of training sequences; hence in the long
run its effects will be weighted proportionally to its probability, as we would hope. Once
again, it can be shown that the TD algorithm will converge to the same values as ADP as the
number of training sequences tends to inﬁnity.
representation instead of learning utilities. We will use the notation Q(s, a) to denote the
U(s) = max
a
Q(s, a) .
Q(s, a) = R(s) + γ


s′
P(s′ | s, a) max
a′
Q(s′, a′) .
As in the ADP learning agent, we can use this equation directly as an update equation for
however, require that a model also be learned, because the equation uses P(s′ | s, a). TheChapter
Reinforcement Learning
inputs: percept, a percept indicating the current state s′ and reward signal r ′
persistent: Q, a table of action values indexed by state and action, initially zero
Nsa, a table of frequencies for state–action pairs, initially zero
s, a, r, the previous state, action, and reward, initially null
if TERMINAL?(s) then Q[s,None] ←r ′
if s is not null then
increment Nsa[s,a]
Q[s,a] ←Q[s,a] + α(Nsa[s, a])(r + γ maxa′ Q[s′, a′] −Q[s,a])
s,a,r ←s′,argmaxa′ f(Q[s′, a′], Nsa[s′, a′]),r ′
return a
a state can be related directly to those of its neighbors.
Q(s, a) ←Q(s, a) + α(R(s) + γ max
a′
Q(s′, a′) −Q(s, a)) ,
which is calculated whenever action a is executed in state s leading to state s′.
exploratory ADP agent—hence the need to keep statistics on actions taken (the table N). If
a simpler exploration policy is used—say, acting randomly on some fraction of steps, where
the fraction decreases over time—then we can dispense with the statistics.
SARSA
Q(s, a) ←Q(s, a) + α(R(s) + γ Q(s′, a′) −Q(s, a)) ,
where a′ is the action actually taken in state s′. The rule is applied at the end of each
are identical. When exploration is happening, however, they differ signiﬁcantly. Because
even when guided by a random or adversarial exploration policy. On the other hand, SARSA
is more realistic: for example, if the overall policy is even partly controlled by other agents, it
like to happen.Generalization in Reinforcement Learning
at a much slower rate than the ADP agent. This is because the local updates do not enforce
no model? In other words, what is the best way to represent the agent function? This is
key historical characteristics of much of AI research is its (often unstated) adherence to the
the agent function is to build a representation of some aspects of the environment in which
the agent is situated.
Some researchers, both inside and outside AI, have claimed that the availability of
essary. There is, however, little to go on but intuition. Our intuition, for what it’s worth, is that
become more apparent. This is borne out even in games such as chess, checkers (draughts),
and backgammon (see next section), where efforts to learn an evaluation function by means
GENERALIZATION IN REINFORCEMENT LEARNING
represented in tabular form with one output value for each input tuple. Such an approach
works reasonably well for small state spaces, but the time to convergence and (for ADP) the
Backgammon and chess are tiny subsets of the real world, yet their state spaces contain on
visit all these states many times in order to learn how to play the game!
One way to handle such problems is to use function approximation, which simply
FUNCTION
APPROXIMATION
representation is viewed as approximate because it might not be the case that the true utility
described an evaluation function for chess that is represented as a weighted linear function
of a set of features (or basis functions) f1, . . . , fn:
BASIS FUNCTION
an enormous compression. Although no one knows the true utility function for chess, noChapter
Reinforcement Learning
it practical to represent utility functions for very large state spaces, but that is not its principal
beneﬁt. The compression achieved by a function approximator allows the learning agent to
generalize from states it has visited to states it has not visited. That is, the most important
tive generalization over input states. To give you some idea of the power of this effect: by
On the ﬂip side, of course, there is the problem that there could fail to be any function
in the chosen hypothesis space that approximates the true utility function sufﬁciently well.
As in all inductive learning, there is a tradeoff between the size of the hypothesis space and
the time it takes to learn the function. A larger hypothesis space increases the likelihood that
a good approximation can be found, but also means that convergence is likely to be delayed.
of the squares are just their x and y coordinates, so we have
For reinforcement learning, it makes more sense to use an online learning algorithm
that updates the parameters after each trial. Suppose we run a trial and the total reward
network learning, we write an error function and compute its gradient with respect to the
parameters. If uj(s) is the observed total reward from state s onward in the jth trial, then
the error is deﬁned as (half) the squared difference of the predicted total and the actual total:
θi ←θi −α ∂Ej(s)
∂θi
∂θi
.
WIDROW–HOFF RULE
DELTA RULE
it can be represented by a program that solves the game exactly every time it is called. We are interested only in
function approximators that use a reasonable amount of computation. It might in fact be better to learn a very
are currently not well understood.Generalization in Reinforcement Learning
allows a reinforcement learner to generalize from its experiences.
We expect that the agent will learn faster if it uses a function approximator, provided
that the hypothesis space is not too large, but includes some functions that are a reasonably
direct utility estimation, both with and without function approximation. The improvement in
world is well suited for a linear utility function because the true utility function is smooth
All is not lost, however! Remember that what matters for linear function approximation
is that the function be linear in the parameters—the features themselves can be arbitrary
θ3

adjust the parameters to try to reduce the temporal difference between successive states. The
are given by
∂θi
for utilities and
θi ←θi + α [R(s) + γ max
a′
∂θi
possible4 approximation to the true function when the function approximator is linear in the
parameters. With active learning and nonlinear functions such as neural networks, all bets
are off: There are some very simple cases in which the parameters can go off to inﬁnity
even though there are good solutions in the hypothesis space. There are more sophisticated
algorithms that can avoid these problems, but at present reinforcement learning with general
function approximators remains a delicate art.
ing problem, because the next percept gives the outcome state. Any of the supervised learning
dict a complete state description rather than just a Boolean classiﬁcation or a single real value.
For a partially observable environment, the learning problem is much more difﬁcult. If we
know what the hidden variables are and how they are causally related to each other and to theChapter
Reinforcement Learning
observable variables, then we can ﬁx the structure of a dynamic Bayesian network and use the
variables and learning the model structure are still open problems. Some practical examples
POLICY SEARCH
The ﬁnal approach we will consider for reinforcement learning problems is called policy
search. In some ways, policy search is the simplest of all the methods in this chapter: the
POLICY SEARCH
idea is to keep twiddling the policy as long as its performance improves, then stop.
Let us begin with the policies themselves. Remember that a policy π is a function that
maps states to actions. We are interested primarily in parameterized representations of π that
have far fewer parameters than there are states in the state space (just as in the preceding
one for each action, and take the action with the highest predicted value:
π(s) = max
a
Q∗.) Another clear instance of the difference is the case where π(s) is calculated using, say,
the policy is a discontinuous function of the parameters when the actions are discrete. (For a
continuous action space, the policy can be a smooth function of the parameters.) That is, there
action to another. This means that the value of the policy may also change discontinuously,
a stochastic policy representation πθ(s, a), which speciﬁes the probability of selecting action
STOCHASTIC POLICY
a in state s. One popular representation is the softmax function:
SOFTMAX FUNCTION
πθ(s, a) = e


a′
e
Softmax becomes nearly deterministic if one action is much better than the others, but itPolicy Search
Now let us look at methods for improving the policy. We start with the simplest case: a
deterministic policy and a deterministic environment. Let ρ(θ) be the policy value, i.e., the
POLICY VALUE
the policy gradient vector ∇θρ(θ) provided ρ(θ) is differentiable. Alternatively, if ρ(θ) is
POLICY GRADIENT
not available in closed form, we can evaluate πθ simply by executing it and observing the
accumulated reward. We can follow the empirical gradient by hill climbing—i.e., evaluating
the change in policy value for small increments in each parameter. With the usual caveats,
this process will converge to a local optimum in policy space.
When the environment (or the policy) is stochastic, things get more difﬁcult. Suppose
we are trying to do hill climbing, which requires comparing ρ(θ) and ρ(θ + Δθ) for some
small Δθ. The problem is that the total reward on each trial may vary widely, so estimates
of the policy value from a small number of trials will be quite unreliable; trying to compare
two such estimates will be even more unreliable. One solution is simply to run lots of trials,
measuring the sample variance and using it to determine that enough trials have been run
to get a reliable indication of the direction of improvement for ρ(θ). Unfortunately, this is
perhaps even dangerous.
For the case of a stochastic policy πθ(s, a), it is possible to obtain an unbiased estimate
we will derive this estimate for the simple case of a nonsequential environment in which the
reward R(a) is obtained immediately after doing action a in the start state s0. In this case,
the policy value is just the expected value of the reward, and we have
∇θρ(θ) = ∇θ


a
πθ(s0, a)R(a) =


a
(∇θπθ(s0, a))R(a) .
Now we perform a simple trick so that this summation can be approximated by samples
generated from the probability distribution deﬁned by πθ(s0, a). Suppose that we have N
trials in all and the action taken on the jth trial is aj. Then
∇θρ(θ) =


a
πθ(s0, a) · (∇θπθ(s0, a))R(a)
πθ(s0, a)
N
N


(∇θπθ(s0, aj))R(aj)
πθ(s0, aj)
.
Thus, the true gradient of the policy value is approximated by a sum of terms involving
generalizes to
N
N


(∇θπθ(s, aj))Rj(s)
πθ(s, aj)
for each state s visited, where aj is executed in s on the jth trial and Rj(s) is the total
reward received from state s onwards in the jth trial.
The resulting algorithm is calledChapter
Reinforcement Learning
Consider the following task: given two blackjack5 programs, determine which is best.
One way to do this is to have each play against a standard “dealer” for a certain number of
hands and then to measure their respective winnings. The problem with this, as we have seen,
is that the winnings of each program ﬂuctuate widely depending on whether it receives good
or bad cards. An obvious solution is to generate a certain number of hands in advance and
have each program play the same set of hands. In this way, we eliminate the measurement
CORRELATED
SAMPLING
applicable to domains for which a simulator is available so that the “random” outcomes of
dom numbers, each of which can be used to run a trial of any policy. Policy search is carried
out by evaluating each candidate policy using the same set of random sequences to determine
the action outcomes. It can be shown that the number of random sequences required to ensure
that the value of every policy is well estimated depends only on the complexity of the policy
space, and not at all on the complexity of the underlying domain.
APPLICATIONS OF REINFORCEMENT LEARNING
applications in game playing, where the transition model is known and the goal is to learn the
utility function, and in robotics, where the model is usually unknown.
Applications to game playing
were some signiﬁcant differences, however, between his program and current methods. First,
ing the state space at a different granularity. A second difference was that the program did
ignored. This means that it is theoretically possible for Samuel’s program not to converge, or
to converge on a strategy designed to lose rather than to win. He managed to avoid this fate
by insisting that the weight for material advantage should always be positive. Remarkably,
this was sufﬁcient to direct the program into areas of weight space corresponding to good
checkers play.
potential of reinforcement learning techniques.
In earlier work (Tesauro and Sejnowski,Applications of Reinforcement Learning
x
θ
Setup for the problem of balancing a long pole on top of a moving cart. The
cart can be jerked left or right by a controller that observes x, θ, ˙x, and ˙θ.
amples of moves labeled with relative values by a human expert. This approach proved
extremely tedious for the expert. It resulted in a program, called NEUROGAMMON, that was
the end of each game. The evaluation function was represented by a fully connected neural
even though the input representation contained just the raw board position with no computed
that may seem like a lot of games, it is only a vanishingly small fraction of the state space.
of the top three human players worldwide. Kit Woolsey, a top player and analyst, said that
“There is no question in my mind that its positional judgment is far better than mine.”
Application to robot control
CART–POLE
INVERTED
PENDULUM
as shown. Several thousand papers in reinforcement learning and control theory have been
lems described earlier in that the state variables x, θ, ˙x, and ˙θ are continuous. The actions are
CONTROLChapter
Reinforcement Learning
space into boxes—hence the name. It then ran trials until the pole fell over or the cart hit the
end of the track. Negative reinforcement was associated with the ﬁnal action in the ﬁnal box
and then propagated back through the sequence. It was found that the discretization caused
some problems when the apparatus was initialized in a position different from those used in
training, suggesting that generalization was not perfect. Improved generalization and faster
approximator such as a neural network. Nowadays, balancing a triple inverted pendulum is a
common exercise—a feat far beyond the capabilities of most humans.
Still more impressive is the application of reinforcement learning to helicopter ﬂight
as well as the PEGASUS algorithm with simulation based on a learned transition model (Ng
rithm was run on the simulator model overnight. A variety of controllers were developed for
different maneuvers. In all cases, performance far exceeded that of an expert human pilot
using remote control. (Image courtesy of Andrew Ng.)Summary
SUMMARY
This chapter has examined the reinforcement learning problem: how an agent can become
forcement learning can be viewed as a microcosm for the entire AI problem, but it is studied
in a number of simpliﬁed settings to facilitate progress. The major points are:
• The overall agent design dictates the kind of information that must be learned. The
reﬂex design, using a policy π.
• Utilities can be learned using three approaches:
direct evidence for learning its utility.
from observations and then uses value or policy iteration to obtain the utilities or
an optimal policy. ADP makes optimal use of the local constraints on utilities of
states imposed through the neighborhood structure of the environment.
cessor states. They can be viewed as simple approximations to the ADP approach
erate pseudoexperiences can, however, result in faster learning.
selection phase. This simpliﬁes the learning problem but potentially restricts the ability
to learn in complex environments, because the agent cannot simulate the results of
possible courses of action.
• When the learning agent is responsible for selecting actions while it learns, it must
trade off the estimated value of those actions against the potential for learning useful
new information. An exact solution of the exploration problem is infeasible, but some
simple heuristics do a reasonable job.
can be used directly to update parameters in representations such as neural networks.
to improve it based on observed performance. The variation in the performance in a
stochastic domain is a serious problem; for simulated domains this can be overcome by
ﬁxing the randomness in advance.
ing continues to be one of the most active areas of machine learning research. ApplicationsChapter
Reinforcement Learning
may consist of thousands or even millions of primitive actions.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
vinced of its effectiveness, writing, “the use of punishments and rewards can at best be a part
machine learning research. Although this work was informal and had a number of ﬂaws,
encing and function approximation. Around the same time, researchers in adaptive control
works using the delta rule. (This early connection between neural networks and reinforcement
learning method with a function approximator. The psychological literature on reinforcement
the operation of reinforcement learning in animals has been provided by investigations into
the foraging behavior of bees; there is a clear neural correlate of the reward signal in the form
in primate brains implements something resembling value function learning (Schultz et al.,
evidence from neuroscientiﬁc and behavioral experiments.
The connection between reinforcement learning and Markov decision processes was
values of all states in a sequence leading up to each transition by an amount that drops off as
rithms make inefﬁcient use of experiences; essentially, they are online regression algorithms
differencing) algorithm is an online algorithm for passive reinforcement learning that givesBibliographical and Historical Notes
Bandit problems, which model the problem of exploration for nonsequential decisions,
ety of exploration methods for sequential decision problems are discussed by Barto et al.
another angle on both model uncertainty and exploration.
Function approximation in reinforcement learning goes back to the work of Samuel,
CMAC
periences, especially those in parts of the state space that are avoided once competence is
The convergence of reinforcement learning algorithms using function approximation is
for policy search. The method of correlated sampling for comparing different conﬁgurations
formal properties.
ing based on the current percept. (One reason is that the stochastic policy is less likely toChapter
Reinforcement Learning
optimal policies in partially observable MDPs are deterministic functions of the belief state
rather than the current percept, so we would expect still better results by keeping track of the
reinforcement learning with belief states.
of primitive actions required to achieve signiﬁcant reward. For example, a robot playing
soccer might make a hundred thousand individual leg motions before scoring a goal. One
common method, used originally in animal training, is called reward shaping. This involves
REWARD SHAPING
supplying the agent with additional rewards, called pseudorewards, for “making progress.”
PSEUDOREWARD
For example, in soccer the real reward is for scoring a goal, but pseudorewards might be
given for making contact with the ball or for kicking it toward the goal. Such rewards can
speed up learning enormously and are simple to provide, but there is a risk that the agent
will learn to maximize the pseudorewards rather than the true rewards; for example, standing
that the agent will still learn the optimal policy provided that the pseudoreward F(s, a, s′)
satisﬁes F(s, a, s′) = γΦ(s′) −Φ(s), where Φ is an arbitrary function of the state. Φ can be
constructed to reﬂect any desirable aspects of the state, such as achievement of subgoals or
distance to a goal state.
ment learning methods, which attempt to solve problems at multiple levels of abstraction—
HIERARCHICAL
REINFORCEMENT
LEARNING
broken down into “obtain possession,” “dribble towards the goal,” and “shoot;” and each of
of arbitrary complexity can be treated just like primitive actions (albeit ones that can take
with a partial program that constrains the agent’s behavior to have a particular hierarchical
PARTIAL PROGRAM
gramming language by adding primitives for unspeciﬁed choices that must be ﬁlled in by
learning. Reinforcement learning is then applied to learn the best behavior consistent with
the partial program. The combination of function approximation, shaping, and hierarchical
natural additive decomposition of the overall utility function into terms that depend on small
The topic of distributed and multiagent reinforcement learning was not touched upon in
the chapter but is of great current interest. In distributed RL, the aim is to devise methods by
which multiple, coordinated agents learn to optimize a common utility function. For example,Bibliographical and Historical Notes
can we devise methods whereby separate subagents for robot navigation and robot obstacle
SUBAGENT
avoidance could cooperatively achieve a combined control system that is globally optimal?
whole process converges to globally optimal solutions.
Multiagent RL is distinguished from distributed RL by the presence of agents who
cannot coordinate their actions (except by explicit communicative acts) and who may not
ponent is changing its policy to defeat the agent. Thus, the environment is nonstationary
Sometimes the reward function is not easy to deﬁne. Consider the task of driving a car.
There are extreme states (such as crashing the car) that clearly should have a large penalty.
But beyond that, it is difﬁcult to be precise about the reward function. However, it is easy
enough for a human to drive for a while and then tell a robot “do it like that.” The robot then
has the task of apprenticeship learning; learning from an example of the task done right,
APPRENTICESHIP
LEARNING
reinforcement learning—ﬁguring out what the reward function must be from an example
INVERSE
REINFORCEMENT
LEARNING
path through that state space. This is useful as a part of apprenticeship learning, or as a part
of doing science—we can understand an animal or robot by working backwards from what it
does to what its reward function must be.
This chapter has dealt only with atomic states—all the agent knows about a state is the
it is also possible to apply reinforcement learning to structured representations rather than
RELATIONAL
REINFORCEMENT
LEARNING
rithms, showing how reinforcement learning weaves together the ideas of learning, planning,
inforcement learning papers are published frequently in Machine Learning, in the Journal of
Machine Learning Research, and in the International Conferences on Machine Learning and
the Neural Information Processing Systems meetings.Chapter
Reinforcement Learning
EXERCISES
For the case of an initially unknown environment model, compare the learning performance
of the direct utility estimation, TD, and ADP algorithms. Do the comparison for the optimal
policy and for several random policies. For which do the utility estimates converge faster?
What happens when the size of the environment is increased? (Try environments with and
without obstacles.)
terminal state. Show that it is possible for a passive ADP agent to learn a transition model
for which its policy π is improper even if π is proper for the true MDP; with such models,
Starting with the passive ADP agent, modify it to use an approximate ADP algorithm
as discussed in the text. Do this in two steps:
a. Implement a priority queue for adjustments to the utility estimates. Whenever a state is
adjusted, all of its predecessors also become candidates for adjustment and should be
added to the queue. The queue is initialized with the state from which the most recent
transition took place. Allow only a ﬁxed number of adjustments.
fect on learning rates and computation time.
Write out the parameter update equations for TD learning with
ˆU(x, y) = θ0 + θ1x + θ2y + θ3
	
signal. Put two reinforcement learning agents into the environment (they may, of course,
share the agent program) and have them play against each other. Apply the generalized TDNATURAL LANGUAGE
PROCESSING
In which we see how to make use of the copious knowledge that is expressed in
natural language.
Homo sapiens is set apart from other species by the capacity for language. Somewhere around
Although chimpanzees, dolphins, and other animals have shown vocabularies of hundreds of
signs, only humans can reliably communicate an unbounded number of qualitatively different
messages on any topic using discrete signs.
Of course, there are other attributes that are uniquely human: no other species wears
clothes, creates representational art, or watches three hours of television a day. But when
There are two main reasons why we want our computer agents to be able to process natural
to acquire information from written language, the focus of this chapter.
There are over a trillion pages of information on the Web, almost all of it in natural
language. An agent that wants to do knowledge acquisition needs to understand (at least
KNOWLEDGE
ACQUISITION
partially) the ambiguous, messy languages that humans use. We examine the problem from
trieval, and information extraction. One common factor in addressing these tasks is the use of
language models: models that predict the probability distribution of language expressions.
LANGUAGE MODEL
LANGUAGE MODELS
Formal languages, such as the programming languages Java or Python, have precisely deﬁned
LANGUAGE
inﬁnite number of legal programs, they cannot be enumerated; instead they are speciﬁed by a
set of rules called a grammar. Formal languages also have rules that deﬁne the meaning or
GRAMMAR
SEMANTICSLanguage Models
Natural languages, such as English or Spanish, cannot be characterized as a deﬁnitive
set of sentences. Everyone agrees that “Not to be invited is sad” is a sentence of English,
but people disagree on the grammaticality of “To be not invited is sad.” Therefore, it is more
fruitful to deﬁne a natural language model as a probability distribution over sentences rather
than a deﬁnitive set. That is, rather than asking if a string of words is or is not a member of
the set deﬁning the language, we instead ask for P(S = words)—what is the probability that
a random sentence would be words.
Natural languages are also ambiguous. “He saw her duck” can mean either that he saw
AMBIGUITY
a waterfowl belonging to her, or that he saw her move to evade something. Thus, again, we
cannot speak of a single meaning for a sentence, but rather of a probability distribution over
possible meanings.
Finally, natural languages are difﬁcult to deal with because they are very large, and
constantly changing. Thus, our language models are, at best, an approximation. We start
with the simplest possible approximations and move up from there.
Ultimately, a written text is composed of characters—letters, digits, punctuation, and spaces
CHARACTERS
in English (and more exotic characters in some other languages). Thus, one of the simplest
we write P(c1:N) for the probability of a sequence of N characters, c1 through cN. In one
N -GRAM MODEL
over characters.)
ceding characters, not on any other characters. So in a trigram model (Markov chain of
We can deﬁne the probability of a sequence of characters P(c1:N) under the trigram model
by ﬁrst factoring with the chain rule and then using the Markov assumption:
P(c1:N) =
N

N

entries, and can be accurately estimated by counting character sequences in a body of text of
CORPUS
Latin word for body.Chapter
Natural Language Processing
is language identiﬁcation: given a text, determine what natural language it is written in. This
LANGUAGE
IDENTIFICATION
is a relatively easy task; even with short texts such as “Hello, world” or “Wie geht es dir,” it
is easy to identify the ﬁrst as English and the second as German. Computer systems identify
Swedish and Norwegian, are confused.
One approach to language identiﬁcation is to ﬁrst build a trigram character model of
characters of each language are needed.) That gives us a model of P(Text | Language), but
we want to select the most probable language given the text, so we apply Bayes’ rule followed
by the Markov assumption to get the most probable language:
ℓ∗= argmax
ℓ
P(ℓ| c1:N)
= argmax
ℓ
P(ℓ)P(c1:N | ℓ)
= argmax
ℓ
P(ℓ)
N

The trigram model can be learned from a corpus, but what about the prior probability P(ℓ)?
We may have some estimate of these values; for example, if we are selecting a random Web
page we know that English is the most likely language and that the probability of Macedonian
trigram model usually selects one language that is several orders of magnitude more probable
than any other.
Other tasks for character models include spelling correction, genre classiﬁcation, and
legal document, a scientiﬁc article, etc. While many features help make this classiﬁcation,
deciding what class they belong to. For example, in the text “Mr. Sopersteen was prescribed
aciphex,” we should recognize that “Mr. Sopersteen” is the name of a person and “aciphex” is
the character sequence “ex ” (“ex” followed by a space) with a drug name and “steen ” with
a person name, and thereby identify words that they have never seen before.
mate of the true probability distribution. For common character sequences such as “ th” any
is very uncommon—no dictionary words start with ht. It is likely that the sequence wouldLanguage Models
an English probability of zero, which seems wrong. We have a problem in generalization: we
want our language models to generalize well to texts they haven’t seen yet. Just because we
ble. Thus, we will adjust our language model so that sequences that have a count of zero in
the training corpus will be assigned a small nonzero probability (and the other counts will be
SMOOTHING
tury: he said that, in the lack of further information, if a random Boolean variable X has been
is, he assumes that with two more trials, one might be true and one false. Laplace smoothing
BACKOFF MODEL
interpolation smoothing is a backoff model that combines trigram, bigram, and unigram
LINEAR
INTERPOLATION
SMOOTHING
models by linear interpolation. It deﬁnes the probability estimate as
on the counts: if we have a high count of trigrams, then we weigh them relatively more; if
only a low count, then we put more weight on the bigram and unigram models. One camp of
researchers has developed ever more sophisticated smoothing models, while the other camp
suggests gathering a larger corpus so that even simple smoothing models work well. Both are
getting at the same goal: reducing the variance in the language model.
example, deﬁning c0 to be a space character or a special “begin text” character. Or we can
Model evaluation
Determine the parameters of the model from the training data. Then evaluate the model on
the validation corpus.
bility the better. This metric is inconvenient because the probability of a large corpus will
describing the probability of a sequence is with a measure called perplexity, deﬁned as
PERPLEXITYChapter
Natural Language Processing
N .
Perplexity can be thought of as the reciprocal of probability, normalized by sequence length.
It can also be thought of as the weighted average branching factor of a model. Suppose there
applies equally to word and character models. The main difference is that the vocabulary—
VOCABULARY
the set of symbols that make up the corpus and the model—is larger. There are only about
more restrictive, for example by treating “A” and “a” as the same symbol or by treating all
punctuation as the same symbol. But with word models we have at least tens of thousands of
symbols, and sometimes millions. The wide range is because it is not clear what constitutes a
word. In English a sequence of letters surrounded by spaces is a word, but in some languages,
like Chinese, words are not separated by spaces, and even in English many decisions must be
OUT OF
VOCABULARY
with word models there is always the chance of a new word that was not seen in the training
corpus, so we need to model that explicitly in our language model. This can be done by
the ﬁrst time any individual word appears it is previously unknown, so replace it with the
To get a feeling for what word models can do, we built unigram, bigram, and trigram
models over the words in this book and then randomly sampled sequences of words from the
models. The results are
Unigram: logical are as are confusion a may right tries agent goal the was . . .
Bigram: systems are very similar computational approach would be represented . . .
Trigram: planning and scheduling are integrated the success of naive bayes model is . . .
Even with this small sample, it should be clear that the unigram model is a poor approximation
of either English or the content of an AI textbook, and that the bigram and trigram models areText Classiﬁcation
can turn now to some language tasks.
TEXT CLASSIFICATION
We now consider in depth the task of text classiﬁcation, also known as categorization: given
TEXT
CLASSIFICATION
tiﬁcation and genre classiﬁcation are examples of text classiﬁcation, as is sentiment analysis
SPAM DETECTION
learning. A training set is readily available: the positive (spam) examples are in my spam
folder, the negative (ham) examples are in my inbox. Here is an excerpt:
Spam: WE CAN TREAT ANYTHING YOU SUFFER FROM JUST TRUST US ...
Ham: The practical signiﬁcance of hypertree width in identifying more ...
Ham: Abstract: We will motivate the problem of social identity clustering: ...
Ham: Good to see you my friend. Hey Peter, It was good to hear from you. ...
Ham: PDS implies convexity of the resulting optimization problem (Kernel Ridge ...
From this excerpt we can start to get an idea of what might be good features to include in
to be indicators of spam (although they would have a nonzero probability in ham as well).
have punctuation embedded in words. Apparently the spammers thought that the word bigram
of spam and ham, or we could handcraft features such as “number of punctuation marks
embedded in words.”
Note that we have two complementary ways of talking about classiﬁcation.
In the
by training on the spam folder, and one model for P(Message | ham) by training on the inbox.
Then we can classify a new message with an application of Bayes’ rule:
argmax
P(c | message) =
argmax
P(message | c) P(c) .
where P(c) is estimated just by counting the total number of spam and ham messages. This
approach works well for spam detection, just as it did for language identiﬁcation.Chapter
Natural Language Processing
pairs and apply a classiﬁcation algorithm h to the feature vector X.
We can make the
as features. This is easiest to see with a unigram model. The features are the words in the
vocabulary: “a,” “aardvark,” . . ., and the values are the number of times each word appears
almost all the features will have count zero. This unigram representation has been called the
bag of words model. You can think of the model as putting the words of the training corpus
BAG OF WORDS
in a bag and then selecting words one at a time. The notion of order of the words is lost; a
models maintain some local notion of word order.
With bigrams and trigrams the number of features is squared or cubed, and we can add
is part of the message, an ID number for the sender of the message, the sender’s number of
previous spam and ham messages, and so on. The choice of features is the most important part
of creating a good spam detector—more important than the choice of algorithm for processing
the features. In part this is because there is a lot of training data, so if we can propose a
feature, the data can accurately determine if it is good or not. It is necessary to constantly
update features, because spam detection is an adversarial task; the spammers modify their
spam in response to the spam detector’s changes.
It can be expensive to run algorithms on a very large feature vector, so often a process
of feature selection is used to keep only the features that best discriminate between spam and
FEATURE SELECTION
ham. For example, the bigram “of the” is frequent in English, and may be equally frequent in
spam and ham, so there is no sense in counting it. Often the top hundred or so features do a
good job of discriminating between classes.
Once we have chosen a set of features, we can apply any of the supervised learning
support vector machines, decision trees, naive Bayes, and logistic regression. All of these
Classiﬁcation by data compression
Another way to think about classiﬁcation is as a problem in data compression. A lossless
DATA COMPRESSION
compression algorithm takes a sequence of symbols, detects repeated patterns in it, and writes
a description of the sequence that is more compact than the original. For example, the text
work by building dictionaries of subsequences of the text, and then referring to entries in the
In effect, compression algorithms are creating a language model. The LZW algorithm
by compression, we ﬁrst lump together all the spam training messages and compress them asInformation Retrieval
a unit. We do the same for the ham. Then when given a new message to classify, we append
it to the spam messages and compress the result. We also append it to the ham and compress
that. Whichever class compresses better—adds the fewer number of additional bytes for the
new message—is the predicted class. The idea is that a spam message will tend to share
dictionary entries with other spam messages and thus will compress better when appended to
a collection that already contains the spam dictionary.
rithms. This is interesting in its own right, and also serves to point out that there is promise
selection: they seem to be captiring some real patterns.
INFORMATION RETRIEVAL
Information retrieval is the task of ﬁnding documents that are relevant to a user’s need for
INFORMATION
RETRIEVAL
and see a list of relevant pages. In this section, we will see how such systems are built. An
information retrieval (henceforth IR) system can be characterized by
IR
a paragraph, a page, or a multipage text.
QUERY LANGUAGE
The query language can be just a list of words, such as [AI book]; or it can specify
a phrase of words that must be adjacent, as in [“AI book”]; it can contain Boolean
book] or [AI book site:www.aaai.org].
RESULT SET
RELEVANT
the query. By relevant, we mean likely to be of use to the person who posed the query,
for the particular information need expressed in the query.
PRESENTATION
The earliest IR systems worked on a Boolean keyword model. Each word in the document
BOOLEAN KEYWORD
MODEL
collection is treated as a Boolean feature that is true of a document if the word occurs in the
document and false if it does not. So the feature “retrieval” is true for the current chapter
distinguish the query [“two words”] from [two words].Chapter
Natural Language Processing
features. A document is relevant only if the expression evaluates to true. For example, the
This model has the advantage of being simple to explain and implement. However,
it has some disadvantages. First, the degree of relevance of a document is a single bit, so
there is no guidance as to how to order the relevant documents for presentation. Second,
Boolean expressions are unfamiliar to users who are not programmers or logicians. Users
ﬁnd it unintuitive that when they want to know about farming in the states of Kansas and
Nebraska they need to issue the query [farming (Kansas OR Nebraska)]. Third, it can be
hard to formulate an appropriate query, even for a skilled user. Suppose we try [information
AND retrieval AND models AND optimization] and get an empty result set. We could try
[information OR retrieval OR models OR optimization], but if that returns too many results,
it is difﬁcult to know what to try next.
IR scoring functions
Most IR systems have abandoned the Boolean model and use models based on the statistics of
word counts. We describe the BM25 scoring function, which comes from the Okapi project
BM25 SCORING
FUNCTION
of Stephen Robertson and Karen Sparck Jones at London’s City College, and has been used
A scoring function takes a document and a query and returns a numeric score; the most
relevant documents have the highest scores. In the BM25 function, the score is a linear
weighted combination of scores for each of the words that make up the query. Three factors
affect the weight of a query term: First, the frequency with which a query term appears in
a document (also known as TF for term frequency). For the query [farming in Kansas],
documents that mention “farming” frequently will have higher scores. Second, the inverse
document frequency of the term, or IDF. The word “in” appears in almost every document,
so it has a high document frequency, and thus a low inverse document frequency, and thus it
is not as important to the query as “farming” or “Kansas.” Third, the length of the document.
about the query. A short document that mentions all the words is a much better candidate.
The BM25 function takes all three of these into account. We assume we have created
an index of the N documents in the corpus so that we can look up TF(qi, dj), the count of
the number of times word qi appears in document dj. We also assume a table of document
frequency counts, DF(qi), that gives the number of documents that contain the word qi.
Then, given a document dj and a query consisting of the words q1:N, we have
BM25(dj, q1:N) =
N


IDF(qi) ·
L )
,
where |dj| is the length of document dj in words, and L is the average document length
in the corpus: L = 
i |di|/N. We have two parameters, k and b, that can be tuned byInformation Retrieval
frequency of word qi, given by
.
Of course, it would be impractical to apply the BM25 scoring function to every document
in the corpus. Instead, systems create an index ahead of time that lists, for each vocabulary
INDEX
word, the documents that contain the word. This is called the hit list for the word. Then when
HIT LIST
given a query, we intersect the hit lists of the query words and only score the documents in
the intersection.
IR system evaluation
How do we know whether an IR system is performing well? We undertake an experiment in
which the system is given a set of queries and the result sets are scored with respect to human
relevance judgments. Traditionally, there have been two measures used in the scoring: recall
and precision. We explain them with the help of an example. Imagine that an IR system has
returned a result set for a single query, for which we know which documents are and are not
in the following table:
In result set Not in result set
Relevant
Not relevant
Precision measures the proportion of documents in the result set that are actually relevant.
PRECISION
Recall measures the proportion of all the relevant documents in the collection that are in
RECALL
to compute, because there is no easy way to examine every page on the Web for relevance.
All we can do is either estimate recall by sampling or ignore recall completely and just judge
precision. In the case of a Web search engine, there may be thousands of documents in the
result set, so it makes more sense to measure precision for several different sizes, such as
entire result set.
It is possible to trade off precision against recall by varying the size of the result set
returned. In the extreme, a system that returns every document in the document collection is
of both measures is the F1 score, a single number that is the harmonic mean of precision and
IR reﬁnements
There are many possible reﬁnements to the system described here, and indeed Web search
engines are continually updating their algorithms as they discover new approaches and as the
Web grows and changes.Chapter
Natural Language Processing
One common reﬁnement is a better model of the effect of document length on relevance.
favor short documents too much and long documents not enough. They propose a pivoted
document length normalization scheme; the idea is that the pivot is the document length at
longer ones get a penalty.
dependent, but we know that some words are correlated: “couch” is closely related to both
“couches” and “sofa.” Many IR systems attempt to account for these correlations.
For example, if the query is [couch], it would be a shame to exclude from the result set
those documents that mention “COUCH” or “couches” but not “couch.” Most IR systems
do case folding of “COUCH” to “couch,” and some use a stemming algorithm to reduce
CASE FOLDING
STEMMING
“couches” to the stem form “couch,” both in the query and the documents. This typically
precision. For example, stemming “stocking” to “stock” will tend to decrease precision for
queries about either foot coverings or ﬁnancial instruments, although it could improve recall
for queries about warehousing. Stemming algorithms based on rules (e.g., remove “-ing”)
cannot avoid this problem, but algorithms based on dictionaries (don’t remove “-ing” if the
word is already listed in the dictionary) can. While stemming has a small effect in English,
it is more important in other languages. In German, for example, it is not uncommon to
ployee). Languages such as Finnish, Turkish, Inuit, and Yupik have recursive morphological
rules that in principle generate words of unbounded length.
The next step is to recognize synonyms, such as “sofa” for “couch.” As with stemming,
SYNONYM
this has the potential for small gains in recall, but can hurt precision. A user who gives the
query [Tim Couch] wants to see results about the football player, not sofas. The problem is
is, anytime there are two words that mean the same thing, speakers of the language conspire
to evolve the meanings to remove the confusion. Related words that are not synonyms also
play an important role in ranking—terms like “leather”, “wooden,” or “modern” can serve
to conﬁrm that the document really is about “couch.” Synonyms and related words can be
found in dictionaries or by looking for correlations in documents or in queries—if we ﬁnd
that many users who ask the query [new sofa] follow it up with the query [new couch], we
can in the future alter [new sofa] to be [new sofa OR new couch].
As a ﬁnal reﬁnement, IR can be improved by considering metadata—data outside of
METADATA
On the Web, hypertext links between documents are a crucial source of information.
LINKS
The PageRank algorithm
PageRank3 was one of the two original ideas that set Google’s search apart from other Web
PAGERANKInformation Retrieval
function HITS(query) returns pages with hub and authority numbers
for each p in pages do
repeat until convergence do
for each p in pages do
p.AUTHORITY ←
i INLINKi(p).HUB
p.HUB ←
i OUTLINKi(p).AUTHORITY
NORMALIZE(pages)
return pages
The HITS algorithm for computing hubs and authorities with respect to a
in every page that links to or is linked from one of the relevant pages. NORMALIZE divides
each page’s score by the sum of the squares of all pages’ scores (separately for both the
authority and hubs scores).
text—the underlined text in a hyperlink—to index a page, even though the anchor text was on
a different page than the one being indexed.) PageRank was invented to solve the problem of
the tyranny of TF scores: if the query is [IBM], how do we make sure that IBM’s home page,
ibm.com, is the ﬁrst result, even if another page mentions the term “IBM” more frequently?
then it would be possible for a Web spammer to create a network of pages and have them all
point to a page of his choosing, increasing the score of that page. Therefore, the PageRank
we will see that the recursion bottoms out properly. The PageRank for a page p is deﬁned as:
N
+ d


i
PR(ini)
C(ini) ,
where PR(p) is the PageRank of page p, N is the total number of pages in the corpus, ini
page ini. The constant d is a damping factor. It can be understood through the random
surfer model: imagine a Web surfer who starts at some random page and begins exploring.
RANDOM SURFER
MODEL
and restarts on a random page anywhere on the Web. The PageRank of page p is then the
probability that the random surfer will be at page p at any point in time. PageRank can be
algorithm, updating ranks until they converge.Chapter
Natural Language Processing
The HITS algorithm
to a query. That means that it must be computed anew for each query—a computational
burden that most search engines have elected not to take on. Given a query, HITS ﬁrst ﬁnds
a set of pages that are relevant to the query. It does that by intersecting hit lists of query
words, and then adding pages in the link neighborhood of these pages—pages that link to or
are linked from one of the pages in the original relevant set.
Each page in this set is considered an authority on the query to the degree that other
AUTHORITY
pages in the relevant set point to it. A page is considered a hub to the degree that it points
HUB
to other authoritative pages in the relevant set. Just as with PageRank, we don’t want to
authorities. Thus, as with PageRank, we iterate a process that updates the authority score of
a page to be the sum of the hub scores of the pages that point to it, and the hub score to be
the sum of the authority scores of the pages it points to. If we then normalize the scores and
repeat k times, the process will converge.
Both PageRank and HITS played important roles in developing our understanding of
Web information retrieval. These algorithms and their extensions are used in ranking billions
of queries daily as search engines steadily develop better ways of extracting yet ﬁner signals
of search relevance.
Question answering
Information retrieval is the task of ﬁnding documents that are relevant to a query, where the
query may be a question, or just a topic area or concept. Question answering is a somewhat
QUESTION
ANSWERING
different task, in which the query really is a question, and the answer is not a ranked list
of documents but rather a short response—a sentence, or even just a phrase. There have
breadth of coverage.
system. It is based on the intuition that most questions will be answered many times on the
Web, so question answering should be thought of as a problem in precision, not recall. We
don’t have to deal with all the different ways that an answer might be phrased—we only
have to ﬁnd one of them. For example, consider the query [Who killed Abraham Lincoln?]
Suppose a system had to answer that question with access only to a single encyclopedia,
whose entry on Lincoln said
John Wilkes Booth altered history with a bullet. He will forever be known as the man
who ended Abraham Lincoln’s life.
To use this passage to answer the question, the system would have to know that ending a life
can be a killing, that “He” refers to Booth, and several other linguistic and semantic facts.Information Extraction
ASKMSR does not attempt this kind of sophistication—it knows nothing about pronoun
how they can be rewritten as queries to a search engine. It knows that [Who killed Abraham
Lincoln] can be rewritten as the query [* killed Abraham Lincoln] and as [Abraham Lincoln
was killed by *]. It issues these rewritten queries and examines the results that come back—
not the full Web pages, just the short summaries of text that appear near the query terms.
phrase match query [“Abraham Lincoln was killed by *”]) would get more weight than one
from a general query rewrite, such as [Abraham OR Lincoln OR killed]. We would expect
“Abraham Lincoln” and “the assassination of” and “Ford’s Theatre.”
starts with “who,” then we ﬁlter on names of people; for “how many” we ﬁlter on numbers, for
“when,” on a date or time. There is also a ﬁlter that says the answer should not be part of the
question; together these should allow us to return “John Wilkes Booth” (and not “Abraham
shorter pieces. For example, in a system that used only bigrams, the answer “John Wilkes
At the Text Retrieval Evaluation Conference (TREC), ASKMSR was rated as one of
the top systems, beating out competitors with the ability to do far more complex language
understanding. ASKMSR relies upon the breadth of the content on the Web rather than on
its own depth of understanding. It won’t be able to handle complex inference patterns like
associating “who killed” with “ended the life of.” But it knows that the Web is so vast that it
can afford to ignore passages like that and wait for a simple passage it can handle.
INFORMATION EXTRACTION
INFORMATION
EXTRACTION
ing for occurrences of a particular class of object and for relationships among objects. A
typical task is to extract instances of addresses from Web pages, with database ﬁelds for
street, city, state, and zip code; or instances of storms from weather reports, with ﬁelds for
temperature, wind speed, and precipitation. In a limited domain, this can be done with high
language models of the phrase structure (noun phrases and verb phrases) of English. But so
traction, we deﬁne limited models that approximate the full English model, and concentrateChapter
Natural Language Processing
In this section we describe six different approaches to information extraction, in order
EXTRACTION
that assumes that the entire text refers to a single object and the task is to extract attributes of
plate (also known as a pattern) for each attribute we would like to extract. The template is
TEMPLATE
deﬁned by a ﬁnite state automaton, the simplest example of which is the regular expression,
REGULAR
EXPRESSION
or regex. Regular expressions are used in Unix commands such as grep, in programming
languages such as Perl, and in word processors such as Microsoft Word. The details vary
slightly from one tool to another and so are best learned from the appropriate manual, but
here we show how to build up a regular expression template for prices in dollars:
matches one or more digits
matches a period followed by two digits
matches a period followed by two digits, or nothing
Templates are often deﬁned with three parts: a preﬁx regex, a target regex, and a postﬁx regex.
For prices, the target regex is as above, the preﬁx would look for strings such as “price:” and
the postﬁx could be empty. The idea is that some clues about an attribute come from the
attribute value itself and some come from the surrounding text.
If a regular expression for an attribute matches the text exactly once, then we can pull
out the portion of the text that is the value of the attribute. If there is no match, all we can do
is give a default value or leave the attribute missing; but if there are several matches, we need
a process to choose among them. One strategy is to have several templates for each attribute,
preﬁx “our price:”; if that is not found, we look for the preﬁx “price:” and if that is not found,
the empty preﬁx. Another strategy is to take all the matches and ﬁnd some way to choose
RELATIONAL
EXTRACTION
which deal with multiple objects and the relations among them. Thus, when these systems
stories about corporate mergers and acquisitions. It can read the storyInformation Extraction
Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local
concern and a Japanese trading house to produce golf clubs to be shipped to Japan.
and extract the relations:
∧Member(e, “Bridgestone Sports Co”) ∧Member(e, “a local concern”)
∧Member(e, “a Japanese trading house”) .
CASCADED
TRANSDUCERS
each automaton receives text as input, transduces the text into a different format, and passes
it along to the next automaton. FASTUS consists of ﬁve stages:
FASTUS’s ﬁrst stage is tokenization, which segments the stream of characters into tokens
arating characters at white space or punctuation does a fairly good job. Some tokenizers also
deal with markup languages such as HTML, SGML, and XML.
The second stage handles complex words, including collocations such as “set up” and
company name might be recognized by the rule
The third stage handles basic groups, meaning noun groups and verb groups. The idea is
to chunk these into units that will be managed by the later stages. We will see how to write
resentable by ﬁnite state automata. The example sentence would emerge from this stage as
the following sequence of tagged groups:
Here NG means noun group, VG is verb group, PR is preposition, and CJ is conjunction.Chapter
Natural Language Processing
The fourth stage combines the basic groups into complex phrases. Again, the aim
unambiguous (or nearly unambiguous) output phrases. One type of combination rule deals
captures one way to describe the formation of a joint venture. This stage is the ﬁrst one in
the cascade where the output is placed into a database template as well as being placed in the
output stream. The ﬁnal stage merges structures that were built up in the previous step. If
the next sentence says “The joint venture will start production in January,” then this step will
notice that there are two references to a joint venture, and that they should be merged into
domain in which it is possible to predetermine what subjects will be discussed, and how they
edge, easing construction of the system. These systems work especially well when they are
on the Web is generated by a program that takes database entries and formats them into Web
tion extraction is less successful at recovering information in highly variable format, such as
text written by humans on a variety of subjects.
Probabilistic models for information extraction
approaches fare poorly. It is too hard to get all the rules and their priorities right; it is better
for sequences with hidden state is the hidden Markov model, or HMM.
traction, we can either build one big HMM for all the attributes or build a separate HMM
for each attribute. We’ll do the second. The observations are the words of the text, and the
hidden states are whether we are in the target, preﬁx, or postﬁx part of the attribute template,
or in the background (not part of a template). For example, here is a brief text and the most
probable (Viterbi) path for that text for two HMMs, one trained to recognize the speaker in a
talk announcement, and one trained to recognize dates. The “-” indicates a background state:
Text:
There will be a seminar by
Dr.
Andrew McCallum on
Friday
Speaker: -
-
-
-
PRE
PRE TARGET TARGET TARGET
POST -
Date:
-
-
-
- -
-
-
-
-
PRE
TARGET
HMMs have two big advantages over FSAs for extraction. First, HMMs are probabilistic, and
thus tolerant to noise. In a regular expression, if a single expected character is missing, theInformation Extraction
that
by
speakers
/
here
will
(
received
has
is
:
with
;
about
how
who
speaker
speak
appointment
seminar
reminder
theater
artist
additionally
dr
professor
robert
michael
mr
w
cavalier
stevens
christel
l
Prefix
Target
Postfix
Hidden Markov model for the speaker of a talk announcement. The two
match a string of any length), the four circles to the left are the preﬁx, and the one on the
HMMs can be trained from data; they don’t require laborious engineering of templates, and
thus they can more easily be kept up to date as text changes over time.
Note that we have assumed a certain level of structure in our HMM templates: they all
consist of one or more target states, and any preﬁx states must precede the targets, postﬁx
states most follow the targets, and other states must be background. This structure makes
it easier to learn HMMs from examples. With a partially speciﬁed structure, the forward–
tween states and the observation model, P(Et | Xt), which says how likely each word is in
each state. For example, the word “Friday” would have high probability in one or more of
the target states of the date HMM, and lower probability elsewhere.
With sufﬁcient training data, the HMM automatically learns a structure of dates that we
talk announcement, as learned from data. The preﬁx covers expressions such as “Speaker:”
and “seminar by,” and the target has one state that covers titles and ﬁrst names and another
state that covers initials and last names.
Once the HMMs have been learned, we can apply them to a text, using the Viterbi
algorithm to ﬁnd the most likely path through the HMM states. One approach is to apply
each attribute HMM separately; in this case you would expect most of the HMMs to spend
most of their time in background states. This is appropriate when the extraction is sparse—
when the number of extracted words is small compared to the length of the text.Chapter
Natural Language Processing
The other approach is to combine all the individual attributes into one big HMM, which
would then ﬁnd a path that wanders through different target attributes, ﬁrst ﬁnding a speaker
target, then a date target, etc. Separate HMMs are better when we expect just one of each
with attributes. With either approach, in the end we have a collection of target attribute
observations, and have to decide what to do with them. If every expected attribute has one
target ﬁller then the decision is easy: we have an instance of the desired relation. If there
systems. HMMs have the advantage of supplying probability numbers that can help make
the choice. If some targets are missing, we need to decide if this is an instance of the desired
relation at all, or if the targets found are false positives. A machine learning algorithm can be
trained to make this choice.
Conditional random ﬁelds for information extraction
abilities that we don’t really need. An HMM is a generative model; it models the full joint
probability of observations and hidden states, and thus can be used to generate samples. That
is, we can use the HMM model not only to parse a text and recover the speaker and date,
but also to generate a random instance of a text containing a speaker and a date. Since we’re
not interested in that task, it is natural to ask whether we might be better off with a model
that doesn’t bother modeling that possibility. All we need in order to understand a text is a
discriminative model, one that models the conditional probability of the hidden attributes
given the observations (the text). Given a text e1:N, the conditional model ﬁnds the hidden
state sequence X1:N that maximizes P(X1:N | e1:N).
sumptions of the Markov model—we can have an xt that is dependent on x1. A framework
for this type of model is the conditional random ﬁeld, or CRF, which models a conditional
CONDITIONAL
RANDOM FIELD
probability distribution of a set of target variables given a set of observed variables. Like
Bayesian networks, CRFs can represent many different structures of dependencies among the
CONDITIONAL
RANDOM FIELD
senting Markov dependencies among variables in a temporal sequence. Thus, HMMs are the
logistic regression, where the predicted target is an entire state sequence rather than a single
binary variable.
Let e1:N be the observations (e.g., words in a document), and x1:N be the sequence of
ﬁeld deﬁnes a conditional probability distribution:
P(x1:N|e1:N) = α e[
PN
function deﬁned as the weighted sum of a collection of k component feature functions:


kInformation Extraction
dure that maximizes the conditional likelihood of the training data. The feature functions are
xi, but also the entire observation (word) sequence e, and the current position in the temporal
sequence, i. This gives us a lot of ﬂexibility in deﬁning features. We can deﬁne a simple
and the current state is SPEAKER:
whenever f1 is true, it increases the probability of the hidden state sequence x1:N . This is
another way of saying “the CRF model should prefer the target state SPEAKER for the word
from data. Now consider a second feature function:
This feature is true if the current state is SPEAKER and the next word is “said.” One would
f1 and f2 can hold at the same time for a sentence like “Andrew said . . . .” In this case, the
two features overlap each other and both boost the belief in x1 = SPEAKER. Because of the
independence assumption, HMMs cannot use overlapping features; CRFs can. Furthermore,
a feature in a CRF can use any part of the sequence e1:N. Features can also be deﬁned over
transitions between states. The features we deﬁned here were binary, but in general, a feature
the types of features we would like to include, the CRF formalism gives us a great deal of
ﬂexibility in deﬁning them. This ﬂexibility can lead to accuracies that are higher than with
less ﬂexible models such as HMMs.
Ontology extraction from large corpora
So far we have thought of information extraction as ﬁnding a speciﬁc set of relations (e.g.,
tion of extraction technology is building a large knowledge base or ontology of facts from
about all types of domains, not just one speciﬁc domain. Second, with a large corpus, this
rather than being extracted from one speciﬁc text.
templates that are very general (not tied to a speciﬁc domain) and have high precision (areChapter
Natural Language Processing
almost always correct when they match) but low recall (do not always match). Here is one of
the most productive templates:
NP such as NP (, NP)* (,)? ((and | or) NP)? .
Here the bold words and commas must appear literally in the text, but the parentheses are
for grouping, the asterisk means repetition of zero or more, and the question mark means
noun phrases; for now just assume that we know some words are nouns and other words (such
as verbs) that we can reliably assume are not part of a simple noun phrase. This template
matches the texts “diseases such as rabies affect your dog” and “supports network protocols
such as DNS,” concluding that rabies is a disease and DNS is a network protocol. Similar
templates can be constructed with the key words “including,” “especially,” and “or other.” Of
course these templates will fail to match many relevant passages, like “Rabies is a disease.”
That is intentional. The “NP is a NP” template does indeed sometimes denote a subcategory
relation, but it often means something else, as in “There is a God” or “She is a little tired.”
miss many statements of a subcategory relationship, but most likely we’ll ﬁnd a paraphrase
of the statement somewhere else in the corpus in a form we can use.
Automated template construction
The subcategory relation is so fundamental that is worthwhile to handcraft a few templates to
help identify instances of it occurring in natural language text. But what about the thousands
of other relations in the world? There aren’t enough AI grad students in the world to create
and debug templates for all of them. Fortunately, it is possible to learn templates from a few
examples, then use the templates to learn more examples, from which more templates can be
set of just ﬁve examples:
(“Isaac Asimov”, “The Robots of Dawn”)
(“David Brin”, “Startide Rising”)
(“James Gleick”, “Chaos—Making a New Science”)
(“Charles Dickens”, “Great Expectations”)
(“William Shakespeare”, “The Comedy of Errors”)
edge of authors or titles. The words in these examples were used in a search over a Web
(Author, Title, Order, Preﬁx, Middle, Postﬁx, URL) ,
where Order is true if the author came ﬁrst and false if the title came ﬁrst, Middle is the
explain the matches. The language of templates was designed to have a close mapping to the
matches themselves, to be amenable to automated learning, and to emphasize high precisionInformation Extraction
(possibly at the risk of lower recall). Each template has the same seven components as a
match. The Author and Title are regexes consisting of any characters (but beginning and
ending in letters) and constrained to have a length from half the minimum length of the
examples to twice the maximum length. The preﬁx, middle, and postﬁx are restricted to
literal strings, not regexes. The middle is the easiest to learn: each distinct middle string in
the set of matches is a distinct candidate template. For each such candidate, the template’s
Preﬁx is then deﬁned as the longest common sufﬁx of all the preﬁxes in the matches, and the
Postﬁx is deﬁned as the longest common preﬁx of all the postﬁxes in the matches. If either of
these is of length zero, then the template is rejected. The URL of the template is deﬁned as
the longest preﬁx of the URLs in the matches.
most productive template was
titles. Given a good set of templates, the system can collect a good set of examples. Given a
good set of examples, the system can build a good set of templates.
The biggest weakness in this approach is the sensitivity to noise. If one of the ﬁrst
few templates is incorrect, errors can propagate quickly. One way to limit this problem is to
not accept a new example unless it is veriﬁed by multiple templates, and not accept a new
template unless it discovers multiple examples that are also found by other templates.
Machine reading
Automated template construction is a big step up from handcrafted template construction, but
it still requires a handful of labeled examples of each relation to get started. To build a large
ontology with many thousands of relations, even that amount of work would be onerous; we
would like to have an extraction system with no human input of any kind—a system that could
would work for any relation. In practice, these systems work on all relations in parallel,
extraction system that is targeted at a few relations and more like a human reader who learns
from the text itself; because of this the ﬁeld has been called machine reading.
MACHINE READING
TEXTRUNNER uses cotraining to boost its performance, but it needs something to bootstrap
ships are expressed in English. The actual bootsrapping starts from a set of labelled examples
that are extracted from the Penn Treebank, a corpus of parsed sentences. For example, fromChapter
Natural Language Processing
to extract the relation (“Einstein,” “received,” “Nobel Prize”).
to extract further examples from unlabeled text. The features in the CRF include function
words like “to” and “of” and “the,” but not nouns and verbs (and not noun phrases or verb
of nouns and verbs.
Type
Template
Example
Frequency
Verb
NP1 Verb NP2
X established Y
Noun–Prep
X settlement with Y
Verb–Prep
NP1 Verb Prep NP2
X moved to Y
Inﬁnitive
NP1 to Verb NP2
X plans to acquire Y
Modiﬁer
NP1 Verb NP2 Noun
X is Y winner
NP1 (, | and | - | :) NP2 NP
X, Y merge
Appositive
NP1 NP (:| ,)? NP2
X hometown : Y
expressed in English.
Web corpus. TEXTRUNNER has extracted hundreds of millions of facts from a corpus of a
It would be better to attribute this to “boiling water” rather than just “water.”
ing to get closer to the goal of machine reading.
SUMMARY
The main points of this chapter are as follows:
mation about a language. They can perform well on such diverse tasks as language
cessing of the data to reduce noise is important.
classiﬁcation algorithms we have previously discussed. Classiﬁcation can also be seen
as a problem in data compression.Bibliographical and Historical Notes
• Information retrieval systems use a very simple language model based on bags of
words, yet still manage to perform well in terms of recall and precision on very large
• Question answering can be handled by an approach based on information retrieval, for
questions that have multiple answers in the corpus. When more answers are available
in the corpus, we can use techniques that emphasize precision rather than recall.
state automata, HMMs, or conditional random ﬁelds, and can be learned from examples.
• In building a statistical language system, it is best to devise a model that can make good
use of available data, even if the model seems overly simplistic.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
of the basic problems of syntactic structure.” This is true, but probabilistic models do provide
sky’s remarks had the unfortunate effect of scaring many people away from statistical models
words of Web text; it is now publicly available. The bag of words model gets its name from
views a document as a mixture of topics, each with its own distribution of words. This model
can be seen as an extension and rationalization of the latent semantic indexing model ofChapter
Natural Language Processing
years of application of naive Bayes techniques to text classiﬁcation and retrieval. Schapire
how to use the EM algorithm to label unlabeled documents, thus learning a better classiﬁ-
language models.
statistics and probabilistic NLP are coming closer together, as each deals with long, structured
sequences chosen from an alphabet of constituents.
The ﬁeld of information retrieval is experiencing a regrowth in interest, sparked by
user interfaces for Web search. The TREC conference, organized by the U.S. government’s
National Institute of Standards and Technology (NIST), hosts an annual competition for IR
systems and publishes proceedings with results. In the ﬁrst seven years of the competition,
performance roughly doubled.
work dominated the early years of the ﬁeld. There are two alternative probabilistic models,
joint probability distribution, but that the choice of model has implications for training the
rates click data—examples of past search queies and the results that were clicked on.
ceedings of the annual SIGIR conference cover recent developments in the ﬁeld.
sage Understand Conferences (MUC), sponsored by the U.S. government. The FASTUSChapter
Natural Language Processing
Write a program to do segmentation of words without spaces. Given a string, such
as the URL “thelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,” return a
list of component words: [“the,” “longest,” “list,” . . .]. This task is useful for parsing URLs,
for spelling correction when words runtogether, and for languages such as Chinese that do
not have spaces between words. It can be solved with a unigram or bigram word model and
a dynamic programming algorithm similar to the Viterbi algorithm.
ﬁer for authorship: given a text, the classiﬁer predicts which of two candidate authors wrote
the text. Obtain samples of text from two different authors. Separate them into training and
test sets. Now train a language model on the training set. You can choose what features to
think may help. Then compute the probability of the text under each language model and
chose the most probable model. Assess the accuracy of this technique. How does accuracy
change as you alter the set of features? This subﬁeld of linguistics is called stylometry; its
STYLOMETRY
successes include the identiﬁcation of the author of the disputed Federalist Papers (Mosteller
This exercise concerns the classiﬁcation of spam email. Create a corpus of spam email
for classiﬁcation: unigram words? bigrams? message length, sender, time of arrival? Then
train a classiﬁcation algorithm (decision tree, naive Bayes, SVM, logistic regression, or some
other algorithm of your choosing) on a training set and report its accuracy on a test set.
Create a test set of ten queries, and pose them to three major Web search engines.
between engines?
Try to ascertain which of the search engines from the previous exercise are using case
folding, stemming, synonyms, and spelling correction.
Write a regular expression or a short program to extract company names. Test it on a
corpus of business news articles. Report your recall and precision.
Consider the problem of trying to evaluate the quality of an IR system that returns a
ranked list of answers (like most Web search engines). The appropriate measure of quality
depends on the presumed model of what the searcher is trying to achieve, and what strategy
she employs. For each of the following models, propose a corresponding numeric measure.
a. The searcher will look at the ﬁrst twenty answers returned, with the objective of getting
as much relevant information as possible.
b. The searcher needs only one relevant document, and will go down the list until she ﬁnds
the ﬁrst one.
c. The searcher has a fairly narrow query and is able to examine all the answers retrieved.
She wants to be sure that she has seen everything in the document collection that isNATURAL LANGUAGE
FOR COMMUNICATION
In which we see how humans communicate with one another in natural language,
and how computer agents might join in the conversation.
Communication is the intentional exchange of information brought about by the production
COMMUNICATION
and perception of signs drawn from a shared system of conventional signs. Most animals use
SIGN
signs to represent important messages: food here, predator nearby, approach, withdraw, let’s
mate. In a partially observable world, communication can help agents be successful because
they can learn information that is observed or inferred by others. Humans are the most chatty
of all species, and if computer agents are to be helpful, they’ll need to learn to speak the
language. In this chapter we look at language models for communication. Models aimed at
deep understanding of a conversation necessarily need to be more complex than the simple
models aimed at, say, spam classiﬁcation. We start with grammatical models of the phrase
structure of sentences, add semantics to the model, and then apply it to machine translation
and speech recognition.
PHRASE STRUCTURE GRAMMARS
trigram probabilities to estimate, and so a corpus of even a trillion words will not be able to
supply reliable estimates for all of them. We can address the problem of sparsity through
generalization. From the fact that “black dog” is more frequent than “dog black” and similar
observations, we can form the generalization that adjectives tend to come before nouns in
English (whereas they tend to follow nouns in French: “chien noir” is more frequent). Of
course there are always exceptions; “galore” is an adjective that follows the noun it modiﬁes.
Despite the exceptions, the notion of a lexical category (also known as a part of speech) such
LEXICAL CATEGORY
as noun or adjective is a useful generalization—useful in its own right, but more so when we
string together lexical categories to form syntactic categories such as noun phrase or verb
SYNTACTIC
CATEGORIES
phrase, and combine these syntactic categories into trees representing the phrase structure
PHRASE STRUCTURE
of sentences: nested phrases, each marked with a category.Phrase Structure Grammars
GENERATIVE CAPACITY
Grammatical formalisms can be classiﬁed by their generative capacity: the set of
ical formalisms that differ only in the form of the rewrite rules. The classes can
guages that can be described by a less powerful class, as well as some additional
languages. Here we list the hierarchy, most powerful class ﬁrst:
Recursively enumerable grammars use unrestricted rules: both sides of the
rewrite rules can have any number of terminal and nonterminal symbols, as in the
expressive power.
sensitive” comes from the fact that a rule such as A X B
→A Y B says that
an X can be rewritten as a Y in the context of a preceding A and a following B.
of n copies of a followed by the same number of bs and then cs).
gle nonterminal symbol. Thus, each rule licenses rewriting the nonterminal as
state machines. They are poorly suited for programming languages, because they
cannot represent constructs such as balanced opening and closing parentheses (a
variation of the anbn language). The closest they can come is representing a∗b∗, a
sequence of any number of as followed by any number of bs.
The grammars higher up in the hierarchy have more expressive power, but
the algorithms for dealing with them are less efﬁcient. Up to the 1980s, linguists
renewed interest in regular grammars, brought about by the need to process and
learn from gigabytes or terabytes of online text very quickly, even at the cost of
a less complete analysis. As Fernando Pereira put it, “The older I get, the further
down the Chomsky hierarchy I go.” To see what he means, compare Pereira and
authors all now work on large text corpora at Google).Chapter
Natural Language for Communication
PROBABILISTIC
GRAMMAR
GRAMMAR
LANGUAGE
means that the grammar assigns a probability to every string. Here is a PCFG rule:
VP
→
|
SYMBOLS
also refers to actual words, which are called terminal symbols. This rule is saying that with
TERMINAL SYMBOL
cation between agents exploring the wumpus world. We call this language E0. Later sections
improve on E0 to make it slightly closer to real English. We are unlikely ever to devise a
complete grammar for English, if only because no two persons would agree entirely on what
constitutes valid English.
The lexicon of E0
First we deﬁne the lexicon, or list of allowable words. The words are grouped into the lexical
LEXICON
categories familiar to dictionary users: nouns, pronouns, and names to denote things; verbs
to denote events; adjectives to modify nouns; adverbs to modify verbs; and function words:
lexicon for the language E0.
Each of the categories ends in . . . to indicate that there are other words in the category.
For nouns, names, verbs, adjectives, and adverbs, it is infeasible even in principle to list all
the words. Not only are there tens of thousands of members in each class, but new ones–
like iPod or biodiesel—are being added constantly. These ﬁve categories are called open
classes. For the categories of pronoun, relative pronoun, article, preposition, and conjunction
OPEN CLASS
we could have listed all the words with a little more work. These are called closed classes;
CLOSED CLASS
they have a small number of words (a dozen or so). Closed classes change over the course
of centuries, not months. For example, “thee” and “thou” were commonly used pronouns in
the 17th century, were on the decline in the 19th, and are seen today only in poetry and some
regional dialects.
The Grammar of E0
PARSE TREE
“that”) followed by a verb phrase. An example of a relative clause is that stinks in “The wumpus that stinks is inPhrase Structure Grammars
Noun
→
Verb
→
Adjective
→
Adverb
→
Pronoun
→
RelPro
→
Name
→
Article
→
Prep
→
Conj
→
Digit
→
The lexicon for E0. RelPro is short for relative pronoun, Prep for preposition,
E0 :
S
→
NP VP
|
S Conj S
NP
→
Pronoun
|
Name
|
Noun
|
Article Noun
|
|
Digit Digit
|
NP PP
|
NP RelClause
VP
→
Verb
|
VP NP
|
VP Adjective
|
VP PP
|
VP Adverb
Adjs
→
Adjective
|
Adjective Adjs
PP
→
Prep NP
RelClause
→
RelPro VP
egories are sentence (S), noun phrase (NP), verb phrase (VP), list of adjectives (Adjs),
prepositional phrase (PP), and relative clause (RelClause).Chapter
Natural Language for Communication
Article
Noun
wumpus
Verb
NP
VP
S
Every
smells
Parse tree for the sentence “Every wumpus smells” according to the grammar
E0. Each interior node of the tree is labeled with its probability. The probability of the tree
parse of the sentence, that number is also the probability of the sentence. The tree can also
be written in linear form as [S [NP [Article every] [Noun wumpus]][VP [Verb smells]]].
gives a constructive proof that the string of words is indeed a sentence according to the rules
of E0. The E0 grammar generates a wide range of English sentences such as the following:
John is in the pit
OVERGENERATION
matical, such as “Me go Boston” and “I smell pits wumpus John.” It also undergenerates:
UNDERGENERATION
there are many sentences of English that it rejects, such as “I think the wumpus is smelly.”
We will see how to learn a better grammar later; for now we concentrate on what we can do
with the grammar we have.
SYNTACTIC ANALYSIS (PARSING)
Parsing is the process of analyzing a string of words to uncover its phrase structure, according
PARSING
top down for a tree that has the words as its leaves, or we can start with the words and search
inefﬁcient, however, because they can end up repeating effort in areas of the search space that
lead to dead ends. Consider the following two sentences:
have to guess whether the ﬁrst word is part of a command or a question and will not be able
to tell if the guess is correct until at least the eleventh word, take or taken. If the algorithm
guesses wrong, it will have to backtrack all the way to the ﬁrst word and reanalyze the whole
sentence under the other interpretation.Syntactic Analysis (Parsing)
List of items
Rule
S
NP VP
NP VP Adjective
NP Verb Adjective
NP Verb dead
NP is dead
Article Noun is dead
Article wumpus is dead
the wumpus is dead
Trace of the process of ﬁnding a parse for the string “The wumpus is dead”
list of items being the words of the sentence, and, on each step, match a string of tokens (...)
To avoid this source of inefﬁciency we can use dynamic programming: every time we
analyze a substring, store the results so we won’t have to reanalyze it later. For example,
record that result in a data structure known as a chart. Algorithms that do this are called chart
CHART
the context of one branch of the search space can work just as well in any other branch of the
the CYK algorithm, after its inventors, John Cocke, Daniel Younger, and Tadeo Kasami.
CYK ALGORITHM
CHOMSKY NORMAL
FORM
The CYK algorithm uses space of O(n2m) for the P table, where n is the number of
words in the sentence, and m is the number of nonterminal symbols in the grammar, and takes
time O(n3m). (Since m is constant for a particular grammar, this is commonly described as
faster algorithms on more restricted grammars. In fact, it is quite a trick for the algorithm to
complete in O(n3) time, given that it is possible for a sentence to have an exponential number
of parse trees. Consider the sentence
Fall leaves fall and spring leaves spring.
It is ambiguous because each word (except “and”) can be either a noun or a verb, and “fall”
and “spring” can be adjectives as well. (For example, one meaning of “Fall leaves fall” isChapter
Natural Language for Communication
N ←LENGTH(words)
M ←the number of nonterminal symbols in grammar
/* Insert lexical rules for each word */
len2 ←length −len1
P[X , start, length] ←MAX(P[X , start, length],
P[Y , start, len1] × P[Z, start + len1, len2] × p)
return P
The CYK algorithm for parsing. Given a sequence of words, it ﬁnds the
most probable derivation for the whole sequence and for each subsequence. It returns the
whole table, P, in which an entry P[X , start, len] is the probability of the most probable
X of length len starting at position start. If there is no X of that size at that location, the
equivalent to “Autumn abandons autumn.) With E0 the sentence has four parses:
[S [S [NP Fall leaves] fall] and [S [NP spring leaves] spring]
[S [S [NP Fall leaves] fall] and [S spring [VP leaves spring]]
[S [S Fall [VP leaves fall]] and [S [NP spring leaves] spring]
[S [S Fall [VP leaves fall]] and [S spring [VP leaves spring]] .
in O(c3) time? The answer is that it doesn’t examine all the parse trees; all it has to do is
compute the probability of the most probable tree. The subtrees are all represented in the P
table, and with a little work we could enumerate them all (in exponential time), but the beauty
of the CYK algorithm is that we don’t have to enumerate them unless we want to.
In practice we are usually not interested in all parses; just the best one or best few. Think
of the CYK algorithm as deﬁning the complete state space deﬁned by the “apply grammar
rule” operator. It is possible to search just part of this space using A∗search. Each stateSyntactic Analysis (Parsing)
[VP were
[VP glazed
[VP did n’t
or
.]
Annotated tree for the sentence “Her eyes were glazed as if she didn’t hear
or even see him.” from the Penn Treebank. Note that in this grammar there is a distinction
matical phenomenon we have not covered yet: the movement of a phrase from one part of
the tree to another. This tree analyzes the phrase “hear or even see him” as consisting of two
cost of a state is the inverse of its probability as deﬁned by the rules applied so far, and there
are various heuristics to estimate the remaining distance to the goal; the best heuristics come
from machine learning applied to a corpus of sentences. With the A∗algorithm we don’t have
to search the entire state space, and we are guaranteed that the ﬁrst parse found will be the
most probable.
Learning probabilities for PCFGs
A PCFG has many rules, with a probability for each rule. This suggests that learning the
iest if we are given a corpus of correctly parsed sentences, commonly called a treebank. The
TREEBANK
Given a corpus of trees, we can create a PCFG just by counting (and smoothing). In the
example above, there are two nodes of the form [S[NP . . .][VP . . .]]. We would count these,
What if a treebank is not available, but we have a corpus of raw unlabeled sentences? It is
still possible to learn a grammar from such a corpus, but it is more difﬁcult. First of all,
we actually have two problems: learning the structure of the grammar rules and learning theChapter
Natural Language for Communication
probabilities associated with each rule. (We have the same distinction in learning Bayes nets.)
We’ll assume that we’re given the lexical and syntactic category names. (If not, we can just
then assume that the grammar includes every possible (X
→Y Z) or (X
→word) rule,
We can then use an expectation–maximization (EM) approach, just as we did in learning
HMMs. The parameters we are trying to learn are the rule probabilities; we start them off at
random or uniform values. The hidden variables are the parse trees: we don’t know whether
the probability that each subsequence is generated by each rule. The M step then estimates
fashion with an algorithm called the inside–outside algorithm in analogy to the forward–
INSIDE–OUTSIDE
ALGORITHM
backward algorithm for HMMs.
The inside–outside algorithm seems magical in that it induces a grammar from unparsed
text. But it has several drawbacks. First, the parses that are assigned by the induced grammars
are often difﬁcult to understand and unsatisfying to linguists. This makes it hard to combine
handcrafted knowledge with automated induction. Second, it is slow: O(n3m3), where n is
the number of words in a sentence and m is the number of grammar categories. Third, the
space of probability assignments is very large, and empirically it seems that getting stuck in
local maxima is a severe problem. Alternatives such as simulated annealing can get closer to
that inside–outside is “computationally intractable for realistic problems.”
However, progress can be made if we are willing to step outside the bounds of learning
solely from unparsed text. One approach is to learn from prototypes: to seed the process with
a dozen or two rules, similar to the rules in E1. From there, more complex rules can be learned
more easily, and the resulting grammar parses English with an overall recall and precision for
but in addition to learning PCFG rules directly from the bracketings, also learning distinctions
between NP and NP −SBJ. The latter is used for the pronoun “she,” the former for the
are many ways in which it would be useful to split a category like NP—grammar induction
systems that use treebanks but automatically split categories do better than those that stick
with the original category set (Petrov and Klein, 2007c). The error rates for automatically
is decreasing.
A Markov model of order two or more, given a sufﬁciently large corpus, will know that “eatAugmented Grammars and Semantic Interpretation
a banana” is more probable. We can combine a PCFG and Markov model to get the best of
both. The simplest approach is to estimate the probability of a sentence with the geometric
mean of the probabilities computed by both models. Then we would know that “eat a banana”
is probable from both the grammatical and lexical point of view. But it still wouldn’t pick up
the relation between “eat” and “banana” in “eat a slightly aging but still palatable banana”
because here the relation is more than two words away. Increasing the order of the Markov
model won’t get at the relation precisely; to do that we can use a lexicalized PCFG, as
described in the next section.
Another problem with PCFGs is that they tend to have too strong a preference for shorter
sentences. In a corpus such as the Wall Street Journal, the average length of a sentence
sentences, such as “He slept,” whereas in the Journal we’re more likely to see something like
“It has been reported by a reliable source that the allegation that he slept is credible.” It seems
the expected sentence length and use that length as a soft global constraint on their sentences.
This is hard to reﬂect in a PCFG.
AUGMENTED GRAMMARS AND SEMANTIC INTERPRETATION
every NP is independent of context, but rather, certain NPs are more likely to appear in one
context, and others in another context.
Lexicalized PCFGs
To get at the relationship between the verb “eat” and the nouns “banana” versus “bandanna,”
LEXICALIZED PCFG
ship between words in the parse tree, not just on the adjacency of words in a sentence. Of
course, we can’t have the probability depend on every word in the tree, because we won’t
tion of the head of a phrase—the most important word. Thus, “eat” is the head of the VP
HEAD
“eat a banana” and “banana” is the head of the NP “a banana.” We use the notation VP(v)
to denote a phrase with category VP whose head word is v. We say that the category VP
is augmented with the head variable v. Here is an augmented grammar that describes the
AUGMENTED
GRAMMAR
verb–object relation:
VP(v) →Verb(v) NP(n)
[P1(v, n)]
VP(v) →Verb(v)
[P2(v)]
NP(n) →Article(a) Adjs(j) Noun(n)
[P3(n, a)]
Noun(banana) →banana
[pn]
. . .
. . .
bility to be relatively high when v is “eat” and n is “banana,” and low when n is “bandanna.”Chapter
Natural Language for Communication
Note that since we are considering only heads, the distinction between “eat a banana” and
probability estimates. Only a few percent of these can come from a corpus; the rest will have
(v, n) pair that we have not seen often (or at all) by backing off to a model that depends
only on v. These objectless probabilities are still very useful; they can capture the distinction
between a transitive verb like “eat”—which will have a high value for P1 and a low value for
P2—and an intransitive verb like “sleep,” which will have the reverse. It is quite feasible to
learn these probabilities from a treebank.
Formal deﬁnition of augmented grammar rules
Augmented rules are complicated, so we will give them a formal deﬁnition by showing how
an augmented rule can be translated into a logical sentence. The sentence will have the form
DEFINITE CLAUSE
GRAMMAR
We’ll use as an example a version of a rule from the lexicalized grammar for NP with one
new piece of notation:
NP(n) →Article(a) Adjs(j) Noun(n) {Compatible(j, n)} .
The new aspect here is the notation {constraint} to denote a logical constraint on some of the
variables; the rule only holds when the constraint is true. Here the predicate Compatible(j, n)
is meant to test whether adjective j and noun n are compatible; it would be deﬁned by a series
for the concatenation of words, Append(s1, . . .), to the list of arguments for the root of the
tree. That gives us
Article(a, s1) ∧Adjs(j, s2) ∧Noun(n, s3) ∧Compatible(j, n)
⇒NP(n, Append(s1, s2, s3)) .
This deﬁnite clause says that if the predicate Article is true of a head word a and a string s1,
and Adjs is similarly true of a head word j and a string s2, and Noun is true of a head word
n and a string s3, and if j and n are compatible, then the predicate NP is true of the head
word n and the result of appending strings s1, s2, and s3.
ment each constituent with one more variable representing the probability of the constituent,
and augment the root with a variable that is the product of the constituent probabilities times
the rule probability.
The translation from grammar rule to deﬁnite clause allows us to talk about parsing
as logical inference. This makes it possible to reason about languages and strings in manyAugmented Grammars and Semantic Interpretation
one of the ﬁrst applications of (and motivations for) the Prolog logic programming language.
It is sometimes possible to run the process backward and do language generation as well as
LANGUAGE
GENERATION
S(Loves(John, Mary), [John, loves, Mary]) .
the process than is afforded by the DCG rules alone.
E1 :
S
→
NPS VP |
. . .
NPS
→
PronounS | Name | Noun | . . .
NPO
→
PronounO | Name | Noun | . . .
VP
→
VP NPO |
. . .
PP
→
Prep NPO
PronounS
→
I | you | he | she | it | . . .
PronounO
→
me | you | him | her | it | . . .
. . .
E2 :
S(head)
→
NP(Sbj, pn, h) VP(pn, head) | . . .
NP(c, pn, head)
→
Pronoun(c, pn, head) | Noun(c, pn, head) | . . .
VP(pn, head)
→
VP(pn, head) NP(Obj, p, h) | . . .
PP(head)
→
Prep(head) NP(Obj, pn, h)
Pronoun(Sbj, 1S, I)
→
I
Pronoun(Sbj , 1P, we)
→
we
Pronoun(Obj, 1S, me)
→
me
Pronoun(Obj, 3P, them)
→
them
. . .
Top: part of a grammar for the language E1, which handles subjective and
objective cases in noun phrases and thus does not overgenerate quite as badly as E0. The
portions that are identical to E0 have been omitted. Bottom: part of an augmented grammar
for E2, with three augmentations: case agreement, subject–verb agreement, and head word.
Sbj, Obj, 1S, 1P and 3P are constants, and lowercase names are variables.
Case agreement and subject–verb agreement
tences such as “Me smell a stench.” To avoid this problem, our grammar would have to know
that “me” is not a valid NP when it is the subject of a sentence. Linguists say that the pronoun
the accusative case. Many languages also have a dative case for words in the indirect object position.Chapter
Natural Language for Communication
splitting NP into two categories, NPS and NPO, to stand for noun phrases in the subjective
and objective case, respectively. We would also need to split the category Pronoun into the
two categories PronounS (which includes “I”) and PronounO (which includes “me”). The
CASE AGREEMENT
E1. Notice that all the NP rules must be duplicated, once for NPS and once for NPO.
Unfortunately, E1 still overgenerates. English requires subject–verb agreement for
SUBJECT–VERB
AGREEMENT
person and number of the subject and main verb of a sentence. For example, if “I” is the
subject, then “I smell” is grammatical, but “I smells” is not. If “it” is the subject, we get the
reverse. In English, the agreement distinctions are minimal: most verbs have one form for
of person and number. There is one exception: the verb “to be” has three forms, “I am / you
are / he is.” So one distinction (case) splits NP two ways, another distinction (person and
number) splits NP three ways, and as we uncover other distinctions we would end up with an
exponential number of subscripted NP forms if we took the approach of E1. Augmentations
are a better approach: they can represent an exponential number of forms as a single rule.
E2, which handles case agreement, subject–verb agreement, and head words. We have just
one NP category, but NP(c, pn, head) has three augmentations: c is a parameter for case,
pn is a parameter for person and number, and head is a parameter for the head word of
the phrase. The other categories also are augmented with heads and other arguments. Let’s
consider one rule in detail:
S(head) →NP(Sbj , pn, h) VP(pn, head) .
an S, but only if the NP has the subjective (Sbj) case and the person and number (pn) of the
NP and VP are identical. If that holds, then we have an S whose head is the same as the
head of the VP. Note the head of the NP, denoted by the dummy variable h, is not part of the
augmentation of the S. The lexical rules for E2 ﬁll in the values of the parameters and are also
Pronoun(Sbj, 1S, I) →I
head “I.” For simplicity we have omitted the probabilities for these rules, but augmentation
nisms. Petrov and Klein (2007c) show how a learning algorithm can automatically split the
NP category into NPS and NPO.
Semantic interpretation
To show how to add semantics to a grammar, we start with an example that is simpler thanAugmented Grammars and Semantic Interpretation
Exp(x) →Exp(x1) Operator(op) Exp(x2) {x = Apply(op, x1, x2)}
Exp(x) →( Exp(x) )
Exp(x) →Number(x)
Number(x) →Digit(x)
able xi represents the semantics of a constituent. Note the use of the {test} notation to deﬁne
logical predicates that must be satisﬁed, but that are not constituents.
Operator(÷)
(
)
+
Operator(+)
÷
COMPOSITIONAL
SEMANTICS
ing what semantic representations we want to associate with what phrases. We use the simple
tion the logical term John, and the sentence as a whole should have as its interpretation the
logical sentence Loves(John, Mary). That much seems clear. The complicated part is the
VP “loves Mary.” The semantic interpretation of this phrase is neither a logical term nor a
complete logical sentence. Intuitively, “loves Mary” is a description that might or might notChapter
Natural Language for Communication
apply to a particular person. (In this case, it applies to John.) This means that “loves Mary”
is a predicate that, when combined with a term that represents a person (the person doing
represent “loves Mary” as the predicate
λx Loves(x, Mary) .
Now we need a rule that says “an NP with semantics obj followed by a VP with semantics
pred yields a sentence whose semantics is the result of applying pred to obj:”
S(pred(obj)) →NP(obj) VP(pred) .
The rule tells us that the semantic interpretation of “John loves Mary” is
(λx Loves(x, Mary))(John) ,
which is equivalent to Loves(John, Mary).
The rest of the semantics follows in a straightforward way from the choices we have
made so far. Because VPs are represented as predicates, it is a good idea to be consistent and
We could just as easily have added semantics to E2; we chose to work with E0 so that the
reader can focus on one type of augmentation at a time.
Adding semantic augmentations to a grammar by hand is laborious and error prone.
Therefore, there have been several projects to learn semantic augmentations from examples.
learns a grammar and a specialized parser for that grammar from examples. The target domain
is natural language database queries. The training examples consist of pairs of word strings
and corresponding semantic forms—for example;
What is the capital of the state with the largest population?
Answer(c, Capital(s, c) ∧Largest(p, State(s) ∧Population(s, p)))
amples and, hopefully, generalizes well to other examples. Applying ILP directly to learn
Fortunately, ILP learners can improve by adding knowledge. In this case, most of the Parse
predicate was deﬁned as a logic program, and CHILL’s task was reduced to inducing the
query tasks.
Complications
The grammar of real English is endlessly complex. We will brieﬂy mention some examples.
Time and tense: Suppose we want to represent the difference between “John loves
TIME AND TENSE
Mary” and “John loved Mary.” English uses verb tenses (past, present, and future) to indicateAugmented Grammars and Semantic Interpretation
S(pred(obj)) →NP(obj) VP(pred)
VP(pred(obj )) →Verb(pred) NP(obj)
NP(obj ) →Name(obj)
Name(John) →John
Name(Mary) →Mary
A grammar that can derive a parse tree and semantic interpretation for “John
loves Mary” (and three other sentences). Each category is augmented with a single argument
representing the semantics.
John
loves
Mary
Name(John)
Name(Mary)
NP(Mary)
NP(John)
S(Loves(John,Mary))
VP(λx Loves(x,Mary))
A parse tree with semantic interpretations for the string “John loves Mary”.
the relative time of an event. One good choice to represent the time of events is the event
This suggests that our two lexical rules for the words “loves” and “loved” should be these:
couraging news; it suggests we are on the right track if we can so easily add a complication
like the tense of verbs (although we have just scratched the surface of a complete grammar
for time and tense). It is also encouraging that the distinction between processes and discrete
ally reﬂected in language use. We can say “John slept a lot last night,” where Sleeping is a
process category, but it is odd to say “John found a unicorn a lot last night,” where Finding
is a discrete event category. A grammar would reﬂect that fact by having a low probability
for adding the adverbial phrase “a lot” to discrete events.
Quantiﬁcation: Consider the sentence “Every agent feels a breeze.” The sentence has
QUANTIFICATION
only one syntactic parse under E0, but it is actually semantically ambiguous; the preferredChapter
Natural Language for Communication
meaning is “For every agent there exists a breeze that the agent feels,” but an acceptable
can be represented as
The standard approach to quantiﬁcation is for the grammar to deﬁne not an actual logical
FORM
by algorithms outside of the parsing process. Those algorithms can have preference rules for
preferring one quantiﬁer scope over another—preferences that need not be reﬂected directly
in the grammar.
Pragmatics: We have shown how an agent can perceive a string of words and use a
PRAGMATICS
grammar to derive a set of possible semantic interpretations. Now we address the problem
situation. The most obvious need for pragmatic information is in resolving the meaning of
indexicals, which are phrases that refer directly to the current situation. For example, in the
INDEXICAL
sentence “I am in Boston today,” both “I” and “today” are indexicals. The word “I” would be
represented by the ﬂuent Speaker, and it would be up to the hearer to resolve the meaning of
the ﬂuent—that is not considered part of the grammar but rather an issue of pragmatics; of
using the context of the current situation to interpret ﬂuents.
Another part of pragmatics is interpreting the speaker’s intent. The speaker’s action is
considered a speech act, and it is up to the hearer to decipher what type of action it is—a
SPEECH ACT
question, a statement, a promise, a warning, a command, and so on. A command such as
sentences. We can easily extend it to cover commands. A command can be formed from
a VP, where the subject is implicitly the hearer. We need to distinguish commands from
statements, so we alter the rules for S to include the type of speech act:
S(Statement(Speaker, pred(obj))) →NP(obj ) VP(pred)
S(Command(Speaker, pred(Hearer))) →VP(pred) .
DEPENDENCIES
“Who did the agent tell you to give the gold to?” the ﬁnal word “to” should be parsed as
[PP to ], where the “ ” denotes a gap or trace where an NP is missing; the missing NP
TRACE
is licensed by the ﬁrst word of the sentence, “who.” A complex system of augmentations is
used to make sure that the missing NPs match up with the licensing words in just the right
way, and prohibit gaps in the wrong places. For example, you can’t have a gap in one branch
of an NP conjunction: “What did he play [NP Dungeons and ]?” is ungrammatical. But
you can have the same gap in both branches of a VP conjunction: “What did you [VP [VP
smell ] and [VP shoot an arrow at ]]?”
Ambiguity: In some cases, hearers are consciously aware of ambiguity in an utterance.
AMBIGUITY
Here are some examples taken from newspaper headlines:Augmented Grammars and Semantic Interpretation
Squad helps dog bite victim.
Police begin campaign to run down jaywalkers.
Helicopter powered by human ﬂies.
Portable toilet bombed; police have nothing to go on.
Teacher strikes idle kids.
Include your children when baking cookies.
Milk drinkers are turning to powder.
Safety experts say school bus passengers should be belted.
But most of the time the language we hear seems unambiguous. Thus, when researchers ﬁrst
began to use computers to analyze language in the 1960s, they were quite surprised to learn
that almost every utterance is highly ambiguous, even though the alternative interpretations
might not be apparent to a native speaker. A system with a large grammar and lexicon might
ﬁnd thousands of interpretations for a perfectly ordinary sentence. Lexical ambiguity, in
LEXICAL AMBIGUITY
which a word has more than one meaning, is quite common; “back” can be an adverb (go
back), an adjective (back door), a noun (the back of the room) or a verb (back up your ﬁles).
a ﬁsh, a socket, or a device for raising heavy objects), or a verb (to jack up a car, to hunt with
a light, or to hit a baseball hard). Syntactic ambiguity refers to a phrase that has multiple
SYNTACTIC
AMBIGUITY
SEMANTIC
AMBIGUITY
for the agent.
Finally, there can be ambiguity between literal and ﬁgurative meanings. Figures of
speech are important in poetry, but are surprisingly common in everyday speech as well. A
metonymy is a ﬁgure of speech in which one object is used to stand for another. When
METONYMY
nies can talk; rather we understand that a spokesperson representing the company made the
ers. Unfortunately, our grammar as it is written is not so facile. To handle the semantics of
viding two objects for the semantic interpretation of every phrase in the sentence: one for the
object that the phrase literally refers to (Chrysler) and one for the metonymic reference (the
spokesperson). We then have to say that there is a relation between the two. In our current
grammar, “Chrysler announced” gets interpreted as
We need to change that to
∧Metonymy(m, x) .Chapter
Natural Language for Communication
This says that there is one entity x that is equal to Chrysler, and another entity m that did
the announcing, and that the two are in a metonymy relation. The next step is to deﬁne what
kinds of metonymy relations can occur. The simplest case is when there is no metonymy at
all—the literal object x and the metonymic object m are identical:
∀m, x (m = x) ⇒Metonymy(m, x) .
For the Chrysler example, a reasonable generalization is that an organization can be used to
stand for a spokesperson of that organization:
Other metonymies include the author for the works (I read Shakespeare) or more generally
the producer for the product (I drive a Honda) and the part for the whole (The Red Sox need
another beer,” are more novel and are interpreted with respect to a situation.
A metaphor is another ﬁgure of speech, in which a phrase with one literal meaning is
METAPHOR
used to suggest a different meaning by way of an analogy. Thus, metaphor can be seen as a
kind of metonymy where the relation is one of similarity.
Disambiguation is the process of recovering the most probable intended meaning of
DISAMBIGUATION
an utterance. In one sense we already have a framework for solving this problem: each rule
has a probability associated with it, so the probability of an interpretation is the product of
the probabilities of the rules that led to the interpretation. Unfortunately, the probabilities
reﬂect how common the phrases are in the corpus from which the grammar was learned,
and thus reﬂect general knowledge, not speciﬁc knowledge of the current situation. To do
disambiguation properly, we need to combine four models:
know about the world, it is more likely that a speaker who says “I’m dead” means “I
am in big trouble” rather than “My life ended, and yet I can still talk.”
ing a certain fact to the hearer. This approach combines models of what the speaker
believes, what the speaker believes the hearer believes, and so on. For example, when
a politician says, “I am not a crook,” the world model might assign a probability of
proposition that he is not a hooked shepherd’s staff. Nevertheless, we select the former
interpretation because it is a more likely thing to say.
that the speaker has the intention of communicating a certain fact.
quence of sounds will be generated, given that the speaker has chosen a given string ofMachine Translation
MACHINE TRANSLATION
Machine translation is the automatic translation of text from one natural language (the source)
to another (the target). It was one of the ﬁrst application areas envisioned for computers
AI is one of the newest ﬁelds in science and engineering. Work started in earnest soon
ogy, AI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other
disciplines.
And here it is translated from English to Danish by an online tool, Google Translate:
AI er en af de nyeste omr˚ader inden for videnskab og teknik. Arbejde startede for alvor
molekylær biologi, er AI jævnligt nævnt som “feltet Jeg ville de ﬂeste gerne være i” af
forskere i andre discipliner.
For those who don’t read Danish, here is the Danish translated back to English. The words
that came out different are in italics:
AI is one of the newest ﬁelds of science and engineering. Work began in earnest just after
biology, AI is frequently mentioned as
“ﬁeld I would most like to be in” by researchers
in other disciplines.
The differences are all reasonable paraphrases, such as frequently mentioned for regularly
cited. The only real error is the omission of the article the, denoted by the
symbol. This is
typical accuracy: of the two sentences, one has an error that would not be made by a native
speaker, yet the meaning is clearly conveyed.
Historically, there have been three main applications of machine translation. Rough
translation, as provided by free online services, gives the “gist” of a foreign sentence or
documentation and sales materials in multiple languages. The original source text is written
in a constrained language that is easier to translate automatically, and the results are usually
cally, but only on highly stereotypical language, such as a weather report.
ing of the text. This is true even for very simple texts—even “texts” of one word. Consider
customers at the moment. Now consider the same word “Open” on a large banner outside a
newly constructed store. It means that the store is now in daily operation, but readers of this
sign would not feel misled if the store closed at night without removing the banner. The two
signs use the identical word to convey different meanings. In German the sign on the door
would be “Offen” while the banner would read “Neu Er¨offnet.”Chapter
Natural Language for Communication
The problem is that different languages categorize the world differently. For example,
the French word “doux” covers a wide range of meanings corresponding approximately to
the English words “soft,” “sweet,” and “gentle.” Similarly, the English word “hard” covers
virtually all uses of the German word “hart” (physically recalcitrant, cruel) and some uses
of the word “schwierig” (difﬁcult). Therefore, representing the meaning of a sentence is
system could use predicates like Open(x), but for translation, the representation language
would have to make more distinctions, perhaps with Open1(x) representing the “Offen” sense
and Open2(x) representing the “Neu Er¨offnet” sense. A representation language that makes
all the distinctions necessary for a set of languages is called an interlingua.
INTERLINGUA
scribed in the source, not just the individual words. For example, to translate the English
word “him,” into Korean, a choice must be made between the humble and honoriﬁc form, a
choice that depends on the social relationship between the speaker and the referent of “him.”
times ﬁnd it difﬁcult to make this choice. As another example, to translate “The baseball hit
the window. It broke.” into French, we must choose the feminine “elle” or the masculine
“il” for “it,” so we must decide whether “it” refers to the baseball or the window. To get the
translation right, one must understand physics as well as language.
Sometimes there is no choice that can yield a completely satisfactory translation. For
example, an Italian love poem that uses the masculine “il sole” (sun) and feminine “la luna”
(moon) to symbolize two lovers will necessarily be altered when translated into German,
where the genders are reversed, and further altered when translated into a language where the
Machine translation systems
All translation systems must model the source and target languages, but systems vary in the
type of models they use. Some systems attempt to analyze the source language text all the way
guage from that representation. This is difﬁcult because it involves three unsolved problems:
creating a complete knowledge representation of everything; parsing into that representation;
and generating sentences from that representation.
Other systems are based on a transfer model. They keep a database of translation rules
TRANSFER MODEL
(or examples), and whenever the rule (or example) matches, they translate directly. Transfer
can occur at the lexical, syntactic, or semantic level. For example, a strictly syntactic rule
maps English [Adjective Noun] to French [Noun Adjective]. A mixed syntactic and lexical
various transfer points.
that translation “is like kissing the bride through a veil.”Machine Translation
Interlingua Semantics
Attraction(NamedJohn, NamedMary, High)
English Words
John loves Mary
French Words
Jean aime Marie
English Syntax
S(NP(John), VP(loves, NP(Mary)))
S(NP(Jean), VP(aime, NP(Marie)))
French Syntax
English Semantics
Loves(John, Mary)
Aime(Jean, Marie)
French Semantics
The Vauquois triangle: schematic diagram of the choices for a machine
based system follows the solid lines, parsing English ﬁrst into a syntactic form, then into
a semantic representation and an interlingua representation, and then through generation to
lines as a shortcut. Different systems make the transfer at different points; some make it at
multiple points.
Statistical machine translation
prise that the most successful machine translation systems are built by training a probabilistic
model using statistics gathered from a large corpus of text. This approach does not need
a complex ontology of interlingua concepts, nor does it need handcrafted grammars of the
lations from which a translation model can be learned. To translate a sentence in, say, English
(e) into French (f), we ﬁnd the string of words f ∗that maximizes
f ∗= argmax
f
P(f | e) = argmax P(e | f) P(f) .
Here the factor P(f) is the target language model for French; it says how probable a given
LANGUAGE MODEL
sentence is in French. P(e|f) is the translation model; it says how probable an English
TRANSLATION
MODEL
sentence is as a translation for a given French sentence. Similarly, P(f | e) is a translation
model from English to French.
Should we work directly on P(f | e), or apply Bayes’ rule and work on P(e | f) P(f)?
rection: P(symptoms | disease) rather than P(disease | symptoms). But in translation both
directions are equally easy. The earliest work in statistical machine translation did apply
Bayes’ rule—in part because the researchers had a good language model, P(f), and wanted
to make use of it, and in part because they came from a background in speech recognition,
cent work in statistical machine translation often optimizes P(f | e) directly, using a more
sophisticated model that takes into account many of the features from the language model.Chapter
Natural Language for Communication
French corpus, as we have seen before. This captures only a partial, local idea of French
The translation model is learned from a bilingual corpus—a collection of parallel texts,
BILINGUAL CORPUS
sentence would just be a lookup task: we would have seen the English sentence before in the
corpus, so we could just return the paired French sentence. But of course our resources are
ﬁnite, and most of the sentences we will be asked to translate will be novel. However, they
will be composed of phrases that we have seen before (even if some phrases are as short as
one word). For example, in this book, common phrases include “in this exercise we will,”
“size of the state space,” “as a function of the” and “notes at the end of the chapter.” If asked
to translate the novel sentence “In this exercise we will compute the size of the state space as a
function of the number of actions.” into French, we should be able to break the sentence into
phrases, ﬁnd the phrases in the English corpus (this book), ﬁnd the corresponding French
phrases (from the French translation of the book), and then reassemble the French phrases
into an order that makes sense in French. In other words, given a source English sentence, e,
ﬁnding a French translation f is a matter of three steps:
P(fi | ei) for the phrasal probability that fi is a translation of ei.
tribution: For each fi, we choose a distortion di, which is the number of words that
DISTORTION
into a corresponding phrase fi, and then these are permuted into the order f1, f3, f4, f2, f5.
We specify the permutation in terms of the distortions di of each French phrase, deﬁned as
where START(fi) is the ordinal number of the ﬁrst word of phrase fi in the French sentence,
Now that we have deﬁned the distortion, di, we can deﬁne the probability distribution
for distortion, P(di). Note that for sentences bounded by length n we have |di| ≤n , and
r´echerche du temps perdu begins and ends with the same word (longtemps), so some translators have decided toMachine Translation
learn than the number of permutations, n!. That is why we deﬁned the permutation in this
circuitous way. Of course, this is a rather impoverished model of distortion. It doesn’t say
that adjectives are usually distorted to appear after the noun when we are translating from
tion probability is completely independent of the words in the phrases—it depends only on
the integer value di. The probability distribution provides a summary of the volatility of the
We’re ready now to put it all together: we can deﬁne P(f, d | e), the probability that
the sequence of phrases f with distortions d is a translation of the sequence of phrases e. We
make the assumption that each phrase translation and each distortion is independent of the
others, and thus we can factor the expression as
P(f, d | e) =

i
P(fi | ei) P(di)
There is a
smelly
wumpus
sleeping
Il y a un
wumpus
qui dort
malodorant
e1
e2
e3
e4
e5
f1
f3
f2
f4
f5
tortion (d) values for each French phrase.
That gives us a way to compute the probability P(f, d | e) for a candidate translation f
and distortion d. But to ﬁnd the best f and d we can’t just enumerate sentences; with maybe
cedure; see the notes at the end of the chapter for details.
HANSARD
languages, and the United Nations publishes multilingual documents. Bilingual text is
also available online; some Web sites publish parallel content with parallel URLs, forChapter
Natural Language for Communication
leading statistical translation systems train on hundreds of millions of words of parallel
text and billions of words of monolingual text.
the corpus into sentences. Periods are strong indicators of the end of a sentence, but
ends a sentence. One way to decide if a period ends a sentence is to train a model
that takes as features the surrounding words and their parts of speech. This approach
algorithm. Even better alignment can be achieved by using landmarks that are common
to both languages, such as numbers, dates, proper names, or words that we know from
a bilingual dictionary have an unambiguous translation. For example, if the 3rd English
that is good evidence that the sentences should be aligned together.
that used for sentence alignment, but requiring iterative improvement. When we start,
we have no way of knowing that “qui dort” aligns with “sleeping,” but we can arrive at
that alignment by a process of aggregation of evidence. Over all the example sentences
frequently in other sentences with “sleeping.” A complete phrase alignment over our
corpus gives us the phrasal probabilities (after appropriate smoothing).
probabilities. Simply count how often distortion occurs in the corpus for each distance
of P(f | e) and P(d) values. We compute the best alignments with the current values
of these parameters in the E step, then update the estimates in the M step and iterate the
process until convergence.
SPEECH RECOGNITION
Speech recognition is the task of identifying a sequence of words uttered by a speaker, given
SPEECH
RECOGNITION
the acoustic signal. It has become one of the mainstream applications of AI—millions ofSpeech Recognition
people interact with speech recognition systems every day to navigate voice mail systems,
search the Web from mobile phones, and other applications. Speech is an attractive option
Speech recognition is difﬁcult because the sounds made by a speaker are ambiguous
the same as “wreck a nice beach” when spoken quickly. Even this short example shows
several of the issues that make speech problematic. First, segmentation: written words in
SEGMENTATION
English have spaces between them, but in fast speech there are no pauses in “wreck a nice”
that would distinguish it as a multiword phrase as opposed to the single word “recognize.”
Second, coarticulation: when speaking quickly the “s” sound at the end of “nice” merges
COARTICULATION
with the “b” sound at the beginning of “beach,” yielding something that is close to a “sp.”
Another problem that does not show up in this example is homophones—words like “to,”
HOMOPHONES
“too,” and “two” that sound the same but differ in meaning.
variables, x1:t, given a sequence of observations e1:t. In this case the state variables are the
words, and the observations are sounds. More precisely, an observation is a vector of features
extracted from the audio signal. As usual, the most likely sequence can be computed with the
help of Bayes’ rule to be:
argmax
ACOUSTIC MODEL
the language model. It speciﬁes the prior probability of each utterance—for example, that
LANGUAGE MODEL
NOISY CHANNEL
MODEL
described a situation in which an original message (the words in our example) is transmitted
over a noisy channel (such as a telephone line) such that a corrupted message (the sounds
in our example) are received at the other end. Shannon showed that no matter how noisy
the channel, it is possible to recover the original message with arbitrarily small error, if we
encode the original message in a redundant enough way. The noisy channel approach has
been applied to speech recognition, machine translation, spelling correction, and other tasks.
Once we deﬁne the acoustic and language models, we can solve for the most likely
rent state Wordt depends only on a ﬁxed number n of previous states—and represent Word t
as a single random variable taking on a ﬁnite set of values, which makes it a Hidden Markov
models. We cover them next.Chapter
Natural Language for Communication
Vowels
Consonants B–N
Consonants P–Z
Phone
Example
Phone
Example
Phone
Example
[iy]
beat
[b]
bet
[p]
pet
[ih]
bit
[ch]
Chet
[r]
rat
[eh]
bet
[d]
debt
[s]
set
[æ]
bat
[f]
fat
[sh]
shoe
[ah]
but
[g]
get
[t]
ten
[ao]
bought
[hh]
hat
[th]
thick
[ow]
boat
[hv]
high
[dh]
that
[uh]
book
[jh]
jet
[dx]
butter
[ey]
bait
[k]
kick
[v]
vet
[er]
Bert
[l]
let
[w]
wet
[ay]
buy
[el]
bottle
[wh]
which
[oy]
boy
[m]
met
[y]
yet
[axr]
diner
[em]
bottom
[z]
zoo
[aw]
down
[n]
net
[zh]
measure
[ax]
about
[en]
button
[ix]
roses
[ng]
sing
[aa]
cot
[eng]
washing
[-]
silence
The ARPA phonetic alphabet, or ARPAbet, listing all the phones used in
netic Alphabet (IPA), which contains the phones in all known languages.
Acoustic model
Sound waves are periodic changes in pressure that propagate through the air. When these
proximates the amplitude of the sound wave—at discrete intervals called the sampling rate.
SAMPLING RATE
precision of each measurement is determined by the quantization factor; speech recognizers
QUANTIZATION
FACTOR
quantization, would require nearly half a megabyte per minute of speech.
Since we only want to know what words were spoken, not exactly what they sounded
PHONE
composed to form all the words in all known human languages. Roughly speaking, a phone
tions: combinations of letters, such as “th” and “ng” produce single phones, and some lettersSpeech Recognition
all the phones that are used in English, with an example of each. A phoneme is the smallest
PHONEME
unit of sound that has a distinct meaning to speakers of a particular language. For example,
the “t” in “stick” sounds similar enough to the “t” in “tick” that speakers of English consider
them the same phoneme. But the difference is signiﬁcant in the Thai language, so there they
are two phonemes. To represent spoken English we want a representation that can distinguish
between different phonemes, but one that need not distinguish the nonphonemic variations in
sound: loud or soft, fast or slow, male or female voice, etc.
First, we observe that although the sound frequencies in speech may be several kHz,
Hz. Therefore, speech systems summarize the properties of the signal over time slices called
FRAME
make sure that we don’t miss a signal because it happens to fall on a frame boundary.
Each frame is summarized by a vector of features. Picking out features from a speech
FEATURE
signal is like listening to an orchestra and saying “here the French horns are playing loudly
and the violins are playing softly.” We’ll give a brief overview of the features in a typical
system. First, a Fourier transform is used to determine the amount of acoustic energy at
about a dozen frequencies. Then we compute a measure called the mel frequency cepstral
coefﬁcient (MFCC) or MFCC for each frequency. We also compute the total energy in
MEL FREQUENCY
CEPSTRAL
COEFFICIENT (MFCC)
the frame. That gives thirteen features; for each one we compute the difference between
is to discretize the values. (It is also possible to extend the HMM model to handle continuous
sound to a sequence of frames with discrete features.
We have seen how to go from the raw acoustic signal to a series of observations, et.
Now we have to describe the (unobservable) states of the HMM and deﬁne the transition
into two levels: word and phone. We’ll start from the bottom: the phone model describes
PHONE MODEL
Analog acoustic signal:
Sampled, quantized 
digital signal:
Frames with features:
Translating the acoustic signal into a sequence of frames. In this diagram
each frame is described by the discretized values of three acoustic features; a real system
would have dozens of features.Chapter
Natural Language for Communication
Phone HMM for [m]:
Output probabilities for the phone HMM:
Onset:
Mid:
End:
FINAL
Mid
End
Onset
outputs, each with its own probability. The MFCC feature labels C1 through C7 are arbitrary,
standing for some combination of feature values.
[t]
[ow]
[m]
[ey]
[ow]
[aa]
[t]
[m]
[ey]
[ow]
[t]
[aa]
[t]
[ah]
[ow]
(a) Word model with dialect variation:
(b) Word model with coarticulation and dialect variations
:
Two pronunciation models of the word “tomato.” Each model is shown as
a transition diagram with states as circles and arrows showing allowed transitions with their
tion effect on the ﬁrst vowel, allowing either the [ow] or the [ah] phone.Speech Recognition
a phone as three states, the onset, middle, and end. For example, the [t] phone has a silent
beginning, a small explosive burst of sound in the middle, and (usually) a hissing at the end.
short “m” sound.
PRONUNCIATION
MODEL
In addition to dialect variation, words can have coarticulation variation. For example,
the [t] phone is produced with the tongue at the top of the mouth, whereas the [ow] has the
tongue near the bottom. When speaking quickly, the tongue doesn’t have time to get into
a model for “tomato” that takes this coarticulation effect into account. More sophisticated
phone models take into account the context of the surrounding phones.
There can be substantial variation in pronunciation for a word. The most common
pronunciation of “because” is [b iy k ah z], but that only accounts for about a quarter of
uses. Another quarter (approximately) substitutes [ix], [ih] or [ax] for the ﬁrst vowel, and the
remainder substitute [ax] or [aa] for the second vowel, [zh] or [s] for the ﬁnal [z], or drop
“be” entirely, leaving “cuz.”
Language model
text learned from a corpus of written sentences. However, spoken language has different
characteristics than written language, so it is better to get a corpus of transcripts of spoken
vocabulary, such as a list of all the airports and cities served, and all the ﬂight numbers.
Part of the design of a voice user interface is to coerce the user into saying things from a
limited set of options, so that the speech recognizer will have a tighter probability distribution
to deal with. For example, asking “What city do you want to go to?” elicits a response with
a highly constrained language model, while asking “How can I help you?” does not.
Building a speech recognizer
The quality of a speech recognition system depends on the quality of all of its components—
processing algorithms used to extract spectral features from the acoustic signal. We have
discussed how the language model can be constructed from a corpus of written text, and we
leave the details of signal processing to other textbooks. We are left with the pronunciation
and phone models. The structure of the pronunciation models—such as the tomato models inChapter
Natural Language for Communication
able for English and other languages, although their accuracy varies greatly. The structure
leaves the probabilities themselves.
As usual, we will acquire the probabilities from a corpus, this time a corpus of speech.
The most common type of corpus to obtain is one that includes the speech signal for each
sentence paired with a transcript of the words. Building a model from this corpus is more
model—the phone sequence for each word and the phone state for each time frame are hidden
variables. In the early days of speech recognition, the hidden variables were provided by
automatically supply the missing data. The idea is simple: given an HMM and an observation
probability of each state at each time step and, by a simple extension, the probability of each
state–state pair at consecutive time steps. These probabilities can be viewed as uncertain
labels. From the uncertain labels, we can estimate new transition and sensor probabilities,
and the EM procedure repeats. The method is guaranteed to increase the ﬁt between model
and data on each iteration, and it generally converges to a much better set of parameter values
The systems with the highest accuracy work by training a different model for each
This training can require several hours of interaction with the speaker, so the systems with
The accuracy of a system depends on a number of factors. First, the quality of the signal
will do much better than a cheap microphone transmitting a signal over phone lines from a
car in trafﬁc with the radio playing. The vocabulary size matters: when recognizing digit
is trying to accomplish a speciﬁc task—book a ﬂight or give directions to a restaurant—the
SUMMARY
Natural language understanding is one of the most important subﬁelds of AI. Unlike most
other areas of AI, natural language understanding requires an empirical investigation of actual
human behavior—which turns out to be complex and interesting.
free grammar) are useful tools for dealing with some aspects of natural language. TheBibliographical and Historical Notes
such as the CYK algorithm, which requires grammar rules to be in Chomsky Normal
Form.
• A treebank can be used to learn a grammar. It is also possible to learn a grammar from
an unparsed corpus of sentences, but this is less successful.
• A lexicalized PCFG allows us to represent that some relationships between words are
more common than others.
ment and pronoun case. Deﬁnite clause grammar (DCG) is a formalism that allows for
augmentations. With DCG, parsing and semantic interpretation (and even generation)
can be done using logical inference.
• Semantic interpretation can also be handled by an augmented grammar.
biguation relies on knowledge about the world, about the current situation, and about
language use.
• Machine translation systems have been implemented using a range of techniques,
quencies. Currently the statistical models are most popular and most successful.
• Speech recognition systems are also primarily based on statistical principles. Speech
systems are popular and useful, albeit imperfect.
• Together, machine translation and speech recognition are two of the big successes of
natural language technology. One reason that the models perform well is that large
corpora are available—both translation and speech are tasks that are performed “in the
wild” by people every day. In contrast, tasks like parsing sentences have been less
successful, in part because no large corpora of parsed sentences are available “in the
wild” and in part because parsing is not useful in and of itself.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
are a reinvention of a technique ﬁrst used by ancient Indian grammarians (especially Panini,
ATTRIBUTE
GRAMMAR
ful for programming languages.Chapter
Natural Language for Communication
features. Petrov and Klein (2007a) show how to get the advantages of lexicalization without
actual lexical augmentations by learning speciﬁc syntactic categories from a treebank that has
general categories; for example, the treebank has the category NP, from which more speciﬁc
categories such as NPO and NPS can be learned.
There have been many attempts to write formal grammars of natural languages, both
in “pure” linguistics and in computational linguistics. There are several comprehensive but
the ﬁrst major grammar formalism to be highly lexicalized. If we carry lexicalization to an
version of dependency grammar is easier to learn than PCFGs.
ﬁcient algorithms were developed in the late 1960s, with a few twists since then (Kasami,
how chart parsing with augmentations can be made efﬁcient in the average case. Church
the result is not a single parse but the K best.
Leading parsers today include those by Petrov and Klein (2007b), which achieved
numbers are not directly comparable, and there is some criticism of the ﬁeld that it is focusing
too narrowly on a few select corpora, and perhaps overﬁtting on them.
Formal semantic interpretation of natural languages originates within philosophy and
is a kind of manifesto for the logical analysis of language, but the books by Dowty et al.
The ﬁrst NLP system to solve an actual task was probably the BASEBALL questionBibliographical and Historical Notes
rocks brought back from the moon by the Apollo program. Roger Schank and his students
all had the task of understanding language. Modern approaches to semantic interpretation
usually assume that the mapping from syntax to semantics will be learned from examples
pretations (similar to a probability distribution), rather than having the grammar generate all
ambiguity is something to be resolved or cherished.
have used Gold’s result to argue that there must be an innate universal grammar that all
UNIVERSAL
GRAMMAR
aren’t given enough input to learn a CFG, so they must already “know” the grammar and be
merely tuning some of its parameters. While this argument continues to hold sway throughout
then, there have been many convincing empirical demonstrations of learning from positive
Grammatical Inference (ICGI). It is possible to learn other grammar formalisms, such as
phrases, categorized into parts of speech and linked by semantic relations such as synonym,Chapter
Natural Language for Communication
In the 1930s Petr Troyanskii applied for a patent for a “translating machine,” but there
tion’s Warren Weaver wrote to Norbert Wiener, suggesting that machine translation might be
possible. Drawing on work in cryptography and information theory, Weaver wrote, “When I
look at an article in Russian, I say: ‘This is really written in English, but it has been coded in
strange symbols. I will now proceed to decode.”’ For the next decade, the community tried
scribes the enthusiasm of this period. However, the U.S. government subsequently reported
lation.” However, limited work continued, and starting in the 1980s, computer power had
increased to the point where the ALPAC ﬁndings were no longer correct.
The basic statistical approach we describe in the chapter is based on early work by the
activated toy dog. Rex jumped out of his doghouse in response to the word “Rex!” (or
actually almost any sufﬁciently loud word). Somewhat more serious work began after World
War II. At AT&T Bell Labs, a system was built for recognizing isolated digits (Davis et al.,
Advanced Research Projects Agency (DARPA) of the United States Department of Defense
of HARPY was derived from a system called DRAGON built by CMU graduate student James
years from now—you won’t recognize it, because speech will come into the interface.” That
searches to be done through speech than through typing on a keyboard.” History will tell if
he is right this time around.
cessing and at the DARPA Workshops on Speech and Natural Language Processing and the
Eurospeech, ICSLP, and ASRU conferences.Chapter
Natural Language for Communication
Consider the following PCFG for simple verb phrases:
a. Which of the following have a nonzero probability as a VP? (i) shoots the duck well
well well
(ii) seems the well well
(iii) shoots the unwell well badly
b. What is the probability of generating “is well well”?
c. What types of ambiguity are exhibited by the phrase in (b)?
d. Given any PCFG, is it possible to calculate the probability that the PCFG generates a
Outline the major differences between Java (or any other computer language with
which you are familiar) and English, commenting on the “understanding” problem in each
pronouns), background knowledge, and what it means to “understand” in the ﬁrst place.
This exercise concerns grammars for very simple languages.
second half is the reverse of the ﬁrst half.
whose second half is the same as the ﬁrst half.
Consider the sentence “Someone walked slowly to the supermarket” and a lexicon
consisting of the following words:
tence? Show the corresponding parse tree(s).Chapter
Natural Language for Communication
Consider the following PCFG:
The sentence “I can ﬁsh” has two parse trees with this grammar. Show the two trees, their
prior probabilities, and their conditional probabilities, given the sentence.
value. The rule for a sentence in this language is
S(n) →A(n) B(n) C(n) .
Show the rule(s) for each of A, B, and C.
Augment the E1 grammar so that it handles article–noun agreement. That is, make
sure that “agents” and “an agent” are NPs, but “agent” and “an agents” are not.
tailing loans to American businesses, depriving even healthy companies of money
for expansion and hiring.
a. Which of the words in this sentence are lexically ambiguous?
b. Find two cases of syntactic ambiguity in this sentence (there are more than two.)
c. Give an instance of metaphor in this sentence.
d. Can you ﬁnd semantic ambiguity?
a. What are the four steps that are mentioned?
b. What step is left out?
c. What is “the material” that is mentioned in the text?
d. What kind of mistake would be expensive?
e. Is it better to do too few things or too many? Why?
Select ﬁve sentences and submit them to an online translation service. Translate
them from English to another language and back to English. Rate the resulting sentences for
grammaticality and preservation of meaning. Repeat the process; does the second round ofPERCEPTION
In which we connect the computer to the raw, unwashed world.
Perception provides agents with information about the world they inhabit by interpreting the
PERCEPTION
response of sensors. A sensor measures some aspect of the environment in a form that can
SENSOR
be used as input by an agent program. The sensor could be as simple as a switch, which gives
one bit telling whether it is on or off, or as complex as the eye. A variety of sensory modalities
are available to artiﬁcial agents. Those they share with humans include vision, hearing, and
touch. Modalities that are not available to the unaided human include radio, infrared, GPS,
and wireless signals. Some robots do active sensing, meaning they send out a signal, such as
radar or ultrasound, and sense the reﬂection of this signal off of the environment. Rather than
trying to cover all of these, this chapter will cover one modality in depth: vision.
ability distribution P(E | S) over the evidence that its sensors provide, given a state of the
world. Bayes’ rule can then be used to update the estimation of the state.
For vision, the sensor model can be broken into two components: An object model
OBJECT MODEL
describes the objects that inhabit the visual world—people, buildings, trees, cars, etc. The
RENDERING MODEL
that produce the stimulus from the world. Rendering models are quite accurate, but they are
ambiguous. For example, a white object under low light may appear as the same color as a
black object under intense light. A small nearby object may look the same as a large distant
object. Without additional evidence, we cannot tell if the image that ﬁlls the frame is a toy
Godzilla or a real monster.
Ambiguity can be managed with prior knowledge—we know Godzilla is not real, so
the image must be a toy—or by selectively choosing to ignore the ambiguity. For example,
the vision system for an autonomous car may not be able to interpret objects that are far in
the distance, but the agent can choose to ignore the problem, because it is unlikely to crash
into an object that is miles away.Image Formation
sors. For example, fruit ﬂies (Drosophila) are in part reﬂex agents: they have cervical giant
ﬁbers that form a direct pathway from their visual system to the wing muscles that initiate an
escape response—an immediate reaction, without deliberation. Flies and many other ﬂying
system extracts an estimate of the distance to the object, and the control system adjusts the
wing muscles accordingly, allowing very fast changes of direction, with no need for a detailed
model of the object.
Compared to the data from other sensors (such as the single bit that tells the vacuum
robot that it has bumped into a wall), visual observations are extraordinarily rich, both in
the detail they can reveal and in the sheer amount of data they produce. A video camera
stimulus should be considered to help the agent make good action choices, and which aspects
should be ignored?
Vision—and all perception—serves to further the agent’s goals, not as
an end to itself.
We can characterize three broad approaches to the problem. The feature extraction
FEATURE
EXTRACTION
approach, as exhibited by Drosophila, emphasizes simple computations applied directly to
the sensor observations. In the recognition approach an agent draws distinctions among the
RECOGNITION
objects it encounters based on visual and other information. Recognition could mean labeling
each image with a yes or no as to whether it contains food that we should forage, or contains
Grandma’s face. Finally, in the reconstruction approach an agent builds a geometric model
RECONSTRUCTION
of the world from an image or a set of images.
dressing these approaches. Understanding these methods requires an understanding of the
processes by which images are formed. Therefore, we now cover the physical and statistical
phenomena that occur in the production of an image.
IMAGE FORMATION
Imaging distorts the appearance of objects. For example, a picture taken looking down a
long straight set of railway tracks will suggest that the rails converge and meet. As another
example, if you hold your hand in front of your eye, you can block out the moon, which is
not smaller than your hand. As you move your hand back and forth or tilt it, your hand will
of these effects are essential for both recognition and reconstruction.
Images without lenses: The pinhole camera
SCENE
image. In the eye, the image is formed on the retina, which consists of two types of cells:
IMAGEChapter
Perception
Imaging distorts geometry. Parallel lines appear to meet in the distance, as
in the image of the railway tracks on the left. In the center, a small hand blocks out most of
a large moon. On the right is a foreshortening effect: the hand is tilted away from the eye,
making it appear shorter than in the center ﬁgure.
million cones. Cones, which are essential for color vision, are of three main types, each of
which is sensitive to a different set of wavelengths. In cameras, the image is formed on an
image plane, which can be a piece of ﬁlm coated with silver halides or a rectangular grid
PIXEL
effect, whose strength depends on the wavelength of the photon. The output of the sensor
is the sum of all effects due to photons observed in some time window, meaning that image
sensors report a weighted average of the intensity of light arriving at the sensor.
To see a focused image, we must ensure that all the photons from approximately the
same spot in the scene arrive at approximately the same point in the image plane. The simplest
way to form a focused image is to view stationary objects with a pinhole camera, which
PINHOLE CAMERA
consists of a pinhole opening, O, at the front of a box, and an image plane at the back of the
enough then nearby photons in the scene will be nearby in the image plane, and the image
will be in focus.
The geometry of scene and image is easiest to understand with the pinhole camera. We
P in the scene, with coordinates (X, Y, Z). P gets projected to the point P ′ in the image
plane with coordinates (x, y, z). If f is the distance from the pinhole to the image plane, then
by similar triangles, we can derive the following equations:
−x
f
= X
Z , −y
f
= Y
Z
⇒
x = −fX
Z
, y = −fY
Z
.
PERSPECTIVE
PROJECTION
that the Z in the denominator means that the farther away an object is, the smaller its imageImage Formation
f
Image 
plane
P′
Y
X
Z
P
Pinhole
era receives light from a the small range of directions that passes through the pinhole. If the
pinhole is small enough, the result is a focused image at the back of the pinhole. The process
of projection means that large, distant objects look the same as smaller, nearby objects. Note
that the image is projected upside down.
will be. Also, note that the minus signs mean that the image is inverted, both left–right and
up–down, compared with the scene.
Under perspective projection, distant objects look small. This is what allows you to
scene in the direction (U, V, W) and passing through the point (X0, Y0, Z0) can be described
Different choices of (X0, Y0, Z0) yield different lines parallel to one another. The projection

f X0 + λU
Z0 + λW , f Y0 + λV
Z0 + λW

.
the image points are nearly the same, whatever the value of (X0, Y0, Z0) (again, think railway
VANISHING POINT
lines with direction (U, V, W). Lines with the same direction share the same vanishing point.
Lens systems
The drawback of the pinhole camera is that we need a small pinhole to keep the image in
focus. But the smaller the pinhole, the fewer photons get through, meaning the image will be
dark. We can gather more photons by keeping the pinhole open longer, but then we will get
motion blur—objects in the scene that move will appear blurred because they send photons
MOTION BLUR
to multiple locations on the image plane. If we can’t keep the pinhole open longer, we can
try to make it bigger. More light will enter, but light from a small patch of object in the scene
will now be spread over a patch on the image plane, causing a blurred image.Chapter
Perception
Iris
Cornea
Fovea
Visual Axis
Optical Axis
Lens
Retina
Optic Nerve
Lens
System
Image plane
Light Source
Lenses collect the light leaving a scene point in a range of directions, and steer
it all to arrive at a single point on the image plane. Focusing works for points lying close to
a focal plane in space; other points will not be focused properly. In cameras, elements of
the lens system move to change the focal plane, whereas in the eye, the shape of the lens is
changed by specialized muscles.
Vertebrate eyes and modern cameras use a lens system to gather sufﬁcient light while
LENS
keeping the image in focus. A large opening is covered with a lens that focuses light from
nearby object locations down to nearby locations in the image plane. However, lens systems
have a limited depth of ﬁeld: they can focus light only from points that lie within a range
DEPTH OF FIELD
of depths (centered around a focal plane). Objects outside this range will be out of focus in
FOCAL PLANE
camera, the lenses move back and forth.
Scaled orthographic projection
Perspective effects aren’t always pronounced. For example, spots on a distant leopard may
look small because the leopard is far away, but two spots that are next to each other will have
about the same size. This is because the difference in distance to the spots is small compared
to the distance to them, and so we can simplify the projection model. The appropriate model
is scaled orthographic projection. The idea is as follows: If the depth Z of points on the
SCALED
ORTHOGRAPHIC
PROJECTION
object varies within some range Z0 ± ΔZ, with ΔZ ≪Z0, then the perspective scaling
the scene coordinates (X, Y, Z) to the image plane become x = sX and y = sY . Scaled
orthographic projection is an approximation that is valid only for those parts of the scene with
not much internal depth variation. For example, scaled orthographic projection can be a good
model for the features on the front of a distant building.
Light and shading
The brightness of a pixel in the image is a function of the brightness of the surface patch in
linearities at the extremes of light and dark, but are linear in the middle). Image brightness isImage Formation
Specularities
Cast shadow
Diffuse reflection, bright
Diffuse reflection, dark
A variety of illumination effects. There are specularities on the metal spoon
and on the milk. The bright diffuse surface is bright because it faces the light direction. The
dark diffuse surface is dark because it is tangential to the illumination direction. The shadows
appear at surface points that cannot see the light source. Photo by Mike Linksvayer (mlinksva
on ﬂickr).
a strong, if ambiguous, cue to the shape of an object, and from there to its identity. People are
the object’s properties. The ﬁrst cause is overall intensity of the light. Even though a white
OVERALL INTENSITY
guish relative brightness well, and perceive the white object as white. Second, different points
in the scene may reﬂect more or less of the light. Usually, the result is that people perceive
REFLECT
these points as lighter or darker, and so see texture or markings on the object. Third, surface
patches facing the light are brighter than surface patches tilted away from the light, an effect
known as shading. Typically, people can tell that this shading comes from the geometry of
SHADING
the object, but sometimes get shading and markings mixed up. For example, a streak of dark
makeup under a cheekbone will often look like a shading effect, making the face look thinner.
DIFFUSE
REFLECTION
ters light evenly across the directions leaving a surface, so the brightness of a diffuse surface
tion, and rough stone are diffuse. Mirrors are not diffuse, because what you see depends on
the direction in which you look at the mirror. The behavior of a perfect mirror is known as
specular reﬂection. Some surfaces—such as brushed metal, plastic, or a wet ﬂoor—display
SPECULAR
REFLECTION
small patches where specular reﬂection has occurred, called specularities. These are easy to
SPECULARITIES
to model all surfaces as being diffuse with specularities.Chapter
Perception
A
B
θ
θ
Two surface patches are illuminated by a distant point source, whose rays are
collects less energy, because it cuts fewer light rays per unit surface area. Patch B, facing the
The main source of illumination outside is the sun, whose rays all travel parallel to one
another. We model this behavior as a distant point light source. This is the most important
DISTANT POINT
LIGHT SOURCE
model of lighting, and is quite effective for indoor scenes as well as outdoor scenes. The
illumination direction and the normal to the surface.
A diffuse surface patch illuminated by a distant point light source will reﬂect some
fraction of the light it collects; this fraction is called the diffuse albedo. White paper and
DIFFUSE ALBEDO
the velvet or the pores of the charcoal). Lambert’s cosine law states that the brightness of a
LAMBERT’S COSINE
LAW
diffuse patch is given by
bright image pixels come from surface patches that face the light directly and dark pixels
reached by the light source, then it is in shadow. Shadows are very seldom a uniform black,
SHADOW
because the shadowed surface receives some light from other sources. Outdoors, the most
important such source is the sky, which is quite bright. Indoors, light reﬂected from other
surfaces illuminates shadowed patches. These interreﬂections can have a signiﬁcant effect
INTERREFLECTIONS
on the brightness of other surfaces, too. These effects are sometimes modeled by adding a
constant ambient illumination term to the predicted intensity.
AMBIENT
ILLUMINATIONColor
Fruit is a bribe that a tree offers to animals to carry its seeds around. Trees have evolved to
have fruit that turns red or yellow when ripe, and animals have evolved to detect these color
changes. Light arriving at the eye has different amounts of energy at different wavelengths;
this can be represented by a spectral energy density function. Human eyes respond to light in
have peak receptiveness at 420mm (blue), 540nm (green), and 570nm (red). The human eye
can capture only a small fraction of the full spectral energy density function—but it is enough
to tell when the fruit is ripe.
The principle of trichromacy states that for any spectral energy density, no matter how
PRINCIPLE OF
TRICHROMACY
complicated, it is possible to construct another spectral energy density consisting of a mixture
of just three colors—usually red, green, and blue—such that a human can’t tell the difference
between the two. That means that our TVs and computer displays can get by with just the
fact, human observers are quite good at ignoring the effects of different colored lights and are
able to estimate the color of the surface under white light, an effect known as color constancy.
COLOR CONSTANCY
Quite accurate color constancy algorithms are now available; simple versions show up in the
“auto white balance” function of your camera. Note that if we wanted to build a camera for
color receptors of the crustacean.
We have seen how light reﬂects off objects in the scene to form an image consisting of, say,
there is a lot of data to deal with. So how do we get started on analyzing this data?
operations because they are the ﬁrst in a pipeline of operations. Early vision operations are
characterized by their local nature (they can be carried out in one part of the image without
regard for anything more than a few pixels away) and by their lack of knowledge: we can
perform these operations without consideration of the objects that might be present in the
hardware—either in a graphics processor unit (GPU) or an eye. We will then look at oneChapter
Perception
A
B
Edge detection
Edges are straight lines or curves in the image plane across which there is a “signiﬁcant”
EDGE
change in image brightness. The goal of edge detection is to abstract away from the messy,
The motivation is that edge contours in the image correspond to important scene contours.
not distinguish between these different types of scene discontinuities; later processing will.
is a difference between the output and an ideal line drawing. There are gaps where no edge
appears, and there are “noise” edges that do not correspond to anything of signiﬁcance in the
scene. Later stages of processing will have to correct for these errors.
How do we detect edges in an image? Consider the proﬁle of image brightness along a
Edges correspond to locations in images where the brightness undergoes a sharp change,
so a naive idea would be to differentiate the image and look for places where the magnitude
These arise because of the presence of noise in the image. If we smooth the image ﬁrst, the
spurious peaks are diminished, as we see in the bottom of the ﬁgure.(a)
(b)
(a) Photograph of a stapler. (b) Edges computed from (a).
to edges, but the function is noisy. Bottom: The derivative of a smoothed version of the
intensity, (I ∗Gσ)′, which can be computed in one step as the convolution I ∗G′
σ. The noisy
The measurement of brightness at a pixel in a CCD camera is based on a physical
process involving the absorption of photons and the release of electrons; inevitably there
will be statistical ﬂuctuations of the measurement—noise. The noise can be modeled withChapter
Perception
a Gaussian probability distribution, with each pixel independent of the others. One way to
smooth an image is to assign to each pixel the average of its neighbors. This tends to cancel
out extreme values. But how many neighbors should we consider—one pixel away, or two, or
more? One good answer is a weighted average that weights the nearest pixels the most, then
gradually decreases the weight for more distant pixels. The Gaussian ﬁlter does just that.
GAUSSIAN FILTER
(Users of Photoshop recognize this as the Gaussian blur operation.) Recall that the Gaussian
Nσ(x) =
√
in one dimension, or
Nσ(x, y) =
in two dimensions.
The application of the Gaussian ﬁlter replaces the intensity I(x0, y0) with the sum, over all
(x, y) pixels, of I(x, y) Nσ(d), where d is the distance from (x0, y0) to (x, y). This kind of
weighted sum is so common that there is a special name and notation for it. We say that the
function h is the convolution of two functions f and g (denoted f ∗g) if we have
CONVOLUTION
h(x) = (f ∗g)(x) =
+∞


f(u) g(x −u)
in one dimension, or
h(x, y) = (f ∗g)(x, y) =
+∞


+∞


f(u, v) g(x −u, y −v)
in two.
So the smoothing function is achieved by convolving the image with the Gaussian, I ∗Nσ. A
larger amount, but at the loss of some detail. Because the Gaussian’s inﬂuence fades quickly
at a distance, we can replace the ±∞in the sums with ±3σ.
gle operation. It is a theorem that for any functions f and g, the derivative of the convolution,
(f ∗g)′, is equal to the convolution with the derivative, f ∗(g′). So rather than smoothing
the image and then differentiating, we can just convolve the image with the derivative of the
smoothing function, N ′
σ. We then mark as edges those peaks in the response that are above
some threshold.
ering the image brightness as a scalar function of the variables x, y, its gradient is a vector
∇I =
 ∂I
∂x
∂I
∂y
 
=

 Ix
Iy

.
Edges correspond to locations in images where the brightness undergoes a sharp change, and
so the magnitude of the gradient, ∥∇I∥, should be large at an edge point. Of independent
interest is the direction of the gradient
∇I
∥∇I∥=


.
ORIENTATIONAs in one dimension, to form the gradient we don’t compute ∇I, but rather ∇(I ∗Nσ),
the gradient after smoothing the image by convolving it with a Gaussian. And again, the
shortcut is that this is equivalent to convolving the image with the partial derivatives of a
Gaussian. Once we have computed the gradient, we can obtain edges by ﬁnding edge points
and linking them together. To tell whether a point is an edge point, we must look at other
points a small distance forward and back along the direction of the gradient. If the gradient
magnitude at one of these points is larger, then we could get a better edge point by shifting
the edge curve very slightly. Furthermore, if the gradient magnitude is too small, the point
cannot be an edge point. So at an edge point, the gradient magnitude is a local maximum
along the direction of the gradient, and the gradient magnitude is above a suitable threshold.
Once we have marked edge pixels by this algorithm, the next stage is to link those pixels
that belong to the same edge curves. This can be done by assuming that any two neighboring
edge pixels with consistent orientations must belong to the same edge curve.
Texture
In everyday language, texture is the visual feel of a surface—what you see evokes what
TEXTURE
the surface might feel like if you touched it (“texture” has the same root as “textile”). In
computational vision, texture refers to a spatially repeating pattern on a surface that can be
sensed visually. Examples include the pattern of windows on a building, stitches on a sweater,
spots on a leopard, blades of grass on a lawn, pebbles on a beach, and people in a stadium.
Sometimes the arrangement is quite periodic, as in the stitches on a sweater; in other cases,
such as pebbles on a beach, the regularity is only statistical.
Whereas brightness is a property of individual pixels, the concept of texture makes sense
only for a multipixel patch. Given such a patch, we could compute the orientation at each
pixel, and then characterize the patch by a histogram of orientations. The texture of bricks in
a wall would have two peaks in the histogram (one vertical and one horizontal), whereas the
texture of spots on a leopard’s skin would have a more uniform distribution of orientations.
makes texture an important clue for object recognition, because other clues, such as edges,
can yield different results in different lighting conditions.
In images of textured objects, edge detection does not work as well as it does for smooth
objects. This is because the most important edges can be lost among the texture elements.
Quite literally, we may miss the tiger for the stripes. The solution is to look for differences in
texture properties, just the way we look for differences in brightness. A patch on a tiger and
a patch on the grassy background will have very different orientation histograms, allowing us
to ﬁnd the boundary curve between them.
Optical ﬂow
Next, let us consider what happens when we have a video sequence, instead of just a single
static image. When an object in the video is moving, or when the camera is moving relative
to an object, the resulting apparent motion in the image is called optical ﬂow. Optical ﬂow
OPTICAL FLOW
describes the direction and speed of motion of features in the image—the optical ﬂow of aChapter
Perception
(a)
(b)
nation levels. The gradient vector ﬁeld (at every eighth pixel) is plotted on top of each one.
Notice that, as the light gets darker, all the gradient vectors get shorter. The vectors do not
rotate, so the gradient orientations do not change.
video of a race car would be measured in pixels per second, not miles per hour. The optical
ﬂow encodes useful information about scene structure. For example, in a video of scenery
taken from a moving train, distant objects have slower apparent motion than close objects;
thus, the rate of apparent motion can tell us something about distance. Optical ﬂow also
of a tennis player. In (c) we display the optical ﬂow vectors computed from these images,
showing that the racket and front leg are moving fastest.
The optical ﬂow vector ﬁeld can be represented at any point (x, y) by its components
vx(x, y) in the x direction and vy(x, y) in the y direction. To measure optical ﬂow we need to
is based on the fact that image patches around corresponding points have similar intensity
patterns. Consider a block of pixels centered at pixel p, (x0, y0), at time t0. This block
of pixels is to be compared with pixel blocks centered at various candidate pixels at (x0 +
Dx, y0 + Dy) at time t0 + Dt. One possible measure of similarity is the sum of squared
differences (SSD):
SUM OF SQUARED
DIFFERENCES
SSD(Dx, Dy) =


(x,y)
Here, (x, y) ranges over pixels in the block centered at (x0, y0). We ﬁnd the (Dx, Dy) that
that for this to work, there needs to be some texture or variation in the scene. If one is lookingresponding to the displacement from one frame to the other. Note how the movement of
the tennis racket and the front leg is captured by the directions of the arrows. (Courtesy of
Thomas Brox.)
algorithms for measuring optical ﬂow rely on a variety of additional constraints when the
scene is only partially textured.
Segmentation of images
Segmentation is the process of breaking an image into regions of similar pixels. Each image
SEGMENTATION
REGIONS
pixel can be associated with certain visual properties, such as brightness, color, and texture.
Within an object, or a single part of an object, these attributes vary relatively little, whereas
tributes. There are two approaches to segmentation, one focusing on detecting the boundaries
to formalize the problem of detecting boundary curves is as a machine learning classiﬁcation
problem. Based on features from a local neighborhood, we want to compute the probability
Pb(x, y, θ) that indeed there is a boundary curve at that pixel along that orientation. Consider
If there is a boundary at (x, y, θ) the two half disks might be expected to differ signiﬁcantly
on differences in histograms of brightness, color, and texture values measured in these two
half disks, and then trained a classiﬁer. For this they used a data set of natural images where
humans had marked the “ground truth” boundaries, and the goal of the classiﬁer was to mark
exactly those boundaries marked by humans and no others.
Boundaries detected by this technique turn out to be signiﬁcantly better than those found
only local context and does not use global consistency constraints.Chapter
Perception
(a)
(b)
(c)
(d)
(a) Original image. (b) Boundary contours, where the higher the Pb value,
the darker the contour. (c) Segmentation into regions, corresponding to a ﬁne partition of
sponding to a coarser partition of the image, resulting in fewer regions. (Courtesy of Pablo
Arbelaez, Michael Maire, Charles Fowlkes, and Jitendra Malik)
The alternative approach is based on trying to “cluster” the pixels into regions based on
problem. The nodes of the graph correspond to pixels, and edges to connections between
pixels. The weight Wij on the edge connecting a pair of pixels i and j is based on how similar
the two pixels are in brightness, color, texture, etc. Partitions that minimize a normalized cut
criterion are then found. Roughly speaking, the criterion for partitioning the graph is to
minimize the sum of weights of connections across the groups of pixels and maximize the
sum of weights of connections within the groups.
cannot be expected to deliver the ﬁnal correct boundaries of all the objects in the scene. To
in the scene. Representing this knowledge is a topic of active research. A popular strategy is
SUPERPIXELS
ﬁnd it easier to deal with hundreds of superpixels rather than millions of raw pixels. How to
OBJECT RECOGNITION BY APPEARANCE
Appearance is shorthand for what an object tends to look like. Some object categories—for
APPEARANCE
example, baseballs—vary rather little in appearance; all of the objects in the category look
about the same under most circumstances. In this case, we can compute a set of features
describing each class of images likely to contain the object, then test it with a classiﬁer.Object Recognition by Appearance
Other object categories—for example, houses or ballet dancers—vary greatly. A house
can have different size, color, and shape and can look different from different angles. A dancer
looks different in each pose, or when the stage lights change colors. A useful abstraction is to
say that some objects are made up of local patterns which tend to move around with respect to
one another. We can then ﬁnd the object by looking at local histograms of detector responses,
which expose whether some part is present but suppress the details of where it is.
Testing each class of images with a learned classiﬁer is an important general recipe.
It works extremely well for faces looking directly at the camera, because at low resolution
and under reasonable lighting, all such faces look quite similar. The face is round, and quite
bright compared to the eye sockets; these are dark, because they are sunken, and the mouth is
a dark slash, as are the eyebrows. Major changes of illumination can cause some variations in
this pattern, but the range of variation is quite manageable. That makes it possible to detect
face positions in an image that contains faces. Once a computational challenge, this feature
is now commonplace in even inexpensive digital cameras.
For the moment, we will consider only faces where the nose is oriented vertically; we
will deal with rotated faces below. We sweep a round window of ﬁxed size over the image,
compute features for it, and present the features to a classiﬁer. This strategy is sometimes
called the sliding window. Features need to be robust to shadows and to changes in brightness
SLIDING WINDOW
caused by illumination changes. One strategy is to build features out of gradient orientations.
Another is to estimate and correct the illumination in each image window. To ﬁnd faces of
different sizes, repeat the sweep over larger or smaller versions of the image. Finally, we
postprocess the responses across scales and locations to produce the ﬁnal set of detections.
Postprocessing is important, because it is unlikely that we have chosen a window size
that is exactly the right size for a face (even if we use multiple sizes). Thus, we will likely
have several overlapping windows that each report a match for a face. However, if we use
a classiﬁer that can report strength of response (for example, logistic regression or a support
vector machine) we can combine these partial overlapping matches at nearby locations to
and scales. To search rotations as well, we use two steps. We train a regression procedure
to estimate the best orientation of any face present in a window. Now, for each window, we
estimate the orientation, reorient the window, then test whether a vertical face is present with
images, and rotated face windows are easy to build (just rotate a window from a training
data set). One trick that is widely used is to take each example window, then produce new
examples by changing the orientation of the window, the center of the window, or the scale
very slightly. This is an easy way of getting a bigger data set that reﬂects real images fairly
well; the trick usually improves performance signiﬁcantly. Face detectors built along these
lines now perform very well for frontal faces (side views are harder).Chapter
Perception
Image
Responses
Detections
suppresion
Correct
illumination
Estimate
orientation
Rotate
window
Features
Classifier
Face ﬁnding systems vary, but most follow the architecture illustrated in
suppression to ﬁnd the strongest local response. The responses are obtained by the process
illustrated on the bottom. We sweep a window of ﬁxed size over larger and smaller versions
of the image, so as to ﬁnd smaller or larger faces, respectively. The illumination in the
window is corrected, and then a regression engine (quite often, a neural net) predicts the
orientation of the face. The window is corrected to this orientation and then presented to a
classiﬁer. Classiﬁer outputs are then postprocessed to ensure that only one face is placed at
each location in the image.
Complex appearance and pattern elements
Many objects produce much more complex patterns than faces do. This is because several
• Foreshortening, which causes a pattern viewed at a slant to be signiﬁcantly distorted.
• Aspect, which causes objects to look different when seen from different directions.
Even as simple an object as a doughnut has several aspects; seen from the side, it looks
like a ﬂattened oval, but from above it is an annulus.
• Occlusion, where some parts are hidden from some viewing directions. Objects can
occlude one another, or parts of an object can occlude other parts, an effect known as
• Deformation, where internal degrees of freedom of the object change its appearance.
For example, people can move their arms and legs around, generating a very wide range
of different body conﬁgurations.
However, our recipe of searching across location and scale can still work. This is because
some structure will be present in the images produced by the object. For example, a picture
of a car is likely to show some of headlights, doors, wheels, windows, and hubcaps, though
they may be in somewhat different arrangements in different pictures. This suggests modeling
objects with pattern elements—collections of parts. These pattern elements may move aroundObject Recognition by Appearance
Foreshortening
Aspect
Occlusion
Deformation
Sources of appearance variation. First, elements can foreshorten, like the
circular patch on the top left. This patch is viewed at a slant, and so is elliptical in the
image. Second, objects viewed from different directions can change shape quite dramatically,
a phenomenon known as aspect. On the top right are three different aspects of a doughnut.
Occlusion causes the handle of the mug on the bottom left to disappear when the mug is
occlusion. Finally, on the bottom right, some objects can deform dramatically.
with respect to one another, but if most of the pattern elements are present in about the right
place, then the object is present. An object recognizer is then a collection of features that can
tell whether the pattern elements are present, and whether they are in about the right place.
The most obvious approach is to represent the image window with a histogram of the
pattern elements that appear there. This approach does not work particularly well, because
too many patterns get confused with one another. For example, if the pattern elements are
color pixels, the French, UK, and Netherlands ﬂags will get confused because they have
approximately the same color histograms, though the colors are arranged in very different
ways. Quite simple modiﬁcations of histograms yield very useful features. The trick is to
preserve some spatial detail in the representation; for example, headlights tend to be at the
successful in a wide variety of recognition applications; we will survey pedestrian detection.
Pedestrian detection with HOG features
cation problem, because cars that can automatically detect and avoid pedestrians might save
many lives. Pedestrians wear many different kinds of clothing and appear in many different
conﬁgurations, but, at relatively low resolution, pedestrians can have a fairly characteristic
appearance. The most usual cases are lateral or frontal views of a walk. In these cases,Chapter
Perception
Image
Orientation
histograms
Positive
components
Negative
components
Local orientation histograms are a powerful feature for recognizing even
tation histograms for patches. We then apply a classiﬁer such as a support vector machine
to ﬁnd the weights for each histogram that best separate the positive examples of pedestrians
of a person. The negative components are less clear; they represent all the patterns that are
we see either a “lollipop” shape — the torso is wider than the legs, which are together in
the stance phase of the walk — or a “scissor” shape — where the legs are swinging in the
walk. We expect to see some evidence of arms and legs, and the curve around the shoulders
and head also tends to visible and quite distinctive. This means that, with a careful feature
There isn’t always a strong contrast between the pedestrian and the background, so it
is better to use orientations than edges to represent the image window. Pedestrians can move
their arms and legs around, so we should use a histogram to suppress some spatial detail in
the feature. We break up the window into cells, which could overlap, and build an orientation
shoulders curve is at the top of the window or at the bottom, but will not change if the head
moves slightly.
One further trick is required to make a good feature. Because orientation features are
means that the distinctive curves on the boundary of a pedestrian are treated in the same way
as ﬁne texture detail in clothing or in the background, and so the signal may be submerged
in noise. We can recover contrast information by counting gradient orientations with weights
that reﬂect how signiﬁcant a gradient is compared to other gradients in the same cell. We
will write || ∇Ix || for the gradient magnitude at point x in the image, write C for the cell
whose histogram we wish to compute, and write wx,C for the weight that we will use for theReconstructing the 3D World
Another example of object recognition, this one using the SIFT feature
ages of a shoe and a telephone that serve as object models. In the center, a test image. On the
right, the shoe and the telephone have been detected by: ﬁnding points in the image whose
SIFT feature descriptions match a model; computing an estimate of pose of the model; and
verifying that estimate. A strong match is usually veriﬁed with rare false positives. Images
orientation at x for this cell. A natural choice of weight is
wx,C =
||∇Ix ||

pared to their neighbors get a large weight. The resulting feature is usually called a HOG
feature (for Histogram Of Gradient orientations).
HOG FEATURE
This feature construction is the main way in which pedestrian detection differs from
face detection. Otherwise, building a pedestrian detector is very like building a face detector.
The detector sweeps a window across the image, computes features for that window, then
most applications, the scale and orientation of typical pedestrians is known. For example, in
driving applications in which a camera is ﬁxed to the car, we expect to view mainly vertical
pedestrians, and we are interested only in nearby pedestrians. Several pedestrian data sets
have been published, and these can be used for training the classiﬁer.
similar techniques can be used to ﬁnd a variety of objects in different contexts.
RECONSTRUCTING THE 3D WORLD
representation of the scene. The fundamental question is this: Given that all points in the
scene that fall along a ray to the pinhole are projected to the same point in the image, how doChapter
Perception
late to ﬁnd the position of a point in the scene.
• We can exploit background knowledge about the physical scene that gave rise to the
image. Given an object model P(Scene) and a rendering model P(Image | Scene), we
can compute a posterior distribution P(Scene | Image).
There is as yet no single uniﬁed theory for scene reconstruction. We survey eight commonly
used visual cues: motion, binocular stereopsis, multiple views, texture, shading, contour,
and familiar objects.
Motion parallax
the image, optical ﬂow, can be a source of information for both the movement of the camera
and depth in the scene. To understand this, we state (without proof) an equation that relates
the optical ﬂow to the viewer’s translational velocity T and the depth in the scene.
The components of the optical ﬂow ﬁeld are
vx(x, y) = −Tx + xTz
Z(x, y)
,
vy(x, y) = −Ty + yTz
Z(x, y)
,
image at (x, y).
Note that both components of the optical ﬂow, vx(x, y) and vy(x, y), are zero at the
FOCUS OF
EXPANSION
ﬁeld. Suppose we change the origin in the x–y plane to lie at the focus of expansion; then
the expressions for optical ﬂow take on a particularly simple form. Let (x′, y′) be the new
vx(x′, y′) =
x′Tz
Z(x′, y′),
vy(x′, y′) =
y′Tz
Z(x′, y′) .
every object in the scene was twice as big and at twice the distance to the camera, the optical
ﬂow ﬁeld would be exactly the same. But we can still extract quite useful information.
instantaneous optical ﬂow ﬁeld cannot provide either the distance Z or the velocity
component Tz, it can provide the ratio of the two and can therefore be used to control
the landing approach. There is considerable experimental evidence that many different
animal species exploit this cue.
value of either of these, but by considering the inverse of the ratio of the optical ﬂow
motion parallax, one we use when we look out of the side window of a moving car or
train and infer that the slower moving parts of the landscape are farther away.Reconstructing the 3D World
Perceived object
Right image
(a)
(b)
Left image
Disparity
Left
Right
Translating a camera parallel to the image plane causes image features to
move in the camera plane. The disparity in positions that results is a cue to depth. If we
superimpose left and right image, as in (b), we see the disparity.
Binocular stereopsis
Most vertebrates have two eyes. This is useful for redundancy in case of a lost eye, but it
helps in other ways too. Most prey have eyes on the side of the head to enable a wider ﬁeld
of vision. Predators have the eyes in the front, enabling them to use binocular stereopsis.
BINOCULAR
STEREOPSIS
The idea is similar to motion parallax, except that instead of using images over time, we use
two (or more) images separated in space. Because a given feature in the scene will be in a
there will be a disparity in the location of the image feature in the two images. You can see
DISPARITY
image and to the right in the left image.
Note that to measure disparity we need to solve the correspondence problem, that is,
determine for a point in the left image, the point in the right image that results from the
projection of the same scene point. This is analogous to what one has to do in measuring
comparing blocks of pixels around corresponding points using the sum of squared differences.
In practice, we use much more sophisticated algorithms, which exploit additional constraints.
Assuming that we can measure disparity, how does this yield information about depth
in the scene? We will need to work out the geometrical relationship between disparity and
depth. First, we will consider the case when both the eyes (or cameras) are looking forward
with their optical axes parallel. The relationship of the right camera to the left camera is then
ﬂow equations from the previous section, if we think of this as resulting from a translationChapter
Perception
b
δZ
P
P0
PR
PL
Left
eye
Z
Right
eye
θ
jection of the two eyes are b apart, and the optical axes intersect at the ﬁxation point P0. The
point P in the scene projects to points PL and PR in the two eyes. In angular terms, the
the horizontal disparity is equal to the ratio of the baseline to the depth, and the vertical
disparity is zero. Given that we know b, we can measure H and recover the depth Z.
Under normal viewing conditions, humans ﬁxate; that is, there is some point in the
FIXATE
at a point P0, which is at a distance Z from the midpoint of the eyes. For convenience,
we will compute the angular disparity, measured in radians. The disparity at the point of
compute the angular displacements of the left and right images of P, which we will call PL
Z −
2Z2
Z2 .
BASELINEReconstructing the 3D World
(a) Four frames from a video sequence in which the camera is moved and
rotated relative to the object. (b) The ﬁrst frame of the sequence, annotated with small boxes
highlighting the features found by the feature detector. (Courtesy of Carlo Tomasi.)
Multiple views
Shape from optical ﬂow or binocular disparity are two instances of a more general framework,
that of exploiting multiple views for recovering depth. In computer vision, there is no reason
for us to be restricted to differential motion or to only use two cameras converging at a ﬁxation
point. Therefore, techniques have been developed that exploit the information available in
multiple views, even from hundreds or thousands of cameras. Algorithmically, there are
three subproblems that need to be solved:
• The correspondence problem, i.e., identifying features in the different images that are
• The relative orientation problem, i.e., determining the transformation (rotation and
translation) between the coordinate systems ﬁxed to the different cameras.
• The depth estimation problem, i.e., determining the depths of various points in the world
for which image plane projections were available in at least two views
nied by numerically stable algorithms for solving for relative orientations and scene depth, is
one of the success stories of computer vision. Results from one such approach due to Tomasi
Texture
Earlier we saw how texture was used for segmenting objects. It can also be used to estimate
texture elements, or texels, in the image. All the paving tiles in (a) are identical in the scene.
TEXEL
They appear different in the image for two reasons:Chapter
Perception
(a)
(b)
then distance ones are viewed at an angle that is farther off the perpendicular, and so
are more foreshortened. The magnitude of the foreshortening effect is proportional to
the texel.
Researchers have developed various algorithms that try to exploit the variation in the
appearance of the projected texels as a basis for determining surface normals. However, the
accuracy and applicability of these algorithms is not anywhere as general as those based on
using multiple views.
Shading
Shading—variation in the intensity of light received from different portions of a surface in a
scene—is determined by the geometry of the scene and by the reﬂectance properties of the
surfaces. In computer graphics, the objective is to compute the image brightness I(x, y),
given the scene geometry and reﬂectance properties of the objects in the scene. Computer
vision aims to invert the process—that is, to recover the geometry and reﬂectance properties,
given the image brightness I(x, y). This has proved to be difﬁcult to do in anything but the
simplest cases.
toward the light source, the surface is brighter, and if it points away, the surface is darker.
We cannot conclude that a dark patch has its normal pointing away from the light; instead,
it could have low albedo. Generally, albedo changes quite quickly in images, and shadingReconstructing the 3D World
(a)
(b)
ery of the surface orientation. The computed surface orientation is indicated by overlaying a
black circle and pointer, transformed as if the circle were painted on the surface at that point.
(b) Recovery of shape from texture for a curved surface (white circle and pointer this time).
changes rather slowly, and humans seem to be quite good at using this observation to tell
whether low illumination, surface orientation, or albedo caused a surface patch to be dark.
To simplify the problem, let us assume that the albedo is known at every surface point. It
is still difﬁcult to recover the normal, because the image brightness is one measurement but
the normal has two unknown parameters, so we cannot simply solve for the normal. The key
to this situation seems to be that nearby normals will be similar, because most surfaces are
smooth—they do not have sharp changes.
The real difﬁculty comes in dealing with interreﬂections. If we consider a typical indoor
scene, such as the objects inside an ofﬁce, surfaces are illuminated not only by the light
sources, but also by the light reﬂected from other surfaces in the scene that effectively serve
as secondary light sources. These mutual illumination effects are quite signiﬁcant and make
it quite difﬁcult to predict the relationship between the normal and the image brightness. Two
surface patches with the same normal might have quite different brightnesses, because one
receives light reﬂected from a large white wall and the other faces only a dark bookcase.
Despite these difﬁculties, the problem is important. Humans seem to be able to ignore the
effects of interreﬂections and get a useful perception of shape from shading, but we know
frustratingly little about algorithms to do this.
Contour
dimensional shape and layout. How? It is a combination of recognition of familiar objects in
the scene and the application of generic constraints such as the following:
• Occluding contours, such as the outlines of the hills. One side of the contour is nearerChapter
Perception
An evocative line drawing. (Courtesy of Isha Malik.)
contour is ﬁgure (nearer), and which is ground (farther). At an occluding contour, the
line of sight is tangential to the surface in the scene.
• Position on the ground plane. Humans, like many other terrestrial animals, are very
often in a scene that contains a ground plane, with various objects at different locations
GROUND PLANE
on this plane. Because of gravity, typical objects don’t ﬂoat in air but are supported by
this ground plane, and we can exploit the very special geometry of this viewing scenario.
tions on the ground plane. Suppose that the eye, or camera, is at a height hc above
bottom is at (X, −hc, Z) and top is at (X, δY −hc, Z). The bottom projects to the
nearer objects (small Z) project to points lower in the image plane; farther objects have
bottoms closer to the horizon.
Objects and the geometric structure of scenes
head appears to subtend just half a degree, Bayesian inference suggests we are looking at a
reasoning supplies us with a method to check the results of a pedestrian detector, as well as a
method to estimate the distance to an object. For example, all pedestrians are about the same
height, and they tend to stand on a ground plane. If we know where the horizon is in an image,
we can rank pedestrians by distance to the camera. This works because we know where theirReconstructing the 3D World
Image plane
Horizon
Ground plane
A
B
C
A
C
B
In an image of people standing on a ground plane, the people whose feet
are closer to the horizon in the image must be farther away (top drawing). This means they
must look smaller in the image (left lower drawing). This means that the size and location of
real pedestrians in an image depend upon one another and on the location of the horizon. To
methods. From this information, and from some likely pedestrians, we can recover a horizon
as shown in the center image. On the right, acceptable pedestrian boxes given this geometric
context. Notice that pedestrians who are higher in the scene must be smaller. If they are not,
feet are, and pedestrians whose feet are closer to the horizon in the image are farther away
be smaller in the image. This means we can rule out some detector responses — if a detector
ﬁnds a pedestrian who is large in the image and whose feet are close to the horizon, it has
found an enormous pedestrian; these don’t exist, so the detector is wrong. In fact, many or
most image windows are not acceptable pedestrian windows, and need not even be presented
to the detector.
There are several strategies for ﬁnding the horizon, including searching for a roughly
horizontal line with a lot of blue above it, and using surface orientation estimates obtained
from texture deformation. A more elegant strategy exploits the reverse of our geometric
constraints. A reasonably reliable pedestrian detector is capable of producing estimates of the
horizon, if there are several pedestrians in the scene at different distances from the camera.
This is because the relative scaling of the pedestrians is a cue to where the horizon is. So we
can extract a horizon estimate from the detector, then use this estimate to prune the pedestrian
detector’s mistakes.Chapter
Perception
If the object is familiar, we can estimate more than just the distance to it, because what it
looks like in the image depends very strongly on its pose, i.e., its position and orientation with
respect to the viewer. This has many applications. For instance, in an industrial manipulation
task, the robot arm cannot pick up an object until the pose is known. In the case of rigid
deﬁned solution based on the alignment method, which we now develop.
ALIGNMENT METHOD
The object is represented by M features or distinguished points m1, m2, . . . , mM in
in some coordinate system that is natural for the object. The points are then subjected to
and then projection to give rise to image feature points p1, p2, . . . , pN on the image plane.
In general, N ̸= M, because some model points may be occluded, and the feature detector
could miss some features (or invent false ones due to noise). We can express this as
pi = Π(Rmi + t) = Q(mi)
is a rotation matrix, t is a translation, and Π denotes perspective projection or one of its
approximations, such as scaled orthographic projection. The net result is a transformation Q
that will bring the model point mi into alignment with the image point pi. Although we do
not know Q initially, we do know (for rigid objects) that Q must be the same for all the model
points.
relating the coordinates of pi to those of mi. In these equations, the unknown quantities
correspond to the parameters of the rotation matrix R and the translation vector t. If we have
enough equations, we ought to be able to solve for Q. We will not give a proof here; we
merely state the following result:
Given three noncollinear points m1, m2, and m3 in the model, and their scaled
orthographic projections p1, p2, and p3 on the image plane, there exist exactly
dimensional image coordinate frame.
These transformations are related by a reﬂection around the image plane and can be computed
three features in the image, we could compute Q, the pose of the object.
Let us specify position and orientation in mathematical terms. The position of a point P
in the scene is characterized by three numbers, the (X, Y, Z) coordinates of P in a coordinate
image. This speciﬁes the ray from the pinhole along which P lies; what we do not know is
the distance. The term “orientation” could be used in two senses:
dimensional rotation relating its coordinate frame to that of the camera.Object Recognition from Structural Information
vector, n—which is a vector specifying the direction that is perpendicular to the surface.
Often we express the surface orientation using the variables slant and tilt. Slant is the
SLANT
TILT
of n on the image plane.
When the camera moves relative to an object, both the object’s distance and its orientation
change. What is preserved is the shape of the object. If the object is a cube, that fact is
SHAPE
not changed when the object moves. Geometers have been attempting to formalize shape for
centuries, the basic concept being that shape is what remains unchanged under some group of
transformations—for example, combinations of rotations and translations. The difﬁculty lies
in ﬁnding a representation of global shape that is general enough to deal with the wide variety
of objects in the real world—not just simple forms like cylinders, cones, and spheres—and yet
can be recovered easily from the visual input. The problem of characterizing the local shape
of a surface is much better understood. Essentially, this can be done in terms of curvature:
how does the surface normal change as one moves in different directions on the surface? For
a plane, there is no change at all. For a cylinder, if one moves parallel to the axis, there is
no change, but in the perpendicular direction, the surface normal rotates at a rate inversely
proportional to the radius of the cylinder, and so on. All this is studied in the subject called
differential geometry.
The shape of an object is relevant for some manipulation tasks (e.g., deciding where to
grasp an object), but its most signiﬁcant role is in object recognition, where geometric shape
along with color and texture provide the most signiﬁcant cues to enable us to identify objects,
classify what is in the image as an example of some class one has seen before, and so on.
OBJECT RECOGNITION FROM STRUCTURAL INFORMATION
Putting a box around pedestrians in an image may well be enough to avoid driving into them.
We have seen that we can ﬁnd a box by pooling the evidence provided by orientations, using
histogram methods to suppress potentially confusing spatial detail. If we want to know more
about what someone is doing, we will need to know where their arms, legs, body, and head lie
in the picture. Individual body parts are quite difﬁcult to detect on their own using a moving
window method, because their color and texture can vary widely and because they are usually
small in images. Often, forearms and shins are as small as two to three pixels wide. Body
parts do not usually appear on their own, and representing what is connected to what could
be quite powerful, because parts that are easy to ﬁnd might tell us where to look for parts that
are small and hard to detect.
Inferring the layout of human bodies in pictures is an important task in vision, because
the layout of the body often reveals what people are doing. A model called a deformable
template can tell us which conﬁgurations are acceptable: the elbow can bend but the head is
DEFORMABLE
TEMPLATE
never joined to the foot. The simplest deformable template model of a person connects lower
arms to upper arms, upper arms to the torso, and so on. There are richer models: for example,Chapter
Perception
we could represent the fact that left and right upper arms tend to have the same color and
texture, as do left and right legs. These richer models remain difﬁcult to work with, however.
The geometry of bodies: Finding arms and legs
For the moment, we assume that we know what the person’s body parts look like (e.g., we
know the color and texture of the person’s clothing). We can model the geometry of the
body as a tree of eleven segments (upper and lower left and right arms and legs respectively,
a torso, a face, and hair on top of the face) each of which is rectangular. We assume that
the position and orientation (pose) of the left lower arm is independent of all other segments
POSE
given the pose of the left upper arm; that the pose of the left upper arm is independent of
all segments given the pose of the torso; and extend these assumptions in the obvious way
to include the right arm and the legs, the face, and the hair. Such models are often called
“cardboard people” models. The model forms a tree, which is usually rooted at the torso. We
will search the image for the best match to this cardboard person using inference methods for
There are two criteria for evaluating a conﬁguration. First, an image rectangle should
look like its segment. For the moment, we will remain vague about precisely what that means,
but we assume we have a function φi that scores how well an image rectangle matches a body
segment. For each pair of related segments, we have another function ψ that scores how
well relations between a pair of image rectangles match those to be expected from the body
segments. The dependencies between segments form a tree, so each segment has only one
parent, and we could write ψi,pa(i). All the functions will be larger if the match is better,
so we can think of them as being like a log probability. The cost of a particular match that
allocates image rectangle mi to body segment i is then


φi(mi) +


ψi,pa(i)(mi, mpa(i)) .
Dynamic programming can ﬁnd the best match, because the relational model is a tree.
It is inconvenient to search a continuous space, and we will discretize the space of image
rectangles. We do so by discretizing the location and orientation of rectangles of ﬁxed size
(the sizes may be different for different segments). Because ankles and knees are different,
could visualize the result as a set of very large stacks of small rectangles of image, cut out at
different locations and orientations. There is one stack per segment. We must now ﬁnd the
best allocation of rectangles to segments. This will be slow, because there are many image
rectangles and, for the model we have given, choosing the right torso will be O(M6) if there
are M image rectangles. However, various speedups are available for an appropriate choice
structure model.
PICTORIAL
STRUCTURE MODEL
Recall our assumption that we know what we need to know about what the person looks
ment matches turns out to be color. Texture features don’t work well in most cases, because
folds on loose clothing produce strong shading patterns that overlay the image texture. TheseObject Recognition from Structural Information
angles and a cardboard person (shown on the left) by scoring the similarity in appearance
ments. Generally, a match is better if the image segments have about the right appearance and
are in about the right place with respect to one another. The appearance model uses average
colors for hair, head, torso, and upper and lower arms and legs. The relevant relations are
shown as arrows. On the right, the best match for a particular image, obtained using dynamic
programming. The match is a fair estimate of the conﬁguration of the body. Figure from
patterns are strong enough to disrupt the true texture of the cloth. In current work, ψ typically
reﬂects the need for the ends of the segments to be reasonably close together, but there are
usually no constraints on the angles. Generally, we don’t know what a person looks like,
and must build a model of segment appearances. We call the description of what a person
looks like the appearance model. If we must report the conﬁguration of a person in a single
APPEARANCE
MODEL
image, we can start with a poorly tuned appearance model, estimate conﬁguration with this,
and this will reveal their appearance.
Coherent appearance: Tracking people in video
Tracking people in video is an important practical problem. If we could reliably report the
location of arms, legs, torso, and head in video sequences, we could build much improved
game interfaces and surveillance systems. Filtering methods have not had much success
with this problem, because people can produce large accelerations and move quite fast. This
exploit the fact that appearance changes very slowly from frame to frame. If we can infer an
appearance model of an individual from the video, then we can use this information in a
pictorial structure model to detect that person in each frame of the video. We can then link
these locations across time to make a track.Chapter
Perception
Lateral walking 
detector
Appearance
model
Body part 
maps
Detected figure
torso
arm
motion blur
& interlacing
taining an appearance model, then applying it. To obtain the appearance model, we scan the
image to ﬁnd a lateral walking pose. The detector does not need to be very accurate, but
should produce few false positives. From the detector response, we can read off pixels that
lie on each body segment, and others that do not lie on that segment. This makes it possible to
build a discriminative model of the appearance of each body part, and these are tied together
into a pictorial structure model of the person being tracked. Finally, we can reliably track by
detecting this model in each frame. As the frames in the lower part of the image suggest, this
There are several ways to infer a good appearance model. We regard the video as a
large stack of pictures of the person we wish to track. We can exploit this stack by looking
for appearance models that explain many of the pictures. This would work by detecting
body segments in each frame, using the fact that segments have roughly parallel edges. Such
detectors are not particularly reliable, but the segments we want to ﬁnd are special. They
will appear at least once in most of the frames of video; such segments can be found by
clustering the detector responses. It is best to start with the torso, because it is big and
because torso detectors tend to be reliable. Once we have a torso appearance model, upper
leg segments should appear near the torso, and so on. This reasoning yields an appearance
segment detector generates lots of false positives. An alternative is to estimate appearance
for many of the frames of video by repeatedly reestimating conﬁguration and appearance; we
then see if one appearance model explains many frames. Another alternative, which is quiteUsing Vision
Some complex human actions produce consistent patterns of appearance
and motion. For example, drinking involves movements of the hand in front of the face. The
c⃝IEEE.
reliable in practice, is to apply a detector for a ﬁxed body conﬁguration to all of the frames. A
good choice of conﬁguration is one that is easy to detect reliably, and where there is a strong
chance the person will appear in that conﬁguration even in a short sequence (lateral walking
is a good choice). We tune the detector to have a low false positive rate, so we know when it
responds that we have found a real person; and because we have localized their torso, arms,
legs, and head, we know what these segments look like.
USING VISION
If vision systems could analyze video and understood what people are doing, we would be
able to: design buildings and public places better by collecting and using data about what
people do in public; build more accurate, more secure, and less intrusive surveillance systems;
and react to their behavior. Applications for reactive interfaces range from computer games
that make a player get up and move around to systems that save energy by managing heat and
light in a building to match where the occupants are and what they are doing.
Some problems are well understood. If people are relatively small in the video frame,
and the background is stable, it is easy to detect the people by subtracting a background image
from the current frame. If the absolute value of the difference is large, this background
subtraction declares the pixel to be a foreground pixel; by linking foreground blobs over
BACKGROUND
SUBTRACTION
time, we obtain a track.
tions. When performed against a simple background, videos of these actions are easy to deal
with. Background subtraction identiﬁes the major moving regions, and we can build HOG
features (keeping track of ﬂow rather than orientation) to present to a classiﬁer. We can detect
consistent patterns of action with a variant of our pedestrian detector, where the orientation
More general problems remain open. The big research question is to link observations
of the body and the objects nearby to the goals and intentions of the moving people. One
source of difﬁculty is that we lack a simple vocabulary of human behavior. Behavior is a lotChapter
Perception
like color, in that people tend to think they know a lot of behavior names but can’t produce
long lists of such words on demand. There is quite a lot of evidence that behaviors combine—
you can, for example, drink a milkshake while visiting an ATM—but we don’t yet know
what the pieces are, how the composition works, or how many composites there might be.
A second source of difﬁculty is that we don’t know what features expose what is happening.
For example, knowing someone is close to an ATM may be enough to tell that they’re visiting
the ATM. A third difﬁculty is that the usual reasoning about the relationship between training
and test data is untrustworthy. For example, we cannot argue that a pedestrian detector is
safe simply because it performs well on a large data set, because that data set may well omit
important, but rare, phenomena (for example, people mounting bicycles). We wouldn’t want
our automated driver to run over a pedestrian who happened to do something unusual.
Words and pictures
Many Web sites offer collections of images for viewing. How can we ﬁnd the images we
want? Let’s suppose the user enters a text query, such as “bicycle race.” Some of the images
will have keywords or captions attached, or will come from Web pages that contain text near
the image. For these, image retrieval can be like text retrieval: ignore the images and match
However, keywords are usually incomplete. For example, a picture of a cat playing in
the street might be tagged with words like “cat” and “street,” but it is easy to forget to mention
the “garbage can” or the “ﬁsh bones.” Thus an interesting task is to annotate an image (which
may already have a few keywords) with additional appropriate keywords.
ample images, and we wish to tag some test images. This problem is sometimes known as
One ﬁnds the training images that are closest to the test image in a feature space metric that
is trained using examples, then reports their tags.
ing data. We can use a version of expectation maximization to guess an initial correspondence
between text and regions, and from that estimate a better decomposition into regions, and so
on.
Reconstruction from many views
Binocular stereopsis works because for each point we have four measurements constraining
three unknown degrees of freedom. The four measurements are the (x, y) positions of the
point in each view, and the unknown degrees of freedom are the (x, y, z) coordinate values of
the point in the scene. This rather crude argument suggests, correctly, that there are geometric
constraints that prevent most pairs of points from being acceptable matches. Many images of
a set of points should reveal their positions unambiguously.
We don’t always need a second picture to get a second view of a set of points. If we
believe the original set of points comes from a familiar rigid 3D object, then we might haveUsing Vision
an object model available as a source of information. If this object model consists of a set of
3D points or of a set of pictures of the object, and if we can establish point correspondences,
we can determine the parameters of the camera that produced the points in the original image.
This is very powerful information. We could use it to evaluate our original hypothesis that
the points come from an object model. We do this by using some points to determine the
parameters of the camera, then projecting model points in this camera and checking to see
whether there are image points nearby.
We have sketched here a technology that is now very highly developed. The technology
can be generalized to deal with views that are not orthographic; to deal with points that are
observed in only some views; to deal with unknown camera properties like focal length; to
tion from very large numbers of points and of views. If the locations of points in the images
are known with some accuracy and the viewing directions are reasonable, very high accuracy
camera and point information can be obtained. Some applications are
textured polygons for use in computer graphics and virtual reality applications. Models
on the Internet.
• Matching moves: To place computer graphics characters into real video, we need to
know how the camera moved for the real video, so that we can render the character
correctly.
• Path reconstruction: Mobile robots need to know where they have been. If they are
moving in a world of rigid objects, then performing a reconstruction and keeping the
camera information is one way to obtain a path.
Using vision for controlling movement
One of the principal uses of vision is to provide information both for manipulating objects—
picking them up, grasping them, twirling them, and so on—and for navigating while avoiding
obstacles. The ability to use vision for these purposes is present in the most primitive of
animal visual systems. In many cases, the visual system is minimal, in the sense that it
extracts from the available light ﬁeld just the information the animal needs to inform its
behavior. Quite probably, modern vision systems evolved from early, primitive organisms
that used a photosensitive spot at one end to orient themselves toward (or away from) the
land on walls. A classic study, What the Frog’s Eye Tells the Frog’s Brain (Lettvin et al.,
His choice of food is determined only by size and movement.”
Let us consider a vision system for an automated vehicle driving on a freeway. The
tasks faced by the driver include the following:Chapter
Perception
a
b
c
(a)
(b)
(c)
This ﬁgure outlines a system built by Michael Goesele and colleagues from the University
of Washington, TU Darmstadt, and Microsoft Research. From a collection of pictures of a
monument taken by a large community of users and posted on the Internet (a), their system
can determine the viewing directions for those pictures, shown by the small black pyramids
in (b) and a comprehensive 3D reconstruction shown in (c).
lanes smoothly when required.
maneuvers if one of them decides to change lanes.
tions to best accomplish these tasks.
For lateral control, one needs to maintain a representation of the position and orientation
tion it is pointing relative to the lane, and the curvature of the lane. This information, along
system. If we have good detailed maps of the road, then the vision system serves to conﬁrm
our position (and to watch for obstacles that are not on the map).
For longitudinal control, one needs to know distances to the vehicles in front. This can
controlled cars can now drive reliably at highway speeds.
ment, now has pretty good solutions. A group at Sarnoff has developed a system based on
two cameras looking forward that track feature points in 3D and use that to reconstruct theSummary
position of the robot relative to the environment. In fact, they have two stereoscopic camera
systems, one looking front and one looking back—this gives greater robustness in case the
robot has to go through a featureless patch due to dark shadows, blank walls, and the like. It is
unlikely that there are no features either in the front or in the back. Now of course, that could
happen, so a backup is provided by using an inertial motion unit (IMU) somewhat akin to the
mechanisms for sensing acceleration that we humans have in our inner ears. By integrating
the sensed acceleration twice, one can keep track of the change in position. Combining the
data from vision and the IMU is a problem of probabilistic evidence fusion and can be tackled
using techniques, such as Kalman ﬁltering, we have studied elsewhere in the book.
In the use of visual odometry (estimation of change in position), as in other problems
of odometry, there is the problem of “drift,” positional errors accumulating over time. The
solution for this is to use landmarks to provide absolute position ﬁxes: as soon as the robot
passes a location in its internal map, it can adjust its estimate of its position appropriately.
Accuracies on the order of centimeters have been demonstrated with the these techniques.
The driving example makes one point very clear: for a speciﬁc task, one does not need
to recover all the information that, in principle, can be recovered from an image. One does
surface adjacent to the freeway, and so on. Instead, a vision system should compute just what
is needed to accomplish the task.
SUMMARY
Although perception appears to be an effortless activity for humans, it requires a signiﬁcant
amount of sophisticated computation. The goal of vision is to extract information needed for
tasks such as manipulation, navigation, and object recognition.
of it from some arbitrary camera position (the graphics problem). Inverting the process
by going from an image to a description of the scene is more difﬁcult.
• To extract the visual information necessary for the tasks of manipulation, navigation,
and recognition, intermediate representations have to be constructed.
Early vision
and regions.
formation about the scene: motion, stereopsis, texture, shading, and contour analysis.
Each of these cues relies on background assumptions about physical scenes to provide
nearly unambiguous interpretations.
estimation. Other possibilities exist.Chapter
Perception
BIBLIOGRAPHICAL AND HISTORICAL NOTES
mon ancestor. Since then, endless variations have developed in different creatures, but the
mice, and Drosophila.
Systematic attempts to understand human vision can be traced back to ancient times.
projection O to the point P. He was well aware of the notion of motion parallax. The use of
perspective in art was developed in ancient Roman culture, as evidenced by art found in the
standing of perspective projection, this time in the context of projection onto planar surfaces,
is usually credited with creating the ﬁrst paintings based on geometrically correct projection
ment of the science of perspective, as it was called in those days, were Leonardo da Vinci and
Albrecht D¨urer. Leonardo’s late 15th century descriptions of the interplay of light and shade
(chiaroscuro), umbra and penumbra regions of shadows, and aerial perspective are still worth
Renaissance art using computer vision techniques.
Although perspective was known to the ancient Greeks, they were curiously confused
by the role of the eyes in vision. Aristotle thought of the eyes as devices emitting rays, rather
in the manner of modern laser range ﬁnders. This mistaken view was laid to rest by the work
of Arab scientists, such as Abu Ali Alhazen, in the 10th century. Alhazen also developed the
camera obscura, a room (camera is Latin for “room” or “chamber”) with a pinhole that casts
an image on the opposite wall. Of course the image was inverted, which caused no end of
side up? This enigma exercised the greatest minds of the era (including Leonardo). Kepler
ﬁrst proposed that the lens of the eye focuses an image on the retina, and Descartes surgically
removed an ox eye and demonstrated that Kepler was right. There was still puzzlement as to
why we do not see everything upside down; today we realize it is just a question of accessing
the retinal data structure in the right way.
In the ﬁrst half of the 20th century, the most signiﬁcant research results in vision were
obtained by the Gestalt school of psychology, led by Max Wertheimer. They pointed out the
importance of perceptual organization: for a human observer, the image is not a collection
ganized into coherent groups. One could trace the motivation in computer vision of ﬁnding
regions and curves back to this insight. The Gestaltists also drew attention to the “ﬁgure–
ground” phenomenon—a contour separating two image regions that, in the world, are at
different depths, appears to belong only to the nearer region, the “ﬁgure,” and not the fartherBibliographical and Historical Notes
region, the “ground.” The computer vision problem of classifying image curves according to
their signiﬁcance in the scene can be thought of as a generalization of this insight.
The period after World War II was marked by renewed activity. Most signiﬁcant was
as texture gradients in the estimation of environmental variables such as surface slant and tilt.
He reemphasized the importance of the stimulus and how rich it was. Gibson emphasized the
about the external environment.
of the earliest publications in the ﬁeld, introducing key ideas such as edge detection and
of “solving” computer vision to a graduate student as a summer project. According to Minsky
the legend is untrue—it was actually an undergraduate student. But it was an exceptional
undergraduate, Gerald Jay Sussman (who is now a professor at MIT) and the task was not to
“solve” vision, but to investigate some aspects of it.
for ﬁnding texture boundaries based on multiscale, multiorientation ﬁltering of images date to
color—for ﬁnding boundary curves in a learning framework was shown by Martin, Fowlkes
ture, naturally lends itself to formulations in which ﬁnding the best partition becomes an
optimization problem. Three leading examples are the Markov Random Fields approach of
Through much of the 1960s, 1970s and 1980s, there were two distinct paradigms in
images. The idea of alignment, also ﬁrst introduced by Roberts, resurfaced in the 1980s in the
based on describing shapes in terms of volumetric primitives, with generalized cylinders,
GENERALIZED
CYLINDER
lem as not signiﬁcant. Their motivating examples were in domains such as optical character
ing the typical variations characteristic of a class of objects and separating them from other
In the late 1990s, these two paradigms started to converge, as both sides adopted the
probabilistic modeling and learning techniques that were becoming popular throughout AI.
Two lines of work contributed signiﬁcantly. One was research on face detection, such as thatChapter
Perception
the power of pattern recognition techniques on clearly important and useful tasks. The other
was the development of point descriptors, which enable one to construct feature vectors from
multiple frames shape could be recovered quite accurately. In the 1990s, with great increase
particularly popular, led by reconstruction algorithms such as the one developed by Debevec,
more generally understanding the visual events in the projection of smooth curved objects,
owes much to the work of Koenderink and van Doorn, which ﬁnds an extensive treatment in
of shape and surface recovery from a single image as a probabilistic inference problem, where
geometrical cues are not modeled explicitly, but used implicitly in a learning framework. A
vision to psychophysics and neurobiology. While many of his speciﬁc models haven’t stood
tional, computational, and implementation level is still illuminating.
and still useful textbooks.
The main journals for computer vision are IEEE Transactions on Pattern Analysis and
ferences include ICCV (International Conference on Computer Vision), CVPR (Computer
Vision and Pattern Recognition), and ECCV (European Conference on Computer Vision).
tion Processing Systems) conference, and work on the interface with computer graphics often
appears at the ACM SIGGRAPH (Special Interest Group in Graphics) conference.Chapter
Perception
E
A
B
C
X
Y
D
behind it.
scene. Draw the image seen at each camera, assuming that all named points are in the same
horizontal plane. What can be concluded from these two images about the relative distances
of points A, B, C, D, and E from the camera baseline, and on what basis?ROBOTICS
In which agents are endowed with physical effectors with which to do mischief.
INTRODUCTION
Robots are physical agents that perform tasks by manipulating the physical world. To do so,
ROBOT
they are equipped with effectors such as legs, wheels, joints, and grippers. Effectors have
EFFECTOR
SENSOR
ploys a diverse set of sensors, including cameras and lasers to measure the environment, and
gyroscopes and accelerometers to measure the robot’s own motion.
Most of today’s robots fall into one of three primary categories. Manipulators, or robot
MANIPULATOR
assembly line or on the International Space Station. Manipulator motion usually involves
a chain of controllable joints, enabling such robots to place their effectors in any position
within the workplace. Manipulators are by far the most common type of industrial robots,
with approximately one million units installed worldwide. Some mobile manipulators are
used in hospitals to assist surgeons. Few car manufacturers could survive without robotic
manipulators, and some manipulators have even been used to generate original artwork.
The second category is the mobile robot. Mobile robots move about their environment
MOBILE ROBOT
using wheels, legs, or similar mechanisms. They have been put to use delivering food in
UGV
PLANETARY ROVER
robots include the twin Mars Exploration Rovers (one is depicted on the cover of this book),
UAV
from the actuator (the control line that communicates a command to the effector).Chapter
Robotics
(a)
(b)
(a) An industrial robotic manipulator for stacking bags on a pallet. Image
courtesy of Nachi Robotic Systems. (b) Honda’s P3 and Asimo humanoid robots.
(a)
(b)
(a) Predator, an unmanned aerial vehicle (UAV) used by the U.S. Military.
Image courtesy of General Atomics Aeronautical Systems. (b) NASA’s Sojourner, a mobile
tonomous underwater vehicles (AUVs) are used in deep sea exploration. Mobile robots
AUV
deliver packages in the workplace and vacuum the ﬂoors at home.
The third type of robot combines mobility with manipulation, and is often called a
MOBILE
MANIPULATOR
HUMANOID ROBOT
early humanoid robots, both manufactured by Honda Corp. in Japan. Mobile manipulatorsRobot Hardware
can apply their effectors further aﬁeld than anchored manipulators can, but their task is made
harder because they don’t have the rigidity that the anchor provides.
The ﬁeld of robotics also includes prosthetic devices (artiﬁcial limbs, ears, and eyes
for humans), intelligent environments (such as an entire house that is equipped with sensors
and effectors), and multibody systems, wherein robotic action is achieved through swarms of
small cooperating robots.
namic, and continuous. Many robot environments are sequential and multiagent as well.
Partial observability and stochasticity are the result of dealing with a large, complex world.
Robot cameras cannot see around corners, and motion commands are subject to uncertainty
due to gears slipping, friction, etc. Also, the real world stubbornly refuses to operate faster
than real time. In a simulated environment, it is possible to use simple algorithms (such as the
trials. In a real environment, it might take years to run these trials. Furthermore, real crashes
really hurt, unlike simulated ones. Practical robotic systems need to embody prior knowledge
about the robot, its physical environment, and the tasks that the robot will perform so that the
robot can learn quickly and perform safely.
inforcement learning. For some of these concepts robotics serves as a challenging example
application. For other concepts this chapter breaks new ground in introducing the continuous
version of techniques that we previously saw only in the discrete case.
ROBOT HARDWARE
So far in this book, we have taken the agent architecture—sensors, effectors, and processors—
as given, and we have concentrated on the agent program. The success of real robots depends
at least as much on the design of sensors and effectors that are appropriate for the task.
Sensors
Sensors are the perceptual interface between robot and environment. Passive sensors, such
PASSIVE SENSOR
as cameras, are true observers of the environment: they capture signals that are generated by
ACTIVE SENSOR
ronment. They rely on the fact that this energy is reﬂected back to the sensor. Active sensors
tend to provide more information than passive sensors, but at the expense of increased power
consumption and with a danger of interference when multiple active sensors are used at the
same time. Whether active or passive, sensors can be divided into three types, depending on
whether they sense the environment, the robot’s location, or the robot’s internal conﬁguration.
Range ﬁnders are sensors that measure the distance to nearby objects. In the early
RANGE FINDER
days of robotics, robots were commonly equipped with sonar sensors. Sonar sensors emit
SONAR SENSORS
directional sound waves, which are reﬂected by objects, with some of the sound making itChapter
Robotics
(a)
(b)
(a) Time of ﬂight camera; image courtesy of Mesa Imaging GmbH. (b) 3D
range image obtained with this camera. The range image makes it possible to detect obstacles
and objects in a robot’s vicinity.
back into the sensor. The time and intensity of the returning signal indicates the distance
to nearby objects. Sonar is the technology of choice for autonomous underwater vehicles.
STEREO VISION
slightly different viewpoints, analyzing the resulting parallax in these images to compute the
range of surrounding objects. For mobile ground robots, sonar and stereo vision are now
rarely used, because they are not reliably accurate.
Most ground robots are now equipped with optical range ﬁnders. Just like sonar sensors,
optical range sensors emit active signals (light) and measure the time until a reﬂection of this
TIME OF FLIGHT
CAMERA
complex arrangements of mirrors or rotating elements. These sensors are called scanning
lidars (short for light detection and ranging). Scanning lidars tend to provide longer ranges
SCANNING LIDARS
than time of ﬂight cameras, and tend to perform better in bright daylight.
Other common range sensors include radar, which is often the sensor of choice for
UAVs. Radar sensors can measure distances of multiple kilometers. On the other extreme
TACTILE SENSORS
skin. These sensors measure range based on physical contact, and can be deployed only for
sensing objects very close to the robot.
A second important class of sensors is location sensors. Most location sensors use
LOCATION SENSORS
ing System (GPS) is the most common solution to the localization problem. GPS measures
GLOBAL
POSITIONING
SYSTEM
transmitting signals on multiple frequencies. GPS receivers can recover the distance to these
satellites by analyzing phase shifts. By triangulating signals from multiple satellites, GPSRobot Hardware
receivers can determine their absolute location on Earth to within a few meters. Differential
GPS involves a second ground receiver with known location, providing millimeter accuracy
DIFFERENTIAL GPS
under ideal conditions. Unfortunately, GPS does not work indoors or underwater. Indoors,
localization is often achieved by attaching beacons in the environment at known locations.
Many indoor environments are full of wireless base stations, which can help robots localize
through the analysis of the wireless signal. Underwater, active sonar beacons can provide a
sense of location, using sound to inform AUVs of their relative distances to those beacons.
The third important class is proprioceptive sensors, which inform the robot of its own
PROPRIOCEPTIVE
SENSOR
motion. To measure the exact conﬁguration of a robotic joint, motors are often equipped
with shaft decoders that count the revolution of motors in small increments. On robot arms,
SHAFT DECODER
shaft decoders can provide accurate information over any period of time. On mobile robots,
shaft decoders that report wheel revolutions can be used for odometry—the measurement of
ODOMETRY
distance traveled. Unfortunately, wheels tend to drift and slip, so odometry is accurate only
over short distances. External forces, such as the current for AUVs and the wind for UAVs,
increase positional uncertainty. Inertial sensors, such as gyroscopes, rely on the resistance
INERTIAL SENSOR
of mass to the change of velocity. They can help reduce uncertainty.
FORCE SENSOR
sors. These are indispensable when robots handle fragile objects or objects whose exact shape
TORQUE SENSOR
would be all too easy to apply too much force and break the bulb. Force sensors allow the
robot to sense how hard it is gripping the bulb, and torque sensors allow it to sense how hard
it is turning. Good sensors can measure forces in all three translational and three rotational
directions. They do this at a frequency of several hundred times a second, so that a robot can
quickly detect unexpected forces and correct its actions before it breaks a light bulb.
Effectors
Effectors are the means by which robots move and change the shape of their bodies. To
understand the design of effectors, it will help to talk about motion and shape in the abstract,
using the concept of a degree of freedom (DOF) We count one degree of freedom for each
DEGREE OF
FREEDOM
independent direction in which a robot, or one of its effectors, can move. For example, a rigid
mobile robot such as an AUV has six degrees of freedom, three for its (x, y, z) location in
space and three for its angular orientation, known as yaw, roll, and pitch. These six degrees
deﬁne the kinematic state2 or pose of the robot. The dynamic state of a robot includes these
KINEMATIC STATE
POSE
DYNAMIC STATE
six plus an additional six dimensions for the rate of change of each kinematic dimension, that
is, their velocities.
For nonrigid bodies, there are additional degrees of freedom within the robot itself. For
example, the elbow of a human arm possesses two degree of freedom. It can ﬂex the upper
arm towards or away, and can rotate right or left. The wrist has three degrees of freedom. It
can move up and down, side to side, and can also rotate. Robot joints also have one, two,
or three degrees of freedom each. Six degrees of freedom are required to place an object,Chapter
Robotics
has exactly six degrees of freedom, created by ﬁve revolute joints that generate rotational
REVOLUTE JOINT
motion and one prismatic joint that generates sliding motion. You can verify that the human
PRISMATIC JOINT
arm as a whole has more than six degrees of freedom by a simple experiment: put your hand
on the table and notice that you still have the freedom to rotate your elbow without changing
the conﬁguration of your hand. Manipulators that have extra degrees of freedom are easier to
control than robots with only the minimum number of DOFs. Many industrial manipulators
therefore have seven DOFs, not six.
R
R
R
P
R
R
θ
(x, y)
(a)
(b)
(a) The Stanford Manipulator, an early robot arm with ﬁve revolute joints (R)
ments. Consider, for example, your average car: it can move forward or backward, and it can
on an open ﬂat surface, one can easily maneuver a car to any (x, y) point, in any orientation.
EFFECTIVE DOF
lable degrees of freedom. We say a robot is nonholonomic if it has more effective DOFs
CONTROLLABLE DOF
NONHOLONOMIC
than controllable DOFs and holonomic if the two numbers are the same. Holonomic robots
are easier to control—it would be much easier to park a car that could move sideways as well
as forward and backward—but holonomic robots are also mechanically more complex. Most
robot arms are holonomic, and most mobile robots are nonholonomic.
Mobile robots have a range of mechanisms for locomotion, including wheels, tracks,
and legs. Differential drive robots possess two independently actuated wheels (or tracks),
DIFFERENTIAL DRIVE
one on each side, as on a military tank. If both wheels move at the same velocity, the robot
moves on a straight line. If they move in opposite directions, the robot turns on the spot. An
alternative is the synchro drive, in which each wheel can move and turn around its own axis.
SYNCHRO DRIVE
To avoid chaos, the wheels are tightly coordinated. When moving straight, for example, all
wheels point in the same direction and move at the same speed. Both differential and synchro
drives are nonholonomic. Some more expensive robots use holonomic drives, which have
three or more wheels that can be oriented and moved independently.
robot’s arms use springs to compensate for gravity, and they provide minimal resistance toRobot Hardware
(a)
(b)
(a) Mobile manipulator plugging its charge cable into a wall outlet. Image
external forces. Such a design minimizes the physical danger to people who might stumble
into such a robot. This is a key consideration in deploying robots in domestic environments.
Legs, unlike wheels, can handle rough terrain. However, legs are notoriously slow on
signs ranging from one leg up to dozens of legs. Legged robots have been made to walk, run,
stable, meaning that it can remain upright while hopping around. A robot that can remain
DYNAMICALLY
STABLE
upright without moving its legs is called statically stable. A robot is statically stable if its
STATICALLY STABLE
legs at the same time, which renders it dynamically stable. The robot can walk on snow and
ice, and it will not fall over even if you kick it (as demonstrated in videos available online).
derwater vehicles use propellers or thrusters, similar to those used on submarines. Robotic
blimps rely on thermal effects to keep themselves aloft.
Sensors and effectors alone do not make a robot. A complete robot also needs a source
of power to drive its effectors. The electric motor is the most popular mechanism for both
ELECTRIC MOTOR
manipulator actuation and locomotion, but pneumatic actuation using compressed gas and
PNEUMATIC
ACTUATION
hydraulic actuation using pressurized ﬂuids also have their application niches.
HYDRAULIC
ACTUATIONChapter
Robotics
(a)
(b)
ROBOTIC PERCEPTION
ment is partially observable, unpredictable, and often dynamic. In other words, robots have
rule of thumb, good internal representations for robots have three properties: they contain
enough information for the robot to make good decisions, they are structured so that they can
be updated efﬁciently, and they are natural in the sense that internal variables correspond to
natural state variables in the physical world.
sent the transition and sensor models of a partially observable environment, and we described
ity distribution over the environment state variables. Several dynamic Bayes net models for
chapter: Xt is the state of the environment (including the robot) at time t, Zt is the observation
received at time t, and At is the action taken after the observation is received.Robotic Perception
Xt
At
Zt
Robot perception can be viewed as temporal inference from sequences of
actions and measurements, as illustrated by this dynamic Bayes network.
servations, and we deal with continuous rather than discrete variables. Thus, we modify the

recursively from the corresponding estimate one time step earlier. This calculation involves
how to recursively estimate this location, by incrementally folding in sensor measurements
MOTION MODEL
Localization and mapping
Localization is the problem of ﬁnding out where things are—including the robot itself.
LOCALIZATION
Knowledge about where things are is at the core of any successful physical interaction with
the environment. For example, robot manipulators must know the location of objects they
seek to manipulate; navigating robots must know where they are to ﬁnd their way around.
To keep things simple, let us consider a mobile robot that moves slowly in a ﬂat 2D
world. Let us also assume the robot is given an exact map of the environment. (An example
by Xt = (xt, yt, θt)⊤. So far so good.Chapter
Robotics
xi, yi
vt Δt
h(xt)
xt
θt
θ
t Δt
ω
Z1
Z2
Z3
Z4
(a)
(b)
(a) A simpliﬁed kinematic model of a mobile robot. The robot is shown as a
circle with an interior line marking the forward direction. The state xt consists of the (xt, yt)
update in position of vtΔt and in orientation of ωtΔt. Also shown is a landmark at (xi, yi)
a given range scan (z1, z2, z3, z4). It is much more likely that the pose on the left generated
the range scan than the pose on the right.
tion of two velocities—a translational velocity vt and a rotational velocity ωt. For small time
intervals Δt, a crude deterministic model of the motion of such robots is given by
 

 

at
) = Xt +
⎛
⎝
ωtΔt
⎞
⎠.
The notation ˆX refers to a deterministic state prediction.
Of course, physical robots are
somewhat unpredictable. This is commonly modeled by a Gaussian distribution with mean
f(Xt, vt, ωt) and covariance Σx. (See Appendix A for a mathematical deﬁnition.)
This probability distribution is the robot’s motion model. It models the effects of the motion
at on the location of the robot.
Next, we need a sensor model. We will consider two kinds of sensor model. The
ﬁrst assumes that the sensors detect stable, recognizable features of the environment called
landmarks. For each landmark, the range and bearing are reported. Suppose the robot’s state
LANDMARK
is xt = (xt, yt, θt)⊤and it senses a landmark whose location is known to be (xi, yi)⊤. Without
exact prediction of the observed range and bearing would be
ˆzt = h(xt) =

 
arctan yi−yt
xi−xt −θt

.Robotic Perception
Again, noise distorts our measurements. To keep things simple, one might assume Gaussian
noise with covariance Σz, giving us the sensor model
P(zt | xt) = N(ˆzt, Σz) .
A somewhat different sensor model is used for an array of range sensors, each of which
has a ﬁxed bearing relative to the robot. Such sensors produce a vector of range values
zt = (z1, . . . , zM)⊤. Given a pose xt, let ˆzj be the exact range along the jth beam direction
from xt to the nearest obstacle. As before, this will be corrupted by Gaussian noise. Typically,
we assume that the errors for the different beam directions are independent and identically
distributed, so we have
P(zt | xt) = α
M

e−(zj−ˆzj)/2σ2.
one of which is reasonably likely to have produced the observed scan and one of which is not.
has the advantage that there is no need to identify a landmark before the range scan can be
if there are visible, identiﬁable landmarks, they may provide instant localization.
multivariate Gaussian, and the particle ﬁlter, which represents the belief state by a collection
of particles that correspond to states. Most modern localization algorithms use one of two
Localization using particle ﬁltering is called Monte Carlo localization, or MCL. The
MONTE CARLO
LOCALIZATION
particles are uniformly distributed based on the prior, indicating global uncertainty about the
robot’s position. In the second image, the ﬁrst set of measurements arrives and the particles
form clusters in the areas of high posterior belief. In the third, enough measurements are
available to push all the particles to a single location.
The Kalman ﬁlter is the other major way to localize. A Kalman ﬁlter represents the
its covariance Σt. The main problem with Gaussian beliefs is that they are only closed under
linear motion models f and linear measurement models h. For nonlinear f or h, the result of
updating a ﬁlter is in general not Gaussian. Thus, localization algorithms using the Kalman
ﬁlter linearize the motion and sensor models. Linearization is a local approximation of a
LINEARIZATION
f(xt, at) (the control at is omitted in this graph since it plays no role in the linearization).
On the right, this function is approximated by a linear function ˜f(xt, at). This linear functionChapter
Robotics
a set of samples for the next time step
inputs: a, robot velocities v and ω
z, range scan z1, . . . , zM
P(X′|X, v, ω), motion model
P(z|z∗), range sensor noise model
m, 2D map of the environment
persistent: S, a vector of samples of size N
local variables: W, a vector of weights of size N
S′, a temporary vector of particles of size N
W ′, a vector of weights of size N
if S is empty then
/* initialization phase */
S[i] ←sample from P(X0)
/* update cycle */
S′[i] ←sample from P(X′|X = S[i], v, ω)
z∗←RAYCAST(j, X = S′[i], m)
W ′[i] ←W ′[i] · P(zj| z∗)
return S
independent noise.
is called (ﬁrst degree) Taylor expansion. A Kalman ﬁlter that linearizes f and h via Taylor
TAYLOR EXPANSION
of estimates of a robot running an extended Kalman ﬁlter localization algorithm. As the
robot moves, the uncertainty in its location estimate increases, as shown by the error ellipses.
Its error decreases as it senses the range and bearing to a landmark with known location
and increases again as the robot loses sight of the landmark. EKF algorithms work well if
landmarks are easily identiﬁed. Otherwise, the posterior distribution may be multimodal, as
In some situations, no map of the environment is available. Then the robot will have to
determine its location relative to a map it doesn’t quite know, at the same time building this
map while it doesn’t quite know its actual location. This problem is important for many robot
applications, and it has been studied extensively under the name simultaneous localization
and mapping, abbreviated as SLAM.
SIMULTANEOUS
LOCALIZATION AND
MAPPING
SLAM problems are solved using many different probabilistic techniques, including
the extended Kalman ﬁlter discussed above. Using the EKF is straightforward: just augmentRobotic Perception
Robot position
(a)
Robot position
(b)
Robot position
(c)
gating in the (symmetric) corridor. (c) Unimodal uncertainty after entering a room and ﬁnding
it to be distinctive.Chapter
Robotics
Xt
μt
Σt
f(Xt, at)
f(μt, at)
Xt
μt
Σt
f(Xt, at)
f(μt, at)
~
f(Xt, at) = f(μt, at) + Ft(Xt − μt)
~
(a)
(b)
robot
landmark
Example of localization using the extended Kalman ﬁlter. The robot moves
on a straight line. As it progresses, its uncertainty increases gradually, as illustrated by the
error ellipses. When it observes a landmark with known position, the uncertainty is reduced.
the state vector to include the locations of the landmarks in the environment. Luckily, the
putation is quite feasible. Richer maps are often obtained using graph relaxation methods,
maximization is also used for SLAM.
Other types of perception
perature, odors, acoustic signals, and so on. Many of these quantities can be estimated using
variants of dynamic Bayes networks. All that is required for such estimators are conditional
sor models that describe the relation of measurements to state variables.
It is also possible to program a robot as a reactive agent, without explicitly reasoningRobotic Perception
(a)
(b)
(c)
Sequence of “drivable surface” classiﬁer results using adaptive vision. In
the vehicle is heading. In (b) the vehicle is commanded to drive off the road, onto a grassy
surface, and the classiﬁer is beginning to classify some of the grass as drivable. In (c) the
vehicle has updated its model of drivable surface to correspond to grass as well as road.
Probabilistic techniques outperform other approaches in many hard perceptual problems such
as localization and mapping. However, statistical techniques are sometimes too cumbersome,
and simpler solutions may be just as effective in practice. To help decide which approach to
take, experience working with real physical robots is your best teacher.
Machine learning in robot perception
Machine learning plays an important role in robot perception. This is particularly the case
EMBEDDING
multaneously discovering a suitable internal representations.
Another machine learning technique enables robots to continuously adapt to broad
the colors: Neon light has a stronger component of green light than sunlight. Yet somehow
don’t think that suddenly their faces turned green. Our perception quickly adapts to the new
lighting conditions, and our brain ignores the differences.
Adaptive perception techniques enable robots to adjust to such changes. One example
ground vehicle adapts its classiﬁer of the concept “drivable surface.” How does this work?
The robot uses a laser to provide classiﬁcation for a small area right in front of the robot.
When this area is found to be ﬂat in the laser range scan, it is used as a positive training
applying this classiﬁer to the full image.Chapter
Robotics
LEARNING
that works well for terrain classiﬁcation into a sensor that can see much farther. That allows
the robot to drive faster, slowing down only when the sensor model says there is a change in
PLANNING TO MOVE
All of a robot’s deliberations ultimately come down to deciding how to move effectors. The
MOTION
location. A greater challenge is the compliant motion problem, in which a robot moves
COMPLIANT MOTION
while being in physical contact with an obstacle. An example of compliant motion is a robot
manipulator that screws in a light bulb, or a robot that pushes a box across a table top.
be described and solved. It turns out that the conﬁguration space—the space of robot states
deﬁned by location, orientation, and joint angles—is a better place to work than the original
3D space. The path planning problem is to ﬁnd a path from one conﬁguration to another in
PATH PLANNING
lem throughout this book; the complication added by robotics is that path planning involves
continuous spaces. There are two main approaches: cell decomposition and skeletonization.
this section, we assume that motion is deterministic and that localization of the robot is exact.
Subsequent sections will relax these assumptions.
Conﬁguration space
We will start with a simple representation for a simple robot motion problem. Consider the
the joints alters the (x, y) coordinates of the elbow and the gripper. (The arm cannot move
dimensional coordinate: (xe, ye) for the location of the elbow relative to the environment and
(xg, yg) for the location of the gripper. Clearly, these four coordinates characterize the full
state of the robot. They constitute what is known as workspace representation, since the
WORKSPACE
REPRESENTATION
coordinates of the robot are speciﬁed in the same coordinate system as the objects it seeks to
especially if the robot and all objects are represented by simple polygonal models.
The problem with the workspace representation is that not all workspace coordinates
straints on the space of attainable workspace coordinates. For example, the elbow position
LINKAGE
CONSTRAINTS
(xe, ye) and the gripper position (xg, yg) are always a ﬁxed distance apart, because they are
joined by a rigid forearm. A robot motion planner deﬁned over workspace coordinates faces
the challenge of generating paths that adhere to these constraints. This is particularly trickyPlanning to Move
shou
elb
s
e
e
table
table
left wall
vertical
obstacle
s
e
s
(a)
(b)
is a box with a ﬂat obstacle hanging from the ceiling. (b) Conﬁguration space of the same
robot. Only white regions in the space are conﬁgurations that are free of collisions. The dot
in this diagram corresponds to the conﬁguration of the robot shown on the left.
ier to plan with a conﬁguration space representation. Instead of representing the state of the
CONFIGURATION
SPACE
robot by the Cartesian coordinates of its elements, we represent the state by a conﬁguration
of the robot’s joints. Our example robot possesses two joints. Hence, we can represent its
the absence of any obstacles, a robot could freely take on any value in conﬁguration space. In
particular, when planning a path one could simply connect the present conﬁguration and the
target conﬁguration by a straight line. In following this path, the robot would then move its
joints at a constant velocity, until a target location is reached.
ally expressed in workspace coordinates, not in conﬁguration space coordinates. This raises
forming conﬁguration space coordinates into workspace coordinates is simple: it involves
a series of straightforward coordinate transformations. These transformations are linear for
prismatic joints and trigonometric for revolute joints. This chain of coordinate transformation
is known as kinematics.
KINEMATICS
The inverse problem of calculating the conﬁguration of a robot whose effector location
is speciﬁed in workspace coordinates is known as inverse kinematics. Calculating the inverse
INVERSE
KINEMATICS
kinematics is hard, especially for robots with many DOFs. In particular, the solution is seldom
same location. (The other conﬁguration would has the elbow below the shoulder.)Chapter
Robotics
e
ss
e
(a)
(b)
Three robot conﬁgurations, shown in workspace and conﬁguration space.
tions for any set of workspace coordinates. Most industrial robots have sufﬁcient degrees
ble, simply imagine that we added a third revolute joint to our example robot, one whose
rotational axis is parallel to the ones of the existing joints. In such a case, we can keep the
location (but not the orientation!) of the gripper ﬁxed and still freely rotate its internal joints,
for most conﬁgurations of the robot. With a few more joints (how many?) we can achieve the
same effect while keeping the orientation of the gripper constant as well. We have already
seen an example of this in the “experiment” of placing your hand on the desk and moving
your elbow. The kinematic constraint of your hand position is insufﬁcient to determine the
conﬁguration of your elbow. In other words, the inverse kinematics of your shoulder–arm
assembly possesses an inﬁnite number of solutions.
workspace. In workspace, such obstacles take on simple geometric forms—especially in
most robotics textbooks, which tend to focus on polygonal obstacles. But how do they look
in conﬁguration space?
into two subspaces: the space of all conﬁgurations that a robot may attain, commonly called
free space, and the space of unattainable conﬁgurations, called occupied space. The white
FREE SPACE
OCCUPIED SPACEPlanning to Move
pied space. The different shadings of the occupied space corresponds to the different objects
in the robot’s workspace; the black region surrounding the entire free space corresponds to
conﬁgurations in which the robot collides with itself. It is easy to see that extreme values of
sides of the robot correspond to the table on which the robot is mounted. The third oval region
corresponds to the left wall. Finally, the most interesting object in conﬁguration space is the
vertical obstacle that hangs from the ceiling and impedes the robot’s motions. This object has
a funny shape in conﬁguration space: it is highly nonlinear and at places even concave. With
a little bit of imagination the reader will recognize the shape of the gripper at the upper left
end. We encourage the reader to pause for a moment and study this diagram. The shape of
the vertical obstacle.
Even if the robot’s workspace is represented by ﬂat polygons, the shape of the free space
can be very complicated. In practice, therefore, one usually probes a conﬁguration space
instead of constructing it explicitly. A planner may generate a conﬁguration and then test to
see if it is in free space by applying the robot kinematics and then checking for collisions in
workspace coordinates.
Cell decomposition methods
The ﬁrst approach to path planning uses cell decomposition—that is, it decomposes the
CELL
DECOMPOSITION
free space into a ﬁnite number of contiguous regions, called cells. These regions have the
shows a square grid decomposition of the space and a solution path that is optimal for this
the shortest path from that cell to the goal. (These values can be computed by a deterministic
shows the corresponding workspace trajectory for the arm. Of course, we can also use the A∗
algorithm to ﬁnd a shortest path.
Such a decomposition has the advantage that it is extremely simple to implement, but
ration spaces, because the number of grid cells increases exponentially with d, the number of
dimensions. Sounds familiar? This is the curse!dimensionality@of dimensionality. Second,
there is the problem of what to do with cells that are “mixed”—that is, neither entirely within
free space nor entirely within occupied space. A solution path that includes such a cell may
not be a real solution, because there may be no way to cross the cell in the desired direction
in a straight line. This would make the path planner unsound. On the other hand, if we insist
that only completely free cells may be used, the planner will be incomplete, because it mightChapter
Robotics
start
goal
start
goal
(a)
(b)
(a) Value function and path found for a discrete grid cell approximation of
the conﬁguration space. (b) The same path visualized in workspace coordinates. Notice how
the robot bends its elbow to avoid a collision with the vertical obstacle.
be the case that the only paths to the goal go through mixed cells—especially if the cell size
is comparable to that of the passageways and clearances in the space. And third, any path
through a discretized state space will not be smooth. It is generally difﬁcult to guarantee that
a smooth solution exists near the discrete path. So a robot may not be able to execute the
solution found through this decomposition.
Cell decomposition methods can be improved in a number of ways, to alleviate some
of these problems. The ﬁrst approach allows further subdivision of the mixed cells—perhaps
using cells of half the original size. This can be continued recursively until a path is found
that lies entirely within free cells. (Of course, the method only works if there is a way to
decide if a given cell is a mixed cell, which is easy only if the conﬁguration space boundaries
have relatively simple mathematical descriptions.) This method is complete provided there is
a bound on the smallest passageway through which a solution must pass. Although it focuses
most of the computational effort on the tricky areas within the conﬁguration space, it still
creates 2d smaller cells. A second way to obtain a complete algorithm is to insist on an exact
cell decomposition of the free space. This method must allow cells to be irregularly shaped
EXACT CELL
DECOMPOSITION
where they meet the boundaries of free space, but the shapes must still be “simple” in the
sense that it should be easy to compute a traversal of any free cell. This technique requires
some quite advanced geometric ideas, so we shall not pursue it further here.
culty that will have to be resolved. The path contains arbitrarily sharp corners; a robot moving
at any ﬁnite speed could not execute such a path. This problem is solved by storing certain
continuous values for each grid cell. Consider an algorithm which stores, for each grid cell,Planning to Move
the exact, continuous state that was attained with the cell was ﬁrst expanded in the search.
ous state as a basis, and apply the continuous robot motion model for jumping to nearby cells.
In doing so, we can now guarantee that the resulting trajectory is smooth and can indeed be
Modiﬁed cost functions
a car knows that a parking space with one millimeter of clearance on either side is not really a
parking space at all; for the same reason, we would prefer solution paths that are robust with
respect to small motion errors.
This problem can be solved by introducing a potential ﬁeld. A potential ﬁeld is a
POTENTIAL FIELD
function deﬁned over state space, whose value grows with the distance to the closest obstacle.
to an obstacle.
This induces an interesting tradeoff. On the one hand, the robot seeks to minimize path length
to the goal. On the other hand, it tries to stay away from obstacles by virtue of minimizing the
potential function. With the appropriate weight balancing the two objectives, a resulting path
derived from the combined cost function, again calculated by value iteration. Clearly, the
resulting path is longer, but it is also safer.
There exist many other ways to modify the cost function. For example, it may be
desirable to smooth the control parameters over time. For example, when driving a car, a
to accommodate in the planning process, unless we make the most recent steering command
a part of the state. However, it is often easy to smooth the resulting trajectory after planning,
world applications.
Skeletonization methods
SKELETONIZATION
the conﬁguration space.
VORONOI GRAPH
ning with a Voronoi graph, the robot ﬁrst changes its present conﬁguration to a point on the
in conﬁguration space. Second, the robot follows the Voronoi graph until it reaches the point
nearest to the target conﬁguration. Finally, the robot leaves the Voronoi graph and moves toChapter
Robotics
start
goal
(a)
(b)
(a) A repelling potential ﬁeld pushes the robot away from obstacles. (b)
Path found by simultaneously minimizing path length and the potential.
(a)
(b)
points in free space.Planning Uncertain Movements
path, but the resulting paths tend to maximize clearance. Disadvantages of Voronoi graph
that they tend to induce unnecessarily large detours when the conﬁguration space is wide
open. Furthermore, computing the Voronoi graph can be difﬁcult, especially in conﬁguration
space, where the shapes of obstacles can be complex.
An alternative to the Voronoi graphs is the probabilistic roadmap, a skeletonization
PROBABILISTIC
ROADMAP
generating a large number of conﬁgurations, and discarding those that do not fall into free
space. Two nodes are joined by an arc if it is “easy” to reach one node from the other–for
example, by a straight line in free space. The result of all this is a randomized graph in the
robot’s free space. If we add the robot’s start and goal conﬁgurations to this graph, path
cause a bad choice of random points may leave us without any paths from start to goal. It
is possible to bound the probability of failure in terms of the number of points generated
and certain geometric properties of the conﬁguration space. It is also possible to direct the
generation of sample points towards the areas where a partial search suggests that a good
path may be found, working bidirectionally from both the start and the goal positions. With
PLANNING UNCERTAIN MOVEMENTS
tic of robotics problems: uncertainty. In robotics, uncertainty arises from partial observability
rors can also arise from the use of approximation algorithms such as particle ﬁltering, which
ronment is modeled perfectly.
Most of today’s robots use deterministic algorithms for decision making, such as the
MOST LIKELY STATE
gorithm. The advantage of this approach is purely computational. Planning paths through
conﬁguration space is already a challenging problem; it would be worse if we had to work
with a full probability distribution over states. Ignoring uncertainty in this way works when
the uncertainty is small. In fact, when the environment model changes over time as the result
of incorporating sensor measurements, many robots plan paths online during plan execution.
ONLINE REPLANNINGChapter
Robotics
Unfortunately, ignoring the uncertainty does not always work. In some problems the
robot’s uncertainty is simply too massive: How can we use a deterministic path planner to
control a mobile robot that has no clue where it is? In general, if the robot’s true state is not
the one identiﬁed by the maximum likelihood rule, the resulting control will be suboptimal.
Depending on the magnitude of the error this can lead to all sorts of unwanted effects, such
as collisions with obstacles.
The ﬁeld of robotics has adopted a range of techniques for accommodating uncertainty.
tainty. If the robot faces uncertainty only in its state transition, but its state is fully observable,
the problem is best modeled as a Markov decision process (MDP). The solution of an MDP is
an optimal policy, which tells the robot what to do in every possible state. In this way, it can
would be much less robust. In robotics, policies are called navigation functions. The value
NAVIGATION
FUNCTION
by following the gradient.
ing robot control problem is a partially observable MDP, or POMDP. In such situations, the
to a POMDP is a policy deﬁned over the robot’s belief state. Put differently, the input to
the policy is an entire probability distribution. This enables the robot to base its decision not
only on what it knows, but also on what it does not know. For example, if it is uncertain
about a critical state variable, it can rationally invoke an information gathering action. This
INFORMATION
GATHERING ACTION
is impossible in the MDP framework, since MDPs assume full observability. Unfortunately,
tive. For example, the coastal navigation heuristic requires the robot to stay near known
COASTAL
NAVIGATION
tic roadmap planning method to the belief space representation. Such methods tend to scale
better to large discrete POMDPs.
Robust methods
ROBUST CONTROL
certainty in each aspect of a problem, but does not assign probabilities to values within the
allowed interval. A robust solution is one that works no matter what actual values occur,
information at all.
PLANNINGPlanning Uncertain Movements
v
Cv
motion
envelope
initial
configuration
possible robot motions. The intended velocity is v, but with uncertainty the actual velocity
could be anywhere in Cv, resulting in a ﬁnal conﬁguration somewhere in the motion envelope,
which means we wouldn’t know if we hit the hole or not.
v
Cv
motion
envelope
initial
configuration
tions. No matter what the error, we know the ﬁnal conﬁguration will be to the left of the
hole.
that the required motions and the relevant features of the environment are very small. At such
small scales, the robot is unable to measure or control its position accurately and may also be
uncertain of the shape of the environment itself; we will assume that these uncertainties are
all bounded. The solutions to FMP problems will typically be conditional plans or policies
that make use of sensor feedback during execution and are guaranteed to work in all situations
consistent with the assumed uncertainty bounds.
GUARDED MOTION
robot’s sensor values, and returns true to indicate the end of the guarded move. The motion
commands are typically compliant motions that allow the effector to slide if the motion
COMPLIANT MOTION
dimensional conﬁguration space with a narrow vertical hole. It could be the conﬁguration
space for insertion of a rectangular peg into a hole or a car key into the ignition. The motion
commands are constant velocities. The termination conditions are contact with a surface. To
model uncertainty in control, we assume that instead of moving in the commanded direction,
the robot’s actual motion lies in the cone Cv about it. The ﬁgure shows what would happenChapter
Robotics
v
Cv
motion
envelope
The second motion command and the envelope of possible motions. Even
with error, we will eventually get into the hole.
if we commanded a velocity straight down from the initial conﬁguration. Because of the
uncertainty in velocity, the robot could move anywhere in the conical envelope, possibly
going into the hole, but more likely landing to one side of it. Because the robot would not
then know which side of the hole it was on, it would not know which way to move.
robot deliberately moves to one side of the hole. The motion command is shown in the ﬁgure,
given that causes the robot to slide along the surface and into the hole. Because all possible
velocities in the motion envelope are to the right, the robot will slide to the right whenever it
hole when it touches it, because all possible velocities are down relative to a vertical surface.
It will keep moving until it reaches the bottom of the hole, because that is its termination
condition. In spite of the control uncertainty, all possible trajectories of the robot terminate
in contact with the bottom of the hole—that is, unless surface irregularities cause the robot to
stick in one place.
fact, it is a good deal harder than planning with exact motions. One can either choose a
ﬁxed number of discrete values for each motion or use the environment geometry to choose
of what sensing is possible for termination (surface contact in this case). It should produce a
multistep conditional plan or policy that is guaranteed to succeed, if such a plan exists.
Our example assumes that the planner has an exact model of the environment, but it is
possible to allow for bounded error in this model as follows. If the error can be described in
terms of parameters, those parameters can be added as degrees of freedom to the conﬁguration
space. In the last example, if the depth and width of the hole were uncertain, we could add
them as two degrees of freedom to the conﬁguration space. It is impossible to move the
robot in these directions in the conﬁguration space or to sense its position directly. But
both those restrictions can be incorporated when describing this problem as an FMP problem
dimensional planning problem, but exactly the same planning techniques can be applied.Moving
during execution is much worse than any of the other costs involved in execution.
MOVING
So far, we have talked about how to plan motions, but not about how to move. Our plans—
particularly those produced by deterministic path planners—assume that the robot can simply
follow any path that the algorithm produces. In the real world, of course, this is not the case.
Robots have inertia and cannot execute arbitrary paths except at arbitrarily slow speeds. In
most cases, the robot gets to exert forces rather than specify positions. This section discusses
methods for calculating these forces.
Dynamics and control
robot by its velocity. For example, in addition to the angle of a robot joint, the dynamic state
also captures the rate of change of the angle, and possibly even its momentary acceleration.
The transition model for a dynamic state representation includes the effect of forces on this
rate of change. Such models are typically expressed via differential equations, which are
DIFFERENTIAL
EQUATION
equations that relate a quantity (e.g., a kinematic state) to the change of the quantity over
time (e.g., velocity). In principle, we could have chosen to plan robot motion using dynamic
models, instead of our kinematic models. Such a methodology would lead to superior robot
sion than the kinematic space, and the curse of dimensionality would render many motion
planning algorithms inapplicable for all but the most simple robots. For this reason, practical
robot system often rely on simpler kinematic path planners.
A common technique to compensate for the limitations of kinematic plans is to use a
separate mechanism, a controller, for keeping the robot on track. Controllers are techniques
CONTROLLER
for generating robot controls in real time using feedback from the environment, so as to
achieve a control objective. If the objective is to keep the robot on a preplanned path, it is
often referred to as a reference controller and the path is called a reference path. Controllers
REFERENCE
CONTROLLER
REFERENCE PATH
that optimize a global cost function are known as optimal controllers. Optimal policies for
OPTIMAL
CONTROLLERS
continuous MDPs are, in effect, optimal controllers.
On the surface, the problem of keeping a robot on a prespeciﬁed path appears to be
relatively straightforward. In practice, however, even this seemingly simple problem has its
attempts to follow a kinematic path. Whenever a deviation occurs—whether due to noise or
to constraints on the forces the robot can apply—the robot provides an opposing force whose
magnitude is proportional to this deviation. Intuitively, this might appear plausible, since
deviations should be compensated by a counterforce to keep the robot on track. However,Chapter
Robotics
(a)
(b)
(c)
cases the robot arm tries to follow the path shown in gray.
vibration is the result of a natural inertia of the robot arm: once driven back to its reference
position the robot then overshoots, which induces a symmetric error with opposite sign. Such
overshooting may continue along an entire trajectory, and the resulting robot motion is far
from desirable.
Before we can deﬁne a better controller, let us formally describe what went wrong.
Controllers that provide force in negative proportion to the observed error are known as P
P CONTROLLER
portional to the error of the robot manipulator. More formally, let y(t) be the reference path,
parameterized by time index t. The control at generated by a P controller has the form:
at = KP (y(t) −xt) .
Here xt is the state of the robot at time t and KP is a constant known as the gain parameter of
GAIN PARAMETER
the controller and its value is called the gain factor); Kp regulates how strongly the controller
corrects for deviations between the actual state xt and the desired one y(t). In our example,
simply slow down the oscillation, but do not solve the problem. In fact, in the absence of
friction, the P controller is essentially a spring law; so it will oscillate indeﬁnitely around a
ﬁxed target location.
Traditionally, problems of this type fall into the realm of control theory, a ﬁeld of
increasing importance to researchers in AI. Decades of research in this ﬁeld have led to a large
number of controllers that are superior to the simple control law given above. In particular, a
reference controller is said to be stable if small perturbations lead to a bounded error between
STABLE
the robot and the reference signal. It is said to be strictly stable if it is able to return to and
STRICTLY STABLEMoving
then stay on its reference path upon such perturbations. Our P controller appears to be stable
but not strictly stable, since it fails to stay anywhere near its reference trajectory.
The simplest controller that achieves strict stability in our domain is a PD controller.
PD CONTROLLER
The letter ‘P’ stands again for proportional, and ‘D’ stands for derivative. PD controllers are
described by the following equation:
at = KP (y(t) −xt) + KD
∂(y(t) −xt)
∂t
.
As this equation suggests, PD controllers extend P controllers by a differential component,
which adds to the value of at a term that is proportional to the ﬁrst derivative of the error
y(t) −xt over time. What is the effect of such a term? In general, a derivative term dampens
the system that is being controlled. To see this, consider a situation where the error (y(t)−xt)
is changing rapidly over time, as is the case for our P controller above. The derivative of this
error will then counteract the proportional term, which will reduce the overall response to
the perturbation. However, if the same error persists and does not change, the derivative will
vanish and the proportional term dominates the choice of control.
does not exhibit any obvious oscillations.
PD controllers do have failure modes, however. In particular, PD controllers may fail
to regulate an error down to zero, even in the absence of external perturbations. Often such
tonomous car driving on a banked surface, for example, may ﬁnd itself systematically pulled
to one side. Wear and tear in robot arms cause similar systematic errors. In such situations,
problem lies in adding a third term to the control law, based on the integrated error over time:
at = KP (y(t) −xt) + KI

(y(t) −xt)dt + KD
∂(y(t) −xt)
∂t
.
Here KI is yet another gain parameter. The term
+
(y(t) −xt)dt calculates the integral of the
signal and the actual state are corrected. If, for example, xt is smaller than y(t) for a long
period of time, this integral will grow until the resulting control at forces this error to shrink.
Integral terms, then, ensure that a controller does not exhibit systematic error, at the expense
of increased danger of oscillatory behavior. A controller with all three terms is called a PID
controller (for proportional integral derivative). PID controllers are widely used in industry,
PID CONTROLLER
for a variety of control problems.
We introduced potential ﬁelds as an additional cost function in robot motion planning, but
they can also be used for generating robot motion directly, dispensing with the path planning
phase altogether. To achieve this, we have to deﬁne an attractive force that pulls the robot
towards its goal conﬁguration and a repellent potential ﬁeld that pushes the robot away fromChapter
Robotics
start
goal
start
goal
(a)
(b)
Potential ﬁeld control. The robot ascends a potential ﬁeld composed of
repelling forces asserted from the obstacles and an attracting force that corresponds to the
goal conﬁguration. (a) Successful path. (b) Local optimum.
the goal conﬁguration, and the value is the sum of the distance to this goal conﬁguration
and the proximity to obstacles. No planning was involved in generating the potential ﬁeld
ﬁeld. In many applications, the potential ﬁeld can be calculated efﬁciently for any given
conﬁguration. Moreover, optimizing the potential amounts to calculating the gradient of the
potential for the present robot conﬁguration. These calculations can be extremely efﬁcient,
dimensionality of the conﬁguration space (the DOFs) in the worst case.
The fact that the potential ﬁeld approach manages to ﬁnd a path to the goal in such
an efﬁcient manner, even over long distances in conﬁguration space, raises the question as
to whether there is a need for planning in robotics at all. Are potential ﬁeld techniques
sufﬁcient, or were we just lucky in our example? The answer is that we were indeed lucky.
approaches the obstacle by simply rotating its shoulder joint, until it gets stuck on the wrong
side of the obstacle. The potential ﬁeld is not rich enough to make the robot bend its elbow
so that the arm ﬁts under the obstacle. In other words, potential ﬁeld control is great for local
robot motion but sometimes we still need global planning. Another important drawback with
potential ﬁelds is that the forces they generate depend only on the obstacle and robot positions,
not on the robot’s velocity. Thus, potential ﬁeld control is really a kinematic method and may
fail if the robot is moving quickly.Moving
S1
S2
S4
S3
push backward
lift up
set down
retract, lift higher
move
forward
no
yes
stuck?
(a)
(b)
(a) Genghis, a hexapod robot.
(b) An augmented ﬁnite state machine
(AFSM) for the control of a single leg. Notice that this AFSM reacts to sensor feedback:
if a leg is stuck during the forward swinging phase, it will be lifted increasingly higher.
Reactive control
So far we have considered control decisions that require some model of the environment for
constructing either a reference path or a potential ﬁeld. There are some difﬁculties with this
approach. First, models that are sufﬁciently accurate are often difﬁcult to obtain, especially
in complex or remote environments, such as the surface of Mars, or for robots that have
few sensors. Second, even in cases where we can devise a model with sufﬁcient accuracy,
computational difﬁculties and localization error might render these techniques impractical.
In some cases, a reﬂex agent architecture using reactive control is more appropriate.
REACTIVE CONTROL
For example, picture a legged robot that attempts to lift a leg over an obstacle. We could
give this robot a rule that says lift the leg a small height h and move it forward, and if the leg
encounters an obstacle, move it back and start again at a higher height. You could say that h
is modeling an aspect of the world, but we can also think of h as an auxiliary variable of the
robot controller, devoid of direct physical meaning.
els of the terrain for path planning. Moreover, even if we added sufﬁciently accurate sensors,
the twelve degrees of freedom (two for each leg) would render the resulting path planning
problem computationally intractable.
mental model. (We have already seen this with the PD controller, which was able to keep a
complex robot arm on target without an explicit model of the robot dynamics; it did, however,
require a reference path generated from a kinematic model.) For the hexapod robot we ﬁrst
choose a gait, or pattern of movement of the limbs. One statically stable gait is to ﬁrst move
GAIT
the right front, right rear, and left center legs forward (keeping the other three ﬁxed), and
then move the other three. This gait works well on ﬂat terrain. On rugged terrain, obstacles
may prevent a leg from swinging forward. This problem can be overcome by a remarkably
simple control rule: when a leg’s forward motion is blocked, simply retract it, lift it higher,Chapter
Robotics
Multiple exposures of an RC helicopter executing a ﬂip based on a policy
learned with reinforcement learning. Images courtesy of Andrew Ng, Stanford University.
it constitutes a reﬂex agent with state, where the internal state is represented by the index of
the current machine state (s1 through s4).
ably robust walking patterns, capable of maneuvering the robot over rugged terrain. Clearly,
trols. Environmental feedback plays a crucial role in the controller’s execution. The software
alone does not specify what will actually happen when the robot is placed in an environment.
ronment is often referred to as emergent behavior. Strictly speaking, all robots discussed
EMERGENT
BEHAVIOR
cally, however, the term has been reserved for control techniques that do not utilize explicit
environmental models. Emergent behavior is also characteristic of biological organisms.
Reinforcement learning control
One particularly exciting form of control is based on the policy search form of reinforcement
is has solved challenging robotics problems for which previously no solution existed. An
nonlinear nature of the aerodynamics involved. Only the most experienced of human pilots
few minutes of computation, learned a policy that can safely execute a ﬂip every time.
Policy search needs an accurate model of the domain before it can ﬁnd a policy. The
input to this model is the state of the helicopter at time t, the controls at time t, and the
of the vehicle, its yaw, pitch, and roll angles, and the rate of change of these six variables.
The controls are the manual controls of of the helicopter: throttle, pitch, elevator, aileron,
and rudder. All that remains is the resulting state—how are we going to deﬁne a model that
accurately says how the helicopter responds to each control? The answer is simple: Let an
expert human pilot ﬂy the helicopter, and record the controls that the expert transmits over
ﬂight sufﬁces to build a predictive model that is sufﬁciently accurate to simulate the vehicle.Robotic Software Architectures
What is remarkable about this example is the ease with which this learning approach
solves a challenging robotics problem. This is one of the many successes of machine learning
in scientiﬁc ﬁelds previously dominated by careful mathematical analysis and modeling.
ROBOTIC SOFTWARE ARCHITECTURES
A methodology for structuring algorithms is called a software architecture. An architecture
SOFTWARE
ARCHITECTURE
includes languages and tools for writing programs, as well as an overall philosophy for how
programs can be brought together.
not be sensed at the time of decision making. For such problems, deliberate planning is a
more appropriate choice.
Consequently, most robot architectures use reactive techniques at the lower levels of
control and deliberative techniques at the higher levels. We encountered such a combination
in our discussion of PD controllers, where we combined a (reactive) PD controller with a
(deliberate) path planner. Architectures that combine reactive and deliberate techniques are
called hybrid architectures.
HYBRID
ARCHITECTURE
Subsumption architecture
SUBSUMPTION
ARCHITECTURE
trollers out of ﬁnite state machines. Nodes in these machines may contain tests for certain
sensor variables, in which case the execution trace of a ﬁnite state machine is conditioned on
the outcome of such a test. Arcs can be tagged with messages that will be generated when
tionally, ﬁnite state machines possess internal timers (clocks) that control the time it takes to
traverse an arc. The resulting machines are refereed to as augmented ﬁnite state machines,
AUGMENTED FINITE
STATE MACHINE
or AFSMs, where the augmentation refers to the use of clocks.
which generates cyclic leg motion for a hexapod walker. This AFSM implements a cyclic
controller, whose execution mostly does not rely on environmental feedback. The forward
swing phase, however, does rely on sensor feedback. If the leg is stuck, meaning that it has
failed to execute the forward swing, the robot retracts the leg, lifts it up a little higher, and
attempts to execute the forward swing once again. Thus, the controller is able to react to
contingencies arising from the interplay of the robot and its environment.
The subsumption architecture offers additional primitives for synchronizing AFSMs,
and for combining output values of multiple, possibly conﬂicting AFSMs. In this way, itChapter
Robotics
In our example, we might begin with AFSMs for individual legs, followed by an AFSM for
as collision avoidance, which might involve backing up and turning.
The idea of composing robot controllers from AFSMs is quite intriguing. Imagine
rate model of the terrain. The conﬁguration space of a robot with six legs, each of which
is driven by two independent motors, totals eighteen dimensions (twelve dimensions for the
conﬁguration of the legs, and six for the location and orientation of the robot relative to its
spaces, we would have to worry about nasty effects such as the robot sliding down a slope.
Because of such stochastic effects, a single path through conﬁguration space would almost
tingencies. In other words, generating motion behavior deliberately is simply too complex a
Unfortunately, the subsumption architecture has its own problems. First, the AFSMs
are driven by raw sensor input, an arrangement that works if the sensor data is reliable and
style robot usually does just one task, and it has no notion of how to modify its controls to
style controllers tend to be difﬁcult to understand. In practice, the intricate interplay between
mers can comprehend. For all these reasons, the subsumption architecture is rarely used in
robotics, despite its great historical importance. However, it has had an inﬂuence on other
architectures, and on individual components of some architectures.
ARCHITECTURE
and a deliberative layer.
REACTIVE LAYER
sensor–action loop. Its decision cycle is often on the order of milliseconds.
The executive layer (or sequencing layer) serves as the glue between the reactive layer
EXECUTIVE LAYER
and the deliberative layer. It accepts directives by the deliberative layer, and sequences them
generated by a deliberative path planner, and make decisions as to which reactive behavior
to invoke. Decision cycles at the executive layer are usually in the order of a second. The
executive layer is also responsible for integrating sensor information into an internal state
representation. For example, it may host the robot’s localization and online mapping routines.Robotic Software Architectures
Touareg interface 
Laser mapper 
Top level control 
Camera interface 
Radar interface 
Radar mapper 
Vision mapper 
UKF Pose estimation 
Wheel velocity 
GPS position 
GPS compass 
IMU interface 
Surface assessment 
Health monitor 
Road finder 
Touch screen UI 
Steering control 
Path planner 
laser map 
vehicle state (pose, velocity) 
velocity limit 
map 
vision map 
vehicle 
state 
obstacle list 
trajectory 
road center 
RDDF database 
driving mode 
Power server interface 
clocks 
emergency stop 
heart beats 
corridor 
    SENSOR INTERFACE                PERCEPTION                  PLANNING&CONTROL        USER INTERFACE 
VEHICLE 
INTERFACE 
RDDF corridor (smoothed and original) 
Process controller 
GLOBAL 
SERVICES 
health status 
data 
Data logger 
File system 
Communication requests 
vehicle state (pose, velocity) 
Communication channels 
Time server 
Software architecture of a robot car.
This software implements a data
pipeline, in which all modules process data simultaneously.
The deliberative layer generates global solutions to complex tasks using planning.
DELIBERATIVE LAYER
Because of the computational complexity involved in generating such solutions, its decision
cycle is often in the order of minutes. The deliberative layer (or planning layer) uses models
for decision making. Those models might be either learned from data or supplied and may
utilize state information gathered at the executive layer.
systems. The decomposition into three layers is not very strict. Some robot software systems
possess additional layers, such as user interface layers that control the interaction with people,
or a multiagent level for coordinating a robot’s actions with that of other robots operating in
the same environment.
Pipeline architecture
PIPELINE
ARCHITECTURE
tion architecture, the pipeline architecture executes multiple process in parallel. However, the
tonomous car. Data enters this pipeline at the sensor interface layer. The perception layer
SENSOR INTERFACE
LAYER
PERCEPTION LAYERChapter
Robotics
(a)
(b)
(a) The Helpmate robot transports food and other medical items in dozens
shelves in fulﬁllment centers. Image courtesy of Kiva Systems.
then updates the robot’s internal models of the environment based on this data. Next, these
models are handed to the planning and control layer, which adjusts the robot’s internal
PLANNING AND
CONTROL LAYER
plans turns them into actual controls for the robot. Those are then communicated back to the
vehicle through the vehicle interface layer.
VEHICLE INTERFACE
LAYER
ception layer processes the most recent sensor data, the control layer bases its choices on
slightly older data. In this way, the pipeline architecture is similar to the human brain. We
don’t switch off our motion controllers when we digest new sensor data. Instead, we perceive,
plan, and act all at the same time. Processes in the pipeline architecture run asynchronously,
for establishing communication between the different elements of the pipeline.
APPLICATION DOMAINS
Here are some of the prime application domains for robotic technology.
Industry and Agriculture. Traditionally, robots have been ﬁelded in areas that require
difﬁcult human labor, yet are structured enough to be amenable to robotic automation. The
sembly, part placement, material handling, welding, and painting. In many of these tasks,
machines that we use to harvest, mine, or excavate earth have been turned into robots. ForApplication Domains
(a)
(b)
(a) Robotic car BOSS, which won the DARPA Urban Challenge. Courtesy
of Carnegie Mellon University. (b) Surgical robots in the operating room. Image courtesy of
da Vinci Surgical Systems.
example, a project at Carnegie Mellon University has demonstrated that robots can strip paint
cise than people in transporting ore in underground mines. Robots have been used to generate
are still in their prototype stages, it is only a matter of time until robots will take over much
of the semimechanical work that is presently performed by people.
Transportation. Robotic transportation has many facets: from autonomous helicopters
ple who are unable to control wheelchairs by themselves, to autonomous straddle carriers that
ing docks. A prime example of indoor transportation robots, or gofers, is the Helpmate robot
food and other items. In factory settings, autonomous vehicles are now routinely deployed
to transport goods in warehouses and between production lines. The Kiva system, shown in
Many of these robots require environmental modiﬁcations for their operation. The most
cons, or barcode tags. An open challenge in robotics is the design of robots that can use
natural cues, instead of artiﬁcial devices, to navigate, particularly in environments such as the
deep ocean where GPS is unavailable.
Robotic cars. Most of use cars every day. Many of us make cell phone calls while
driving. Some of us even text. The sad result: more than a million people die every year in
trafﬁc accidents. Robotic cars like BOSS and STANLEY offer hope: Not only will they make
driving much safer, but they will also free us from the need to pay attention to the road during
our daily commute.
Progress in robotic cars was stimulated by the DARPA Grand Challenge, a race overChapter
Robotics
(a)
(b)
(a) A robot mapping an abandoned coal mine. (b) A 3D map of the mine
acquired by the robot.
had ever been accomplished before. Stanford’s STANLEY vehicle completed the course in less
Challenge, a complicated road race on city streets where robots faced other robots and had to
obey trafﬁc rules.
Health care. Robots are increasingly used to assist surgeons with instrument placement
a system. Robots have become indispensable tools in a range of surgical procedures, such as
hip replacements, thanks to their high precision. In pilot studies, robotic devices have been
found to reduce the danger of lesions when performing colonoscopy. Outside the operating
room, researchers have begun to develop robotic aides for elderly and handicapped people,
tion and provide comfort. Researchers are also working on robotic devices for rehabilitation
that aid people in performing certain exercises.
Hazardous environments. Robots have assisted people in cleaning up nuclear waste,
most notably in Chernobyl and Three Mile Island. Robots were present after the collapse
of the World Trade Center, where they entered structures deemed too dangerous for human
search and rescue crews.
toriously dangerous task. A number of research projects are presently developing prototype
robots for clearing mineﬁelds, on land and at sea. Most existing robots for these tasks are
omy is an important next step.
Exploration. Robots have gone where no one has gone before, including the surface
and retrieving satellites and in building the International Space Station. Robots also help
shows a robot mapping an abandoned coal mine, along with a 3D model of the mine acquiredApplication Domains
(a)
(b)
of University of Washington and Carnegie Mellon University.
of an active volcano to acquire data for climate research. Unmanned air vehicles known as
drones are used in military operations. Robots are becoming very effective tools for gathering
DRONE
information in domains that are difﬁcult (or dangerous) for people to access.
vice robots assist individuals in performing daily tasks. Commercially available domestic
service robots include autonomous vacuum cleaners, lawn mowers, and golf caddies. The
world’s most popular mobile robot is a personal service robot: the robotic vacuum cleaner
More than three million Roombas have been sold.
ROOMBA
Roomba can navigate autonomously and perform its tasks without human help.
Other service robots operate in public places, such as robotic information kiosks that
vice tasks require human interaction, and the ability to cope robustly with unpredictable and
dynamic environments.
Entertainment. Robots have begun to conquer the entertainment and toy industry.
ROBOTIC SOCCER
cer, but played with autonomous mobile robots. Robot soccer provides great opportunities
for research in AI, since it raises a range of problems relevant to many other, more serious
robot applications. Annual robotic soccer competitions have attracted large numbers of AI
researchers and added a lot of excitement to the ﬁeld of robotics.
Human augmentation. A ﬁnal application domain of robotic technology is that of
human augmentation. Researchers have developed legged walking machines that can carry
people around, very much like a wheelchair. Several research efforts presently focus on the
development of devices that make it easier for people to walk or move their arms by providing
additional forces through extraskeletal attachments. If such devices are attached permanently,Chapter
Robotics
may serve as a prosthetic device in the future.
operation involves carrying out tasks over long distances with the aid of robotic devices.
A popular conﬁguration for robotic teleoperation is the master–slave conﬁguration, where
a robot manipulator emulates the motion of a remote human operator, measured through a
haptic interface. Underwater vehicles are often teleoperated; the vehicles can go to a depth
that would be dangerous for humans but can still be guided by the human operator. All these
systems augment people’s ability to interact with their environments. Some projects go as far
as replicating humans, at least at a very superﬁcial level. Humanoid robots are now available
commercially through several companies in Japan.
SUMMARY
Robotics concerns itself with intelligent agents that manipulate the physical world. In this
chapter, we have learned the following basics of robot hardware and software.
• Robots are equipped with sensors for perceiving their environment and effectors with
which they can assert physical forces on their environment.
Most robots are either
manipulators anchored at ﬁxed locations or mobile robots that can move.
sensor data. To do so, we need an internal representation and a method for updating
this internal representation over time. Common examples of hard perceptual problems
include localization, mapping, and object recognition.
• Probabilistic ﬁltering algorithms such as Kalman ﬁlters and particle ﬁlters are useful
for robot perception. These techniques maintain the belief state, a posterior distribution
over state variables.
• The planning of robot motion is usually done in conﬁguration space, where each point
speciﬁes the location and orientation of the robot and its joint angles.
• Conﬁguration space search algorithms include cell decomposition techniques, which
decompose the space of all conﬁgurations into ﬁnitely many cells, and skeletonization
motion planning problem is then solved using search in these simpler structures.
• A path found by a search algorithm can be executed by using the path as the reference
trajectory for a PID controller. Controllers are necessary in robotics to accommodate
small perturbations; path planning alone is usually insufﬁcient.
tance to obstacles and the goal location. Potential ﬁeld techniques may get stuck in
local minima, but they can generate motion directly without the need for path planning.
• Sometimes it is easier to specify a robot controller directly, rather than deriving a path
from an explicit model of the environment. Such controllers can often be written as
simple ﬁnite state machines.Bibliographical and Historical Notes
ture enables programmers to compose robot controllers from interconnected ﬁnite state
software that integrate deliberation, sequencing of subgoals, and control. The related
sponding to perception, modeling, planning, control, and robot interfaces.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
structed mechanically, end up resenting their masters and decide to take over. It appears
a much longer history, however. In ancient Greek mythology, a mechanical man named Talos
was supposedly designed and built by Hephaistos, the Greek god of metallurgy. Wonderful
being one early example—but the complex behaviors they exhibited were entirely ﬁxed in
UNIMATE
was also the year when Devol obtained the ﬁrst U.S. patent on a robot. Eleven years later, in
veloped by Kawasaki with robots supplied by Engelberger and Devol’s company Unimation.
This development initiated a major revolution that took place mostly in Japan and the U.S.,
PUMA
robot, short for Programmable Universal Machine for Assembly. The PUMA robot, initially
developed for General Motors, was the de facto standard for robotic manipulation for the two
decades that followed. At present, the number of operating robots is estimated at one million
worldwide, more than half of which are installed in Japan.
The literature on robotics research can be divided roughly into two parts: mobile robots
mobile robot was “Shakey,” developed at what was then the Stanford Research Institute (nowChapter
Robotics
to integrate perception, planning, and execution, and much subsequent research in AI was
inﬂuenced by this remarkable achievement. Shakey appears on the cover of this book with
on autonomous vehicles.
The ﬁeld of robotic mapping has evolved from two distinct origins. The ﬁrst thread
multaneous localization and mapping problem. This algorithm was ﬁrst implemented by
thread began with the development of the occupancy grid representation for probabilistic
OCCUPANCY GRID
topological rather than metric mapping, motivated by models of human spatial cognition. A
ization and mapping problem, which gave rise to the development of nonlinear optimization
duced the EM algorithm into the ﬁeld of robotic mapping for data association. An overview
Although Kalman ﬁltering was well known as a localization method in control theory for
decades, the general probabilistic formulation of the localization problem did not appear
in the AI literature until much later, through the work of Tom Dean and colleagues (Dean
MARKOV
LOCALIZATION
BLACKWELLIZED
PARTICLE FILTER
The study of manipulator robots, originally called hand–eye machines, has evolved
HAND–EYE
MACHINES
of papers by Schwartz and Sharir on what they called piano movers problems (Schwartz
PIANO MOVERS
Recursive cell decomposition for conﬁguration space planning was originated by BrooksBibliographical and Historical Notes
VISIBILITY GRAPH
The control of robots as dynamical systems—whether for manipulation or navigation—
has generated a huge literature that is barely touched on by this chapter. Important works
together control theory and AI planning systems. Three classic textbooks on the mathematics
of grasping is also important in robotics—the problem of determining a stable grasp is quite
GRASPING
HAPTIC FEEDBACK
robotics, this idea was viewed as a practical solution to the collision avoidance problem, and
VECTOR FIELD
HISTOGRAMS
Navigation functions, the robotics version of a control policy for deterministic MDPs, were
in the context of autonomous helicopter control.
The topic of software architectures for robots engenders much religious debate. The
tant competitions. The earliest competition, AAAI’s annual mobile robot competition, beganChapter
Robotics
plex, found their way to the registration desk, registered for the conference, and even gave a
ROBOCUP
skills over the past decade.
DARPA GRAND
CHALLENGE
URBAN CHALLENGE
in an urban environment with other trafﬁc. Carnegie Mellon University’s robot BOSS took
provide an introduction into probabilistic robotics. The premiere conference for robotics is
Robotics: Science and Systems Conference, followed by the IEEE International Conference
tion, the International Journal of Robotics Research, and Robotics and Autonomous Systems.
EXERCISES
Monte Carlo localization is biased for any ﬁnite sample size—i.e., the expected value
of the location computed by the algorithm differs from the true expected value—because of
the way particle ﬁltering works. In this question, you are asked to quantify this bias.
To simplify, consider a world with four possible robot locations: X = {x1, x2, x3, x4}.
perfectly acceptable if more than one sample is generated for any of the locations X. Let Z
be a Boolean sensor variable characterized by the following conditional probabilities:
P(¬z | x1)
=
P(¬z | x2)
=
P(¬z | x3)
=
P(¬z | x4)
=Chapter
Robotics
manipulation, so the rods do not interfere with the movement of the blocks. A conﬁguration
indicating the permitted and excluded zones.
ﬁguration. Consider a potential function
where D(A, B) is the distance between the closest points of A and B.
a. Show that hill climbing in this potential ﬁeld will get stuck in a local minimum.
b. Describe a potential ﬁeld where hill climbing will solve this particular problem. You
need not work out the exact numerical coefﬁcients needed, just the general form of the
solution. (Hint: Add a term that “rewards” the hill climber for moving A out of B’s
way, even in a case like this where this does not reduce the distance from A to B in the
above sense.)
the inverse kinematics for this arm. Under what exact conditions is the solution unique?
vironment, described by an n × n Boolean array. Illustrate your algorithm by plotting the
This exercise explores the relationship between workspace and conﬁguration space
obstacle shown in each of the diagrams. Draw the corresponding arm conﬁgurations in
obstacles and the different locations of the obstacles in these individual ﬁgures.)
of the robot arm in workspace. Please ignore the shaded regions in this exercise.
collision constraint (light shading). Draw, for each diagram, the workspace obstacle
that corresponds to the darkly shaded area.Chapter
Robotics
robot
sensor
range
goal
b. Now suppose that the robot has imperfect control of its own rotation, and that, if it
In that case, if the robot attempts to carry out the sequence of actions in (A), there is
a range of possible ending states. What are the minimal and maximal values of the
c. Let us modify the model in (B) to a probabilistic model in which, when the robot
(Do not attempt to calculate the true mean or the true distribution.)
The point of this exercise is that rotational uncertainty quickly gives rise to a lot of
positional uncertainty and that dealing with rotational uncertainty is painful, whether
uncertainty is treated in terms of hard intervals or probabilistically, due to the fact that
coordinates are known at all times, as are those of its goal location. However, the locations
of the obstacles are unknown. The robot can sense obstacles in its immediate proximity, as
to address all possible grid worlds with a valid path from the start to the goal location.
a. Design a deliberate controller that guarantees that the robot always reaches its goal
location if at all possible. The deliberate controller can memorize measurements in the
form of a map that is being acquired as the robot moves. Between individual moves, it
may spend arbitrary time deliberating.PHILOSOPHICAL
FOUNDATIONS
In which we consider what it means to think and whether artifacts could and
should ever do so.
Philosophers have been around far longer than computers and have been trying to resolve
some questions that relate to AI: How do minds work? Is it possible for machines to act
intelligently in the way that people do, and if they did, would they have real, conscious
minds? What are the ethical implications of intelligent machines?
First, some terminology: the assertion that machines could act as if they were intelligent
is called the weak AI hypothesis by philosophers, and the assertion that machines that do so
WEAK AI
are actually thinking (not just simulating thinking) is called the strong AI hypothesis.
STRONG AI
Most AI researchers take the weak AI hypothesis for granted, and don’t care about the
strong AI hypothesis—as long as their program works, they don’t care whether you call it a
simulation of intelligence or real intelligence. All AI researchers should be concerned with
the ethical implications of their work.
WEAK AI: CAN MACHINES ACT INTELLIGENTLY?
of intelligence can be so precisely described that a machine can be made to simulate it.” Thus,
AI was founded on the assumption that weak AI is possible. Others have asserted that weak
AI is impossible: “Artiﬁcial intelligence pursued within the cult of computationalism stands
ﬁned AI as the quest for the best agent program on a given architecture. With this formulation,
AI is by deﬁnition possible: for any digital architecture with k bits of program storage there
are exactly 2k agent programs, and all we have to do to ﬁnd the best one is enumerate and test
them all. This might not be feasible for large k, but philosophers deal with the theoretical,
not the practical.Weak AI: Can Machines Act Intelligently?
Our deﬁnition of AI works well for the engineering problem of ﬁnding a good agent,
given an architecture. Therefore, we’re tempted to end this section right now, answering the
ing two architectures—human and machine. Furthermore, they have traditionally posed the
question not in terms of maximizing expected utility but rather as, “Can machines think?”
CAN MACHINES
THINK?
chines Can Think . . . is about as relevant as the question of whether Submarines Can Swim.”
CAN SUBMARINES
SWIM?
The American Heritage Dictionary’s ﬁrst deﬁnition of swim is “To move through water by
means of the limbs, ﬁns, or tail,” and most people agree that submarines, being limbless,
cannot swim. The dictionary also deﬁnes ﬂy as “To move through the air by means of wings
ever, neither the questions nor the answers have any relevance to the design or capabilities of
airplanes and submarines; rather they are about the usage of words in English. (The fact that
ships do swim in Russian only ampliﬁes this point.). The practical possibility of “thinking
gested that instead of asking whether machines can think, we should ask whether machines
can pass a behavioral intelligence test, which has come to be called the Turing Test. The test
TURING TEST
is for a program to have a conversation (via online typed messages) with an interrogator for
ﬁve minutes. The interrogator then has to guess if the conversation is with a program or a
well enough to pass the test. He was wrong—programs have yet to fool a sophisticated judge.
On the other hand, many people have been fooled when they didn’t know they might
be chatting with a computer. The ELIZA program and Internet chatbots such as MGONZ
chatbot CYBERLOVER has attracted the attention of law enforcement because of its penchant
for tricking fellow chatters into divulging enough personal information that their identity can
since his paper appeared. We will look at some of them.
The argument from disability
ples of X, Turing lists the following:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right
from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone
fall in love with it, learn from experience, use words properly, be the subject of its own
thought, have as much diversity of behavior as man, do something really new.Chapter
Philosophical Foundations
In retrospect, some of these are rather easy—we’re all familiar with computers that “make
to “make someone fall in love with it”—the teddy bear. Computer chess expert David Levy
nose diseases; and do hundreds of other tasks as well as or better than humans. Computers
ogy, biology, computer science, and other ﬁelds. Each of these required performance at the
level of a human expert.
Given what we now know about computers, it is not surprising that they do well at
combinatorial problems such as playing chess. But algorithms also perform at human levels
tive tasks such as predicting the success of a student in a training program or the recidivism
learning algorithms (such as linear regression or naive Bayes) predict better than the experts.
The Educational Testing Service has used an automated program to grade millions of essay
It is clear that computers can do many things as well as or better than humans, including
things that people believe require great human insight and understanding. This does not mean,
of course, that computers use insight and understanding in performing these tasks—those are
not part of behavior, and we address such questions elsewhere—but the point is that one’s
ﬁrst guess about the mental processes required to produce a given behavior is often wrong. It
is also true, of course, that there are many tasks at which computers do not yet excel (to put
The mathematical objection
formal axiomatic system F powerful enough to do arithmetic, it is possible to construct a
• G(F) is a sentence of F, but cannot be proved within F.
• If F is consistent, then G(F) is true.
“probably we’ll live to see machines do it.” He didn’t.Weak AI: Can Machines Act Intelligently?
are mentally inferior to humans, because machines are formal systems that are limited by the
incompleteness theorem—they cannot establish the truth of their own G¨odel sentence—while
humans have no such limitation. This claim has caused decades of controversy, spawning a
that repeat the claim with some fresh twists (such as the hypothesis that humans are different
because their brains operate by quantum gravity). We will examine only three of the problems
with the claim.
First, G¨odel’s incompleteness theorem applies only to formal systems that are powerful
enough to do arithmetic. This includes Turing machines, and Lucas’s claim is in part based
on the assertion that computers are Turing machines. This is a good approximation, but is not
quite true. Turing machines are inﬁnite, whereas computers are ﬁnite, and any computer can
therefore be described as a (very large) system in propositional logic, which is not subject to
G¨odel’s incompleteness theorem. Second, an agent should not be too ashamed that it cannot
establish the truth of some sentence while other agents can. Consider the sentence
J. R. Lucas cannot consistently assert that this sentence is true.
If Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucas
cannot consistently assert it, and hence it must be true. We have thus demonstrated that there
is a sentence that Lucas cannot consistently assert while other people (and machines) can. But
that does not make us think less of Lucas. To take another example, no human could compute
seconds. Still, we do not see this as a fundamental limitation in the human’s ability to think.
Humans were behaving intelligently for thousands of years before they invented mathematics,
so it is unlikely that formal mathematical reasoning plays more than a peripheral role in what
it means to be intelligent.
Third, and most important, even if we grant that computers have limitations on what
they can prove, there is no evidence that humans are immune from those limitations. It is
mans can do X using their own informal method, without giving any evidence for this claim.
rem, because any rigorous proof would require a formalization of the claimed unformalizable
human talent, and hence refute itself. So we are left with an appeal to intuition that humans
can somehow perform superhuman feats of mathematical insight. This appeal is expressed
with arguments such as “we must assume our own consistency, if thought is to be possible at
true for everyday reasoning, but it is also true for careful mathematical thought. A famousChapter
Philosophical Foundations
The argument from informality
ing as the “argument from informality of behavior.” Essentially, this is the claim that human
puters can do no more than follow a set of rules, they cannot generate behavior as intelligent
as that of humans. The inability to capture everything in a set of logical rules is called the
qualiﬁcation problem in AI.
QUALIFICATION
PROBLEM
The principal proponent of this view has been the philosopher Hubert Dreyfus, who
has produced a series of inﬂuential critiques of artiﬁcial intelligence: What Computers Can’t
GOFAI is supposed to claim that all
intelligent behavior can be captured by a system that reasons logically from a set of facts and
rules describing the domain. It therefore corresponds to the simplest logical agent described
se, but rather against one particular way of programming them. It is reasonable to suppose,
Can’t Do might have had less impact.
Under Dreyfus’s view, human expertise does include knowledge of some rules, but only
as a “holistic context” or “background” within which humans operate. He gives the example
of appropriate social behavior in giving and receiving gifts: “Normally one simply responds
in the appropriate circumstances by giving an appropriate gift.” One apparently has “a direct
sense of how things are done and what to expect.” The same claim is made in the context of
chess playing: “A mere chess master might need to ﬁgure out what to do, but a grandmaster
just sees the board as demanding a certain move . . . the right response just pops into his or her
is done at a level that is not open to introspection by the conscious mind. But that does not
mean that the thought processes do not exist. The important question that Dreyfus does not
answer is how the right move gets into the grandmaster’s head. One is reminded of Daniel
saw her in half; he simply makes it appear that he does. “But how does he do that?” we
ask. “Not our department,” say the philosophers.
select correct responses instantaneously. In making this proposal, Dreyfus and Dreyfus in
effect move from being AI critics to AI theorists—they propose a neural network architectureWeak AI: Can Machines Act Intelligently?
organized into a vast “case library,” but point out several problems. Fortunately, all of their
problems have been addressed, some with partial success and some with total success. Their
problems include the following:
edge. They claim no one has any idea how to incorporate background knowledge into
are techniques for using prior knowledge in learning algorithms. Those techniques,
however, rely on the availability of knowledge in explicit form, something that Dreyfus
and Dreyfus strenuously deny. In our view, this is a good reason for a serious redesign
of current models of neural processing so that they can take advantage of previously
learned knowledge in the way that other learning algorithms do.
the prior identiﬁcation of relevant inputs and correct outputs. Therefore, they claim,
it cannot operate autonomously without the help of a human trainer. In fact, learning
of features, “there is no known way of adding new features should the current set prove
inadequate to account for the learned facts.” In fact, new methods such as support
vector machines handle large feature sets very well. With the introduction of large
features, although much more work is needed.
to extract aspects relevant to the current situation. But, Dreyfus and Dreyfus claim,
“Currently, no details of this mechanism are understood or even hypothesized in a way
that could guide AI research.” In fact, the ﬁeld of active vision, underpinned by the
directing sensors, and already some robots have incorporated the theoretical results
large part by an active sensing system of this kind.
In sum, many of the issues Dreyfus has focused on—background commonsense knowledge,
the qualiﬁcation problem, uncertainty, learning, compiled forms of decision making—are
indeed important issues, and have by now been incorporated into standard intelligent agent
design. In our view, this is evidence of AI’s progress, not of its impossibility.
One of Dreyfus’ strongest arguments is for situated agents rather than disembodied
logical inference engines. An agent whose understanding of “dog” comes only from a limited
set of logical sentences such as “Dog(x) ⇒Mammal(x)” is at a disadvantage compared
to an agent that has watched dogs run, has played fetch with them, and has been licked byChapter
Philosophical Foundations
surroundings.” To understand how human (or other animal) agents work, we have to consider
the whole agent, not just the agent program. Indeed, the embodied cognition approach claims
EMBODIED
COGNITION
that it makes no sense to consider the brain separately: cognition takes place within a body,
which is embedded in an environment. We need to study the system as a whole; the brain
augments its reasoning by referring to the environment, as the reader does in perceiving (and
creating) marks on paper to transfer knowledge. Under the embodied cognition program,
robotics, vision, and other sensors become central, not peripheral.
STRONG AI: CAN MACHINES REALLY THINK?
Many philosophers have claimed that a machine that passes the Turing Test would still not
be actually thinking, but would be only a simulation of thinking. Again, the objection was
Not until a machine could write a sonnet or compose a concerto because of thoughts and
emotions felt, and not by the chance fall of symbols, could we agree that machine equals
brain—that is, not only write it but know that it had written it.
Turing calls this the argument from consciousness—the machine has to be aware of its own
mental states and actions. While consciousness is an important subject, Jefferson’s key point
actually relates to phenomenology, or the study of direct experience: the machine has to
actually feel emotions. Others focus on intentionality—that is, the question of whether the
thing in the real world.
Turing’s response to the objection is interesting. He could have presented reasons that
machines can in fact be conscious (or have phenomenology, or have intentions). Instead, he
why should we insist on a higher standard for machines than we do for humans? After all,
in ordinary life we never have any direct evidence about the internal mental states of other
humans. Nevertheless, Turing says, “Instead of arguing continually over this point, it is usual
to have the polite convention that everyone thinks.”
chines if only he had experience with ones that act intelligently. He cites the following dialog,
which has become such a part of AI’s oral tradition that we simply have to include it:
HUMAN: In the ﬁrst line of your sonnet which reads “shall I compare thee to a summer’s
day,” would not a “spring day” do as well or better?
MACHINE: It wouldn’t scan.
HUMAN: How about “a winter’s day.” That would scan all right.
MACHINE: Yes, but nobody wants to be compared to a winter’s day.
HUMAN: Would you say Mr. Pickwick reminded you of Christmas?
MACHINE: In a way.
HUMAN: Yet Christmas is a winter’s day, and I do not think Mr. Pickwick would mind
the comparison.Strong AI: Can Machines Really Think?
MACHINE: I don’t think you’re serious. By a winter’s day one means a typical winter’s
day, rather than a special one like Christmas.
One can easily imagine some future time in which such conversations with machines are
commonplace, and it becomes customary to make no linguistic distinction between “real”
urea was synthesized for the ﬁrst time by Frederick W¨ohler. Prior to this event, organic and
inorganic chemistry were essentially disjoint enterprises and many thought that no process
could exist that would convert inorganic chemicals into organic material. Once the synthesis
was accomplished, chemists agreed that artiﬁcial urea was urea, because it had all the right
terial that inorganic material could never have were faced with the impossibility of devising
any test that could reveal the supposed deﬁciency of artiﬁcial urea.
artiﬁcial thinking, no matter how impressive, will never be real. For example, the philosopher
No one supposes that a computer simulation of a storm will leave us all wet . . . Why on
earth would anyone in his right mind suppose a computer simulation of mental processes
While it is easy to agree that computer simulations of storms do not make us wet, it is not
clear how to carry this analogy over to computer simulations of mental processes. After
all, a Hollywood simulation of a storm using sprinklers and wind machines does make the
actors wet, and a video game simulation of a storm does make the simulated characters wet.
Most people are comfortable saying that a computer simulation of addition is addition, and
of chess is chess. In fact, we typically speak of an implementation of addition or chess, not a
simulation. Are mental processes more like storms, or more like addition?
Turing’s answer—the polite convention—suggests that the issue will eventually go
away by itself once machines reach a certain level of sophistication. This would have the
effect of dissolving the difference between weak and strong AI. Against this, one may insist
that there is a factual issue at stake: humans do have real minds, and machines might or
might not. To address this factual issue, we need to understand how it is that humans have
real minds, not just bodies that generate neurophysiological processes. Philosophical efforts
to solve this mind–body problem are directly relevant to the question of whether machines
MIND–BODY
PROBLEM
could have real minds.
erties) and the physical processes of the body, concluding that the two must exist in separate
DUALISM
alists is the question of how the mind can control the body if the two are really separate.
Descartes speculated that the two might interact through the pineal gland, which simply begs
the question of how the mind controls the pineal gland.Chapter
Philosophical Foundations
The monist theory of mind, often called physicalism, avoids this problem by asserting
MONISM
PHYSICALISM
the mind is not separate from the body—that mental states are physical states. Most modern
philosophers of mind are physicalists of one form or another, and physicalism allows, at least
in principle, for the possibility of strong AI. The problem for physicalists is to explain how
physical states—in particular, the molecular conﬁgurations and electrochemical processes of
the brain—can simultaneously be mental states, such as being in pain, enjoying a hamburger,
MENTAL STATES
knowing that one is riding a horse, or believing that Vienna is the capital of Austria.
Mental states and the brain in a vat
Physicalist philosophers have attempted to explicate what it means to say that a person—and,
by extension, a computer—is in a particular mental state. They have focused in particular on
intentional states. These are states, such as believing, knowing, desiring, fearing, and so on,
INTENTIONAL STATE
that refer to some aspect of the external world. For example, the knowledge that one is eating
a hamburger is a belief about the hamburger and what is happening to it.
If physicalism is correct, it must be the case that the proper description of a person’s
mental state is determined by that person’s brain state. Thus, if I am currently focused on
eating a hamburger in a mindful way, my instantaneous brain state is an instance of the class of
mental states “knowing that one is eating a hamburger.” Of course, the speciﬁc conﬁgurations
of all the atoms of my brain are not essential: there are many conﬁgurations of my brain, or
of other people’s brain, that would belong to the same class of mental states. The key point is
that the same brain state could not correspond to a fundamentally distinct mental state, such
as the knowledge that one is eating a banana.
velously engineered vat. The vat sustains your brain, allowing it to grow and develop. At the
same time, electronic signals are fed to your brain from a computer simulation of an entirely
ﬁctitious world, and motor signals from your brain are intercepted and used to modify the
would have lived, had your brain not been placed in the vat, including simulated eating of
simulated hamburgers. Thus, you could have a brain state identical to that of someone who is
really eating a real hamburger, but it would be literally false to say that you have the mental
state “knowing that one is eating a hamburger.” You aren’t eating a hamburger, you have
never even experienced a hamburger, and you could not, therefore, have such a mental state.
This example seems to contradict the view that brain states determine mental states. One
way to resolve the dilemma is to say that the content of mental states can be interpreted from
two different points of view. The “wide content” view interprets it from the point of view
WIDE CONTENT
of an omniscient outside observer with access to the whole situation, who can distinguish
differences in the world. Under this view, the content of mental states involves both the brain
state and the environment history. Narrow content, on the other hand, considers only the
NARROW CONTENT
vat “hamburger”-“eater” is the same in both cases.Strong AI: Can Machines Really Think?
Wide content is entirely appropriate if one’s goals are to ascribe mental states to others
who share one’s world, to predict their likely behavior and its effects, and so on. This is the
setting in which our ordinary language about mental content has evolved. On the other hand,
if one is concerned with the question of whether AI systems are really thinking and really
do have mental states, then narrow content is appropriate; it simply doesn’t make sense to
say that whether or not an AI system is really thinking depends on conditions outside that
system. Narrow content is also relevant if we are thinking about designing AI systems or
understanding their operation, because it is the narrow content of a brain state that determines
what will be the (narrow content of the) next brain state. This leads naturally to the idea that
what matters about a brain state—what makes it have one kind of mental content and not
another—is its functional role within the mental operation of the entity involved.
Functionalism and the brain replacement experiment
The theory of functionalism says that a mental state is any intermediate causal condition
FUNCTIONALISM
between input and output. Under functionalist theory, any two systems with isomorphic
causal processes would have the same mental states. Therefore, a computer program could
have the same mental states as a person. Of course, we have not yet said what “isomorphic”
really means, but the assumption is that there is some level of abstraction below which the
speciﬁc implementation does not matter.
periment. This thought experiment was introduced by the philosopher Clark Glymour and
the input–output behavior and connectivity of all the neurons in the human brain are perfectly
understood. Suppose further that we can build microscopic electronic devices that mimic this
ulous surgical technique can replace individual neurons with the corresponding electronic
devices without interrupting the operation of the brain as a whole. The experiment consists
of gradually replacing all the neurons in someone’s head with electronic devices.
We are concerned with both the external behavior and the internal experience of the
subject, during and after the operation. By the deﬁnition of the experiment, the subject’s
external behavior must remain unchanged compared with what would be observed if the
cannot easily be ascertained by a third party, the subject of the experiment ought at least to
be able to record any changes in his or her own conscious experience. Apparently, there is
a direct clash of intuitions as to what would happen. Moravec, a robotics researcher and
functionalist, is convinced his consciousness would remain unaffected. Searle, a philosopher
and biological naturalist, is equally convinced his consciousness would vanish:
You ﬁnd, to your total amazement, that you are indeed losing control of your external
behavior. You ﬁnd, for example, that when doctors test your vision, you hear them say
“We are holding up a red object in front of you; please tell us what you see.” You wantChapter
Philosophical Foundations
to cry out “I can’t see anything. I’m going totally blind.” But you hear your voice saying
in a way that is completely out of your control, “I see a red object in front of me.” . . .
your conscious experience slowly shrinks to nothing, while your externally observable
main the same while the subject gradually becomes unconscious, it must be the case that the
ness would be reﬂected in external behavior—“Help, I’m shrinking!” or words to that effect.
seems an unlikely claim to have to make.
Second, consider what happens if we do ask the subject questions concerning his or
her conscious experience during the period when no real neurons remain. By the conditions
of the experiment, we will get responses such as “I feel ﬁne. I must say I’m a bit surprised
because I believed Searle’s argument.” Or we might poke the subject with a pointed stick and
observe the response, “Ouch, that hurt.” Now, in the normal course of affairs, the skeptic can
dismiss such outputs from AI programs as mere contrivances. Certainly, it is easy enough to
because we have replicated the functional properties of a normal human brain, we assume
that the electronic brain contains no such contrivances. Then we must have an explanation of
the manifestations of consciousness produced by the electronic brain that appeals only to the
functional properties of the neurons. And this explanation must also apply to the real brain,
which has the same functional properties. There are three possible conclusions:
brains are still operating in the electronic version, which is therefore conscious.
and are missing from the electronic brain, which is therefore not conscious.
phers call an epiphenomenal role—something that happens, but casts no shadow, as it were,
EPIPHENOMENON
on the observable world. Furthermore, if consciousness is indeed epiphenomenal, then it
scious experience of pain. Instead, the brain must contain a second, unconscious mechanism
that is responsible for the “Ouch.”
the level of the neuron can also operate at the level of any larger functional unit—a clump
of neurons, a mental module, a lobe, a hemisphere, or the whole brain. That means that if
you accept the notion that the brain replacement experiment shows that the replacement brain
is conscious, then you should also believe that consciousness is maintained when the entire
brain is replaced by a circuit that updates its state and maps from inputs to outputs via a huge
lookup table. This is disconcerting to many people (including Turing himself), who have
the intuition that lookup tables are not conscious—or at least, that the conscious experiences
generated during table lookup are not the same as those generated during the operation of aStrong AI: Can Machines Really Think?
and generating beliefs, introspections, goals, and so on.
Biological naturalism and the Chinese Room
BIOLOGICAL
NATURALISM
gram having the same functional structure with the same input–output behavior; we would
require that the program be running on an architecture with the same causal power as neurons.
To support his view, Searle describes a hypothetical system that is clearly running a program
and passes the Turing Test, but that equally clearly (according to Searle) does not understand
anything of its inputs and outputs. His conclusion is that running the appropriate program
(i.e., having the right outputs) is not a sufﬁcient condition for being a mind.
The system consists of a human, who understands only English, equipped with a rule
book, written in English, and various stacks of paper, some blank, some with indecipherable
inscriptions. (The human therefore plays the role of the CPU, the rule book is the program,
and the stacks of paper are the storage device.) The system is inside a room with a small
bols. The human ﬁnds matching symbols in the rule book, and follows the instructions. The
instructions may include writing symbols on new slips of paper, ﬁnding symbols in the stacks,
rearranging the stacks, and so on. Eventually, the instructions will cause one or more symbols
to be transcribed onto a piece of paper that is passed back to the outside world.
So far, so good. But from the outside, we see a system that is taking input in the form
of Chinese sentences and generating answers in Chinese that are as “intelligent” as those
not understand Chinese (given). The rule book and the stacks of paper, being just pieces of
paper, do not understand Chinese. Therefore, there is no understanding of Chinese. Hence,
according to Searle, running the right program does not necessarily generate understanding.
gument. Several commentators, including John McCarthy and Robert Wilensky, proposed
what Searle calls the systems reply. The objection is that asking if the human in the room
understands Chinese is analogous to asking if the CPU can take cube roots. In both cases,
the answer is no, and in both cases, according to the systems reply, the entire system does
have the capacity in question. Certainly, if one asks the Chinese Room whether it understands
Chinese, the answer would be afﬁrmative (in ﬂuent Chinese). By Turing’s polite convention,
this should be enough. Searle’s response is to reiterate the point that the understanding is not
in the human and cannot be in the paper, so there cannot be any understanding. He seems to
be relying on the argument that a property of the whole must reside in one of the parts. Yet
millions of years has no bearing on the logical structure of the argument. One aim of philosophical training is to
develop a ﬁnely honed sense of which objections are germane and which are not.Chapter
Philosophical Foundations
water is wet, even though neither H nor O2 is. The real claim made by Searle rests upon the
From the ﬁrst three axioms Searle concludes that programs are not sufﬁcient for minds. In
other words, an agent running a program might be a mind, but it is not necessarily a mind just
by virtue of running the program. From the fourth axiom he concludes “Any other system
capable of causing minds would have to have causal powers (at least) equivalent to those
of brains.” From there he infers that any artiﬁcial brain would have to duplicate the causal
powers of brains, not just run a particular program, and that human brains do not produce
mental phenomena solely by virtue of running a program.
distinction between syntax and semantics that seems to be closely related to the distinction
between narrow and wide content. On the one hand, we can view computers as manipulating
syntactic symbols; on the other, we can view them as manipulating electric current, which
happens to be what brains mostly do (according to our current understanding). So it seems
we could equally say that brains are syntactic.
grams are not sufﬁcient for minds—does follow. But the conclusion is unsatisfactory—all
cepted. According to Searle, the point of the Chinese Room argument is to provide intuitions
INTUITION PUMP
unsupported, or that in general Searle’s argument is unconvincing. The argument stirs up
combatants, but has done little to change anyone’s opinion. Searle remains undeterred, and
has recently started calling the Chinese Room a “refutation” of strong AI rather than just an
tuitions to fall back on when deciding what entities are minds. The argument purports to show
that the Chinese Room is not a mind by virtue of running the program, but the argument says
nothing about how to decide whether the room (or a computer, some other type of machine,
or an alien) is a mind by virtue of some other reason. Searle himself says that some machines
do have minds: humans are biological machines with minds. According to Searle, human
brains may or may not be running something like an AI program, but if they are, that is not
the reason they are minds. It takes more to make a mind—according to Searle, something
iﬁed. It should be noted, however, that neurons evolved to fulﬁll functional roles—creaturesStrong AI: Can Machines Really Think?
with neurons were learning and deciding long before consciousness appeared on the scene. It
would be a remarkable coincidence if such neurons just happened to generate consciousness
because of some causal powers that are irrelevant to their functional capabilities; after all, it
is the functional capabilities that dictate survival of the organism.
In the case of the Chinese Room, Searle relies on intuition, not proof: just look at the
room; what’s there to be a mind? But one could make the same argument about the brain:
just look at this collection of cells (or of atoms), blindly operating according to the laws of
biochemistry (or of physics)—what’s there to be a mind? Why can a hunk of brain be a mind
while a hunk of liver cannot? That remains the great mystery.
Consciousness, qualia, and the explanatory gap
Running through all the debates about strong AI—the elephant in the debating room, so
to speak—is the issue of consciousness. Consciousness is often broken down into aspects
CONSCIOUSNESS
experience: why it is that it feels like something to have certain brain states (e.g., while eating
a hamburger), whereas it presumably does not feel like anything to have other physical states
(e.g., while being a rock). The technical term for the intrinsic nature of experiences is qualia
QUALIA
(from the Latin word meaning, roughly, “such things”).
Qualia present a challenge for functionalist accounts of the mind because different
qualia could be involved in what are otherwise isomorphic causal processes. Consider, for
INVERTED
SPECTRUM
son X when seeing red objects is the same experience that the rest of us experience when
seeing green objects, and vice versa. X still calls red objects “red,” stops for red trafﬁc lights,
and agrees that the redness of red trafﬁc lights is a more intense red than the redness of the
setting sun. Yet, X’s subjective experience is just different.
Qualia are challenging not just for functionalism but for all of science. Suppose, for the
sake of argument, that we have completed the process of scientiﬁc research on the brain—we
have found that neural process P12 in neuron N177 transforms molecule A into molecule B,
and so on, and on. There is simply no currently accepted form of reasoning that would lead
from such ﬁndings to the conclusion that the entity owning those neurons has any particular
subjective experience. This explanatory gap has led some philosophers to conclude that
EXPLANATORY GAP
humans are simply incapable of forming a proper understanding of their own consciousness.
attributing them to a philosophical confusion.
Turing himself concedes that the question of consciousness is a difﬁcult one, but denies
that it has much relevance to the practice of AI: “I do not wish to give the impression that I
sarily need to be solved before we can answer the question with which we are concerned in
ligently. The additional project of making them conscious is not one that we are equipped to
take on, nor one whose success we would be able to determine.Chapter
Philosophical Foundations
THE ETHICS AND RISKS OF DEVELOPING ARTIFICIAL INTELLIGENCE
So far, we have concentrated on whether we can develop AI, but we must also consider
whether we should. If the effects of AI technology are more likely to be negative than positive,
then it would be the moral responsibility of workers in the ﬁeld to redirect their research.
Many new technologies have had unintended negative side effects: nuclear ﬁssion brought
Chernobyl and the threat of global destruction; the internal combustion engine brought air
robots that have conquered the world by making themselves indispensable.
All scientists and engineers face ethical considerations of how they should act on the
job, what projects should or should not be done, and how they should be handled. See the
to pose some fresh problems beyond that of, say, building bridges that don’t fall down:
• People might lose their jobs to automation.
• People might have too much (or too little) leisure time.
• People might lose their sense of being unique.
• AI systems might be used toward undesirable ends.
• The use of AI systems might result in a loss of accountability.
• The success of AI might mean the end of the human race.
We will look at each issue in turn.
come dependent on computers in general, and select AI programs in particular. For example,
sumer credit. Credit card applications, charge approvals, and fraud detection are now done
by AI programs. One could say that thousands of workers have been displaced by these AI
programs, but in fact if you took away the AI programs these jobs would not exist, because
human labor would add an unacceptable cost to the transactions. So far, automation through
information technology in general and AI in particular has created more jobs than it has
program is an “intelligent agent” designed to assist a human, loss of jobs is less of a concern
than it was when AI focused on “expert systems” designed to replace humans. But some
researchers think that doing the complete job is the right goal for AI. In reﬂecting on the 25th
AI that could pass the employment test rather than the Turing Test—a robot that could learn
to do any one of a range of jobs. We may end up in a future where unemployment is high, but
even the unemployed serve as managers of their own cadre of robot workers.
People might have too much (or too little) leisure time. Alvin Tofﬂer wrote in Future
where the main problem in life is deciding which of several hundred TV channels to select.”The Ethics and Risks of Developing Artiﬁcial Intelligence
The only one of these predictions that has come close to panning out is the number of TV
been forced to work longer hours. In an industrial economy, rewards are roughly proportional
increases the pace of technological innovation and thus contributes to this overall trend, but
AI also holds the promise of allowing us to take some time off and let our automated agents
threats that AI poses to society. One of Weizenbaum’s principal arguments is that AI research
makes possible the idea that humans are automata—an idea that results in a loss of autonomy
or even of humanity. We note that the idea has been around much longer than AI, going back
Homo sapiens at the same level as other species. AI, if widely successful, may be at least as
was to those of the 19th century.
AI systems might be used toward undesirable ends. Advanced technologies have
often been used by the powerful to suppress their rivals. As the number theorist G. H. Hardy
existing inequalities in the distribution of wealth, or more directly promotes the destruction
of human life.” This holds for all sciences, AI being no exception. Autonomous AI systems
holds that military robots are like medieval armor taken to its logical extreme: no one would
have moral objections to a soldier wanting to wear a helmet when being attacked by large,
the other hand, robotic weapons pose additional risks. To the extent that human decision
making is taken out of the ﬁring loop, robots may end up making decisions that lead to the
killing of innocent civilians. At a larger scale, the possession of powerful robots (like the
possession of sturdy helmets) may give a nation overconﬁdence, causing it to go to war more
recklessly than necessary. In most wars, at least one party is overconﬁdent in its military
abilities—otherwise the conﬂict would have been resolved peacefully.
widespread wiretapping, and hence to a loss of civil liberties. He didn’t foresee a world with
terrorist threats that would change the balance of how much surveillance people are willing toChapter
Philosophical Foundations
His prediction has in part come true: the U.K. now has an extensive network of surveillance
cameras, and other countries routinely monitor Web trafﬁc and telephone calls. Some accept
that computerization leads to a loss of privacy—Sun Microsystems CEO Scott McNealy has
privacy is inevitable, and the way to combat the asymmetry of power of the state over the
balancing of privacy and security; individual rights and community.
sphere that prevails in the United States, legal liability becomes an important issue. When a
physician relies on the judgment of a medical expert system for a diagnosis, who is at fault if
methods in medicine, it is now accepted that negligence cannot be shown if the physician
trophic for the patient. The question should therefore be “Who is at fault if the diagnosis is
unreasonable?” So far, courts have held that medical expert systems play the same role as
soning behind any decision and for using their own judgment in deciding whether to accept
the system’s recommendations. In designing medical expert systems as agents, therefore,
the actions should be thought of not as directly affecting the patient but as inﬂuencing the
cians, doctors might become legally liable if they don’t use the recommendations of an expert
net. Some progress has been made in incorporating constraints into intelligent agents so that
lem is magniﬁed when money changes hands. If monetary transactions are made “on one’s
behalf” by an intelligent agent, is one liable for the debts incurred? Would it be possible for
an intelligent agent to have assets itself and to perform electronic trades on its own behalf?
So far, these questions do not seem to be well understood. To our knowledge, no program
has been granted legal status as an individual for the purposes of ﬁnancial transactions; at
present, it seems unreasonable to do so. Programs are also not considered to be “drivers”
for the purposes of enforcing trafﬁc regulations on real highways. In California law, at least,
there do not seem to be any legal sanctions to prevent an automated vehicle from exceeding
the speed limits, although the designer of the vehicle’s control mechanism would be liable in
the case of an accident. As with human reproductive technology, the law has yet to catch up
with the new developments.
The success of AI might mean the end of the human race. Almost any technology
has the potential to cause harm in the wrong hands, but with AI and robotics, we have the new
problem that the wrong hands might belong to the technology itself. Countless science ﬁction
stories have warned about robots or robot–human cyborgs running amok. Early examplesThe Ethics and Risks of Developing Artiﬁcial Intelligence
they represent the unknown, just like the witches and ghosts of tales from earlier eras, or the
poses a bigger risk than traditional software. We will look at three sources of risk.
First, the AI system’s state estimation may be incorrect, causing it to do the wrong
thing. For example, an autonomous car might incorrectly estimate the position of a car in the
adjacent lane, leading to an accident that might kill the occupants. More seriously, a missile
defense system might erroneously detect an attack and launch a counterattack, leading to
the death of billions. These risks are not really risks of AI systems—in both cases the same
mistake could just as easily be made by a human as by a computer. The correct way to mitigate
error does not propagate through the system unchecked.
Second, specifying the right utility function for an AI system to maximize is not so
easy. For example, we might propose a utility function designed to minimize human suffering,
are, however, we’ll always ﬁnd a way to suffer even in paradise; so the optimal decision for
the AI system is to terminate the human race as soon as possible—no humans, no suffering.
With AI systems, then, we need to be very careful what we ask for, whereas humans would
have no trouble realizing that the proposed utility function cannot be taken literally. On the
Humans sometimes use their intelligence in aggressive ways because humans have some
innately aggressive tendencies, due to natural selection. The machines we build need not be
innately aggressive, unless we decide to build them that way (or unless they emerge as the
end product of a mechanism design that encourages aggressive behavior). Fortunately, there
are techniques, such as apprenticeship learning, that allows us to specify a utility function by
example. One can hope that a robot that is smart enough to ﬁgure out how to terminate the
human race is also smart enough to ﬁgure out that that was not the intended utility function.
Third, the AI system’s learning function may cause it to evolve into a system with
unintended behavior. This scenario is the most serious, and is unique to AI systems, so we
Let an ultraintelligent machine be deﬁned as a machine that can far surpass all the
ULTRAINTELLIGENT
MACHINE
intellectual activities of any man however clever. Since the design of machines is one of
these intellectual activities, an ultraintelligent machine could design even better machines;
there would then unquestionably be an “intelligence explosion,” and the intelligence of
man would be left far behind. Thus the ﬁrst ultraintelligent machine is the last invention
that man need ever make, provided that the machine is docile enough to tell us how to
keep it under control.Chapter
Philosophical Foundations
TECHNOLOGICAL
SINGULARITY
years, we will have the technological means to create superhuman intelligence. Shortly after,
the human era will be ended.” Good and Vinge (and many others) correctly note that the curve
of technological progress (on many measures) is growing exponentially at present (consider
Moore’s Law). However, it is a leap to extrapolate that the curve will continue to a singularity
the exponential growth eventually tapers off. Sometimes new technologies step in when the
history to go on, it is difﬁcult to extrapolate hundreds of years ahead.
pecially important attribute, and if you have enough of it, all problems can be solved. But
we know there are limits on computability and computational complexity. If the problem
of deﬁning ultraintelligent machines (or even approximations to them) happens to fall in the
even exponential progress in technology won’t help—the speed of light puts a strict upper
bound on how much computing can be done; problems beyond that limit will not be solved.
We still don’t know where those upper bounds are.
Vinge is concerned about the coming singularity, but some computer scientists and
children,” the robots we create, which may surpass us in intelligence. There is even a new
word—transhumanism—for the active social movement that looks forward to this future in
TRANSHUMANISM
which humans are merged with—or replaced by—robotic and biotech inventions. Sufﬁce it
to say that such issues present a challenge for most moral theorists, who take the preservation
of human life and the human species to be a good thing. Ray Kurzweil is currently the most
The Singularity will allow us to transcend these limitations of our biological bodies and
brain. We will gain power over our fates. Our mortality will be in our own hands. We
will be able to live as long as we want (a subtly different statement from saying we will
live forever). We will fully understand human thinking and will vastly extend and expand
its reach. By the end of this century, the nonbiological portion of our intelligence will be
trillions of trillions of times more powerful than unaided human intelligence.
Kurzweil also notes the potential dangers, writing “But the Singularity will also amplify the
ability to act on our destructive inclinations, so its full story has not yet been written.”
If ultraintelligent machines are a possibility, we humans would do well to make sure
that we design their predecessors in such a way that they design themselves to treat us well.
laws of robotics:
come to harm.
conﬂict with the First Law.The Ethics and Risks of Developing Artiﬁcial Intelligence
the First or Second Law.
laws. In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the
robot is found wandering in a circle around the selenium source. Every time it heads toward
the source, it senses a danger, and the third law causes it to veer away. But every time it
veers away, the danger recedes, and the power of the second law takes over, causing it to
veer back towards the selenium. The set of points that deﬁne the balancing point between
the two laws deﬁnes a circle. This suggests that the laws are not logical absolutes, but rather
are weighed against each other, with a higher weighting for the earlier laws. Asimov was
probably thinking of an architecture based on control theory—perhaps a linear combination
of factors—while today the most likely architecture would be a probabilistic reasoning agent
that reasons over probability distributions of outcomes, and maximizes utility as deﬁned by
the three laws. But presumably we don’t want our robots to prevent a human from crossing
the street because of the nonzero chance of harm. That means that the negative utility for
harm to a human must be much greater than for disobeying, but that each of the utilities is
ﬁnite, not inﬁnite.
FRIENDLY AI
that friendliness (a desire not to harm humans) should be designed in from the start, but that
the designers should recognize both that their own designs may be ﬂawed, and that the robot
will learn and evolve over time. Thus the challenge is one of mechanism design—to deﬁne a
mechanism for evolving AI systems under a system of checks and balances, and to give the
systems utility functions that will remain friendly in the face of such changes.
sired responses to circumstances, change over time. For example, if technology had allowed
the time, it would be ﬁghting today to reestablish slavery and abolish women’s right to vote.
On the other hand, if we build an AI agent today and tell it to evolve its utility function, how
can we assure that it won’t reason that “Humans think it is moral to kill annoying insects, in
part because insect brains are so primitive. But human brains are primitive compared to my
powers, so it must be moral for me to kill humans.”
risk to society. Similarly, Marvin Minsky once suggested that an AI program designed to
solve the Riemann Hypothesis might end up taking over all the resources of Earth to build
more powerful supercomputers to help achieve its goal. The moral is that even if you only
want your program to play chess or prove theorems, if you give it the capability to learn
and alter itself, you need safeguards. Omohundro concludes that “Social structures which
cause individuals to bear the cost of their negative externalities would go a long way toward
ensuring a stable and positive future,” This seems to be an excellent idea for society in general,
regardless of the possibility of ultraintelligent machines.
to sacriﬁce its own life to save a human.Chapter
Philosophical Foundations
We should note that the idea of safeguards against change in utility function is not a
whose song was so alluring it compelled sailors to cast themselves into the sea. Knowing it
would have that effect on him, Ulysses ordered his crew to bind him to the mast so that he
could be built into AI systems.
Finally, let us consider the robot’s point of view. If robots become conscious, then to
treat them as mere “machines” (e.g., to take them apart) might be immoral. Science ﬁction
on a story by Brian Aldiss about an intelligent robot who was programmed to believe that
he was human and fails to understand his eventual abandonment by his owner–mother. The
story (and the movie) argue for the need for a civil rights movement for robots.
SUMMARY
This chapter has addressed the following issues:
• Philosophers use the term weak AI for the hypothesis that machines could possibly
behave intelligently, and strong AI for the hypothesis that such machines would count
as having actual minds (as opposed to simulated minds).
havioral test. He anticipated many objections to the possibility of thinking machines.
Few AI researchers pay attention to the Turing Test, preferring to concentrate on their
systems’ performance on practical tasks, rather than the ability to imitate humans.
• There is general agreement in modern times that mental states are brain states.
• Arguments for and against strong AI are inconclusive. Few mainstream AI researchers
believe that anything signiﬁcant hinges on the outcome of the debate.
• Consciousness remains a mystery.
• We identiﬁed six potential threats to society posed by AI and related technology. We
concluded that some of the threats are either unlikely or differ little from threats posed
ation: that ultraintelligent machines might lead to a future that is very different from
erations lead inevitably to the conclusion that we must weigh carefully, and soon, the
possible consequences of AI research.
BIBLIOGRAPHICAL AND HISTORICAL NOTESBibliographical and Historical Notes
to deride symbolic approaches, not all philosophers are critical of GOFAI. Some are, in fact,
can best be understood through a computational model, not only in principle but also as a
way of conducting research at present, and has speciﬁcally rebutted Dreyfus’s criticisms of
analyzing belief revision, makes connections with AI research on truth maintenance systems.
Proponents of the importance of embodiment in cognition include the philosophers
body and the subjective interpretation of reality afforded by our senses, and Heidegger, whose
internal model, use props in the world (such as paper and pencil as well as computers) to
present arguments for how the body helps shape cognition.
cient times to the present. In the Phaedo, Plato speciﬁcally considered and rejected the idea
that the mind could be an “attunement” or pattern of organization of the parts of the body, a
viewpoint that approximates the functionalist viewpoint in modern philosophy of mind. He
decided instead that the mind had to be an immortal, immaterial soul, separable from the
body and different in substance—the viewpoint of dualism. Aristotle distinguished a variety
of souls (Greek ψυχη) in living things, some of which, at least, he described in a functionalist
ical inﬂuence was toward mechanism and physicalism. He explicitly conceived of animals as
automata, and he anticipated the Turing Test, writing “it is not conceivable [that a machine]
should produce different arrangements of words so as to give an appropriately meaningful
answer to whatever is said in its presence, as even the dullest of men can do” (Descartes,
effect of making it easier to conceive of humans as automata as well, even though he himself
that humans are automata.
Modern analytic philosophy has typically accepted physicalism, but the variety of views
on the content of mental states is bewildering. The identiﬁcation of mental states with brain
TWIN EARTHS
device to generate identical brain states with different (wide) content.Chapter
Philosophical Foundations
Functionalism is the philosophy of mind most naturally suggested by AI. The idea that
mental states correspond to classes of brain states deﬁned functionally is due to Putnam
ism is Daniel Dennett, whose ambitiously titled work Consciousness Explained (Dennett,
as an objective self, that consciousness is the subjective appearance of a world. The inverted
Functionalism has come under attack from authors who claim that they do not account
Made out of Meat, in which alien robotic explorers who visit earth are incredulous to ﬁnd
alent of Searle believes that he can think due to the special causal powers of robotic circuits;
Written four years after the publication of Darwin’s On the Origins of Species and at a time
when the most sophisticated machines were steam engines, Butler’s article on Darwin Among
the Machines envisioned “the ultimate development of mechanical consciousness” by natural
The philosophical literature on minds, brains, and related topics is large and difﬁcult to
accessible work, and the online Stanford Encyclopedia of Philosophy offers many excellent
Brain Sciences, abbreviated BBS, is a major journal devoted to philosophical and scientiﬁc
debates about AI and neuroscience. Topics of ethics and responsibility in AI are covered in
the journals AI and Society and Journal of Artiﬁcial Intelligence and Law.AI: THE PRESENT AND
FUTURE
In which we take stock of where we are and where we are going, this being a good
thing to do before continuing.
rational agents—that is, agents whose actions maximize their expected utility given their
percept histories. We showed that the design problem depends on the percepts and actions
available to the agent, the utility function that the agent’s behavior should satisfy, and the
nature of the environment. A variety of different agent designs are possible, ranging from
the components of these designs can have a number of different instantiations—for example,
logical or probabilistic reasoning, and atomic, factored, or structured representations of states.
The intervening chapters presented the principles by which these components operate.
For all the agent designs and components, there has been tremendous progress both in
our scientiﬁc understanding and in our technological capabilities. In this chapter, we stand
agents is the right goal in the ﬁrst place. (The answer is, “Not really, but it’s OK for now.”)
AGENT COMPONENTS
Let’s see where the state of the art stands for each of the components.
Interaction with the environment through sensors and actuators: For much of the
tems were built in such a way that humans had to supply the inputs and interpret the outputs,Agent Components
Agent
Environment
Sensors
How happy I will be
in such a state
State
How the world evolves
What my actions do
Utility
Actuators
What action I
should do now
What it will be like
if I do action A
What the world
is like now
ning were largely absent. This was due in part to the great expense and engineering effort
required to get real robots to work at all. The situation has changed rapidly in recent years
possible to combine millions of MEMS devices to produce powerful macroscopic actuators.
systems to embedded robotic systems. The state of robotics today is roughly comparable to
experiment with PCs, but it would take another decade before they became commonplace.
Keeping track of the state of the world: This is one of the core capabilities required
for an intelligent agent. It requires both perception and updating of internal representations.
uncertain environments. Current ﬁltering and perception algorithms can be combined to do a
can handle quite large environments, they are still dealing with a factored representation—Chapter
AI: The Present and Future
that the application of these ideas for tracking complex environments will yield huge beneﬁts.
However, we are still faced with a daunting task of deﬁning general, reusable representation
in general; only for isolated, simple domains. It is possible that a new focus on probabilistic
encoding of knowledge) will allow for progress.
representation requirements here are the same as for keeping track of the world; the primary
difﬁculty is coping with courses of action—such as having a conversation or a cup of tea—
that consist eventually of thousands or millions of primitive steps for a real agent. It is only
by imposing hierarchical structure on behavior that we humans cope at all. We saw in
thermore, work in hierarchical reinforcement learning has succeeded in combining some
Utility as an expression of preferences: In principle, basing rational decisions on the
maximization of expected utility is completely general and avoids many of the problems of
however, there has been very little work on constructing realistic utility functions—imagine,
for example, the complex web of interacting preferences that must be understood by an agent
operating as an ofﬁce assistant for a human being. It has proven very difﬁcult to decompose
preferences over complex states in the same way that Bayes nets decompose beliefs over
complex states. One reason may be that preferences over states are really compiled from
Even if the reward function is simple, the corresponding utility function may be very complex.
This suggests that we take seriously the task of knowledge engineering for reward functions
as a way of conveying to our agents what it is that we want them to do.
niques have been developed that can cope with quite large problems, reaching or exceeding
human capabilities in many tasks—as long as we are dealing with a predeﬁned vocabulary
of features and concepts. On the other hand, machine learning has made very little progress
on the important problem of constructing new representations at levels of abstraction higher
than the input vocabulary. In computer vision, for example, learning complex concepts such
as Classroom and Cafeteria would be made unnecessarily difﬁcult if the agent were forced
to work from pixels as the input representation; instead, the agent needs to be able to form
intermediate concepts ﬁrst, such as Desk and Tray, without explicit human supervision.
Similar considerations apply to learning behavior: HavingACupOfTea is a very importantAgent Architectures
much simpler actions such as RaiseArm and Swallow? Perhaps this will incorporate some
of the ideas of deep belief networks—Bayesian networks that have multiple layers of hidden
DEEP BELIEF
NETWORKS
sentations to structured representations, particularly hierarchical representations. The work
Unless we understand such issues, we are faced with the daunting task of constructing
large commonsense knowledge bases by hand, an approach that has not fared well to date.
There is great promise in using the Web as a source of natural language text, images, and
videos to serve as a comprehensive knowledge base, but so far machine learning algorithms
are limited in the amount of organized knowledge they can extract from these sources.
AGENT ARCHITECTURES
The answer is, “All of them!” We have seen that reﬂex responses are needed for situations
plan ahead. A complete agent must be able to do both, using a hybrid architecture. One
HYBRID
ARCHITECTURE
important property of hybrid architectures is that the boundaries between different decision
formation at the deliberative level into more efﬁcient representations, eventually reaching the
they save away a generalized version of the solution for use by the reﬂex component. A
less studied problem is the reversal of this process: when the environment changes, learned
reﬂexes may no longer be appropriate and the agent must return to the deliberative level to
produce new behaviors.
Agents also need ways to control their own deliberations. They must be able to cease
deliberating when action is demanded, and they must be able to use the time available for
that sees an accident ahead must decide in a split second either to brake or to take evasive
action. It should also spend that split second thinking about the most important questions,
such as whether the lanes to the left and right are clear and whether there is a large truck
close behind, rather than worrying about wear and tear on the tires or where to pick up theChapter
AI: The Present and Future
Percepts
Compilation
deliberation
Reflex system
Actions
Compilation serves to convert deliberative decision making into more efﬁ-
cient, reﬂexive mechanisms.
agent will never have long enough to solve the decision problem exactly.
Clearly, there is a pressing need for general methods of controlling deliberation, rather
ANYTIME
ALGORITHM
an algorithm whose output quality improves gradually over time, so that it has a reasonable
for a brief description of metalevel decision making.) Example of an anytime algorithms
THEORETIC
METAREASONING
tions. The value of a computation depends on both its cost (in terms of delaying action) and
its beneﬁts (in terms of improved decision quality). Metareasoning techniques can be used to
erty. Metareasoning is expensive, of course, and compilation methods can be applied so that
the overhead is small compared to the costs of the computations being controlled. Metalevel
reinforcement learning may provide another way to acquire effective policies for controlling
deliberation: in essence, computations that lead to better decisions are reinforced, while those
that turn out to have no effect are penalized. This approach avoids the myopia problems of
REFLECTIVE
ARCHITECTURE
tecture that enables deliberation about the computational entities and actions occurring within
the architecture itself. A theoretical foundation for reﬂective architectures can be built by
deﬁning a joint state space composed from the environment state and the computational state
and backward chaining to disappear from AI systems, to be replaced by general methods thatAre We Going in the Right Direction?
ARE WE GOING IN THE RIGHT DIRECTION?
The preceding section listed many advances and many opportunities for further progress. But
climbing a tree; one can report steady progress, all the way to the top of the tree. In this
section, we consider whether AI’s current path is more like a tree climb or a rocket trip.
also said that
cated environments. The computational demands are just too high. For most of the book,
however, we will adopt the working hypothesis that perfect rationality is a good starting
point for analysis.
Now it is time to consider again what exactly the goal of AI is. We want to build agents, but
with what speciﬁcation in mind? Here are four possibilities:
Perfect rationality. A perfectly rational agent acts at every instant in such a way as to
PERFECT
RATIONALITY
maximize its expected utility, given the information it has acquired from the environment. We
have seen that the calculations necessary to achieve perfect rationality in most environments
are too time consuming, so perfect rationality is not a realistic goal.
CALCULATIVE
RATIONALITY
on this property. A calculatively rational agent eventually returns what would have been the
rational choice at the beginning of its deliberation. This is an interesting property for a system
to exhibit, but in most environments, the right answer at the wrong time is of no value. In
able overall performance; unfortunately, the theoretical basis of calculative rationality does
BOUNDED
RATIONALITY
imately perfect) rationality and replaced it with bounded rationality, a descriptive theory of
decision making by real agents. He wrote,
The capacity of the human mind for formulating and solving complex problems is very
small compared with the size of the problems whose solution is required for objectively
jective rationality.
He suggested that bounded rationality works primarily by satisﬁcing—that is, deliberating
only long enough to come up with an answer that is “good enough.” Simon won the Nobel
to be a useful model of human behaviors in many cases. It is not a formal speciﬁcation
for intelligent agents, however, because the deﬁnition of “good enough” is not given by the
theory. Furthermore, satisﬁcing seems to be just one of a large range of methods used to cope
with bounded resources.Chapter
AI: The Present and Future
Bounded optimality (BO). A bounded optimal agent behaves as well as possible, given its
BOUNDED
OPTIMALITY
computational resources. That is, the expected utility of the agent program for a bounded
optimal agent is at least as high as the expected utility of any other agent program running on
the same machine.
Of these four possibilities, bounded optimality seems to offer the best hope for a strong
theoretical foundation for AI. It has the advantage of being possible to achieve: there is always
at least one best program—something that perfect rationality lacks. Bounded optimal agents
are actually useful in the real world, whereas calculatively rational agents usually are not, and
satisﬁcing agents might or might not be, depending on how ambitious they are.
The traditional approach in AI has been to start with calculative rationality and then
make compromises to meet resource constraints. If the problems imposed by the constraints
are minor, one would expect the ﬁnal design to be similar to a BO agent design. But as the
resource constraints become more critical—for example, as the environment becomes more
complex—one would expect the two designs to diverge. In the theory of bounded optimality,
these constraints can be handled in a principled fashion.
As yet, little is known about bounded optimality. It is possible to construct bounded
timal programs does not depend too strongly on the details of the computer being used. It
byte machine made a signiﬁcant difference to the design of the BO program. One way to
make sure this cannot happen is to be slightly more relaxed about the criteria for bounded
ASYMPTOTIC
BOUNDED
OPTIMALITY
Suppose a program P is bounded optimal for a machine M in a class of environments E,
where the complexity of environments in E is unbounded. Then program P ′ is ABO for M
in E if it can outperform P by running on a machine kM that is k times faster (or larger)
than M. Unless k were enormous, we would be happy with a program that was ABO for
a nontrivial environment on a nontrivial architecture. There would be little point in putting
enormous effort into ﬁnding BO rather than ABO programs, because the size and speed of
available machines tends to increase by a constant factor in a ﬁxed amount of time anyway.
We can hazard a guess that BO or ABO programs for powerful computers in complex
environments will not necessarily have a simple, elegant structure. We have already seen that
a variety of forms of knowledge and decision making; learning and compilation mechanisms
knowledge. A bounded optimal agent must adapt to the environment in which it ﬁnds itself,
so that eventually its internal organization will reﬂect optimizations that are speciﬁc to the
particular environment. This is only to be expected, and it is similar to the way in which
racing cars restricted by engine capacity have evolved into extremely complex designs. WeWhat If AI Does Succeed?
suspect that a science of artiﬁcial intelligence based on bounded optimality will involve a
good deal of study of the processes that allow an agent program to converge to bounded
optimality and perhaps less concentration on the details of the messy programs that result.
In sum, the concept of bounded optimality is proposed as a formal task for AI research
that is both well deﬁned and feasible. Bounded optimality speciﬁes optimal programs rather
than optimal actions. Actions are, after all, generated by programs, and it is over programs
that designers have control.
WHAT IF AI DOES SUCCEED?
the protagonist causes consternation by asking a panel of eminent but contradictory literary
theorists the following question: “What if you were right?” None of the theorists seems to
have considered this question before, perhaps because debating unfalsiﬁable theories is an end
in itself. Similar confusion can be evoked by asking AI researchers, “What if you succeed?”
more powerful than dumb ones, but will that power be used for good or ill? Those who strive
to develop AI have a responsibility to see that the impact of their work is a positive one. The
scope of the impact will depend on the degree of success of AI. Even modest successes in AI
development is practiced. AI has made possible new applications such as speech recognition
systems, inventory control systems, surveillance systems, robots, and search engines.
their daily lives. So far, computerized communication networks, such as cell phones and the
Internet, have had this kind of pervasive effect on society, but AI has not. AI has been at work
tions for every purchase made on the Web—but has not been visible to the average consumer.
We can imagine that truly useful personal assistants for the ofﬁce or the home would have a
tion in the short term. Automated assistants for driving could prevent accidents, saving tens
of thousands of lives per year. A technological capability at this level might also be applied
to the development of autonomous weapons, which many view as undesirable. Some of the
biggest societal problems we face today—such as the harnessing of genomic information for
treating disease, the efﬁcient management of energy resources, and the veriﬁcation of treaties
concerning nuclear weapons—are being addressed with the help of AI technologies.
telligence and beyond—would change the lives of a majority of humankind. The very nature
of our work and play would be altered, as would our view of intelligence, consciousness, and
man autonomy, freedom, and even survival. For these reasons, we cannot divorce AI researchChapter
AI: The Present and Future
Which way will the future go? Science ﬁction authors seem to favor dystopian futures
over utopian ones, probably because they make for more interesting plots. But so far, AI
seems to ﬁt in with other revolutionary technologies (printing, plumbing, air travel, telephony)
whose negative repercussions are outweighed by their positive aspects.
In conclusion, we see that AI has made great progress in its short history, but the ﬁnal
valid today:
We can see only a short distance ahead,
but we can see that much remains to be done.A
MATHEMATICAL
BACKGROUND
COMPLEXITY ANALYSIS AND O() NOTATION
Computer scientists are often faced with the task of comparing algorithms to see how fast
they run or how much memory they require. There are two approaches to this task. The ﬁrst
is benchmarking—running the algorithms on a computer and measuring speed in seconds
BENCHMARKING
and memory consumption in bytes. Ultimately, this is what really matters, but a benchmark
can be unsatisfactory because it is so speciﬁc: it measures the performance of a particular
program written in a particular language, running on a particular computer, with a particular
compiler and particular input data. From the single result that the benchmark provides, it
puter, or data set. The second approach relies on a mathematical analysis of algorithms,
ANALYSIS OF
ALGORITHMS
independently of the particular implementation and input, as discussed below.
Asymptotic analysis
We will consider algorithm analysis through the following example, a program to compute
the sum of a sequence of numbers:
function SUMMATION(sequence) returns a number
sum ←sum + sequence[i]
return sum
The ﬁrst step in the analysis is to abstract over the input, in order to ﬁnd some parameter or
terized by the length of the sequence, which we will call n. The second step is to abstract
over the implementation, to ﬁnd some measure that reﬂects the running time of the algorithm
but is not tied to a particular compiler or computer. For the SUMMATION program, this could
be just the number of lines of code executed, or it could be more detailed, measuring the
number of additions, assignments, array references, and branches executed by the algorithm.Appendix
A.
Mathematical background
Either way gives us a characterization of the total number of steps taken by the algorithm as
a function of the size of the input. We will call this characterization T(n). If we count lines
If all programs were as simple as SUMMATION, the analysis of algorithms would be a
trivial ﬁeld. But two problems make it more complicated. First, it is rare to ﬁnd a parameter
like n that completely characterizes the number of steps taken by an algorithm. Instead, the
best we can usually do is compute the worst case Tworst(n) or the average case Tavg(n).
Computing an average means that the analyst must assume some distribution of inputs.
The second problem is that algorithms tend to resist exact analysis. In that case, it is
necessary to fall back on an approximation. We say that the SUMMATION algorithm is O(n),
meaning that its measure is at most a constant times n, with the possible exception of a few
small values of n. More formally,
T(n) is O(f(n)) if T(n) ≤kf(n) for some k, for all n > n0 .
ASYMPTOTIC
ANALYSIS
tion that, as n asymptotically approaches inﬁnity, an O(n) algorithm is better than an O(n2)
algorithm. A single benchmark ﬁgure could not substantiate such a claim.
The O() notation abstracts over constant factors, which makes it easier to use, but less
precise, than the T() notation. For example, an O(n2) algorithm will always be worse than
Despite this drawback, asymptotic analysis is the most widely used tool for analyzing
ations (by ignoring the constant factor k) and the exact content of the input (by considering
only its size n) that the analysis becomes mathematically feasible. The O() notation is a good
compromise between precision and ease of analysis.
NP and inherently hard problems
The analysis of algorithms and the O() notation allow us to talk about the efﬁciency of a
particular algorithm. However, they have nothing to say about whether there could be a better
algorithm for the problem at hand. The ﬁeld of complexity analysis analyzes problems rather
COMPLEXITY
ANALYSIS
than algorithms. The ﬁrst gross division is between problems that can be solved in polynomial
time and problems that cannot be solved in polynomial time, no matter what algorithm is
used. The class of polynomial problems—those which can be solved in time O(nk) for some
k—is called P. These are sometimes called “easy” problems, because the class contains those
problems with running times like O(log n) and O(n). But it also contains those with time
O(n1000), so the name “easy” should not be taken too literally.
Another important class of problems is NP, the class of nondeterministic polynomial
problems. A problem is in this class if there is some algorithm that can guess a solution and
then verify whether the guess is correct in polynomial time. The idea is that if you have an
arbitrarily large number of processors, so that you can try all the guesses at once, or you are
very lucky and always guess right the ﬁrst time, then the NP problems become P problems.
One of the biggest open questions in computer science is whether the class NP is equivalentVectors, Matrices, and Linear Algebra
to the class P when one does not have the luxury of an inﬁnite number of processors or
omniscient guessing. Most computer scientists are convinced that P ̸= NP; that NP problems
Those who are interested in deciding whether P = NP look at a subclass of NP called the
and thus refers to the hardest problems in the class NP. It has been proven that either all
interesting, but the class is also of practical interest because many important problems are
propositional logic, is there an assignment of truth values to the proposition symbols of the
sentence that makes it true? Unless a miracle occurs and P = NP, there can be no algorithm
that solves all satisﬁability problems in polynomial time. However, AI is more interested in
do quite well on many problems.
The class #P (pronounced “sharp P”) is the set of counting problems corresponding to
decision problem. For example, deciding whether a bipartite graph has a perfect matching
can be done in time O(V E) (where the graph has V vertices and E edges), but the counting
that it is hard as any problem in #P and thus at least as hard as any NP problem.
Another class is the class of PSPACE problems—those that require a polynomial amount
could turn out that P = NP.
VECTORS, MATRICES, AND LINEAR ALGEBRA
VECTOR
face characters for vector names, although some authors use arrows or bars over the names:
⃗x or ¯y. The elements of a vector can be accessed using subscripts: z = ⟨z1, z2, . . . , zn⟩. One
confusing point: this book is synthesizing work from many subﬁelds, which variously callAppendix
A.
Mathematical background
The length of a vector is denoted |x| and is computed by taking the square root of the
sum of the squares of the elements: |x| =

scalar product) of two vectors is the sum of the products of corresponding elements, that is,
x · y = 
Euclidean space. Vector addition is then equivalent to placing the tail of one vector at the
between x and y.
A matrix is a rectangular array of values arranged into rows and columns. Here is a
MATRIX
⎛
⎝
⎞
⎠
guages, Ai,j is often written A[i,j] or A[i][j].
The sum of two matrices is deﬁned by adding their corresponding elements; for example
(A + B)i,j = Ai,j + Bi,j. (The sum is undeﬁned if A and B have different sizes.) We can also
deﬁne the multiplication of a matrix by a scalar: (cA)i,j = cAi,j. Matrix multiplication (the
product of two matrices) is more complicated. The product AB is deﬁned only if A is of size
a × b and B is of size b × c (i.e., the second matrix has the same number of rows as the ﬁrst
has columns); the result is a matrix of size a × c. If the matrices are of appropriate size, then
the result is
(AB)i,k =


j
Ai,jBj,k .
Matrix multiplication is not commutative, even for square matrices: AB ̸= BA in general.
It is, however, associative: (AB)C = A(BC). Note that the dot product can be expressed in
terms of a transpose and a matrix multiplication:x · y = x⊤y.
IDENTITY MATRIX
It has the property that AI = A for all A. The transpose of A, written A⊤is formed by
TRANSPOSE
turning rows into columns and vice versa, or, more formally, by A⊤i,j = Aj,i. The inverse of
INVERSE
SINGULAR
the inverse does not exist. For a nonsingular matrix, it can be computed in O(n3) time.
nated by inverting a matrix of coefﬁcients. Consider the following set of equations, for which
we want a solution in x, y, and z:Probability Distributions
We can represent this system as the matrix equation A x = b, where
A =
⎛
⎝
⎞
⎠,
x =
⎛
⎝
x
y
z
⎞
⎠,
b =
⎛
⎝
⎞
⎠.
to x = A−1b. After inverting A and multiplying by b, we get the answer
x =
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
⎞
⎠.
PROBABILITY DISTRIBUTIONS
A probability is a measure over a set of events that satisﬁes three axioms:
where X is a random variable representing an event and xi are the possible values of
X. In general, random variables are denoted by uppercase letters and their values by
lowercase letters.
vidual events; that is, P(X = x1 ∨X = x2) = P(X = x1) + P(X = x2), where x1 and
x2 are disjoint.
A probabilistic model consists of a sample space of mutually exclusive possible outcomes,
together with a probability measure for each outcome. For example, in a model of the weather
comes constitutes an event. For example, the event of precipitation is the subset consisting of
{rainy, snowy}.
We use P(X) to denote the vector of values ⟨P(X = x1), . . . , P(X = xn)⟩. We also
use P(xi) as an abbreviation for P(X = xi) and 
x P(x) for n
tionally independent if P(B|A) = P(B) (or equivalently, P(A|B) = P(A)). For continuous
PROBABILITY
DENSITY FUNCTION
also denote as P(·), but which has a slightly different meaning from the discrete probability
function. The density function P(x) for a random variable X, which might be thought of as
P(X = x), is intuitively deﬁned as the ratio of the probability that X falls into an interval
around x, divided by the width of the interval, as the interval width goes to zero:
P(x) = limAppendix
A.
Mathematical background
The density function must be nonnegative for all x and must have
 ∞
−∞
CUMULATIVE
PROBABILITY
DENSITY FUNCTION
bility of a random variable being less than x:
 x
−∞
P(u) du .
Note that the probability density function has units, whereas the discrete probability function
is unitless. For example, if values of X are measured in seconds, then the density is measured
One of the most important probability distributions is the Gaussian distribution, also
GAUSSIAN
DISTRIBUTION
ation σ (and therefore variance σ2) is deﬁned as
P(x) =
σ
√
STANDARD NORMAL
DISTRIBUTION
a vector x in n dimensions, there is the multivariate Gaussian distribution:
MULTIVARIATE
GAUSSIAN
P(x) =

(2π)n|Σ|
“
(x−μ)⊤Σ
”
,
In one dimension, we can deﬁne the cumulative distribution function F(x) as the
CUMULATIVE
DISTRIBUTION
probability that a random variable will be less than x. For the normal distribution, this is
F(x) =
x

−∞
σ
√
CENTRAL LIMIT
THEOREM
dent random variables and taking their mean tends to a normal distribution as n tends to
inﬁnity. This holds for almost any collection of random variables, even if they are not strictly
independent, unless the variance of any ﬁnite subset of variables dominates the others.
The expectation of a random variable, E(X), is the mean or average value, weighted
EXPECTATION
by the probability of each value. For a discrete variable it is:
E(X) =


i
xi P(X = xi) .
For a continuous variable, replace the summation with an integral over the probability density
function, P(x):
E(X) =
∞

−∞
xP(x) dx ,Bibliographical and Historical Notes
The root mean square, RMS, of a set of values (often samples of a random variable) is
ROOT MEAN SQUARE
the square root of the mean of the squares of the values,
RMS(x1, . . . , xn) =

x2
n
.
The covariance of two random variables is the expectation of the product of their differences
COVARIANCE
from their means:
cov(X, Y ) = E((X −μX)(Y −μY )) .
The covariance matrix, often denoted Σ, is a matrix of covariances between elements of a
COVARIANCE MATRIX
vector of random variables. Given X = ⟨X1, . . . Xn⟩⊤, the entries of the covariance matrix
are as follows:
Σi,j = cov(Xi, Xj) = E((Xi −μi)(Xj −μj)) .
A few more miscellaneous points: we use log(x) for the natural logarithm, loge(x). We use
argmaxx f(x) for the value of x for which f(x) is maximal.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The O() notation so widely used in computer science today was ﬁrst introduced in the context
the Turing award, the highest honor in computer science, for their work.B
NOTES ON LANGUAGES
AND ALGORITHMS
DEFINING LANGUAGES WITH BACKUS–NAUR FORM (BNF)
In this book, we deﬁne several languages, including the languages of propositional logic
guage is deﬁned as a set of strings where each string is a sequence of symbols. The languages
terize the set. We do that with a grammar. The particular type of grammar we use is called a
GRAMMAR
our grammars in a formalism called Backus–Naur form (BNF). There are four components
BACKUS–NAUR
FORM (BNF)
to a BNF grammar:
• A set of terminal symbols. These are the symbols or words that make up the strings of
TERMINAL SYMBOL
the language. They could be letters (A, B, C, . . .) or words (a, aardvark, abacus, . . .),
or whatever symbols are appropriate for the domain.
NONTERMINAL
SYMBOL
ple, the nonterminal symbol NounPhrase in English denotes an inﬁnite set of strings
including “you” and “the big slobbery dog.”
• A start symbol, which is the nonterminal symbol that denotes the complete set of
START SYMBOL
strings of the language. In English, this is Sentence; for arithmetic, it might be Expr,
and for programming languages it is Program.
• A set of rewrite rules, of the form LHS
→
RHS, where LHS is a nonterminal
symbol and RHS is a sequence of zero or more symbols. These can be either terminal
A rewrite rule of the form
means that whenever we have two strings categorized as a NounPhrase and a VerbPhrase,
we can append them together and categorize the result as a Sentence. As an abbreviation,Describing Algorithms with Pseudocode
Here is a BNF grammar for simple arithmetic expressions:
Expr
→
Expr Operator Expr | ( Expr ) | Number
Number
→
Digit | Number Digit
Digit
→
Operator
→
+ |
−|
÷ |
×
use slightly different notations for BNF; for example, you might see ⟨Digit⟩instead of Digit
DESCRIBING ALGORITHMS WITH PSEUDOCODE
The algorithms in this book are described in pseudocode. Most of the pseudocode should be
formulas or ordinary English to describe parts that would otherwise be more cumbersome. A
few idiosyncrasies should be noted.
• Persistent variables: We use the keyword persistent to say that a variable is given an
initial value the ﬁrst time a function is called and retains that value (or the value given to
it by a subsequent assignment statement) on all subsequent calls to the function. Thus,
persistent variables are like global variables in that they outlive a single call to their
function, but they are accessible only within the function. The agent programs in the
book use persistent variables for memory. Programs with persistent variables can be
Smalltalk. In functional languages, they can be implemented by functional closures
over an environment containing the required variables.
• Functions as values: Functions and procedures have capitalized names, and variables
have lowercase italic names. So most of the time, a function call looks like FN(x).
However, we allow the value of a variable to be a function; for example, if the value of
• for each: The notation “for each x in c do” means that the loop is executed with the
variable x bound to successive elements of the collection c.
and Visual Basic (which use end).
second to y. The same idea is used in “for each x, y in pairs do” and can be used to
swap two variables: “x, y ←y, x”
• Generators and yield: the notation “generator G(x) yields numbers” deﬁnes G as a
generator function. This is best understood by an example. The code fragment shown inAppendix
B.
Notes on Languages and Algorithms
while true do
yield i
PRINT(p)
Example of a generator function and its invocation within a loop.
returns a generator, which in turn yields one value each time the loop code asks for the
next element of the collection. Even though the collection is inﬁnite, it is enumerated
one element at a time.
• Lists: [x, y, z] denotes a list of three elements. [ﬁrst|rest] denotes a list formed by
adding ﬁrst to the list rest. In Lisp, this is the cons function.
• Sets: {x, y, z} denotes a set of three elements. {x : p(x)} denotes the set of all elements
x for which p(x) is true.
ONLINE HELP
Most of the algorithms in the book have been implemented in Java, Lisp, and Python at our
online code repository:
aima.cs.berkeley.edu
The same Web site includes instructions for sending comments, corrections, or suggestions
for improving the book, and for joining discussion lists.This page intentionally left blank 