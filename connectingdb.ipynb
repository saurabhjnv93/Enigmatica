{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from AnsweringClass import *\n",
    "ans = AnsweringClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all_books_paragraphs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_number</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>embedding</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>FUNDAMENTALS OF MACHINE LEARNING FOR\\nPREDICTI...</td>\n",
       "      <td>[-8.55935141e-02 -3.08090486e-02  4.21223417e-...</td>\n",
       "      <td>fundamentals-of-machine-learning-for-predictiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MIT Press books may be purchased at special qu...</td>\n",
       "      <td>[-1.51165545e-01 -4.82687764e-02  3.75046209e-...</td>\n",
       "      <td>fundamentals-of-machine-learning-for-predictiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kelleher, Brian Mac Namee, and Aoife DArcy. In...</td>\n",
       "      <td>[-2.09216531e-02 -2.10685264e-02  2.43631154e-...</td>\n",
       "      <td>fundamentals-of-machine-learning-for-predictiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>As such, the book goes beyond the standard top...</td>\n",
       "      <td>[-7.86022916e-02 -5.77687770e-02  1.44656338e-...</td>\n",
       "      <td>fundamentals-of-machine-learning-for-predictiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The fact that the book\\nprovides case studies ...</td>\n",
       "      <td>[-3.77877764e-02 -5.17287245e-03 -7.59665016e-...</td>\n",
       "      <td>fundamentals-of-machine-learning-for-predictiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paragraph_number                                          paragraph  \\\n",
       "0                 1  FUNDAMENTALS OF MACHINE LEARNING FOR\\nPREDICTI...   \n",
       "1                 2  MIT Press books may be purchased at special qu...   \n",
       "2                 3  Kelleher, Brian Mac Namee, and Aoife DArcy. In...   \n",
       "3                 4  As such, the book goes beyond the standard top...   \n",
       "4                 5  The fact that the book\\nprovides case studies ...   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [-8.55935141e-02 -3.08090486e-02  4.21223417e-...   \n",
       "1  [-1.51165545e-01 -4.82687764e-02  3.75046209e-...   \n",
       "2  [-2.09216531e-02 -2.10685264e-02  2.43631154e-...   \n",
       "3  [-7.86022916e-02 -5.77687770e-02  1.44656338e-...   \n",
       "4  [-3.77877764e-02 -5.17287245e-03 -7.59665016e-...   \n",
       "\n",
       "                                              Source  \n",
       "0  fundamentals-of-machine-learning-for-predictiv...  \n",
       "1  fundamentals-of-machine-learning-for-predictiv...  \n",
       "2  fundamentals-of-machine-learning-for-predictiv...  \n",
       "3  fundamentals-of-machine-learning-for-predictiv...  \n",
       "4  fundamentals-of-machine-learning-for-predictiv...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbcon= db.connect_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [float(i) for i in emb]\n",
    "que = \"What is loss function.\"\n",
    "emb = ans.get_sentence_embedding(que)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn,cor = db.connect_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `lst` is a Python list like [3, 1, 2]\n",
    "select_query = \"\"\"\n",
    "    SELECT paragraph, \n",
    "           1 - (embedding <=> %s::VECTOR) AS cosine_similarity\n",
    "    FROM AI_ML_Data\n",
    "    ORDER BY cosine_similarity DESC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "# Convert Python list to a PostgreSQL-compatible array string\n",
    "vector_str = f\"[{', '.join(map(str, lst))}]\"  # Convert list [3, 1, 2] to \"[3, 1, 2]\"\n",
    "\n",
    "# Pass the vector string as a parameter\n",
    "cor.execute(select_query, (vector_str,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = cor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = ''\n",
    "for i in row:\n",
    "    ans = ans+i[0]\n",
    "ans = ans.replace(\"\\n\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The loss\\nfunction L(x, y, ˆy) is deﬁned as the amount of utility lost by predicting h(x) = ˆy when the\\ncorrect answer is f(x) = y:\\nL(x, y, ˆy) = Utility(result of using y given an input x)\\n−Utility(result of using ˆy given an input x)Evaluating and Choosing the Best Hypothesis\\nValidation Set Error\\nError rates on training data (lower, dashed line) and validation data (upper,\\ntotes, and then choose the tree with minimal error on the validation set; in this case the tree\\nThis is the most general formulation of the loss function.There are many possible ways\\nto deﬁne such loss functions, and here we list a few examples. In all the examples\\nwe deﬁne ℓ(h, (¯x, y)) = ∆(h(¯x), y), for some function ∆: S∞\\na loss function is almost never used in practice as it does not distinguish\\nbetween the case in which π(y′) is almost equal to π(y) and the case in\\nwhich π(y′) is completely diﬀerent from π(y).A common choice of loss function in regression problems is the squared loss given\\n{y(x) −t}2p(x, t) dx dt. ﬂexible function y(x), we can do this formally using the calculus of variations to\\nSolving for y(x), and using the sum and product rules of probability, we obtain\\ntp(t|x) dt = Et[t|x]The regression function y(x),\\nwhich minimizes the expected\\nsquared loss, is given by the\\ntiple target variables represented by the vector t, in which case the optimal solution\\nis the conditional average y(x) = Et[t|x].Our goal is then to minimize the total loss\\nincurred. Note that some authors consider instead a utility function, whose value\\nsimply the negative of the loss, and throughout this text we shall use the loss function\\nmatrix. For instance, in our cancer example, we might have a loss matrix of the form\\nThe optimal solution is the one which minimizes the loss function.However,\\nthe loss function depends on the true class, which is unknown. For a given input\\nvector x, our uncertainty in the true class is expressed through the joint probability\\ndistribution p(x, Ck) and so we seek instead to minimize the average loss, where the\\naverage is computed with respect to this distribution, which is given by\\nimplies that for each x we should minimize \\x05\\nthe product rule p(x, Ck) = p(Ck|x)p(x) to eliminate the common factor of p(x).'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss function L(x, y, ˆy) is deﬁned as the amount of utility lost by predicting h(x) = ˆy when the correct answer is f(x) = y: L(x, y, ˆy) = Utility(result of using y given an input x) −Utility(result of using ˆy given an input x)Evaluating and Choosing the Best Hypothesis Validation Set Error Error rates on training data (lower, dashed line) and validation data (upper, totes, and then choose the tree with minimal error on the validation set; in this case the tree This is the most general formulation of the loss function.There are many possible ways to deﬁne such loss functions, and here we list a few examples. In all the examples we deﬁne ℓ(h, (¯x, y)) = ∆(h(¯x), y), for some function ∆: S∞ a loss function is almost never used in practice as it does not distinguish between the case in which π(y′) is almost equal to π(y) and the case in which π(y′) is completely diﬀerent from π(y).A common choice of loss function in regression problems is the squared loss given {y(x) −t}2p(x, t) dx dt. ﬂexible function y(x), we can do this formally using the calculus of variations to Solving for y(x), and using the sum and product rules of probability, we obtain tp(t|x) dt = Et[t|x]The regression function y(x), which minimizes the expected squared loss, is given by the tiple target variables represented by the vector t, in which case the optimal solution is the conditional average y(x) = Et[t|x].Our goal is then to minimize the total loss incurred. Note that some authors consider instead a utility function, whose value simply the negative of the loss, and throughout this text we shall use the loss function matrix. For instance, in our cancer example, we might have a loss matrix of the form The optimal solution is the one which minimizes the loss function.However, the loss function depends on the true class, which is unknown. For a given input vector x, our uncertainty in the true class is expressed through the joint probability distribution p(x, Ck) and so we seek instead to minimize the average loss, where the average is computed with respect to this distribution, which is given by implies that for each x we should minimize \u0005 the product rule p(x, Ck) = p(Ck|x)p(x) to eliminate the common factor of p(x).\n"
     ]
    }
   ],
   "source": [
    "print(ans.replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor.close()\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13426"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"all_books_paragraphs.csv\")\n",
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
