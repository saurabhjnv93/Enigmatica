,paragraph_number,paragraph
0,1,Understanding Machine Learning:
1,2,From Theory to Algorithms
2,3,This copy is for personal use only. Not for distribution.
3,4,Do not post. Please link to:
4,5,"Please note: This copy is almost, but not entirely, identical to the printed version of the book. In particular, page numbers are not identical (but section numbers are the"
5,6,Understanding Machine Learning
6,7,"Machine learning is one of the fastest growing areas of computer science, pled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical lowing a presentation of the basics of the ﬁeld, the book covers a wide books. These include a discussion of the computational complexity of mic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as an advanced undergraduate or beginning graduate course, the text makes dents and nonexpert readers in statistics, computer science, mathematics,"
7,8,"Science and Engineering at The Hebrew University, Israel."
8,9,"University of Waterloo, Canada."
9,10,"The Hebrew University, Jerusalem"
10,11,"University of Waterloo, Canada"
11,12,Cambridge University Press is part of the University of Cambridge.
12,13,"It furthers the University’s mission by disseminating knowledge in the pursuit of education, learning and research at the highest international levels of excellence."
13,14,"This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press."
14,15,Printed in the United States of America
15,16,A catalog record for this publication is available from the British Library
16,17,Library of Congress Cataloging in Publication Data
17,18,"Cambridge University Press has no responsibility for the persistence or accuracy of and does not guarantee that any content on such Web sites is, or will remain, accurate or appropriate."
18,19,"The term machine learning refers to the automated detection of meaningful patterns in data. In the past couple of decades it has become a common tool in almost any task that requires information extraction from large data sets. We are surrounded by a machine learning based technology: search engines learn how learns to ﬁlter our email messages, and credit card transactions are secured by a software that learns how to detect frauds. Digital cameras learn to detect recognize voice commands. Cars are equipped with accident prevention systems that are built using machine learning algorithms. Machine learning is also widely used in scientiﬁc applications such as bioinformatics, medicine, and astronomy."
19,20,"One common feature of all of these applications is that, in contrast to more traditional uses of computers, in these cases, due to the complexity of the patterns detailed speciﬁcation of how such tasks should be executed. Taking example from intelligent beings, many of our skills are acquired or reﬁned through learning from our experience (rather than following explicit instructions given to us). Machine learning tools are concerned with endowing programs with the ability to “learn” duction to the main concepts underlying machine learning: What is learning?"
20,21,"How can a machine learn? How do we quantify the resources needed to learn a given concept? Is learning always possible? Can we know if the learning process succeeded or failed? rithms. We chose to present algorithms that on one hand are successfully used in practice and on the other hand give a wide spectrum of diﬀerent learning techniques. Additionally, we pay speciﬁc attention to algorithms appropriate for come increasingly “digitized” and the amount of data available for learning is dramatically increasing. As a result, in many applications data is plentiful and computation time is the main bottleneck. We therefore explicitly quantify both the amount of data and the amount of computation time needed to learn a given"
21,22,"The book is divided into four parts. The ﬁrst part aims at giving an initial eralization of Valiant’s Probably Approximately Correct (PAC) learning model, which is a ﬁrst solid answer to the question “what is learning?”. We describe the Empirical Risk Minimization (ERM), Structural Risk Minimization (SRM), and Minimum Description Length (MDL) learning rules, which shows “how can a machine learn”. We quantify the amount of data needed for learning using the ERM, SRM, and MDL rules and show how learning might fail by deriving quired for learning. In the second part of the book we describe various learning algorithms. For some of the algorithms, we ﬁrst present a more general learning principle, and then show how the algorithm follows the principle. While the ﬁrst two parts of the book focus on the PAC model, the third part extends the scope by presenting a wider variety of learning models. Finally, the last part of the book is devoted to advanced theory. the reader is assumed to be comfortable with basic notions of probability, linear algebra, analysis, and algorithms. The ﬁrst three parts of the book are intended for ﬁrst year graduate students in computer science, engineering, mathematics, or statistics. It can also be accessible to undergraduate students with the adequate background. The more advanced chapters can be used by researchers intending to gather a deeper theoretical understanding."
22,23,"The book is based on Introduction to Machine Learning courses taught by Shai sity of Waterloo. The ﬁrst draft of the book grew out of the lecture notes for the exercises. Alon, to whom we are indebted for his help throughout the entire making of the book, has also prepared a solution manual."
23,24,"We are deeply grateful for the most valuable work of Dana Rubinstein. Dana has scientiﬁcally proofread and edited the manuscript, transforming it from"
24,25,"Special thanks to Amit Daniely, who helped us with a careful read of the advanced part of the book and also wrote the advanced chapter on multiclass learnability. We are also grateful for the members of a book reading club in"
25,26,"Jerusalem that have carefully read and constructively criticized every line of vani, Aharon Birnbaum, Alon Cohen, Alon Gonen, Roi Livni, Ofer Meshi, Dan"
26,27,"Rosenbaum, Dana Rubinstein, Shahar Somin, Alon Vinnikov, and Yoav Wald."
27,28,"We would also like to thank Gal Elidan, Amir Globerson, Nika Haghtalab, Shie"
28,29,"Mannor, Amnon Shashua, Nati Srebro, and Ruth Urner for helpful discussions."
29,30,When Do We Need Machine Learning?
30,31,Relations to Other Fields
31,32,How to Read This Book
32,33,Possible Course Plans Based on This Book
33,34,A Formal Model – The Statistical Learning Framework
34,35,Empirical Risk Minimization
35,36,Something May Go Wrong – Overﬁtting
36,37,Empirical Risk Minimization with Inductive Bias
37,38,Finite Hypothesis Classes
38,39,A Formal Learning Model
39,40,A More General Learning Model
40,41,Releasing the Realizability Assumption – Agnostic PAC
41,42,The Scope of Learning Problems Modeled
42,43,Bibliographic Remarks
43,44,Learning via Uniform Convergence
44,45,Uniform Convergence Is Suﬃcient for Learnability
45,46,Finite Classes Are Agnostic PAC Learnable
46,47,Personal use only. Not for distribution. Do not post.
47,48,Bibliographic Remarks
48,49,Bibliographic Remarks
49,50,Axis Aligned Rectangles
50,51,The Fundamental Theorem of PAC learning
51,52,Sauer’s Lemma and the Growth Function
52,53,Uniform Convergence for Classes of Small Eﬀective Size
53,54,Bibliographic remarks
54,55,Nonuniform Learnability
55,56,Nonuniform Learnability
56,57,Characterizing Nonuniform Learnability
57,58,Structural Risk Minimization
58,59,Minimum Description Length and Occam’s Razor
59,60,Other Notions of Learnability – Consistency
60,61,Discussing the Diﬀerent Notions of Learnability
61,62,Bibliographic Remarks
62,63,The Runtime of Learning
63,64,Computational Complexity of Learning
64,65,Implementing the ERM Rule
65,66,Axis Aligned Rectangles
66,67,Boolean Conjunctions
67,68,"Eﬃciently Learnable, but Not by a Proper ERM"
68,69,Bibliographic Remarks
69,70,From Theory to Algorithms
70,71,Linear Programming for the Class of Halfspaces
71,72,Perceptron for Halfspaces
72,73,The VC Dimension of Halfspaces
73,74,Linear Regression for Polynomial Regression Tasks
74,75,Bibliographic Remarks
75,76,Eﬃcient Implementation of ERM for Decision Stumps
76,77,Linear Combinations of Base Hypotheses
77,78,AdaBoost for Face Recognition
78,79,Bibliographic Remarks
79,80,Model Selection and Validation
80,81,Model Selection Using SRM
81,82,Validation for Model Selection
82,83,What to Do If Learning Fails
83,84,Convex Learning Problems
84,85,"Convexity, Lipschitzness, and Smoothness"
85,86,Convex Learning Problems
86,87,Learnability of Convex Learning Problems
87,88,Surrogate Loss Functions
88,89,Bibliographic Remarks
89,90,Regularization and Stability
90,91,Regularized Loss Minimization
91,92,Stable Rules Do Not Overﬁt
92,93,Tikhonov Regularization as a Stabilizer
93,94,Smooth and Nonnegative Loss
94,95,Bibliographic Remarks
95,96,Stochastic Gradient Descent
96,97,Calculating Subgradients
97,98,Subgradients of Lipschitz Functions
98,99,Stochastic Gradient Descent (SGD)
99,100,Adding a Projection Step
100,101,Other Averaging Techniques
101,102,SGD for Risk Minimization
102,103,SGD for Regularized Loss Minimization
103,104,Bibliographic Remarks
104,105,Support Vector Machines
105,106,Optimality Conditions and “Support Vectors”*
106,107,Bibliographic Remarks
107,108,Embeddings into Feature Spaces
108,109,Kernels as a Way to Express Prior Knowledge
109,110,Bibliographic Remarks
110,111,"Multiclass, Ranking, and Complex Prediction Problems"
111,112,Linear Multiclass Predictors
112,113,Generalized Hinge Loss
113,114,Multiclass SVM and SGD
114,115,Structured Output Prediction
115,116,Linear Predictors for Ranking
116,117,Bipartite Ranking and Multivariate Performance Measures
117,118,Linear Predictors for Bipartite Ranking
118,119,Bibliographic Remarks
119,120,Decision Tree Algorithms
120,121,Implementations of the Gain Measure
121,122,Bibliographic Remarks
122,123,The “Curse of Dimensionality”
123,124,Bibliographic Remarks
124,125,Feedforward Neural Networks
125,126,Learning Neural Networks
126,127,The Expressive Power of Neural Networks
127,128,The Sample Complexity of Neural Networks
128,129,The Runtime of Learning Neural Networks
129,130,SGD and Backpropagation
130,131,Bibliographic Remarks
131,132,Additional Learning Models
132,133,Online Classiﬁcation in the Realizable Case
133,134,Online Classiﬁcation in the Unrealizable Case
134,135,Online Convex Optimization
135,136,The Online Perceptron Algorithm
136,137,Bibliographic Remarks
137,138,Graph Laplacian and Relaxed Graph Cuts
138,139,Unnormalized Spectral Clustering
139,140,A High Level View of Clustering
140,141,Bibliographic Remarks
141,142,Dimensionality Reduction
142,143,Principal Component Analysis (PCA)
143,144,A More Eﬃcient Solution for the Case d ≫m
144,145,Implementation and Demonstration
145,146,PCA or Compressed Sensing?
146,147,Bibliographic Remarks
147,148,Maximum Likelihood Estimator
148,149,Maximum Likelihood and Empirical Risk Minimization
149,150,Generalization Analysis
150,151,Linear Discriminant Analysis
151,152,Latent Variables and the EM Algorithm
152,153,EM as an Alternate Maximization Algorithm
153,154,Bibliographic Remarks
154,155,Feature Selection and Generation
155,156,Greedy Selection Approaches
156,157,Feature Manipulation and Normalization
157,158,Examples of Feature Transformations
158,159,Bibliographic Remarks
159,160,Rademacher Complexities
160,161,The Rademacher Complexity
161,162,Rademacher Complexity of Linear Classes
162,163,Generalization Bounds for SVM
163,164,Generalization Bounds for Predictors with Low ℓ1 Norm
164,165,Bibliographic Remarks
165,166,From Covering to Rademacher Complexity via Chaining
166,167,Bibliographic Remarks
167,168,Proof of the Fundamental Theorem of Learning Theory
168,169,The Upper Bound for the Agnostic Case
169,170,The Lower Bound for the Agnostic Case
170,171,The Upper Bound for the Realizable Case
171,172,Multiclass Learnability
172,173,The Natarajan Dimension
173,174,The Multiclass Fundamental Theorem
174,175,Calculating the Natarajan Dimension
175,176,Linear Multiclass Predictors
176,177,On Good and Bad ERMs
177,178,Bibliographic Remarks
178,179,Axis Aligned Rectangles
179,180,Separating Polynomials
180,181,Separation with Margin
181,182,Bibliographic Remarks
182,183,Bibliographic Remarks
183,184,Measure Concentration
184,185,"The subject of this book is automated learning, or, as we will more often call it, Machine Learning (ML). That is, we wish to program computers so that they can “learn” from input available to them. Roughly speaking, learning is the process of converting experience into expertise or knowledge. The input to a learning algorithm is training data, representing experience, and the output is some expertise, which usually takes the form of another computer program this concept, we’ll have to be more explicit about what we mean by each of the involved terms: What is the training data our programs will access? How can the process of learning be automated? How can we evaluate the success of such a process (namely, the quality of the output of a learning program)? mal learning. Some of the most fundamental issues in ML arise already in that context, which we are all familiar with."
185,186,"Bait Shyness – Rats Learning to Avoid Poisonous Baits: When rats encounter food items with novel look or smell, they will ﬁrst eat very small amounts, and subsequent feeding will depend on the ﬂavor of the food and its physiological eﬀect. If the food produces an ill eﬀect, the novel food will often be associated with the illness, and subsequently, the rats will not eat it. Clearly, there is a learning mechanism in play here – the animal used past experience with some food to acquire expertise in detecting the safety of this food. If past experience with the food was negatively labeled, the animal predicts that it will also have a negative eﬀect when encountered in the future."
186,187,"Inspired by the preceding example of successful learning, let us demonstrate a typical machine learning task. Suppose we would like to program a machine that to the way rats learn how to avoid poisonous baits. The machine will simply"
187,188,"Personal use only. Not for distribution. Do not post. it will be moved to the user’s inbox folder. ful, it lacks an important aspect of learning systems – the ability to label unseen examples to broader generalization. This is also referred to as inductive reasoning or inductive inference. In the bait shyness example presented previously, after the rats encounter an example of a certain type of food, they apply their attitude toward it on new, unseen examples of food of similar smell and taste. To achieve generalization in the spam ﬁltering task, the learner can scan the previously seen ingly. Such a system would potentially be able correctly to predict the label of"
188,189,"However, inductive reasoning might lead us to false conclusions. To illustrate this, let us consider again an example from animal learning."
189,190,"Pigeon Superstition: In an experiment performed by the psychologist B. F. Skinner, he placed a bunch of hungry pigeons in a cage. An automatic mechanism had been attached to the cage, delivering food to the pigeons at regular intervals with no reference whatsoever to the birds’ behavior. The hungry pigeons went around the cage, and when food was ﬁrst delivered, it found each pigeon engaged in some activity (pecking, turning the head, etc.). The arrival of food reinforced each bird’s speciﬁc action, and consequently, each bird tended to spend some more time doing that very same action. That, in turn, increased the chance that the next random food delivery would ﬁnd each bird engaged in that activity again. What results is a chain of events that reinforces the pigeons’ association ing when it was ﬁrst delivered. They subsequently continue to perform these"
190,191,What distinguishes learning mechanisms that result in superstition from useful learning? This question is crucial to the development of automated learners.
191,192,"While human learners can rely on common sense to ﬁlter out random meaningless learning conclusions, once we export the task of learning to a machine, we must provide well deﬁned crisp principles that will protect the program from reaching senseless or useless conclusions. The development of such principles is a central goal of the theory of machine learning."
192,193,"What, then, made the rats’ learning more successful than that of the pigeons?"
193,194,"As a ﬁrst step toward answering this question, let us have a closer look at the bait shyness phenomenon in rats."
194,195,"Bait Shyness revisited – rats fail to acquire conditioning between food and electric shock or between sound and nausea: The bait shyness mechanism in rats turns out to be more complex than what one may expect. In experiments unpleasant stimulus that follows food consumption is replaced by, say, electrical shock (rather than nausea), then no conditioning occurs. Even after repeated trials in which the consumption of some food is followed by the administration of unpleasant electrical shock, the rats do not tend to avoid that food. Similar failure of conditioning occurs when the characteristic of the food that implies nausea (such as taste or smell) is replaced by a vocal signal. The rats seem to have some “built in” prior knowledge telling them that, while temporal correlation between food and nausea can be causal, it is unlikely that there would be a causal relationship between food consumption and electrical shocks or between"
195,196,We conclude that one distinguishing feature between the bait shyness learning and the pigeon superstition is the incorporation of prior knowledge that biases the learning mechanism. This is also referred to as inductive bias. The pigeons in the experiment are willing to adopt any explanation for the occurrence of food.
196,197,"However, the rats “know” that food cannot cause an electric shock and that the of that food. The rats’ learning process is biased toward detecting some kind of patterns while ignoring other temporal correlations between events."
197,198,"It turns out that the incorporation of prior knowledge, biasing the learning process, is inevitable for the success of learning algorithms (this is formally stated tools for expressing domain expertise, translating it into a learning bias, and quantifying the eﬀect of such a bias on the success of learning is a central theme of the theory of machine learning. Roughly speaking, the stronger the prior knowledge (or prior assumptions) that one starts the learning process with, the easier it is to learn from further examples. However, the stronger these prior assumptions are, the less ﬂexible the learning is – it is bound, a priori, by the commitment to these assumptions. We shall discuss these issues explicitly in"
198,199,When Do We Need Machine Learning?
199,200,When do we need machine learning rather than directly program our computers to carry out the task at hand? Two aspects of a given problem may call for the use of programs that learn and improve on the basis of their “experience”: the problem’s complexity and the need for adaptivity.
200,201,"Tasks That Are Too Complex to Program. ing how we do them is not suﬃciently elaborate to extract a well deﬁned program. Examples of such tasks include driving, speech recognition, and image understanding. In all of these tasks, state of the art machine learning programs, programs that “learn from their experience,” achieve quite satisfactory results, once exposed to suﬃciently many training examples."
201,202,"• Tasks beyond Human Capabilities: Another wide family of tasks that sis of very large and complex data sets: astronomical data, turning ysis of genomic data, Web search engines, and electronic commerce."
202,203,"With more and more available digitally recorded data, it becomes obvious that there are treasures of meaningful information buried in data archives that are way too large and too complex for humans to make sense of. Learning to detect meaningful patterns in large nation of programs that learn with the almost unlimited memory capacity and ever increasing processing speed of computers opens"
203,204,"Adaptivity. One limiting feature of programmed tools is their rigidity – once the program has been written down and installed, it stays unchanged."
204,205,"However, many tasks change over time or from one user to another."
205,206,"Machine learning tools – programs whose behavior adapts to their input data – oﬀer a solution to such issues; they are, by nature, adaptive to changes in the environment they interact with. Typical successful applications of machine learning to such problems include programs that decode handwritten text, where a ﬁxed program can adapt to variations between the handwriting of diﬀerent users; spam detection programs, speech recognition programs."
206,207,"Learning is, of course, a very wide domain. Consequently, the ﬁeld of machine ing tasks. We give a rough taxonomy of learning paradigms, aiming to provide some perspective of where the content of this book sits within the wide ﬁeld of"
207,208,"We describe four parameters along which learning paradigms can be classiﬁed. tween the learner and the environment, one can divide learning tasks according to the nature of that interaction. The ﬁrst distinction to note is the diﬀerence between supervised and unsupervised learning. As an versus the task of anomaly detection. For the spam detection task, we sage. In contrast, for the task of anomaly detection, all the learner gets learner’s task is to detect “unusual” messages."
208,209,"More abstractly, viewing learning as a process of “using experience to gain expertise,” supervised learning describes a scenario in which the “experience,” a training example, contains signiﬁcant information (say, quired expertise is aimed to predict that missing information for the test data. In such cases, we can think of the environment as a teacher that “supervises” the learner by providing the extra information (labels). In unsupervised learning, however, there is no distinction between training and test data. The learner processes input data with the goal of coming up with some summary, or compressed version of that data. Clustering a data set into subsets of similar objets is a typical example of such a"
209,210,"There is also an intermediate learning setting in which, while the training examples contain more information than the test examples, the ples. For example, one may try to learn a value function that describes for ter than the Black’s. Yet, the only information available to the learner at training time is positions that occurred throughout actual chess games, labeled by who eventually won that game. Such learning frameworks are mainly investigated under the title of reinforcement learning."
210,211,"Active versus Passive Learners Learning paradigms can vary by the role played by the learner. We distinguish between “active” and “passive” learners. An active learner interacts with the environment at training time, say, by posing queries or performing experiments, while a passive learner only observes the information provided by the environment (or the teacher) without inﬂuencing or directing it. Note that the learner of a ing to them. In an active setting, one could imagine asking users to label"
211,212,"Helpfulness of the Teacher When one thinks about human learning, of a baby at home or a student at school, the process often involves a helpful ful for achieving the learning goal. In contrast, when a scientist learns about nature, the environment, playing the role of the teacher, can be best thought of as passive – apples drop, stars shine, and the rain falls narios by postulating that the training data (or the learner’s experience) is generated by some random process. This is the basic building block in the branch of “statistical learning.” Finally, learning also occurs when the learner’s input is generated by an adversarial “teacher.” This may be the case in the spam ﬁltering example (if the spammer makes an eﬀort to mislead the spam ﬁltering designer) or in learning to detect fraud. when no milder setup can be safely assumed. If you can learn against an adversarial teacher, you are guaranteed to succeed interacting any odd"
212,213,"Online versus Batch Learning Protocol The last parameter we mention is the distinction between situations in which the learner has to respond online, throughout the learning process, and settings in which the learner has to engage the acquired expertise only after having a chance to process large amounts of data. For example, a stockbroker has to make daily decisions, based on the experience collected so far. He may become an expert over time, but might have made costly mistakes in the process. In contrast, in many data mining settings, the learner – the data miner – has large amounts of training data to play with before having to output"
213,214,In this book we shall discuss only a subset of the possible learning paradigms.
214,215,"Our main focus is on supervised statistical batch learning with a passive learner (for example, trying to learn how to generate patients’ prognoses, based on large archives of records of patients that were independently collected and are already labeled by the fate of the recorded patients). We shall also brieﬂy discuss online learning and batch unsupervised learning (in particular, clustering)."
215,216,Relations to Other Fields
216,217,"As an interdisciplinary ﬁeld, machine learning shares common threads with the tion. It is naturally a subﬁeld of computer science, as our goal is to program machines so that they will learn. In a sense, machine learning can be viewed as rience into expertise or to detect meaningful patterns in complex sensory data is a cornerstone of human (and animal) intelligence. However, one should note that, in contrast with traditional AI, machine learning is not trying to build automated imitation of intelligent behavior, but rather to use the strengths and ing tasks that fall way beyond human capabilities. For example, the ability to scan and process huge databases allows machine learning programs to detect patterns that are outside the scope of human perception."
217,218,"The component of experience, or training, in machine learning often refers to data that is randomly generated. The task of the learner is to process such vironment from which these examples are picked. This description of machine learning highlights its close relationship with statistics. Indeed there is a lot in common between the two disciplines, in terms of both the goals and techniques used. There are, however, a few signiﬁcant diﬀerences of emphasis; if a doctor comes up with the hypothesis that there is a correlation between smoking and heart disease, it is the statistician’s role to view samples of patients and check sis testing). In contrast, machine learning aims to use the data gathered from samples of patients to come up with a description of the causes of heart disease."
218,219,The hope is that automated techniques may be able to ﬁgure out meaningful patterns (or hypotheses) that may have been missed by the human observer.
219,220,"In contrast with traditional statistics, in machine learning in general, and mic issues are pivotal. We develop algorithms to perform the learning tasks and are concerned with their computational eﬃciency. Another diﬀerence is that while statistics is often interested in asymptotic behavior (like the convergence theory of machine learning focuses on ﬁnite sample bounds. Namely, given the size of available samples, machine learning theory aims to ﬁgure out the degree of accuracy that a learner can expect on the basis of such samples."
220,221,"There are further diﬀerences between these two disciplines, of which we shall mention only one more here. While in statistics it is common to work under the ting, where the learner assumes as little as possible about the nature of the data distribution and allows the learning algorithm to ﬁgure out which models requires some technical preliminaries, and we will come back to it later in the"
221,222,How to Read This Book
222,223,"The ﬁrst part of the book provides the basic theoretical principles that underlie machine learning (ML). In a sense, this is the foundation upon which the rest of the book is built. This part could serve as a basis for a minicourse on the theoretical foundations of ML."
223,224,"The second part of the book introduces the most commonly used algorithmic approaches to supervised machine learning. A subset of these chapters may also be used for introducing machine learning in a general AI course to computer science, Math, or engineering students."
224,225,"The third part of the book extends the scope of discussion from statistical classiﬁcation to other learning models. It covers online learning, unsupervised learning, dimensionality reduction, generative models, and feature learning."
225,226,"The fourth part of the book, Advanced Theory, is geared toward readers who niques that serve to analyze and drive forward the ﬁeld of theoretical machine"
226,227,"The Appendixes provide some technical tools used in the book. In particular, we list basic results from measure concentration and linear algebra."
227,228,"A few sections are marked by an asterisk, which means they are addressed to more advanced students. Each chapter is concluded with a list of exercises. A solution manual is provided in the course Web site."
228,229,Possible Course Plans Based on This Book
229,230,Most of the notation we use throughout the book is either standard or deﬁned on the spot. In this section we describe our main conventions and provide a this section and return to it if during the reading of the book some notation is
230,231,"Often, we would like to emphasize that some object is a vector and then we by xi. We use uppercase letters to denote matrices, sets, and sequences. The meaning should be clear from the context. As we will see momentarily, the input of a learning algorithm is a sequence of training examples. We denote by z an abstract example and by S = z1, . . . , zm a sequence of m examples. Historically,"
231,232,"S is often referred to as a training set; however, we will always assume that S is a sequence rather than a set. A sequence of m vectors is denoted by x1, . . . , xm."
232,233,"The ith element of xt is denoted by xt,i."
233,234,"Throughout the book, we make use of basic notions from probability. We z ∼D to denote that z is sampled according to D. Given a random variable {true, false} we also use Pz∼D[f(z)] to denote D({z : f(z) = true}). In the next chapter we will also introduce the notation Dm to denote the probability over Zm induced by sampling (z1, . . . , zm) where each point zi is sampled from"
234,235,D independently of the other points.
235,236,"In general, we have made an eﬀort to avoid asymptotic notation. However, we"
236,237,"The user who is not familiar with measure theory can skip the few footnotes and remarks regarding more formal measurability deﬁnitions and assumptions. the set of real numbers the set of natural numbers asymptotic notation (see text) the ith element of a vector ⟨x, x⟩(the ℓ2 norm of x) = maxi |xi| (the ℓ∞norm of x) the number of nonzero elements of x a d × k matrix over R the (i, j) element of A a sequence of m vectors the jth element of the ith vector in the sequence the values of a vector w during an iterative algorithm the ith element of the vector w(t) instances domain (a set) labels domain (a set) examples domain (a set) hypothesis class (a set) a distribution over some set (usually over Z or over X) sampling z according to D a sequence of m examples sampling S = z1, . . . , zm i.i.d. according to D probability and expectation of a random variable"
237,238,Gaussian distribution with expectation µ and covariance C the natural logarithm x0 such that for all x > x0 we have f(x) ≤αg(x). We write f = Ω(g) if there f = ω(g) is deﬁned analogously. The notation f = Θ(g) means that f = O(g) such that f(x) = O(g(x) logk(g(x))).
238,239,"The inner product between vectors x and w is denoted by ⟨x, w⟩. Whenever we space and then ⟨x, w⟩= Pd ⟨w, w⟩. We omit the subscript from the ℓ2 norm when it is clear from the context. We also use other ℓp norms, ∥w∥p = (P i |wi| and ∥w∥∞= maxi |wi|. whenever the minimum is not achievable. However, in the context of this book the distinction between inﬁmum and minimum is often of little interest. Hence, to simplify the presentation, we sometimes use the min notation even when inf is more adequate. An analogous remark applies to max versus sup."
239,240,"Let us begin our mathematical analysis by showing how successful learning can be achieved in a relatively simpliﬁed setting. Imagine you have just arrived in some small Paciﬁc island. You soon ﬁnd out that papayas are a signiﬁcant ingredient in the local diet. However, you have never before tasted papayas. You have to learn how to predict whether a papaya you see in the market is tasty or not."
240,241,"First, you need to decide which features of a papaya your prediction should be based on. On the basis of your previous experience with other fruits, you decide to use two features: the papaya’s color, ranging from dark green, through orange and red to dark brown, and the papaya’s softness, ranging from rock hard to mushy. Your input for ﬁguring out your prediction rule is a sample of papayas that you have examined for color and softness and then tasted and found out whether they were tasty or not. Let us analyze this task as a demonstration of the considerations involved in learning problems."
241,242,Our ﬁrst step is to describe a formal model aimed to capture such learning
242,243,A Formal Model – The Statistical Learning Framework
243,244,"• The learner’s input: In the basic statistical learning setting, the learner has access to the following: – Domain set: An arbitrary set, X. This is the set of objects that we tioned before, the domain set will be the set of all papayas. Usually, these domain points will be represented by a vector of features (like the papaya’s color and softness). We also refer to domain points as instances and to X as instance space. – Label set: For our current discussion, we will restrict the label set to – Training data: S = ((x1, y1) . . . (xm, ym)) is a ﬁnite sequence of pairs in"
244,245,"X ×Y: that is, a sequence of labeled domain points. This is the input that the learner has access to (like a set of papayas that have been"
245,246,"Personal use only. Not for distribution. Do not post. tasted and their color, softness, and tastiness). Such labeled examples are often called training examples. We sometimes also refer to S as a"
246,247,"• The learner’s output: The learner is requested to output a prediction rule, siﬁer. The predictor can be used to predict the label of new domain points."
247,248,"In our papayas example, it is a rule that our learner will employ to predict whether future papayas he examines in the farmers’ market are going to be tasty or not. We use the notation A(S) to denote the hypothesis that a learning algorithm, A, returns upon receiving the training sequence S. generated. First, we assume that the instances (the papayas we encounter) are generated by some probability distribution (in this case, representing the environment). Let us denote that probability distribution over X by"
248,249,"D. It is important to note that we do not assume that the learner knows anything about this distribution. For the type of learning tasks we discuss, this could be any arbitrary probability distribution. As to the labels, in the current discussion we assume that there is some “correct” labeling function, the next chapter. The labeling function is unknown to the learner. In fact, this is just what the learner is trying to ﬁgure out. In summary, each pair in the training data S is generated by ﬁrst sampling a point xi according to D and then labeling it by f."
249,250,"• Measures of success: We deﬁne the error of a classiﬁer to be the probability that it does not predict the correct label on a random data point generated by the aforementioned underlying distribution. That is, the error of h is the probability to draw a random instance x, according to the distribution"
250,251,"D, such that h(x) does not equal f(x)."
251,252,"D, assigns a number, D(A), which determines how likely it is to observe a we also use the notation Px∼D[π(x)] to express D(A). = D({x : h(x) ̸= f(x)})."
252,253,"That is, the error of such h is the probability of randomly choosing an example x for which h(x) ̸= f(x). The subscript (D, f) indicates that the error is measured with respect to the probability distribution D and the twice in S and some algorithms can take into account the order of examples in S. σ-algebra of subsets of X, over which D is deﬁned. We will formally deﬁne our measurability assumptions in the next chapter. correct labeling function f. We omit this subscript when it is clear from ization error, the risk, or the true error of h, and we will use these names interchangeably throughout the book. We use the letter L for the error, since we view this error as the loss of the learner. We will later also discuss other possible formulations of such loss."
253,254,"• A note about the information available to the learner The learner is blind to the underlying distribution D over the world and to the labeling function f. In our papayas example, we have just arrived in a new island and we have no clue as to how papayas are distributed and how to predict their tastiness. The only way the learner can interact with the environment is through observing the training set."
254,255,In the next section we describe a simple learning paradigm for the preceding setup and analyze its performance.
255,256,Empirical Risk Minimization
256,257,"As mentioned earlier, a learning algorithm receives as input a training set S, sampled from an unknown distribution D and labeled by some target function fact that the output predictor depends on S). The goal of the algorithm is to ﬁnd hS that minimizes the error with respect to the unknown D and f."
257,258,"Since the learner does not know what D and f are, the true error is not directly available to the learner. A useful notion of error that can be calculated by the learner is the training error – the error the classiﬁer incurs over the training"
258,259,The terms empirical error and empirical risk are often used interchangeably
259,260,"Since the training sample is the snapshot of the world that is available to the learner, it makes sense to search for a solution that works well on that data."
260,261,This learning paradigm – coming up with a predictor h that minimizes LS(h) – is called Empirical Risk Minimization or ERM for short.
261,262,Something May Go Wrong – Overﬁtting
262,263,"Although the ERM rule seems very natural, without being careful, this approach"
263,264,"To demonstrate such a failure, let us go back to the problem of learning to predict the taste of a papaya on the basis of its softness and color. Consider a sample as depicted in the following:"
264,265,"Assume that the probability distribution D is such that instances are distributed uniformly within the gray square and the labeling function, f, determines the"
265,266,"Consider the following predictor: representation of it using polynomials. Clearly, no matter what the sample is, error). On the other hand, the true error of any classiﬁer that predicts the label"
266,267,"We have found a predictor whose performance on the training set is excellent, yet its performance on the true “world” is very poor. This phenomenon is called overﬁtting. Intuitively, overﬁtting occurs when our hypothesis ﬁts the training data “too well” (perhaps like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion)."
267,268,Empirical Risk Minimization with Inductive Bias
268,269,"We have just demonstrated that the ERM rule might lead to overﬁtting. Rather than giving up on the ERM paradigm, we will look for ways to rectify it. We will search for conditions under which there is a guarantee that ERM does not overﬁt, namely, conditions under which when the ERM predictor has good performance with respect to the training data, it is also highly likely to perform well over the underlying data distribution."
269,270,"A common solution is to apply the ERM learning rule over a restricted search space. Formally, the learner should choose in advance (before seeing the data) a set of predictors. This set is called a hypothesis class and is denoted by H. Each with the lowest possible error over S. Formally, where argmin stands for the set of hypotheses in H that achieve the minimum value of LS(h) over H. By restricting the learner to choosing a predictor from"
270,271,"H, we bias it toward a particular set of predictors. Such restrictions are often called an inductive bias. Since the choice of such a restriction is determined before the learner sees the training data, it should ideally be based on some prior knowledge about the problem to be learned. For example, for the papaya taste prediction problem we may choose the class H to be the set of predictors that are determined by axis aligned rectangles (in the space determined by the color and softness coordinates). We will later show that ERMH over this class is guaranteed not to overﬁt. On the other hand, the example of overﬁtting that we have seen previously, demonstrates that choosing H to be a class of predictors does not suﬃce to guarantee that ERMH will not overﬁt."
271,272,"A fundamental question in learning theory is, over which hypothesis classes"
272,273,ERMH learning will not result in overﬁtting. We will study this question later
273,274,"Intuitively, choosing a more restricted hypothesis class better protects us against overﬁtting but at the same time might cause us a stronger inductive bias. We will get back to this fundamental tradeoﬀlater."
274,275,Finite Hypothesis Classes
275,276,"The simplest type of restriction on a class is imposing an upper bound on its size (that is, the number of predictors h in H). In this section, we show that if H is a ﬁnite class then ERMH will not overﬁt, provided it is based on a suﬃciently large training sample (this size requirement will depend on the size of H)."
276,277,"Limiting the learner to prediction rules within some ﬁnite hypothesis class may be considered as a reasonably mild restriction. For example, H can be the set of the hypothesis class becomes a ﬁnite class."
277,278,"Let us now analyze the performance of the ERMH learning rule assuming that let hS denote a result of applying ERMH to S, namely,"
278,279,"In this chapter, we make the following simplifying assumption (which will be relaxed in the next chapter)."
279,280,"There exists h⋆∈H s.t. random samples, S, where the instances of S are sampled according to D and"
280,281,The realizability assumption implies that for every ERM hypothesis we have rather than its empirical risk.
281,282,"Clearly, any guarantee on the error with respect to the underlying distribution,"
282,283,"D, for an algorithm that has access only to a sample S should depend on the relationship between D and S. The common assumption in statistical machine learning is that the training sample S is generated by sampling points from the distribution D independently of each other. Formally,"
283,284,"• The i.i.d. assumption: The examples in the training set are independently and identically distributed (i.i.d.) according to the distribution D. That is, every xi in S is freshly sampled according to D and then labeled according to the labeling function, f. We denote this assumption by S ∼Dm where by applying D to pick each element of the tuple independently of the other members of the tuple."
284,285,"Intuitively, the training set S is a window through which the learner gets partial information about the distribution D over the world and the labeling function, f. The larger the sample gets, the more likely it is to reﬂect more accurately the distribution and labeling used to generate it."
285,286,"Since L(D,f)(hS) depends on the training set, S, and that training set is picked by a random process, there is randomness in the choice of the predictor hS and, consequently, in the risk L(D,f)(hS). Formally, we say that it is a random variable. It is not realistic to expect that with full certainty S will suﬃce to direct the learner toward a good classiﬁer (from the point of view of D), as there is always some probability that the sampled training data happens to be very nonrepresentative of the underlying D. If we go back to the papaya tasting example, there is always some (small) chance that all the papayas we papayas in our island are tasty. In such a case, ERMH(S) may be the constant distribution of papapyas in the island). We will therefore address the probability to sample a training set for which L(D,f)(hS) is not too large. Usually, we denote conﬁdence parameter of our prediction. duce another parameter for the quality of prediction, the accuracy parameter, stances that will lead to failure of the learner. Formally, let S|x = (x1, . . . , xm) be the instances of the training set. We would like to upper bound"
286,287,"Let HB be the set of “bad” hypotheses, that is, other words, this event will only happen if our sample is in the set of misleading samples, M. Formally, we have shown that"
287,288,Note that we can rewrite M as union bound – a basic property of probabilities.
288,289,"For any two sets A, B and a distribution D we"
289,290,D({xi : h(xi) = f(xi)}).
290,291,For each individual sampling of an element of the training set we have
291,292,"A graphical illustration which explains how we used the union bound is given in is, the smaller each of these colored ovals becomes. The union bound formalizes the fact that the area representing the training sets that are misleading with respect to colored ovals. Therefore, it is bounded by |HB| times the maximum size of a colored oval. Any sample S outside the colored ovals cannot cause the ERM rule to overﬁt. and let m be an integer that satisﬁes"
292,293,"Then, for any labeling function, f, and for any distribution, D, for which the have that for every ERM hypothesis, hS, it holds that"
293,294,"The preceeding corollary tells us that for a suﬃciently large m, the ERMH rule of Probably Approximately Correct (PAC) learning. be very unnatural, the goal of this exercise is to show that it can be described as a thresholded polynomial. That is, show that given a training set S ="
294,295,It follows that learning the class of all thresholded polynomials using the ERM rule may lead to overﬁtting.
295,296,"Show that the expected value of LS(h) over the choice of S|x equals L(D,f)(h),"
296,297,"S|x∼Dm[LS(h)] = L(D,f)(h). classiﬁer h(a1,b1,a2,b2) by h(a1,b1,a2,b2)(x1, x2) ="
297,298,The class of all axis aligned rectangles in the plane is deﬁned as
298,299,Note that this is an inﬁnite size hypothesis class. Throughout this exercise we rely on the realizability assumption. positive examples in the training set. Show that A is an ERM.
299,300,"Hint: Fix some distribution D over X, let R∗= R(a∗ angle that generates the labels, and let f be the corresponding hypothesis. to D) of the rectangle R1 = R(a∗ b1, a2, b2 be numbers such that the probability masses of the rectangles"
300,301,• Show that R(S) ⊆R∗.
301,302,• Show that if S contains (positive) examples in all of the rectangles
302,303,"R1, R2, R3, R4, then the hypothesis returned by A has error of at contain an example from Ri."
303,304,• Use the union bound to conclude the argument.
304,305,A Formal Learning Model
305,306,In this chapter we deﬁne our main formal learning model – the PAC learning
306,307,"In the previous chapter we have shown that for a ﬁnite hypothesis class, if the"
307,308,ERM rule with respect to that class is applied on a suﬃciently large training sample (whose size is independent of the underlying distribution or labeling function) then the output hypothesis will be probably approximately correct.
308,309,"More generally, we now deﬁne Probably Approximately Correct (PAC) learning."
309,310,A hypothesis class H is PAC learnable
310,311,"The deﬁnition of Probably Approximately Correct learnability contains two siﬁer is to meet that accuracy requirement (corresponds to the “probably” part proximations are inevitable. Since the training set is randomly generated, there ample, there is always some chance that the training set will contain only one domain point, sampled over and over again). Furthermore, even when we are lucky enough to get a training sample that does faithfully represent D, because it is just a ﬁnite sample, there may always be some ﬁne details of D that it fails"
311,312,Personal use only. Not for distribution. Do not post.
312,313,"A Formal Learning Model for making minor errors. that is, how many examples are required to guarantee a probably approximately"
313,314,"H – for example, for a ﬁnite class we showed that the sample complexity depends on log the size of H."
314,315,"Note that if H is PAC learnable, there are many functions mH that satisfy the requirements given in the deﬁnition of PAC learnability. Therefore, to be precise, we will deﬁne the sample complexity of learning H to be the “minimal function,”"
315,316,Let us now recall the conclusion of the analysis of ﬁnite hypothesis classes from the previous chapter. It can be rephrased as stating:
316,317,Every ﬁnite hypothesis class is PAC learnable with sample a class is not its ﬁniteness but rather a combinatorial measure called the VC
317,318,A More General Learning Model
318,319,"The model we have just described can be readily generalized, so that it can be made relevant to a wider scope of learning tasks. We consider generalizations in"
319,320,"Removing the Realizability Assumption bution D and labeling function f provided that the realizability assumption is met. For practical learning tasks, this assumption may be too strong (can we determines which papayas are tasty?). In the next subsection, we will describe the agnostic PAC model in which this realizability assumption is waived."
320,321,Learning Problems beyond Binary Classiﬁcation
321,322,"The learning task that we have been discussing so far has to do with predicting a binary label to a given example (like being tasty or not). However, many learning tasks take a diﬀerent form. For example, one may wish to predict a real valued a ﬁnite set of labels (like the topic of the main story in tomorrow’s paper). It turns out that our analysis of learning can be readily extended to such and many other scenarios by allowing a variety of loss functions. We shall discuss that in"
322,323,Releasing the Realizability Assumption – Agnostic PAC Learning
323,324,"Recall that the realizability assumption requires that there exists h⋆∈H such not hold. Furthermore, it is maybe more realistic not to assume that the labels are fully determined by the features we measure on input elements (in the case of the papayas, it is plausible that two papayas of the same color and softness will have diﬀerent taste). In the following, we relax the realizability assumption by generating distribution."
324,325,"Formally, from now on, let D be a probability distribution over X × Y, where, as before, X is our domain set and Y is a set of labels (usually we will consider can view such a distribution as being composed of two parts: a distribution Dx over unlabeled domain points (sometimes called the marginal distribution) and a conditional probability over labels for each domain point, D((x, y)|x). In the papaya example, Dx determines the probability of encountering a papaya whose probability is the probability that a papaya with color and hardness represented by x is tasty. Indeed, such modeling allows for two papayas that share the same color and hardness to belong to diﬀerent taste categories."
325,326,The empirical and the True Error Revised
326,327,"For a probability distribution, D, over X × Y, one can measure how likely h is to make an error when labeled points are randomly drawn according to D. We redeﬁne the true error (or risk) of a prediction rule h to be = D({(x, y) : h(x) ̸= y})."
327,328,"We would like to ﬁnd a predictor, h, for which that error will be minimized."
328,329,"However, the learner does not know the data generating D. What the learner does have access to is the training data, S. The deﬁnition of the empirical risk"
329,330,"A Formal Learning Model remains the same as before, namely, that LS(h) = LD(uniform over S)(h). minimizes the true risk, LD(h)."
330,331,"The Bayes Optimal Predictor. the Bayes optimal predictor fD is optimal, in the sense that no other classiﬁer,"
331,332,"Unfortunately, since we do not know D, we cannot utilize this optimal predictor fD. What the learner does have access to is the training sample. We can now present the formal deﬁnition of agnostic PAC learnability, which is a natural extension of the deﬁnition of PAC learnability to the more realistic, nonrealizable, learning setup we have just discussed."
332,333,"Clearly, we cannot hope that the learning algorithm will ﬁnd a hypothesis"
333,334,"Furthermore, as we shall prove later, once we make no prior assumptions a predictor that is as good as the Bayes optimal one. Instead, we require that the learning algorithm will ﬁnd a predictor whose error is not much larger than the best possible error of a predictor in some given benchmark hypothesis class."
334,335,"Of course, the strength of such a requirement depends on the choice of that"
335,336,"A hypothesis class H is agnostic generated by D, the algorithm returns a hypothesis h such that, with probability"
336,337,"Clearly, if the realizability assumption holds, agnostic PAC learning provides alizes the deﬁnition of PAC learning. When the realizability assumption does not hold, no learner can guarantee an arbitrarily small error. Nevertheless, under the deﬁnition of agnostic PAC learning, a learner can still declare success if its error is not much larger than the best error achievable by a predictor from the class H."
337,338,"This is in contrast to PAC learning, in which the learner is required to achieve a small error in absolute terms and not relative to the best error achievable by the hypothesis class."
338,339,The Scope of Learning Problems Modeled
339,340,We next extend our model so that it can be applied to a wide variety of learning tasks. Let us consider some examples of diﬀerent learning tasks.
340,341,Multiclass Classiﬁcation Our classiﬁcation does not have to be binary.
341,342,"Take, for example, the task of document classiﬁcation: We wish to design a program that will be able to classify given documents according to topics (e.g., news, sports, biology, medicine). A learning algorithm for such a task will have access to examples of correctly classiﬁed documents and, on the basis of these examples, should output a program that can take as input a new document and output a topic classiﬁcation for that document. Here, the domain set is the set of all potential documents. Once again, we would usually represent documents by a set of features that could include counts of diﬀerent key words in the document, as well as other possibly relevant features like the size of the document or its origin. The label set in this task will be the set of possible document topics (so Y will be some large ﬁnite set). Once we determine our domain and label sets, the other components of our framework look exactly the same as in the papaya tasting example;"
342,343,"Our training sample will be a ﬁnite sequence of (feature vector, label) pairs, the learner’s output will be a function from the domain set to the label set, and, ﬁnally, for our measure of success, we can use the probability, over (document, topic) pairs, of the event that our predictor suggests a wrong"
343,344,"• Regression In this task, one wishes to ﬁnd some simple pattern in the data – a functional relationship between the X and Y components of the data. For example, one wishes to ﬁnd a linear function that best predicts a baby’s birth weight on the basis of ultrasound measures of his head circumference, abdominal circumference, and femur length. Here, our domain set X is some subset of R3 (the three ultrasound measurements), and the set of “labels,”"
344,345,"Y, is the the set of real numbers (the weight in grams). In this context, it is more adequate to call Y the target set. Our training data as well as the learner’s output are as before (a ﬁnite sequence of (x, y) pairs, and a function from X to Y respectively). However, our measure of success is"
345,346,A Formal Learning Model by the expected square diﬀerence between the true labels and their predicted
346,347,To accommodate a wide range of learning tasks we generalize our formalism of the measure of success as follows:
347,348,Generalized Loss Functions
348,349,"Given any set H (that plays the role of our hypotheses, or models) and some domain Z let ℓbe any function from H×Z to the set of nonnegative real numbers,"
349,350,We call such functions loss functions.
350,351,"Note that for prediction problems, we have that Z = X × Y. However, our notion of the loss function is generalized beyond prediction tasks, and therefore it allows Z to be any domain of examples (for instance, in unsupervised learning domain and a label domain). with respect to a probability distribution D over Z, namely, domly according to D. Similarly, we deﬁne the empirical risk to be the expected loss over a given sample S = (z1, . . . , zm) ∈Zm, namely, sion tasks are as follows: the loss function is"
351,352,This loss function is used in binary or multiclass classiﬁcation problems.
352,353,"• Square Loss: Here, our random variable z ranges over the set of pairs X ×Y and the loss function is"
353,354,This loss function is used in regression problems.
354,355,We will later see more examples of useful instantiations of loss functions.
355,356,"To summarize, we formally deﬁne agnostic PAC learnability for general loss hypothesis class H is agnostic PAC learnable with respect to a set Z and a and for every distribution D over Z, when running the learning algorithm on where LD(h) = Ez∼D[ℓ(h, z)]."
356,357,"In the aforementioned deﬁnition, deﬁne LD(h) to be the expected value of this random variable. For that, we need to require that the function ℓ(h, ·) is measurable. Formally, we assume that there is a σ-algebra of subsets of Z, over which the probability D is deﬁned, and that and our assumption on ℓis equivalent to the assumption that for every h, the ceding deﬁnition, we required that the algorithm will return a hypothesis from"
357,358,"H. In some situations, H is a subset of a set H′, and the loss function can be naturally extended to be a function from H′ × Z to the reals. In this case, we may allow the algorithm to return a hypothesis h′ ∈H′, as long as it satisﬁes a hypothesis from H′ is called representation independent learning, while proper tation independent learning is sometimes called “improper learning,” although there is nothing improper in representation independent learning."
358,359,"In this chapter we deﬁned our main formal learning model – PAC learning. The basic model relies on the realizability assumption, while the agnostic variant does"
359,360,"A Formal Learning Model not impose any restrictions on the underlying distribution over the examples. We also generalized the PAC model to arbitrary loss functions. We will sometimes refer to the most general model simply as PAC learning, omitting the “agnostic” preﬁx and letting the reader infer what the underlying loss function is from the context. When we would like to emphasize that we are dealing with the original we will discuss other notions of learnability."
360,361,Bibliographic Remarks tions follows the works of Vladimir Vapnik and Alexey Chervonenkis (Vapnik &
361,362,"Valiant’s deﬁnition also requires that the runtime of the learning algorithm will be polynomial in these quantities. In contrast, we chose to distinguish between the statistical aspect of learning and the computational aspect of learning. We introduce the full PAC learning model of Valiant. For expository reasons, we use the term PAC learning even when we ignore the runtime aspect of learning. binary classiﬁcation task. Suppose that H is PAC learnable and its sample complexity is given by mH(·, ·). Show that mH is monotonically nonincreasing"
362,363,"The realizability assumption here implies that the true hypothesis f labels negatively all examples in the domain, perhaps except one. in the realizable setup."
363,364,"PAC learnable (assume realizability), and its sample complexity is bounded literal over the variables x1, . . . , xd is a simple Boolean function that takes the"
364,365,"In Boolean logic, the product is denoted using the ∧sign. For example, the"
365,366,"We consider the hypothesis class of all conjunctions of literals over the d (and similarly any conjunction involving a literal and its negation) is allowed that there exists a Boolean conjunction that generates the labels. Thus, each example (x, y) ∈X × Y consists of an assignment to the d Boolean variables"
366,367,"Then, the training set S might contain the following instances:"
367,368,Prove that the hypothesis class of all conjunctions over d variables is
368,369,"PAC learnable and bound its sample complexity. Propose an algorithm that implements the ERM rule, whose runtime is polynomial in d · m."
369,370,"Suppose we are getting a sample S of m examples, such that the instances are independent but are not identically distributed; the ith instance is sampled from Di and then yi is set to be f(xi). Let ¯Dm denote the average, that is, ¯Dm = (D1 + · · · + Dm)/m."
370,371,"A Formal Learning Model cessful agnostic PAC learner for H, then A is also a successful PAC learner bution D, the Bayes optimal predictor fD is optimal, in the sense that for probability distribution, D, if than B, if it is better than B with respect to all probability distributions D x is predicted by tossing a coin with bias h(x) toward Heads and predicting (x, y) is deﬁned to be |h(x) −y|, which is exactly the probability that the prediction of h will not be equal to y. Note that if h is deterministic, that"
371,372,"Bayes optimal predictor has the smallest risk (w.r.t. the loss function better than any other learning algorithm with respect to D. bution, D, and a learning algorithm B such that A is not better than B cles: one that generates positive examples and one that generates negative examples, both according to the underlying distribution D on X. Formally,"
372,373,"A ⊂X +. Similarly, D−is the distribution over X −induced by D. standard deﬁnition of PAC learnability except that here the learner has access of the two training sets, and possibly over the nondeterministic decisions made"
373,374,Learning via Uniform Convergence
374,375,"The ﬁrst formal learning model that we have discussed was the PAC model. hypothesis class is PAC learnable. In this chapter we will develop a general tool, uniform convergence, and apply it to show that any ﬁnite class is learnable in the agnostic PAC model with general loss functions, as long as the range loss function is bounded."
375,376,Uniform Convergence Is Suﬃcient for Learnability
376,377,The idea behind the learning condition discussed in this chapter is very simple.
377,378,"Recall that, given a hypothesis class, H, the ERM learning paradigm works as follows: Upon receiving a training sample, S, the learner evaluates the risk (or error) of each h in H on the given sample and outputs a member of H that minimizes this empirical risk. The hope is that an h that minimizes the empirical risk with respect to S is a risk minimizer (or has risk close to the minimum) with respect to the true data probability distribution as well. For that, it suﬃces to ensure that the empirical risks of all members of H are good approximations of their true risk. Put another way, we need that uniformly over all hypotheses in the hypothesis class, the empirical risk will be close to the true risk, as formalized (w.r.t. domain Z, hypothesis class H, loss function ℓ, and distribution D) if the ERM learning rule is guaranteed to return a good hypothesis."
378,379,Assume that a training set S is
379,380,"Z, hypothesis class H, loss function ℓ, and distribution D). Then, any output of"
380,381,Personal use only. Not for distribution. Do not post. where the ﬁrst and third inequalities are due to the assumption that S is
381,382,The preceding lemma implies that to ensure that the ERM rule is an agnostic uniform convergence condition formalizes this requirement.
382,383,We say that a hypothesis class H has the uniform convergence property (w.r.t. a domain Z and a loss function ℓ) if there exists a function mUC
383,384,"Similar to the deﬁnition of sample complexity for PAC learning, the function vergence property, namely, how many examples we need to ensure that with"
384,385,The term uniform here refers to having a ﬁxed sample size that works for all members of H and over all possible probability distributions over the domain. uniform convergence.
385,386,If a class H has the uniform convergence property with a is a successful agnostic PAC learner for H.
386,387,Finite Classes Are Agnostic PAC Learnable
387,388,PAC learnable will follow once we establish that uniform convergence holds for a ﬁnite hypothesis class.
388,389,"To show that uniform convergence holds we follow a two step argument, similar second step employs a measure concentration inequality. We now explain these two steps in detail."
389,390,Learning via Uniform Convergence
390,391,"Equivalently, we need to show that of this inequality is small enough (for a suﬃciently large m). That is, we will show that for any ﬁxed hypothesis, h, (which is chosen in advance prior to the sampling of the training set), the gap between the true and empirical risks, |LS(h) −LD(h)|, is likely to be small."
391,392,"Recall that LD(h) = Ez∼D[ℓ(h, z)] and that LS(h) = each zi is sampled i.i.d. from D, the expected value of the random variable ℓ(h, zi) is LD(h). By the linearity of expectation, it follows that LD(h) is also the expected value of LS(h). Hence, the quantity |LD(h)−LS(h)| is the deviation of the random variable LS(h) from its expectation. We therefore need to show that the measure of LS(h) is concentrated around its expected value."
392,393,"A basic statistical fact, the law of large numbers, states that when m goes to inﬁnity, empirical averages converge to their true expectation. This is true for"
393,394,"LS(h), since it is the empirical average of m i.i.d random variables. However, since the law of large numbers is only an asymptotic result, it provides no information about the gap between the empirically estimated error and its true value for any given, ﬁnite, sample size."
394,395,"Instead, we will use a measure concentration inequality due to Hoeﬀding, which quantiﬁes the gap between empirical averages and their expected value."
395,396,"The proof can be found in Appendix B. random variables. Furthermore, LS(h) ="
396,397,"Finally, if we choose"
397,398,"Let H be a ﬁnite hypothesis class, let Z be a domain, and let property with sample complexity"
398,399,"Furthermore, the class is agnostically PAC learnable using the ERM algorithm with sample complexity"
399,400,"While the preceding corollary only applies to ﬁnite hypothesis classes, there is a simple trick that allows us to get a very good estimate of the practical sample complexity of inﬁnite hypothesis classes. Consider a hypothesis class that is parameterized by d parameters. For inﬁnite size. However, if we are going to learn this hypothesis class in practice, using a computer, we will probably maintain real numbers using ﬂoating point is parameterized by d numbers, in practice we learn a hypothesis class of size at"
400,401,"Learning via Uniform Convergence ity has the deﬁciency of being dependent on the speciﬁc representation of real less, the discretization trick can be used to get a rough estimate of the sample complexity in many practical situations."
401,402,If the uniform convergence property holds for a hypothesis class H then in most cases the empirical risks of hypotheses in H will faithfully represent their true risks. Uniform convergence suﬃces for agnostic PAC learnability using the ERM rule. We have shown that ﬁnite hypothesis classes enjoy the uniform convergence property and are hence agnostic PAC learnable.
402,403,Bibliographic Remarks
403,404,Classes of functions for which the uniform convergence property holds are also
404,405,"Francesco Paolo Cantelli, who proved the ﬁrst uniform convergence result in mental theorem of learning theory states that in binary classiﬁcation problems, uniform convergence is not only a suﬃcient condition for learnability but is also a necessary condition. This is not the case for more general learning problems pler looking requirement about averages (or expectations). Prove that the following two statements are equivalent (for any learning algorithm A, any (where ES∼Dm denotes the expectation over samples S of size m). the sample complexity satisﬁes learner, and result in overﬁtting. To overcome this problem, we restricted the search space to some hypothesis class H. Such a hypothesis class can be viewed as reﬂecting some prior knowledge that the learner has about the task – a belief example, in our papayas taste problem, on the basis of our previous experience predicts (at least approximately) the papaya’s tastiness."
405,406,"Is such prior knowledge really necessary for the success of learning? Maybe there exists some kind of universal learner, that is, a learner who has no prior knowledge about a certain task and is ready to be challenged by any task? Let us elaborate on this point. A speciﬁc learning task is deﬁned by an unknown distribution D over X × Y, where the goal of the learner is to ﬁnd a predictor there exist a learning algorithm A and a training set size m, such that for every distribution D, if A receives m i.i.d. examples from D, there is a high chance it outputs a predictor h that has a low risk."
406,407,"Lunch theorem states that no such universal learner exists. To be more precise, the theorem states that for binary classiﬁcation prediction tasks, for every learner there exists a distribution on which it fails. We say that the learner fails if, upon receiving i.i.d. examples from that distribution, its output hypothesis is likely another learner that will output a hypothesis with a small risk. In other words, the theorem states that no learner can succeed on all learnable tasks – every learner has tasks on which it fails while other learners succeed."
407,408,"Therefore, when approaching a particular learning problem, deﬁned by some distribution D, we should have some prior knowledge on D. One type of such prior knowledge is that D comes from some speciﬁc parametric family of distributions. type of prior knowledge on D, which we assumed when deﬁning the PAC learning model, is that there exists h in some predeﬁned hypothesis class H, such that is small. In a sense, this weaker assumption on D is a prerequisite for using the"
408,409,"Personal use only. Not for distribution. Do not post. agnostic PAC model, in which we require that the risk of the output hypothesis"
409,410,"In the second part of this chapter we study the beneﬁts and pitfalls of using a hypothesis class as a means of formalizing prior knowledge. We decompose the error of an ERM algorithm over a class H into two components. The ﬁrst component reﬂects the quality of our prior knowledge, measured by the minimal also called the approximation error, or the bias of the algorithm toward choosing a hypothesis from H. The second component is the error due to overﬁtting, which depends on the size or the complexity of the class H and is called the estimation error. These two terms imply a tradeoﬀbetween choosing a more complex H (which can decrease the bias but increases the risk of overﬁtting) or a less complex H (which might increase the bias but decreases the potential"
410,411,"In this part we prove that there is no universal learner. We do this by showing that no learner can succeed on all learning tasks, as formalized in the following"
411,412,Let A be any learning algorithm for the task
412,413,"This theorem states that for every learner, there exists a task on which it fails, even though that task can be successfully learned by another learner. Indeed, a esis class H = {f}, or more generally, ERM with respect to any ﬁnite hypothesis"
413,414,Let C be a subset of X of size 2m. The intuition of the proof is that any learning algorithm that observes only half of the instances in C has no information on what should be the labels of the rest of the instances in C.
414,415,"Therefore, there exists a “reality,” that is, some target function f, that would contradict the labels that A(S) predicts on the unobserved instances in C. functions by f1, . . . , fT . For each such function, let Di be a distribution over"
415,416,"We will show that for every algorithm, A, that receives a training set of m"
416,417,"Clearly, this means that for every algorithm, A′, that receives a training set of m"
417,418,"It is easy to verify that the preceding suﬃces for showing that P[LD(A′(S)) ≥ possible sequences of m examples from C. Denote these sequences by S1, . . . , Sk."
418,419,"Also, if Sj = (x1, . . . , xm) we denote by Si j the sequence containing the instances in Sj labeled by the function fi, namely, Si j = ((x1, fi(x1)), . . . , (xm, fi(xm))). If the distribution is Di then the possible training sets A can receive are Si and all these training sets have the same probability of being sampled. Therefore,"
419,420,"Using the facts that “maximum” is larger than “average” and that “average” is larger than “minimum,” we have if and only if c = vr. Since for such a pair we must have Si consider an ERM predictor over the hypothesis class H of all the functions f from from the domain to the label set is considered a good candidate. According to the in H, and in particular the ERM predictor, will fail on some learning task."
420,421,"Therefore, this class is not PAC learnable, as formalized in the following corollary:"
421,422,Let X be an inﬁnite domain set and let H be the set of all
422,423,"Assume, by way of contradiction, that the class is learnable. Choose in particular for the algorithm A), there exists a distribution D such that with desired contradiction."
423,424,"How can we prevent such failures? We can escape the hazards foreseen by the task, to avoid the distributions that will cause us to fail when learning that task."
424,425,Such prior knowledge can be expressed by restricting our hypothesis class.
425,426,"But how should we choose a good hypothesis class? On the one hand, we want to believe that this class includes the hypothesis that has no error at all (in the"
426,427,"PAC setting), or at least that the smallest error achievable by a hypothesis from this class is indeed rather small (in the agnostic setting). On the other hand, we have just seen that we cannot simply choose the richest class – the class of all functions over the given domain. This tradeoﬀis discussed in the following"
427,428,"To answer this question we decompose the error of an ERMH predictor into two components as follows. Let hS be an ERMH hypothesis. Then, we can write"
428,429,"• The Approximation Error – the minimum risk achievable by a predictor in the hypothesis class. This term measures how much risk we have because we restrict ourselves to a speciﬁc class, namely, how much inductive bias we have. The approximation error does not depend on the sample size and is determined by the hypothesis class chosen. Enlarging the hypothesis class can decrease the approximation error."
429,430,"Under the realizability assumption, the approximation error is zero. In minimal yet inevitable error, because of the possible nondeterminism of the world in this model. Sometimes in the literature the term approximation error refers not to"
430,431,"• The Estimation Error – the diﬀerence between the approximation error and the error achieved by the ERM predictor. The estimation error results because the empirical risk (i.e., training error) is only an estimate of the true risk, and so the predictor minimizing the empirical risk is only an estimate of the predictor minimizing the true risk."
431,432,"The quality of this estimation depends on the training set size and on the size, or complexity, of the hypothesis class. As we have shown, for creases with m. We can think of the size of H as a measure of its complexity."
432,433,"In future chapters we will deﬁne other complexity measures of hypothesis complexity tradeoﬀ. On one hand, choosing H to be a very rich class decreases the approximation error but at the same time might increase the estimation error, as a rich H might lead to overﬁtting. On the other hand, choosing H to be a very small set reduces the estimation error but might increase the approximation error or, in other words, might lead to underﬁtting. Of course, a great choice for"
433,434,"H is the class that contains only one classiﬁer – the Bayes optimal classiﬁer. But the Bayes optimal classiﬁer depends on the underlying distribution D, which we do not know (indeed, learning would have been unnecessary had we known D). sonable estimation error. In many cases, empirical research focuses on designing good hypothesis classes for a certain domain. Here, “good” means classes for which the approximation error would not be excessively high. The idea is that siﬁer, we still have some prior knowledge of the speciﬁc problem at hand, which enables us to design hypothesis classes for which both the approximation error and the estimation error are not too large. Getting back to our papayas example, we do not know how exactly the color and hardness of a papaya predict its taste, but we do know that papaya is a fruit and on the basis of previous experience be a good predictor. learner has to be speciﬁed to some task, and use some prior knowledge about that task, in order to succeed. So far we have modeled our prior knowledge by restricting our output hypothesis to be a member of a chosen hypothesis class."
434,435,"When choosing this hypothesis class, we face a tradeoﬀ, between a larger, or more complex, class that is more likely to have a small approximation error, and a more restricted class that would guarantee that the estimation error will be small. In the next chapter we will study in more detail the behavior of the"
435,436,"Bibliographic Remarks tion, but these are rather diﬀerent from the theorem we prove here. The theorem we prove here is closely related to lower bounds in VC theory, as we will study in the next chapter. (BMI), age (A), level of physical activity (P), and income (I)."
436,437,You have to choose between two algorithms; the ﬁrst picks an axis aligned rectangle in the two dimensional space spanned by the features BP and BMI and the other picks an axis aligned rectangle in the ﬁve dimensional space spanned by all the preceding features.
437,438,"Namely, let A be a learning algorithm for the task of binary classiﬁcation. Let m be any number smaller than |X|/k, representing a training set size. Then, proximation error and estimation error. The approximation error depends on the ﬁt of our prior knowledge (as reﬂected by the choice of the hypothesis class"
438,439,"H) to the underlying unknown distribution. In contrast, the deﬁnition of PAC learnability requires that the estimation error would be bounded uniformly over"
439,440,"Our current goal is to ﬁgure out which classes H are PAC learnable, and to characterize exactly the sample complexity of learning a given hypothesis class."
440,441,"So far we have seen that ﬁnite classes are learnable, but that the class of all functions (over an inﬁnite size domain) is not. What makes one class learnable determines their sample complexity? able, and thus, ﬁniteness of the hypothesis class is not a necessary condition for learnability. We then present a remarkably crisp characterization of the family loss. This characterization was ﬁrst discovered by Vladimir Vapnik and Alexey provide several examples, and then state the fundamental theorem of statistical"
441,442,"ERM rule, and uniform convergence. complexity of a hypothesis class is upper bounded by the log of its size. To show that the size of the hypothesis class is not the right characterization of its sample"
442,443,"Let H be the set of threshold functions over the real line, namely,"
443,444,"Personal use only. Not for distribution. Do not post. size. Nevertheless, the following lemma shows that H is learnable in the PAC model using the ERM algorithm."
444,445,"Let H be the class of thresholds as deﬁned earlier. Then, H is a0 < a⋆< a1 be such that is positive we set b0 = −∞and if no example in S is negative we set b1 = ∞)."
445,446,"Let bS be a threshold corresponding to an ERM hypothesis, hS, which implies"
446,447,"S∼Dm[b0 < a0 ∨b1 > a1], and using the union bound we can bound the preceding by"
447,448,"The event b0 < a0 happens if and only if all examples in S are not in the interval ability, it is not a necessary condition. As we will show, a property called the restricting the hypothesis class, for any learning algorithm, an adversary can construct a distribution for which the learning algorithm will perform poorly, bution. To do so, the adversary used a ﬁnite set C ⊂X and considered a family of distributions that are concentrated on elements of C. Each distribution was fail, the adversary used the power of choosing a target function from the set of"
448,449,"When considering PAC learnability of a hypothesis class H, the adversary achieves a zero risk. Since we are considering distributions that are concentrated on elements of C, we should study how H behaves on C, which leads to the following deﬁnition."
449,450,Let H be a class of functions from X we say that H shatters the set C. Formally:
450,451,A hypothesis class H shatters a ﬁnite set C ⊂X
451,452,Let H be the class of threshold functions over R. Take a set
452,453,"Getting back to the construction of an adversarial distribution as in the proof is shattered by H, the adversary is not restricted by H, as they can construct maintaining the realizability assumption. This immediately yields: m be a training set size. Assume that there exists a set C ⊂X of size 2m that is shattered by H. Then, for any learning algorithm, A, there exist a distribution D learn H using m examples. Intuitively, if a set C is shattered by H, and we receive a sample containing half the instances of C, the labels of these instances give us no information about the labels of the rest of the instances in C – every possible labeling of the rest of the instances can be explained by some hypothesis in H. Philosophically,"
453,454,"If someone can explain every phenomenon, his explanations are worthless."
454,455,"This leads us directly to the deﬁnition of the VC dimension. denoted VCdim(H), is the maximal size of a set C ⊂X that can be shattered by H. If H can shatter sets of arbitrarily large size we say that H has inﬁnite learnability. But before delving into more theory, we ﬁrst show several examples. show that VCdim(H) = d we need to show that we have shown that for an arbitrary set C = {c1}, H shatters C; therefore by an interval and therefore H does not shatter C. We therefore conclude that"
455,456,Axis Aligned Rectangles
456,457,"Let H be the class of axis aligned rectangles, formally: h(a1,a2,b1,b2)(x1, x2) = leftmost point (whose ﬁrst coordinate is the smallest in C), a rightmost point (ﬁrst coordinate is the largest), a lowest point (second coordinate is the smallest), and a highest point (second coordinate is the largest). Without loss of generality, denote C = {c1, . . . , c5} and let c5 be the point that was not selected. Now, axis aligned rectangle. Indeed, such a rectangle must contain c1, . . . , c4; but in this case the rectangle contains c5 as well, because its coordinates are within the intervals deﬁned by the selected points. So, C is not shattered by H, and"
457,458,"Let H be a ﬁnite class. Then, clearly, for any set C we have |HC| ≤|H| and thus C shows that the PAC learnability of ﬁnite classes follows from the more general for some integer k, and consider the class of threshold functions (as deﬁned in large, the gap between log2(|H|) and VCdim(H) can be arbitrarily large. parameters deﬁning the hypothesis class. While this is often the case, it is not always true. Consider, for example, the domain X = R, and the hypothesis class is possible to prove that VCdim(H) = ∞, namely, for every d, one can ﬁnd d"
458,459,"The Fundamental Theorem of PAC learning converse statement is also true, leading to the fundamental theorem of statistical"
459,460,The proof of the theorem is given in the next section. mines the sample complexity.
460,461,"Then, there are absolute constants C1, C2 such that:"
461,462,We stated the fundamental theorem for binary classiﬁcation tasks.
462,463,"A similar result holds for some other learning problems such as regression with the absolute loss or the squared loss. However, the theorem does not hold for all learning tasks. In particular, learnability is sometimes possible even though the uniform convergence property does not hold (we will see an example in but learnability is possible with other learning rules. is based on two main claims:"
463,464,"• If VCdim(H) = d, then even though H might be inﬁnite, when restricting it to a ﬁnite set C ⊂X, its “eﬀective” size, |HC|, is only O(|C|d). That is, the size of HC grows polynomially rather than exponentially with |C|. This claim is often referred to as Sauer’s lemma, but it has also been stated and proved independently by Shelah and by Perles. The formal statement is show that uniform convergence holds whenever the hypothesis class has a “small eﬀective size.” By “small eﬀective size” we mean classes for which |HC| grows polynomially with |C|."
464,465,Sauer’s Lemma and the Growth Function
465,466,"We deﬁned the notion of shattering, by considering the restriction of H to a ﬁnite set of instances. The growth function measures the maximal “eﬀective” size of"
466,467,H on a set of m examples. Formally:
467,468,Let H be a hypothesis class. Then the
468,469,"In words, τH(m) is the number of diﬀerent functions from a set C of size m to tiful lemma, proposed independently by Sauer, Shelah, and Perles, shows that polynomially rather than exponentially with m."
469,470,"Let H be a hypothesis class with VCdim(H) ≤ d < ∞. Then, for all m, τH(m) ≤Pd"
470,471,Proof of Sauer’s Lemma *
471,472,To prove the lemma it suﬃces to prove the following stronger claim: For any
472,473,"C = {c1, . . . , cm} we have d then no set whose size is larger than d is shattered by H and therefore sets of size m. Fix H and C = {c1, . . . , cm}. Denote C′ = {c2, . . . , cm} and in addition, deﬁne the following two sets:"
473,474,"It is easy to verify that |HC| = |Y0| + |Y1|. Additionally, since Y0 = HC′, using the induction assumption (applied on H and C′) we have that"
474,475,"Next, deﬁne H′ ⊆H to be = (h(c1), h(c2), . . . , h(cm)}, namely, H′ contains pairs of hypotheses that agree on C′ and diﬀer on c1. Using using the inductive assumption (now applied on H′ and C′) we obtain that"
475,476,"Overall, we have shown that which concludes our proof."
476,477,Uniform Convergence for Classes of Small Eﬀective Size
477,478,"In this section we prove that if H has small eﬀective size then it enjoys the uniform convergence property. Formally,"
478,479,"Let H be a class and let τH be its growth function. Then, for the uniform convergence property holds. We will prove that"
479,480,For simplicity assume that suﬃcient condition for the preceding to hold is that
480,481,The upper bound on mUC is not the tightest possible. A tighter analysis that yields the bounds given in
481,482,"We will start by showing that the theorem follows directly from the preceding using Markov’s inequality (see additional i.i.d. sample. Therefore,"
482,483,A generalization of the triangle inequality yields
483,484,S′∼Dm[LS′(h) −LS(h)]
484,485,"S′∼Dm |LS′(h) −LS(h)|,"
485,486,S′∼Dm |LS′(h) −LS(h)| ≤
486,487,"S = z1, . . . , zm and S′ = z′ m. Since all of these 2m vectors are chosen i.i.d., nothing will change if we replace the name of the random vector zi with the name of the random vector z′ i. If we do it, instead of the term (ℓ(h, z′ i) −ℓ(h, zi)). It follows that for and by the linearity of expectation it also equals"
487,488,"Next, ﬁx S and S′, and let C be the instances appearing in S and S′. Then, we"
488,489,"Combining all with the deﬁnition of τH, we have shown that"
489,490,"The fundamental theorem of learning theory characterizes PAC learnability of is a combinatorial property that denotes the maximal sample size that can be required for PAC learning. The theorem also shows that if a problem is at all learnable, then uniform convergence holds and therefore the problem is learnable"
490,491,"Bibliographic remarks relation to the deﬁnition of PAC learnability is due to Blumer, Ehrenfeucht, the Natarajan dimension characterizes learnability of some multiclass learning"
491,492,Sauer’s lemma has been proved by Sauer in response to a problem of Erdos
492,493,"Weiss asked Perles about such a result in the context of ergodic theory, and"
493,494,"Perles, who forgot that he had proved it once, proved it again. Vapnik and"
494,495,"Chervonenkis proved the lemma in the context of statistical learning theory. hypothesis classes if H′ ⊆H then VCdim(H′) ≤VCdim(H). dimension of each of the following classes (and prove your claims): a parity function hI as follows. On a binary vector x = (x1, x2, . . . , xn) ∈ dimension d, and every subset A of the domain,"
495,496,Show that there are cases in which the previous two inequalities are strict by equalities. Demonstrate all four combinations of = and <. axis aligned rectangles in Rd. We have already seen that VCdim(H2
496,497,"Prove that in general, VCdim(Hd con be the class of Boolean class is ﬁnite and thus (agnostic) PAC learnable. In this question we calculate that is shattered by Hd sponds to hi) contains some literal ℓi which is false on ci and true on cj for each j ̸= i. Use the Pigeonhole principle to show that there must be a to derive a contradiction to the requirements from the conjunctions hi, hj."
497,498,"Monotonicity here means that the conjunctions do not contain negations. pothesis. We augment Hd much lower than that: where VCdim(H) = ⌊log2(|H|)⌋. can be bounded above by) the number of parameters one needs to set in order to deﬁne each hypothesis in the class. For instance, if H is the class of axis aligned rectangles in Rd, then VCdim(H) = 2d, which is equal to the number of parameters used to deﬁne a rectangle in Rd. Here is an example that shows that this is not always the case. We will see that a hypothesis class might be very complex and even not learnable, although it has a small number of"
498,499,"Consider the domain X = R, and the hypothesis class"
499,500,Hint: There is more than one way to prove the required result. One option
500,501,"Hint: Take a set of k examples and assume that they are shattered by the union class. Therefore, the union class can produce all 2k possible labelings on these examples. Use Sauer’s lemma to show that the union class cannot produce more than rkd labelings. Therefore, 2k < rkd. Now deﬁning concept classes over Rn and show a connection between the VC dimension of such classes and their algebraic properties. Given a function a class F of real valued functions we deﬁne a corresponding class of functions tions is linearly closed addition and scalar multiplication of functions are deﬁned point wise, namely, is linearly closed then we can view it as a vector space over the reals. For a"
501,502,"Hypothesis classes that have a representation as POS(F + g) for some vector space of functions F and some function g are called Dudley classes. deﬁned earlier, VCdim(POS(F + g)) = VCdim(POS(F)). dimension of the corresponding class POS(F) equals the linear dimension of F (as a vector space). Hint: Let f1, . . . , fd be a basis for the vector space that this mapping induces a matching between functions over Rn of the n denote the class of functions deﬁned by polynomial inequalities variable polynomial is the maximal sum of variable exponents over all of its terms. For example, the degree of p(x) = 3x3 n (as a function of d and n)."
502,503,Nonuniform Learnability
503,504,"The notions of PAC learnability discussed so far in the book allow the sample sizes to depend on the accuracy and conﬁdence parameters, but they are uniform quently, classes that are learnable in that respect are limited (they must have more relaxed, weaker notions of learnability. We discuss the usefulness of such notions and provide characterization of the concept classes that are learnable using these deﬁnitions."
504,505,We begin this discussion by deﬁning a notion of “nonuniform learnability” that pared. We then provide a characterization of nonuniform learnability and show that nonuniform learnability is a strict relaxation of agnostic PAC learnability.
505,506,"We also show that a suﬃcient condition for nonuniform learnability is that H is new learning paradigm, which is called Structural Risk Minimization (SRM). In yields the Minimum Description Length (MDL) paradigm. The MDL paradigm notion of learnability. Finally, we discuss the signiﬁcance and usefulness of the diﬀerent notions of learnability."
506,507,Nonuniform Learnability “Nonuniform learnability” allows the sample size to be nonuniform with respect to the diﬀerent hypotheses with which the learner is competing. We say that a
507,508,"In PAC learnability, this notion of “competitiveness” is not very useful, as we are looking for a hypothesis with an absolute low risk (in the realizable case) or"
508,509,Personal use only. Not for distribution. Do not post.
509,510,"Nonuniform Learnability with a low risk compared to the minimal risk achieved by hypotheses in our class (in the agnostic case). Therefore, the sample size depends only on the accuracy and conﬁdence parameters. In nonuniform learnability, however, we allow the which we are competing. Formally,"
510,511,"A hypothesis class H is nonuniformly learnable if there exist a learning algorithm, A, and a function mNUL over the choice of S ∼Dm it holds that"
511,512,"In both types of learnability, we require that the output hypothesis will be between these two notions of learnability is the question of whether the sample size m may depend on the hypothesis h to which the error of A(S) is compared. ability. That is, if a class is agnostic PAC learnable then it is also nonuniformly"
512,513,Characterizing Nonuniform Learnability
513,514,"Our goal now is to characterize nonuniform learnability. In the previous chapter we have found a crisp characterization of PAC learnable classes, by showing dimension is ﬁnite. In the following theorem we ﬁnd a diﬀerent characterization for nonuniform learnable classes for the task of binary classiﬁcation. able if and only if it is a countable union of agnostic PAC learnable hypothesis"
514,515,"Let H be a hypothesis class that can be written as a countable union of hypothesis classes, H = S convergence property. Then, H is nonuniformly learnable. form learnability. The proof of this theorem will be given in the next section by"
515,516,"First assume that H = S nostic PAC learnable. Using the fundamental theorem of statistical learning, it follows that each Hn has the uniform convergence property. Therefore, using"
516,517,"For the other direction, assume that H is nonuniform learnable using some for any distribution D that satisﬁes the realizability assumption with respect to dimension of Hn must be ﬁnite, and therefore Hn is agnostic PAC learnable. ation of agnostic PAC learnability; namely, there are hypothesis classes that are nonuniform learnable but are not agnostic PAC learnable."
517,518,"Consider a binary classiﬁcation problem with the instance domain degree n; namely, Hn is the set of all classiﬁers of the form h(x) = sign(p(x)) the class of all polynomial classiﬁers over R. It is easy to verify that VCdim(H) ="
518,519,Structural Risk Minimization
519,520,"So far, we have encoded our prior knowledge by specifying a hypothesis class"
520,521,"H, which we believe includes a good predictor for the learning task at hand."
521,522,"Yet another way to express our prior knowledge is by specifying preferences over hypotheses within H. In the Structural Risk Minimization (SRM) paradigm, we do so by ﬁrst assuming that H can be written as H = S hypothesis class, Hn, such that a higher weight reﬂects a stronger preference for the hypothesis class. In this section we discuss how to learn with such prior knowledge. In the next section we describe a couple of important weighting schemes, including Minimum Description Length."
522,523,Nonuniform Learnability
523,524,"Concretely, let H be a hypothesis class that can be written as H = S"
524,525,"For example, H may be the class of all polynomial classiﬁers where each Hn is"
525,526,"In words, we have a ﬁxed sample size m, and we are interested in the lowest possible upper bound on the gap between empirical and true risks achievable by using a sample of m examples. a weight function over the hypothesis classes H1, H2, . . .. Such a weight function can reﬂect the importance that the learner attributes to each hypothesis class, or some measure of the complexity of diﬀerent hypothesis classes. If H is a ﬁnite all hypothesis classes. This equal weighting corresponds to no a priori preference to any hypothesis class. Of course, if one believes (as prior knowledge) that a certain hypothesis class is more likely to contain the correct target function, then it should be assigned a larger weight, reﬂecting this prior knowledge. When"
526,527,"H is a (countable) inﬁnite union of hypothesis classes, a uniform weighting is not possible but many other weighting schemes may work. For example, one can convenient way to deﬁne weighting functions using description languages."
527,528,The SRM rule follows a “bound minimization” approach. This means that the goal of the paradigm is to ﬁnd a hypothesis that minimizes a certain upper bound on the true risk. The bound that the SRM rule wishes to minimize is given in the following theorem.
528,529,H be a hypothesis class that can be written as H = S
529,530,Hn satisﬁes the uniform convergence property with a sample complexity function concludes our proof.
530,531,"The SRM paradigm searches for h that minimizes this bound, as formalized in the following pseudocode:"
531,532,Structural Risk Minimization (SRM) n Hn where Hn has uniform convergence with mUC
532,533,"Unlike the ERM paradigm discussed in previous chapters, we no longer just care about the empirical risk, LS(h), but we are willing to trade some of our bias is smaller, for the sake of a smaller estimation error."
533,534,"Next we show that the SRM paradigm can be used for nonuniform learning of every class, which is a countable union of uniformly converging hypothesis"
534,535,"Let H be a hypothesis class such that H = S each Hn has the uniform convergence property with sample complexity mUC n2π2 . Then, H is nonuniformly learnable using the SRM rule with rate"
535,536,Nonuniform Learnability
536,537,Let A be the SRM algorithm with respect to the weighting function w.
537,538,"The preceding holds in particular for the hypothesis A(S) returned by the SRM rule. By the deﬁnition of SRM we obtain that addition, from the uniform convergence property of each Hn we have that with"
538,539,"It turns out that, for any inﬁnite domain set, X, the class of all binary valued that, in some sense, the no free lunch theorem holds for nonuniform learning as well: namely, whenever the domain is not ﬁnite, there exists no nonuniform learner with respect to the class of all deterministic binary classiﬁers (although for each such classiﬁer there exists a trivial algorithm that learns it – ERM with respect to the hypothesis class that contains only this classiﬁer). prior knowledge, or bias, of a nonuniform learner for H is weaker – it is searching ciﬁc Hn. The cost of this weakening of prior knowledge is the increase in sample"
539,540,"Assume that for all n, VCdim(Hn) = n. Since mUC"
540,541,"That is, the cost of relaxing the learner’s prior knowledge from a speciﬁc Hn that contains the target h to a countable union of classes depends on the log of the index of the ﬁrst class in which h resides. That cost increases with the index of the class, which can be interpreted as reﬂecting the value of knowing a good priority order on the hypotheses in H."
541,542,Minimum Description Length and Occam’s Razor
542,543,"Let H be a countable hypothesis class. Then, we can write H as a countable union of singleton classes, namely, H = S and the SRM rule becomes"
543,544,"It follows that in this case, the prior knowledge is solely determined by the weight we assign to each hypothesis. We assign higher weights to hypotheses that we believe are more likely to be the correct one, and in the learning algorithm we prefer hypotheses that have higher weights. tion over H, which is derived from the length of descriptions given to hypotheses."
544,545,"Having a hypothesis class, one can wonder about how we describe, or represent, each hypothesis in the class. We naturally ﬁx some description language. This las. In any of these languages, a description consists of ﬁnite strings of symbols (or characters) drawn from some ﬁxed alphabet. We shall now formalize these"
545,546,"Let H be the hypothesis class we wish to describe. Fix some ﬁnite set Σ of symbols (or “characters”), which we call the alphabet. For concreteness, we"
546,547,"The set of all ﬁnite length strings is denoted Σ∗. A description language for H called “the description of h,” and its length is denoted by |h|. distinct h, h′, d(h) is not a preﬁx of d(h′). That is, we do not allow that any collections of strings enjoy the following combinatorial property:"
547,548,"Nonuniform Learnability of outcomes is a member of S; at that point, stop. For each σ ∈S, let P(σ) be the probability that this process generates the string σ. Note that since S is we will stop only once the sequence of outcomes equals σ. We therefore get that, for every σ ∈S, P(σ) = esis class, H, gives rise to a weighting function w over that hypothesis class – we will simply set w(h) = free description language for H. Then, for every sample size, m, every conﬁdence where |h| is the length of d(h). . In particular, it suggests trading oﬀempirical risk for saving description length. This yields the Minimum Description Length"
548,549,Minimum Description Length (MDL)
549,550,H is a countable hypothesis class
550,551,"Let H be the class of all predictors that can be implemented using binary string obtained by running the gzip command on the program (this yields risk, the true risk of the one that has shorter description can be bounded by a lower value. Thus, this result can be viewed as conveying a philosophical message:"
551,552,"A short explanation (that is, a hypothesis that has a short length) tends to be more valid than a long explanation."
552,553,"This is a well known principle, called Occam’s razor, after William of Ockham, it explicitly. Here, we provide one possible justiﬁcation to this principle. The sense of having a longer description), the larger the sample size it has to ﬁt to guarantee that it has a small true risk, LD(h)."
553,554,"At a second glance, our Occam razor claim might seem somewhat problematic."
554,555,"In the context in which the Occam razor principle is usually invoked in science, the language according to which complexity is measured is a natural language, sume that we have two hypotheses such that |h′| is much smaller than |h|. By the preceding result, if both have the same error on a given training set, S, then the true error of h may be much higher than the true error of h′, so one should prefer h′ over h. However, we could have chosen a diﬀerent description language,"
555,556,Suddenly it looks as if one should prefer h over h′. But these are the same h and h′ for which we argued two sentences ago that h′ should be preferable. Where is
556,557,"Indeed, there is no inherent generalizability diﬀerence between hypotheses."
557,558,"The crucial aspect here is the dependency order between the initial choice of language (or, preference over hypotheses) and the training set. As we know from fore seeing the data, then we are guaranteed a rather small estimation error term . Choosing a description language (or, equivalently, some weighting of hypotheses) is a weak form of committing to a hypothesis."
558,559,"Rather than committing to a single hypothesis, we spread out our commitment eralization bound holds. Just as the choice of a single hypothesis to be evaluated by a sample can be arbitrary, so is the choice of description language."
559,560,Nonuniform Learnability
560,561,Other Notions of Learnability – Consistency
561,562,The notion of learnability can be further relaxed by allowing the needed sample probability distribution D (that is used to generate the training sample and to determine the risk). This type of performance guarantee is captured by the notion of consistency1 of a learning rule.
562,563,"Let Z be a domain set, let P be a set of ing rule A is consistent with respect to H and P if there exists a function the choice of S ∼Dm it holds that"
563,564,"The notion of consistency is, of course, a relaxation of our previous notion of nonuniform learnability. Clearly if an algorithm nonuniformly learns a class"
564,565,"H it is also universally consistent for that class. The relaxation is strict in the sense that there are consistent learning rules that are not successful nonuniform universally consistent for the class of all binary classiﬁers over N. However, as we have argued before, this class is not nonuniformly learnable."
565,566,"Consider the classiﬁcation prediction algorithm Memorize deﬁned as follows. The algorithm memorizes the training examples, and, given a test point x, it predicts the majority label among all labeled instances of x that exist in the training sample (and some ﬁxed default label if no instance of x appears algorithm is universally consistent for every countable domain X and a ﬁnite"
566,567,"Intuitively, it is not obvious that the Memorize algorithm should be viewed as a learner, since it lacks the aspect of generalization, namely, of using observed data to predict the labels of unseen examples. The fact that Memorize is a consistent algorithm for the class of all functions over any countable domain set therefore raises doubt about the usefulness of consistency guarantees. Furthermore, the probability (corresponding to weak consistency) or almost sure convergence (corresponding to strong consistency). distributions” we mean all probability distributions that have Ωcontained in their associated family of measurable subsets. which led to overﬁtting, is in fact the Memorize algorithm. In the next section we discuss the signiﬁcance of the diﬀerent notions of learnability and revisit the"
567,568,"Discussing the Diﬀerent Notions of Learnability ness. As is usually the case, the usefulness of a mathematical deﬁnition depends on what we need it for. We therefore list several possible goals that we aim to tions in light of these goals."
568,569,"What Is the Risk of the Learned Hypothesis? rithm is bounding the risk of the output predictor. Here, both PAC learning and nonuniform learning give us an upper bound on the true risk of the learned hypothesis based on its empirical risk. Consistency guarantees do not provide such a bound. However, it is always possible to estimate the risk of the output"
569,570,"How Many Examples Are Required to Be as Good as the Best Hypothesis ples we need to collect in order to learn it. Here, PAC learning gives a crisp answer. However, for both nonuniform learning and consistency, we do not know in advance how many examples are required to learn H. In nonuniform learning this number depends on the best hypothesis in H, and in consistency it also depends on the underlying distribution. In this sense, PAC learning is the only useful deﬁnition of learnability. On the ﬂip side, one should keep in mind that even if the estimation error of the predictor we learn is small, its risk may still be large if H has a large approximation error. So, for the question “How many examples are required to be as good as the Bayes optimal predictor?” even PAC guarantees do not provide us with a crisp answer. This reﬂects the fact that the usefulness of PAC learning relies on the quality of our prior knowledge."
570,571,"PAC guarantees also help us to understand what we should do next if our learning algorithm returns a hypothesis with a large risk, since we can bound the part of the error that stems from estimation error and therefore know how much of the error is attributed to approximation error. If the approximation error is large, we know that we should use a diﬀerent hypothesis class. Similarly, if a nonuniform algorithm fails, we can consider a diﬀerent weighting function over (subsets of) hypotheses. However, when a consistent algorithm fails, we have no idea whether this is because of the estimation error or the approximation error. Furthermore, even if we are sure we have a problem with the estimation"
571,572,"Nonuniform Learnability error term, we do not know how many more examples are needed to make the estimation error small."
572,573,How to Learn? How to Express Prior Knowledge?
573,574,"Maybe the most useful aspect of the theory of learning is in providing an answer to the question of “how to learn.” The deﬁnition of PAC learning yields the knowledge. It gives us a crisp way to encode prior knowledge by choosing a hypothesis class, and once this choice is made, we have a generic learning rule –"
574,575,"ERM. The deﬁnition of nonuniform learnability also yields a crisp way to encode prior knowledge by specifying weights over (subsets of) hypotheses of H. Once this choice is made, we again have a generic learning rule – SRM. The SRM rule is also advantageous in model selection tasks, where prior knowledge is partial."
575,576,"Consider the problem of ﬁtting a one dimensional polynomial to data; namely, the hypothesis class of polynomials. However, we might be uncertain regarding which degree d would give the best results for our data set: A small degree might not ﬁt the data well (i.e., it will have a large approximation error), whereas a high degree might lead to overﬁtting (i.e., it will have a large estimation error)."
576,577,"It is easy to see that the empirical risk decreases as we enlarge the degree. and would overﬁt. On the other hand, if we choose too small a hypothesis class, (i.e., a large approximation error). In contrast, we can use the SRM rule on the set of all polynomials, while ordering subsets of H according to their degree, and this will yield a 3rd degree polynomial since the combination of its empirical risk and the bound on its estimation error is the smallest. In other words, the"
577,578,"SRM rule enables us to select the right model on the basis of the data itself. The price we pay for this ﬂexibility (besides a slight increase of the estimation error relative to PAC learning w.r.t. the optimal degree) is that we do not know in advance how many examples are needed to compete with the best hypothesis in tion of consistency does not yield a natural learning paradigm or a way to encode prior knowledge. In fact, in many cases there is no need for prior knowledge at all. For example, we saw that even the Memorize algorithm, which intuitively should not be called a learning algorithm, is a consistent algorithm for any class deﬁned over a countable domain and a ﬁnite label set. This hints that consistency is a very weak requirement."
578,579,Which Learning Algorithm Should We Prefer?
579,580,"One may argue that even though consistency is a weak requirement, it is desirable that a learning algorithm will be consistent with respect to the set of all functions from X to Y, which gives us a guarantee that for enough training examples, we will always be as good as the Bayes optimal predictor. Therefore, if we have two algorithms, where one is consistent and the other one is not consistent, we should prefer the consistent algorithm. However, this argument is problematic for two reasons. First, maybe it is the case that for most “natural” distributions we will observe in practice that the sample complexity of the consistent algorithm will be so large so that in every practical situation we will not obtain enough examples to enjoy this guarantee. Second, it is not very hard to make any PAC or nonuniform learner consistent with respect to the class of all functions from"
580,581,"X to Y. Concretely, consider a countable domain, X, a ﬁnite label set Y, and a hypothesis class, H, of functions from X to Y. We can make any nonuniform learner for H be consistent with respect to the class of all classiﬁers from X to Y using the following simple trick: Upon receiving a training set, we will ﬁrst run the nonuniform learner over the training set, and then we will obtain a bound on the true risk of the learned predictor. If this bound is small enough we are done. Otherwise, we revert to the Memorize algorithm. This simple modiﬁcation makes the algorithm consistent with respect to all functions from X to Y. Since it is easy to make any algorithm consistent, it may not be wise to prefer one algorithm over the other just because of consistency considerations. that no algorithm can learn the class of all classiﬁers over an inﬁnite domain."
581,582,"In contrast, in this chapter we saw that the Memorize algorithm is consistent with respect to the class of all classiﬁers over a countable inﬁnite domain. To understand why these two statements do not contradict each other, let us ﬁrst theorem implies the following: For any algorithm, A, and a training set size, m,"
582,583,"Nonuniform Learnability will get a sample of m i.i.d. training examples, labeled by h⋆, then A is likely to return a classiﬁer with a larger error."
583,584,"The consistency of Memorize implies the following: For every distribution over depends on the distribution and on h⋆) such that if Memorize receives at least m examples it is likely to return a classiﬁer with a small error. and then ﬁnd a distribution and a labeling function that are bad for this training set size. In contrast, in consistency guarantees, we ﬁrst ﬁx the distribution and the labeling function, and only then do we ﬁnd a training set size that suﬃces for learning this particular distribution and labeling function."
584,585,"We introduced nonuniform learnability as a relaxation of PAC learnability and consistency as a relaxation of nonuniform learnability. This means that even ability. We discussed the usefulness of the diﬀerent deﬁnitions of learnability. tion Length scheme, where hypotheses with shorter descriptions are preferred, language), which we can learn (nonuniformly) using the MDL scheme. tice. The ability to learn this class is impressive, and, seemingly, this chapter should have been the last chapter of this book. This is not the case, because of the computational aspect of learning: that is, the runtime needed to apply the learning rule. For example, to implement the MDL paradigm with respect to grams, which will take forever. Even the implementation of the ERM paradigm number – much larger than the number of atoms in the visible universe. In the next chapter we formally deﬁne the computational complexity of learning. In the second part of this book we will study hypothesis classes for which the ERM or"
585,586,SRM schemes can be implemented eﬃciently.
586,587,Bibliographic Remarks the second part of this book. classiﬁcation. Show that it is impossible to assign weights to the hypotheses
587,588,"• H could be learnt nonuniformly using these weights. That is, the weighting"
588,589,"• The weights would be monotonically nondecreasing. That is, if i < j, then"
589,590,"• (*) Deﬁne such a function w when for all n Hn is countable (possibly length of h, according to some ﬁxed description language. Consider the MDL learning paradigm in which the algorithm returns:"
590,591,Nonuniform Learnability
591,592,Prove a bound on LD(hS)−LD(h∗
592,593,"B) in terms of B, the conﬁdence parameter"
593,594,"• Note: Such bounds are known as oracle inequalities in the literature: We wish to estimate how good we are compared to a reference classiﬁer (or ability: namely, that, over any inﬁnite domain, the class of all functions is not learnable even under the relaxed nonuniform variation of learning."
594,595,"Recall that an algorithm, A, nonuniformly learns a hypothesis class H if there exists a function mNUL"
595,596,If such an algorithm exists then we say that H is nonuniformly learnable. which VCdim(Hn) = ∞.
596,597,"Hint: Given a class H that shatters some inﬁnite set K, and a sequence of is nonuniformly learnable but not PAC learnable. is not nonuniformly learnable."
597,598,Let X be a countable domain and let D be a probability distribution over X.
598,599,S∼Dm [∃xi : (D({xi}) > η and xi /∈S)] ≤ne−ηm. valued) functions over any countable domain.
599,600,The Runtime of Learning
600,601,"So far in the book we have studied the statistical perspective of learning, namely, how many samples are needed for learning. In other words, we focused on the amount of information learning requires. However, when considering automated plexity of a task: that is, how much computation is involved in carrying out a learning task. Once a suﬃcient training sample is available to the learner, there is some computation to be done to extract a hypothesis or ﬁgure out the label of a given test instance. These computational resources are crucial in any practical application of machine learning. We refer to these two types of resources as the sample complexity and the computational complexity. In this chapter, we turn our attention to the computational complexity of learning. text of the computational complexity of general algorithmic tasks. This area has comments that follow summarize the basic ideas of that general theory that are most relevant to our discussion. chine the algorithm is being implemented on (e.g., what the clock rate of the machine’s CPU is). To avoid dependence on the speciﬁc machine, it is common to analyze the runtime of algorithms in an asymptotic sense. For example, we rithm on any machine that satisﬁes the requirements of some accepted abstract ing: there exist constants c and n0, which can depend on the actual machine, such that, for any value of n > n0, the runtime in seconds of sorting any n items will be at most c n log(n). It is common to use the term feasible or eﬃciently computable for tasks that can be performed by an algorithm whose running time is O(p(n)) for some polynomial function p. One should note that this type of analysis depends on deﬁning what is the input size n of any instance to which cussed in the common computational complexity literature, this input size is clearly deﬁned; the algorithm gets an input instance, say, a list to be sorted, or an arithmetic operation to be calculated, which has a well deﬁned size (say, the"
601,602,"Personal use only. Not for distribution. Do not post. number of bits in its representation). For machine learning tasks, the notion of an input size is not so clear. An algorithm aims to detect some pattern in a data set and can only access random samples of that data."
602,603,"We start the chapter by discussing this issue and deﬁne the computational complexity of learning. For advanced students, we also provide a detailed formal plementing the ERM rule. We ﬁrst give several examples of hypothesis classes where the ERM rule can be eﬃciently implemented, and then consider some tation is computationally hard. It follows that hardness of implementing ERM does not imply hardness of learning. Finally, we brieﬂy discuss how one can show hardness of a given learning task, namely, that no learning algorithm can solve"
603,604,"Computational Complexity of Learning pothesis class, H, a loss function, ℓ, and a training set of examples from Z that are sampled i.i.d. according to an unknown distribution D. Given parameters"
604,605,"As mentioned before, the actual runtime of an algorithm in seconds depends on the speciﬁc machine. To allow machine independent analysis, we use the standard approach in computational complexity theory. First, we rely on a notion of an abstract machine, such as a Turing machine (or a Turing machine over the reals sense, while ignoring constant factors, thus the speciﬁc machine is not important as long as it implements the abstract machine. Usually, the asymptote is with algorithm mentioned before, we analyze the runtime as a function of the number of items that need to be sorted."
605,606,"In the context of learning algorithms, there is no clear notion of “input size.”"
606,607,"One might deﬁne the input size to be the size of the training set the algorithm receives, but that would be rather pointless. If we give the algorithm a very ing problem, the algorithm can simply ignore the extra examples. Therefore, a sequently, the runtime available for a learning algorithm should not increase as we increase the size of the training set. Just the same, we can still analyze the runtime as a function of natural parameters of the problem such as the target accuracy, the conﬁdence of achieving that accuracy, the dimensionality of the"
607,608,"The Runtime of Learning domain set, or some measures of the complexity of the hypothesis class with which the algorithm’s output is compared."
608,609,"To illustrate this, consider a learning algorithm for the task of learning axis a sequence of the problems is ﬁxed, one can analyze the asymptotic runtime as a function of variables of that sequence."
609,610,"Before we introduce the formal deﬁnition, there is one more subtlety we need to tackle. On the basis of the preceding, a learning algorithm can “cheat,” by transferring the computational burden to the output hypothesis. For example, the algorithm can simply deﬁne the output hypothesis to be the function that stores the training set in its memory, and whenever it gets a test example x it calculates the ERM hypothesis on the training set and applies it on x. Note that in this case, our algorithm has a ﬁxed output (namely, the function that we have just described) and can run in constant time. However, learning is still hard – the hardness is now in implementing the output classiﬁer to obtain a label prediction. To prevent this “cheating,” we shall require that the output of a learning algorithm must be applied to predict the label of a new example in time that does not exceed the runtime of training (that is, computing the output classiﬁer from the input training sample). In the next subsection the advanced reader may ﬁnd a formal deﬁnition of the computational complexity of learning."
610,611,"The deﬁnition that follows relies on a notion of an underlying abstract machine, which is usually either a Turing machine or a Turing machine over the reals. We will measure the computational complexity of an algorithm using the number of “operations” it needs to perform, where we assume that for any machine that implements the underlying abstract machine there exists a constant c such that any such “operation” can be performed on the machine using c seconds. tational complexity of a ﬁxed learning problem (determined by a triplet (Z, H, ℓ) – a domain set, a benchmark hypothesis class, and a loss function). Then, in the second step we consider the rate of change of that complexity along a sequence algorithm, A, we say that A solves the learning task in time O(f) if there exists some constant number c, such that for every probability distribution D"
611,612,"• The output of A, denoted hA, can be applied to predict the label of a new is deﬁned by a domain Zn, a hypothesis class Hn, and a loss function ℓn."
612,613,Let A be a learning algorithm designed for solving learning problems of
613,614,"A with respect to the preceding sequence is O(g), if for all n, A solves the"
614,615,"We say that A is an eﬃcient algorithm with respect to a sequence (Zn, Hn, ℓn) lem can be solved eﬃciently depends on how it can be broken into a sequence of speciﬁc learning problems. For example, consider the problem of learning a ﬁnite hypothesis class. As we showed in previous chapters, the ERM rule over example takes a constant time, it is possible to implement the ERM rule in time in polynomial time. Furthermore, if we deﬁne a sequence of problems in which |Hn| = n, then the exhaustive search is still considered to be eﬃcient. However, if ity is still polynomial in n but the computational complexity of the exhaustive search algorithm grows exponentially with n (thus, rendered ineﬃcient)."
615,616,Implementing the ERM Rule
616,617,"Given a hypothesis class H, the ERMH rule is maybe the most natural learning paradigm. Furthermore, for binary classiﬁcation problems we saw that if learning is at all possible, it is possible with the ERM rule. In this section we discuss the computational complexity of implementing the ERM rule for several hypothesis sponding ERMH rule can be deﬁned as follows:"
617,618,The Runtime of Learning
618,619,"This section studies the runtime of implementing the ERM rule for several examples of learning tasks. ably mild restriction. For example, H can be the set of all predictors that can be amples of useful ﬁnite classes are any hypothesis class that can be parameterized by a ﬁnite number of parameters, where we are satisﬁed with a representation of each of the parameters using a ﬁnite number of bits, for example, the class of axis aligned rectangles in the Euclidean space, Rd, when the parameters deﬁning any given rectangle are speciﬁed up to some limited precision."
619,620,"As we have shown in previous chapters, the sample complexity of learning a calculate the empirical risk, LS(h), and return a hypothesis that minimizes ple takes a constant amount of time, k, the runtime of this exhaustive search becomes k|H|m, where m is the size of the training set. If we let m to be the upper bound on the sample complexity mentioned, then the runtime becomes"
620,621,"The linear dependence of the runtime on the size of H makes this approach ineﬃcient (and unrealistic) for large classes. Formally, if we deﬁne a sequence of problems (Zn, Hn, ℓn)∞ at most n bits of code, then the runtime grows exponentially with n, implying that the exhaustive search approach is unrealistic for practical use. In fact, this problem is one of the reasons we are dealing with other hypothesis classes, like classes of linear predictors, which we will encounter in the next chapter, and not just focusing on ﬁnite classes."
621,622,"It is important to realize that the ineﬃciency of one algorithmic approach mentation exists. Indeed, we will show examples in which the ERM rule can be implemented eﬃciently."
622,623,Axis Aligned Rectangles
623,624,"Let Hn be the class of axis aligned rectangles in Rn, namely, h(a1,...,an,b1,...,bn)(x, y) ="
624,625,Eﬃciently Learnable in the Realizable Case
625,626,"Consider implementing the ERM rule in the realizable case. That is, we are given a training set S = (x1, y1), . . . , (xm, ym) of examples, such that there exists an such an axis aligned rectangle with a zero training error, namely, a rectangle that is consistent with all the labels in S. ai to be the minimal value of the i’th coordinate of a positive example in S and bi to be the maximal value of the i’th coordinate of a positive example in S."
626,627,"It is easy to verify that the resulting rectangle has zero training error and that the runtime of ﬁnding each ai and bi is O(m). Hence, the total runtime of this"
627,628,Not Eﬃciently Learnable in the Agnostic Case
628,629,"In the agnostic case, we do not assume that some hypothesis h perfectly predicts the labels of all the examples in the training set. Our goal is therefore to ﬁnd h that minimizes the number of examples for which yi ̸= h(xi). It turns out that for many common hypothesis classes, including the classes of axis aligned rectangles we consider here, solving the ERM problem in the agnostic setting is in H). That is, unless P = NP, there is no algorithm whose running time is polynomial in m and n that is guaranteed to ﬁnd an ERM hypothesis for these esis class, say, axis aligned rectangles in some ﬁxed dimension, n, then there exist eﬃcient learning algorithms for this class. In other words, there are successful dependence on the dimension n is not polynomial)."
629,630,"To see this, recall the implementation of the ERM rule we presented for the realizable case, from which it follows that an axis aligned rectangle is determined form an exhaustive search over all subsets of the training set of size at most 2n examples and construct a rectangle from each such subset. Then, we can pick"
630,631,"The Runtime of Learning the rectangle with the minimal training error. This procedure is guaranteed to ﬁnd an ERM hypothesis, and the runtime of the procedure is mO(n). It follows that if n is ﬁxed, the runtime is polynomial in the sample size. This does not contradict the aforementioned hardness result, since there we argued that unless"
631,632,"Boolean Conjunctions expressed as a proposition formula of the form xi1 ∧. . . ∧xik ∧¬xj1 ∧. . . ∧¬xjr, or appears with a negation sign, or does not appear at all, and we also have the all negative formula). Hence, the sample complexity of learning Hn"
632,633,Eﬃciently Learnable in the Realizable Case
633,634,"Next, we show that it is possible to solve the ERM problem for Hn polynomial in n and m. The idea is to deﬁne an ERM conjunction by including in the hypothesis conjunction all the literals that do not contradict any positively (or conjunctions). Let h0 be the conjunction of all possible literals. That is, positively. Now, since we consider learning in the realizable setup, there exists"
634,635,"C, that is consistent with all the examples in legal ERM hypothesis. Note that the running time of this algorithm is O(mn)."
635,636,Not Eﬃciently Learnable in the Agnostic Case
636,637,"As in the case of axis aligned rectangles, unless P = NP, there is no algorithm whose running time is polynomial in m and n that guaranteed to ﬁnd an ERM hypothesis for the class of Boolean conjunctions in the unrealizable case."
637,638,We next show that a slight generalization of the class of Boolean conjunctions leads to intractability of solving the ERM problem even in the realizable case.
638,639,"Boolean formula of the form h(x) = A1(x) ∨A2(x) ∨A3(x), where each Ai(x) is a Boolean conjunction (as deﬁned in the previous section). The output of h(x) is"
639,640,"3DNF is at most 33n. Hence, the sample complexity of learning Hn"
640,641,"However, from the computational perspective, this learning problem is hard."
641,642,"RP = NP, there is no polynomial time algorithm that properly learns a sequence n. By “properly” we mean that the algorithm should output a hypothesis that is formula it is a proper learner and therefore it is hard to implement it. The proof"
642,643,"Eﬃciently Learnable, but Not by a Proper ERM"
643,644,"In the previous section we saw that it is impossible to implement the ERM rule eﬃciently for the class Hn is possible to learn this class eﬃciently, but using ERM with respect to a larger"
644,645,"Representation Independent Learning Is Not Hard is no contradiction to the hardness result mentioned in the previous section as we now allow “representation independent” learning. That is, we allow the learning a larger hypothesis class so that the new class is easily learnable. The learning"
645,646,"The Runtime of Learning esis class; hence the name “representation independent” learning. We emphasize that in most situations, returning a hypothesis with good predictive ability is what we are really interested in doing. u, v, w there is a variable in the range of ψ indicating if u ∨v ∨w is true or false. with the same truth table. Since we assume that the data is realizable, we can"
646,647,"Furthermore, the sample complexity of learning the class of conjunctions in the this approach is polynomial in n."
647,648,"Intuitively, the idea is as follows. We started with a hypothesis class for which learning is hard. We switched to another representation where the hypothesis class is larger than the original class but has more structure, which allows for a more eﬃcient ERM search. In the new representation, solving the ERM problem"
648,649,We have just demonstrated that the computational hardness of implementing
649,650,"ERMH does not imply that such a class H is not learnable. How can we prove that a learning problem is computationally hard? tography is the opposite of learning. In learning we try to uncover some rule underlying the examples we see, whereas in cryptography, the goal is to make sure that nobody will be able to discover some secret, in spite of having access to some partial information about it. On that high level intuitive sense, results about the cryptographic security of some system translate into results about the unlearnability of some corresponding task. Regrettably, currently one has no way of proving that a cryptographic protocol is not breakable. Even the common assumption of P ̸= NP does not suﬃce for that (although it can be shown to be necessary for most common cryptographic scenarios). The common approach tographic assumptions. The more these are used as a basis for cryptography, the stronger is our belief that they really hold (or, at least, that algorithms that will refute them are hard to come by). ity from cryptographic assumptions. Many cryptographic systems rely on the assumption that there exists a one way function. Roughly speaking, a one way vert. More formally, f can be computed in time poly(n) but for any randomized polynomial time algorithm A, and for every polynomial p(·),"
650,651,"P[f(A(f(x))) = f(x)] < where the probability is taken over a random choice of x according to the uniform f is hard to invert, once one has access to its secret key, inverting f becomes feasible. Such functions are parameterized by their secret key. lated by some polynomial time algorithm. That is, we ﬁx an algorithm that given a secret key (representing one function in Fn) and an input vector, it calculates the value of the function corresponding to the secret key on the input vector in polynomial time. Consider the task of learning the class of the corresponding by some secret key sn of size polynomial in n, the class Hn ized by these keys and its size is at most 2p(n). Its sample complexity is therefore polynomial in n. We claim that there can be no eﬃcient learner for this class. If there were such a learner, L, then by sampling uniformly at random a polynomial labeled training sample of pairs (f(x), x), which should suﬃce for our learner to the range of f), which would violate the one way property of f."
651,652,"A more detailed treatment, as well as a concrete example, can be found in"
652,653,"The Runtime of Learning the class of functions that can be calculated by small Boolean circuits is not eﬃciently learnable, even in the realizable case."
653,654,"The runtime of learning algorithms is asymptotically analyzed as a function of sis class, our measure of accuracy, our measure of conﬁdence, or the size of the mented eﬃciently. For example, we derived eﬃcient algorithms for solving the"
654,655,"ERM problem for the class of Boolean conjunctions and the class of axis aligned rectangles, under the realizability assumption. However, implementing ERM for perspective, there is no diﬀerence between the realizable and agnostic cases (i.e., implementing ERM is hard even in the realizable case, yet the class is eﬃciently learnable by another algorithm."
655,656,"Hardness of implementing the ERM rule for several natural hypothesis classes has motivated the development of alternative learning methods, which we will discuss in the next part of this book."
656,657,"Bibliographic Remarks size of hypotheses in the class. A detailed discussion and thorough bibliographic learning rule (in the agnostic case) that given a training set of size m, runs"
657,658,Hint: Use dynamic programming.
658,659,"Assume that there is a learning algorithm that implements the ERM rule in the realizable case such that the output hypothesis of the algorithm for each class Hn only depends on O(n) examples out of the training set. Furthermore, assume that such a hypothesis can be calculated given these O(n) examples in time O(n), and that the empirical risk of each such hypothesis can be evaluated in time O(mn). For example, if Hn is the class of axis aligned rectangles in Rn, we saw that it is possible to ﬁnd an ERM hypothesis in the realizable case that is deﬁned by at most 2n examples. Prove that in such cases, it is possible to ﬁnd an ERM hypothesis for Hn in the unrealizable case in time O(mn mO(n)). halfspaces, HSn, for a domain X = Rn. This is the class of all functions of tationally hard. More precisely, we consider the sequence of problems in which the dimension n grows linearly and the number of examples m is set to be some constant times n."
659,660,"Rm (that is, a system of m linear inequalities in n variables, x = (x1, . . . , xn)), ﬁnd a subsystem containing as many inequalities as possible that has a solution (such a subsystem is called feasible)."
660,661,"Show that any algorithm that ﬁnds an ERMHSn hypothesis for any training size m, n. Hint: Deﬁne a mapping that transforms linear inequalities in n variables into labeled points in Rn, and a mapping that transforms vectors in Rn to halfspaces, such that a vector w satisﬁes an inequality q if and only if the labeled point that corresponds to q is classiﬁed correctly by the halfspace corresponding to w. Conclude that the problem of empirical risk time polynomial in the sample size, m, and the Euclidean dimension, n, then every problem in the class NP can be solved in polynomial time). halfspaces in Rn. In this exercise, we wish to show that ERMHn size, m, also grows linearly with n."
661,662,"Given a graph G = (V, E), and a number k, determine whether there exists a"
662,663,"The Runtime of Learning k : that is, to prove that if there is an algorithm that solves the ERMHn polynomial in k, n, and the sample size m, then there is a polynomial time"
663,664,"Given a graph G = (V, E), let {v1 . . . vn} be the vertices in V . Construct k that has zero error over S(G)"
664,665,Use the fact that halfspaces are convex sets to show that it cannot be true that two vertices that are connected by an edge have the same zero error over S(G).
665,666,"Hint: Given a coloring f of the vertices of G, we should come up with k hyperplanes, h1 . . . hk whose intersection is a perfect classiﬁer for S(G). alent to hardness of proper PAC learning. Recall that by “properness” of the algorithm we mean that it must output a hypothesis from the hypothesis class. To formalize this statement, we ﬁrst need the following deﬁnition."
666,667,"The complexity class Randomized Polynomial (RP) time is the class of all decision problems (that is, problems in which on any instance one has to ﬁnd out whether the answer is YES or NO) for which there exists a probabilistic algorithm (namely, the algorithm is allowed to ﬂip random coins while it is running) with these properties:"
667,668,• On any input instance the algorithm runs in polynomial time in the input
668,669,"• If the correct answer is NO, the algorithm must return NO."
669,670,"• If the correct answer is YES, the algorithm returns YES with probability"
670,671,"Clearly the class RP contains the class P. It is also known that RP is contained in the class NP. It is not known whether any equality holds among these three complexity classes, but it is widely believed that NP is strictly solved by a randomized polynomial time algorithm."
671,672,"• Show that if a class H is properly PAC learnable by a polynomial time algorithm, then the ERMH problem is in the class RP. In particular, this class of intersections of halfspaces discussed in the previous exercise), then, unless NP = RP, there exists no polynomial time proper PAC learning algorithm for H."
672,673,"Hint: Assume you have an algorithm A that properly PAC learns a an algorithm B for solving the ERMH problem in random polynomial error on S is zero, apply the PAC learning algorithm to the uniform uniform distribution). Show that the algorithm just described satisﬁes the requirements for being a RP solver for ERMH."
673,674,From Theory to Algorithms
674,675,"In this chapter we will study the family of linear predictors, one of the most useful families of hypothesis classes. Many learning algorithms that are being widely used in practice rely on linear predictors, ﬁrst and foremost because of the ability to learn them eﬃciently in many cases. In addition, linear predictors are intuitive, are easy to interpret, and ﬁt the data reasonably well in many natural learning problems."
675,676,"We will introduce several hypothesis classes belonging to this family – halfspaces, vant learning algorithms: linear programming and the Perceptron algorithm for the class of halfspaces and the Least Squares algorithm for linear regression."
676,677,"This chapter is focused on learning linear predictors using the ERM approach; however, in later chapters we will see alternative paradigms for learning these"
677,678,"First, we deﬁne the class of aﬃne functions as hw,b(x) = ⟨w, x⟩+ b ="
678,679,"It will be convenient also to use the notation and returns as output the scalar ⟨w, x⟩+ b. be the sign function, and for regression problems, where Y = R, φ is simply the"
679,680,"It may be more convenient to incorporate b, called the bias, into w as an"
680,681,"Personal use only. Not for distribution. Do not post. hw,b(x) = ⟨w, x⟩+ b = ⟨w′, x′⟩."
681,682,"It follows that each aﬃne function in Rd can be rewritten as a homogenous linear to each input vector. Therefore, whenever it simpliﬁes the presentation, we will omit the bias term and refer to Ld as the class of homogenous linear functions of the form hw(x) = ⟨w, x⟩."
682,683,Throughout the book we often use the general term “linear functions” for both aﬃne functions and (homogenous) linear functions.
683,684,"The ﬁrst hypothesis class we consider is the class of halfspaces, designed for halfspaces is deﬁned as follows:"
684,685,"To illustrate this hypothesis class geometrically, it is instructive to consider that are “above” the hyperplane, that is, share an acute angle with w, are labeled positively. Instances that are “below” the hyperplane, that is, share an obtuse angle with w, are labeled negatively. can learn halfspaces using the ERM paradigm, as long as the sample size is . Therefore, we now discuss how to implement an ERM procedure able case. In the context of halfspaces, the realizable case is often referred to as the “separable” case, since it is possible to separate with a hyperplane all the positive examples from all the negative examples. Implementing the ERM rule in the nonseparable case (i.e., the agnostic case) is known to be computationally separable data. The most popular one is to use surrogate loss functions, namely, to learn a halfspace that does not necessarily minimize the empirical risk with implemented eﬃciently even in the nonseparable case. We will study surrogate"
685,686,Linear Programming for the Class of Halfspaces
686,687,"Linear programs (LP) are problems that can be expressed as maximizing a linear function subject to linear inequalities. That is,"
687,688,"We will show that the ERM problem for halfspaces in the realizable case can be expressed as a linear program. For simplicity, we assume the homogenous case. Let S = {(xi, yi)}m realizable case, an ERM predictor should have zero errors on the training set."
688,689,"Equivalently, we are looking for some vector w for which"
689,690,"Let w∗be a vector that satisﬁes this condition (it must exist since we assume realizability). Deﬁne γ = mini(yi⟨w∗, xi⟩) and let ¯w = w∗ γ . Therefore, for all i"
690,691,We have thus shown that there exists a vector that satisﬁes
691,692,"And clearly, such a vector is an ERM predictor. follows. Set A to be the m × d matrix whose rows are the instances multiplied by yi. That is, Ai,j = yi xi,j, where xi,j is the j’th element of the vector xi. Let"
692,693,"The LP form requires a maximization objective, yet all the w that satisfy the constraints are equal candidates as output hypotheses. Thus, we set a “dummy”"
693,694,Perceptron for Halfspaces
694,695,"A diﬀerent implementation of the ERM rule is the Perceptron algorithm of labeled by w(t), namely, an example for which sign(⟨w(t), xi⟩) ̸= yi. Then, the"
695,696,Perceptron updates w(t) by adding to it the instance xi scaled by the label yi.
696,697,"Hence, the update of the Perceptron guides the solution to be “more correct” on input: A training set (x1, y1), . . . , (xm, ym)"
697,698,"The following theorem guarantees that in the realizable case, the algorithm stops with all sample points correctly classiﬁed."
698,699,"Assume that (x1, y1), . . . , (xm, ym) is separable, let B = min{∥w∥:"
699,700,"By the deﬁnition of the stopping condition, if the Perceptron stops it must have separated all the examples. We will show that if the Perceptron runs"
700,701,"Let w⋆be a vector that achieves the minimum in the deﬁnition of B. That is,"
701,702,"The idea of the proof is to show that after performing T iterations, the cosine"
702,703,"RB . That is, we will show that which will conclude our proof. while on iteration t, if we update using example (xi, yi) we have that = ⟨w⋆, yixi⟩= yi⟨w⋆, xi⟩"
703,704,"Therefore, after performing T iterations, we get: where the last inequality is due to the fact that example i is necessarily such verge. However, the convergence rate depends on the parameter B, which in some situations might be exponentially large in d. In such cases, it would be better to implement the ERM problem by solving a linear program, as described in the previous section. Nevertheless, for many natural data sets, the size of B is not too large, and the Perceptron converges quite fast."
704,705,The VC Dimension of Halfspaces
705,706,"To compute the VC dimension of halfspaces, we start with the homogenous case."
706,707,The VC dimension of the class of homogenous halfspaces in Rd
707,708,"First, consider the set of vectors e1, . . . , ed, where for every i the vector by the class of homogenous halfspaces. Indeed, for every labeling y1, . . . , yd, set w = (y1, . . . , yd), and then ⟨w, ei⟩= yi for all i. ﬁrst assume that both of them are nonempty. Then, which leads to a contradiction. Finally, if J (respectively, I) is empty then the which still leads to a contradiction."
708,709,"The VC dimension of the class of nonhomogenous halfspaces in homogenous halfspaces. But, using the reduction we have shown in the beginning tween some “explanatory” variables and some real valued outcome. Cast as a bel set Y is the set of real numbers. We would like to learn a linear function for example, predicting the weight of a baby as a function of her age and weight"
709,710,The hypothesis class of linear regression predictors is simply the set of linear
710,711,"Next we need to deﬁne a loss function for regression. While in classiﬁcation the deﬁnition of the loss is straightforward, as ℓ(h, (x, y)) simply indicates whether the former over the latter. We therefore need to deﬁne how much we shall be “penalized” for the discrepancy between h(x) and y. One common way is to use"
711,712,"For this loss function, the empirical risk function is called the Mean Squared"
712,713,"In the next subsection, we will see how to implement the ERM rule for linear regression with respect to the squared loss. Of course, there are a variety of other loss functions that one can use, for example, the absolute value loss function, ℓ(h, (x, y)) = |h(x) −y|. The ERM rule for the absolute value loss function can sample complexity of linear regression is by relying on the “discretization trick” each element of the vector w and the bias b using a ﬁnite number of bits (say function will be bounded. Later in the book we will describe more rigorous means to analyze the sample complexity of regression problems. esis class of linear regression predictors with respect to the squared loss. The"
713,714,"ERM problem with respect to this class, given a training set S, and using the homogenous version of Ld, is to ﬁnd"
714,715,"To solve the problem we calculate the gradient of the objective function and compare it to zero. That is, we need to solve"
715,716,We can rewrite the problem as the problem Aw = b where
716,717,If A is invertible then the solution to the ERM problem is
717,718,"The case in which A is not invertible requires a few standard tools from linear algebra, which are available in Appendix C. It can be easily shown that if the training instances do not span the entire space of Rd then A is not invertible."
718,719,"Nevertheless, we can always ﬁnd a solution to the system Aw = b because b is in the range of A. Indeed, since A is symmetric we can write it using its eigenvalue decomposition as A = V DV ⊤, where D is a diagonal matrix and V is an orthonormal matrix (that is, V ⊤V is the identity d × d matrix). Deﬁne"
719,720,"Let vi denote the i’th column of V . Then, we have"
720,721,"That is, A ˆw is the projection of b onto the span of those vectors vi for which those vi, and b is in the linear span of the xi, we obtain that A ˆw = b, which concludes our argument."
721,722,Linear Regression for Polynomial Regression Tasks
722,723,"Some learning tasks call for nonlinear predictors, such as polynomial predictors."
723,724,"Take, for instance, a one dimensional polynomial function of degree n, that is, p(x) = a0 + a1x + a2x2 + · · · + anxn depict a training set that is better ﬁtted using a 3rd degree polynomial predictor than using a linear predictor. gression predictors, namely, where p is a one dimensional polynomial of degree n, parameterized by a vector of coeﬃcients (a0, . . . , an). Note that X = R, since this is a one dimensional polynomial, and Y = R, as this is a regression problem."
724,725,"One way to learn this class is by reduction to the problem of linear regression, which we have already shown how to solve. To translate a polynomial regression p(ψ(x)) = a0 + a1x + a2x2 + · · · + anxn = ⟨a, ψ(x)⟩ and we can ﬁnd the optimal vector of coeﬃcients a by using the Least Squares algorithm as shown earlier."
725,726,"However, logistic regression is used for classiﬁcation tasks: We can interpret h(x) the class of linear functions Ld. In particular, the sigmoid function used in logistic regression is the logistic function, deﬁned as"
726,727,"The hypothesis class is therefore (where for simplicity we are using homogenous halfspace corresponding to a vector w is sign(⟨w, x⟩). Therefore, the predictions of the halfspace hypothesis and the logistic hypothesis are very similar whenever"
727,728,"In contrast, the halfspace hypothesis always outputs a deterministic prediction"
728,729,"Next, we need to specify a loss function. That is, we should deﬁne how bad it"
729,730,"Therefore, any reasonable loss function would increase monotonically with gistic loss function used in logistic regression penalizes hw based on the log of"
730,731,"Therefore, given a training set S = (x1, y1), . . . , (xm, ym), the ERM problem associated with logistic regression is"
731,732,"The advantage of the logistic loss function is that it is a convex function with respect to w; hence the ERM problem can be solved eﬃciently using standard methods. We will study how to learn with convex functions, and in particular specify a simple algorithm for minimizing convex functions, in later chapters. ity of a given data set assuming a speciﬁc parametric probability function. We"
732,733,"The family of linear predictors is one of the most useful families of hypothesis classes, and many learning algorithms that are being widely used in practice rely on linear predictors. We have shown eﬃcient algorithms for learning linear to the squared and logistic losses in the unrealizable case. In later chapters we will present the properties of the loss function that enable eﬃcient learning. edge, that some linear predictor attains low risk with respect to the underlying distribution. In the next chapter we show how to construct nonlinear predictors by composing linear predictors on top of simple classes. This will enable us to employ linear predictors for a variety of prior knowledge assumptions."
733,734,"Bibliographic Remarks absolute value loss function, ℓ(h, (x, y)) = |h(x) −y|, as a linear program; namely, show how to write the problem as a linear program. x1, . . . , xm span Rd. m, there exist a vector w∗∈Rd (for some appropriate d) and a sequence of examples {(x1, y1), . . . , (xm, ym)} such that the following hold:"
734,735,• When running the Perceptron on this sequence of examples it makes m updates before converging.
735,736,Hint: Choose d = m and for every i choose xi = ei.
736,737,"Hint: Set each xi to be a third dimensional vector of the form (a, b, yi), where (≤) rather than equalities (=), and ﬁgure out scenarios where the inequality actually holds with equality. ceptron will perform the same number of iterations as the vanilla Perceptron and will converge to a vector that points to the same direction as the output of the vanilla Perceptron. (closed) balls in Rd, that is, that if x1, . . . , xm are shattered by Bd then φ(x1), . . . , φ(xm) are shattered"
737,738,"Boosting is an algorithmic paradigm that grew out of a theoretical question and became a very practical machine learning tool. The boosting approach uses a generalization of linear predictors to address two major issues that have been a sum of approximation error and estimation error. The more expressive the hypothesis class the learner is searching over, the smaller the approximation error is, but the larger the estimation error becomes. A learner is thus faced with the problem of picking a good tradeoﬀbetween these two considerations. The boosting paradigm allows the learner to have smooth control over this tradeoﬀ."
738,739,"The learning starts with a basic class (that might have a large approximation error), and as it progresses the class that the predictor may belong to grows"
739,740,"The second issue that boosting addresses is the computational complexity of of ﬁnding an ERM hypothesis may be computationally infeasible. A boosting algorithm ampliﬁes the accuracy of weak learners. Intuitively, one can think of a weak learner as an algorithm that uses a simple “rule of thumb” to output a slightly better than a random guess. When a weak learner can be implemented eﬃciently, boosting provides a tool for aggregating such weak hypotheses to approximate gradually good predictors for larger, and harder to learn, classes. rithm, AdaBoost (a shorthand for Adaptive Boosting). The AdaBoost algorithm outputs a hypothesis that is a linear combination of simple hypotheses. In other words, AdaBoost relies on the family of hypothesis classes obtained by composing a linear predictor on top of simple classes. We will show that AdaBoost enables us to control the tradeoﬀbetween the approximation and estimation errors by varying a single parameter."
740,741,"AdaBoost demonstrates a general theme, that will recur later in the book, of expanding the expressiveness of linear predictors by composing them on top of"
741,742,AdaBoost stemmed from the theoretical question of whether an eﬃcient weak learner can be “boosted” into an eﬃcient strong learner. This question was raised
742,743,"Personal use only. Not for distribution. Do not post. a graduate student at MIT. However, the proposed mechanism was not very algorithm, which was the ﬁrst truly practical implementation of boosting. This simple and elegant algorithm became hugely popular, and Freund and Schapire’s work has been recognized by numerous awards."
743,744,"Furthermore, boosting is a great example for the practical impact of learning theory. While boosting originated as a purely theoretical problem, it has led to popular and widely used algorithms. Indeed, as we shall demonstrate later in this chapter, AdaBoost has been successfully used for learning to detect faces in holds with respect to H, D, f, then when running the learning algorithm on learnable class can be learned using any ERM algorithm. However, the deﬁnition of PAC learning and the fundamental theorem of learning theory ignores the are cases in which implementing the ERM rule is computationally hard (even in the realizable case)."
744,745,"However, perhaps we can trade computational hardness with the requirement for accuracy. Given a distribution D and a target labeling function f, maybe there exists an eﬃciently computable learning algorithm whose error is just slightly better than a random guess? This motivates the following deﬁnition. assumption holds with respect to H, D, f, then when running the learning the algorithm returns a hypothesis h such that, with probability of at least"
745,746,"This deﬁnition is almost identical to the deﬁnition of PAC learning, which here we will call strong learning, with one crucial diﬀerence: Strong learnability implies the ability to ﬁnd an arbitrarily good classiﬁer (with error rate at most rate is slightly better than what a random labeling would give us. The hope is that it may be easier to come up with eﬃcient weak learners than with eﬃcient (full) PAC learners. class H has a VC dimension d, then the sample complexity of PAC learning H , where C1 is a constant. Applying this with"
746,747,"This implies that from the statistical perspective (i.e., if we ignore computational complexity), weak learnability is also characterized by the VC dimension of H and therefore is just as hard as PAC (strong) learning. However, when we do consider computational complexity, the potential advantage of weak learning is that maybe there is an algorithm that satisﬁes the requirements of weak learning and can be implemented eﬃciently."
747,748,"One possible approach is to take a “simple” hypothesis class, denoted B, and to apply ERM with respect to B as the weak learning algorithm. For this to work, we need that B will satisfy two requirements:"
748,749,• ERMB is eﬃciently implementable.
749,750,"• For every sample that is labeled by some hypothesis from H, any ERMB"
750,751,"Then, the immediate question is whether we can boost an eﬃcient weak learner into an eﬃcient strong learner. In the next section we will show that this is indeed possible, but before that, let us show an example in which eﬃcient weak learnability of a class H is possible using a base hypothesis class B."
751,752,"To see that, we ﬁrst show that for every distribution that is consistent with every classiﬁer in H consists of three regions (two unbounded rays and a center interval) with alternate labels. For any pair of such regions, there exists a decision stump that agrees with the labeling of these two components. Note that for every distribution D over R and every partitioning of the line into three such regions, error hypothesis. A decision stump that disagrees with h only on such a region"
752,753,We see that ERMB is a γ-weak learner for H. We next show how to implement the ERM rule eﬃciently for decision stumps.
753,754,Eﬃcient Implementation of ERM for Decision Stumps
754,755,"Let X = Rd and consider the base hypothesis class of decision stumps over Rd,"
755,756,"We will show how to implement an ERM rule, namely, how to ﬁnd a decision stump that minimizes LS(h). Furthermore, since in the next section we will show that AdaBoost requires ﬁnding a hypothesis with a small risk relative to some distribution over S, we will show here how to minimize such risk functions."
756,757,"Concretely, let D be a probability vector in Rm (that is, all elements of D are have to calculate a sum over m examples; therefore the runtime of this approach would be O(dm2). We next show a simple trick that enables us to minimize the objective in time O(dm)."
757,758,"The observation is as follows. Suppose we have calculated the objective for in which we sort the examples with respect to each coordinate, the minimization problem can be performed in time O(dm). This yields the following pseudocode."
758,759,"ERM for Decision Stumps training set S = (x1, y1), . . . , (xm, ym) distribution vector D sort S using the j’th coordinate, and denote"
759,760,"AdaBoost (short for Adaptive Boosting) is an algorithm that has access to a weak learner and ﬁnds a hypothesis with a low empirical risk. The AdaBoost algorithm receives as input a training set of examples S = (x1, y1), . . . , (xm, ym), where for each i, yi = f(xi) for some labeling function f. The boosting process proceeds in a sequence of consecutive rounds. At round t, the booster ﬁrst deﬁnes a distribution over the examples in S, denoted D(t). That is, D(t) ∈Rm to the weak learner. (That way, the weak learner can construct i.i.d. examples according to D(t) and f.) The weak learner is assumed to return a “weak” hypothesis, ht, whose error,"
760,761,"That is, the weight of ht is inversely proportional to the error of ht. At the end of the round, AdaBoost updates the distribution so that examples on which ht errs will get a higher probability mass while examples on which ht is correct will get a lower probability mass. Intuitively, this will force the weak learner to focus on the problematic examples in the next round. The output of the AdaBoost algorithm is a “strong” classiﬁer that is based on a weighted sum of all the weak hypotheses. The pseudocode of AdaBoost is presented in the following. training set S = (x1, y1), . . . , (xm, ym) invoke weak learner ht = WL(D(t), S) output the hypothesis hs(x) = sign"
761,762,The following theorem shows that the training error of the output hypothesis decreases exponentially fast with the number of boosting rounds.
762,763,Let S be a training set and assume that at each iteration of the training error of the output hypothesis of AdaBoost is at most
763,764,"For each t, denote ft = P is fT . In addition, denote that for every round t,"
764,765,"To do so, we ﬁrst note that using a simple inductive argument, for all t and i,"
765,766,"Each iteration of AdaBoost involves O(m) operations as well as a single call to the weak learner. Therefore, if the weak learner can be implemented eﬃciently (as happens in the case of ERM with respect to decision stumps) then the total training process will be eﬃcient. the union bound, the probability that the weak learner will not fail at all of the distributions over the training set, in many cases we can implement the weak case, for example, in the weak learner that ﬁnds the minimum value of LD(h) for decision stumps, as described in the previous section."
766,767,"AdaBoost goes to zero as T grows. However, what we really care about is the true risk of the output hypothesis. To argue about the true risk, we note that the output of AdaBoost is in fact a composition of a halfspace over the predictions of the T weak hypotheses constructed by the weak learner. In the next section we show that if the weak hypotheses come from a base hypothesis class of low true risk of the output of AdaBoost would not be very far from its empirical risk."
767,768,Linear Combinations of Base Hypotheses
768,769,"As mentioned previously, a popular approach for constructing a weak learner is to apply the ERM rule with respect to a base hypothesis class (e.g., ERM over decision stumps). We have also seen that boosting outputs a composition of a halfspace over the predictions of the weak hypotheses. Therefore, given a base hypothesis class B (e.g., decision stumps), the output of AdaBoost will be a member of the following class: tained by ﬁrst applying the T base hypotheses to construct the vector ψ(x) = (h1(x), . . . , hT (x)) ∈RT , and then applying the (homogenous) halfspace deﬁned"
769,770,"In this section we analyze the estimation error of L(B, T) by bounding the aBoost grows linearly with T. On the other hand, the empirical risk of AdaBoost decreases with T. In fact, as we demonstrate later, T can be used to decrease the approximation error of L(B, T). Therefore, the parameter T of AdaBoost"
770,771,"To demonstrate how the expressive power of L(B, T) increases with T, consider the simple example, in which X = R and the base class is Decision Stumps, mogenous) halfspaces on R."
771,772,"Now, let H be the rather complex class (compared to halfspaces on the line) yields a more expressive hypothesis class but on the other hand might increase dimension of L(B, T) for any base class B. by ˜O(VCdim(B) T) (the ˜O notation ignores constants and logarithmic factors). chotomies (i.e., labelings) induced by B over C. Therefore, we need to choose dichotomies we can construct is upper bounded by that C is shattered, we must have that the preceding is at least 2m, which yields which concludes our proof. recognition. The value of g for type A or B is the diﬀerence between the sum of the pixels within two rectangular regions. These regions have the same size and shape and are horizontally or vertically adjacent. For type C, the value of g is the sum within two outside rectangles subtracted from the sum in a center rectangle. For type D, we compute the diﬀerence between diagonal pairs of rectangles."
772,773,AdaBoost for Face Recognition
773,774,"We now turn to a base hypothesis that has been proposed by Viola and Jones for the task of face recognition. In this task, the instance space is images, represented as matrices of gray level values of pixels. To be concrete, let us take images of given an image as input, should output whether the image is of a human face or"
774,775,"Each hypothesis in the base class is of the form h(x) = f(g(x)), where f is a to a scalar. Each function g is parameterized by"
775,776,"To calculate g we stretch the mask t to ﬁt the rectangle R and then calculate the sum of the pixels (that is, sum of their gray level values) that lie within the red rectangles and subtract it from the sum of pixels in the blue rectangles. weak learner for the base hypothesis class by ﬁrst calculating all the possible outputs of g on each image, and then apply the weak learner of decision stumps described in the previous subsection. It is possible to perform the ﬁrst step very and then overlayed on a typical training face in the bottom row. The ﬁrst feature measures the difference in intensity between the region of the eyes and a region across the upper cheeks. The feature capitalizes on the observation that the eye region is often darker than the cheeks. The second feature compares the intensities in the eye regions to the intensity across the bridge of the nose. directly increases computation time."
776,777,"The Attentional Cascade tion performance while radically reducing computation time. The key insight is that smaller, and therefore before more complex classiﬁers are called upon to achieve low false positive rates. imize false negatives. The initial AdaBoost threshold, , is designed to yield a low error rate on the training data. A lower threshold yields higher detection rates and higher false positive rates. Based on cessing with very few operations:"
777,778,Viola and Jones. The two features are shown in the top row and then overlaid on a typical training face in the bottom row. The ﬁrst feature measures the diﬀerence in intensity between the region of the eyes and a region across the upper cheeks. The feature capitalizes on the observation that the eye region is often darker than the cheeks. The second feature compares the intensities in the eye regions to the intensity across the bridge of the nose. eﬃciently by a preprocessing step in which we calculate the integral image of running it with the base features proposed by Viola and Jones.
778,779,Boosting is a method for amplifying the accuracy of weak learners. In this chapter we described the AdaBoost algorithm. We have shown that after T iterations of
779,780,"AdaBoost, it returns a hypothesis from the class L(B, T), obtained by composing a linear classiﬁer on T hypotheses from a base class B. We have demonstrated how the parameter T controls the tradeoﬀbetween approximation and estimation errors. In the next chapter we will study how to tune parameters such as T, based"
780,781,Bibliographic Remarks
781,782,"As mentioned before, boosting stemmed from the theoretical question of whether an eﬃcient weak learner can be “boosted” into an eﬃcient strong learner (Kearns"
782,783,"Boosting can be viewed from many perspectives. In the purely theoretical context, AdaBoost can be interpreted as a negative result: If strong learning of a hypothesis class is computationally hard, so is weak learning of this class. This negative result can be useful for showing hardness of agnostic PAC learning of a class B based on hardness of PAC learning of some other class H, as long as shown that PAC learning of the class of intersection of halfspaces is hard (even in the realizable case). This hardness result can be used to show that agnostic for a single halfspace can yield a weak learner for the class of intersection of halfspaces, and since such a weak learner can be boosted, we will obtain a strong learner for the class of intersection of halfspaces."
783,784,AdaBoost also shows an equivalence between the existence of a weak learner and separability of the data using a linear classiﬁer over the predictions of base hypotheses. This result is closely related to von Neumann’s minimax theorem
784,785,"AdaBoost is also related to the concept of margin, which we will study later on of research that this ﬁeld has produced."
785,786,Suggest a procedure that relies on A and learns H in the usual agnostic
786,787,PAC learning model and has a sample complexity of stant function deﬁned according to the same thresholds as h. mechanism to “force” the weak learner to focus on the problematic examples in the next iteration. In this question we will ﬁnd some rigorous justiﬁcation
787,788,"We proved an upper bound of O(dT log(dT)), where d = VCdim(B). Here we wish to prove an almost matching lower bound. However, that will not be the case for all classes B."
788,789,"VCdim(L(B, T)). Find a class B for which VCdim(B) = VCdim(L(B, T))"
789,790,Hint: Take X to be a ﬁnite set.
790,791,"• For the lower bound, assume d = 2k. Let A be a k × d matrix whose a set of k vectors in Rd. Show that this set is shattered by decision"
791,792,"Hint: Construct a set of T from the previous question, and the rows of the matrices 2A, 3A, 4A, . . . , T"
792,793,"Show that the resulting set is shattered by L(Bd, T). image of A, denoted by I(A), is the matrix B such that Bi,j = P"
793,794,• Show that I(A) can be calculated from A in time linear in the size of A.
794,795,"• Show how every Viola and Jones feature can be calculated from I(A) in a constant amount of time (that is, the runtime does not depend on the size of the rectangle deﬁning the feature)."
795,796,Model Selection and Validation
796,797,"In the previous chapter we have described the AdaBoost algorithm and have oﬀ. But, how do we set T in practice? More generally, when approaching some practical problem, we usually can think of several algorithms that may yield a good solution, each of which might have several parameters. How can we choose the best algorithm for the particular problem at hand? And how do we set the algorithm’s parameters? This task is often called model selection."
797,798,"To illustrate the model selection task, consider the problem of learning a one set as depicted in the ﬁgure."
798,799,"However, we might be uncertain regarding which degree d would give the best results for our data set: A small degree may not ﬁt the data well (i.e., it will have a large approximation error), whereas a high degree may lead to overﬁtting (i.e., it will have a large estimation error). In the following we depict the result risk decreases as we enlarge the degree. However, looking at the graphs, our"
799,800,It follows that the empirical risk alone is not enough for model selection.
800,801,Personal use only. Not for distribution. Do not post.
801,802,"In this chapter we will present two approaches for model selection. The ﬁrst approach is based on the Structural Risk Minimization (SRM) paradigm we tradeoﬀ(such as the degree of the ﬁtted polynomial in the preceding example or the parameter T in AdaBoost). The second approach relies on the concept of validation. The basic idea is to partition the training set into two sets. One is used for training each of the candidate models, and the second is used for deciding which of them yields the best results. mation and estimation errors. More generally, if our learning algorithm fails to ﬁnd a predictor with a small risk, it is important to understand whether we"
802,803,"Model Selection Using SRM show how SRM can be used for tuning the tradeoﬀbetween bias and complexity without deciding on a speciﬁc hypothesis class in advance. Consider a countable sequence of hypothesis classes H1, H2, H3, . . .. For example, in the problem of polynomial regression mentioned, we can take Hd to be the set of polynomials of degree at most d. Another example is taking Hd to be the class L(B, d) used by AdaBoost, as described in the previous chapter."
803,804,"We assume that for every d, the class Hd enjoys the uniform convergence of the class Hd multiplied by a universal constant (the one appearing in the by AdaBoost, the function g will simply grow with d."
804,805,"Recall that the SRM rule follows a “bound minimization” approach, where in"
805,806,Model Selection and Validation and a complexity term that depends on d. The SRM rule will search for d and
806,807,"Getting back to the example of polynomial regression described earlier, even though the empirical risk of the 10th degree polynomial is smaller than that of the 3rd degree polynomial, we would still prefer the 3rd degree polynomial since its complexity (as reﬂected by the value of the function g(d)) is much smaller."
807,808,"While the SRM approach can be useful in some situations, in many practical we present a more practical approach. dictor of a learning algorithm. So far we have derived bounds on the estimation error of a hypothesis class, which tell us that for all hypotheses in the class, the true risk is not very far from the empirical risk. However, these bounds might be tributions. A more accurate estimation of the true risk can be obtained by using some of the training data as a validation set, over which one can evalutate the success of the algorithm’s output predictor. This procedure is called validation."
808,809,"Naturally, a better estimation of the true risk is useful for model selection, as ditional set of examples, independent of the training set, and using the empirical error on this validation set as our estimator. Formally, let V = (x1, y1), . . . , (xmv, ymv) be a set of fresh mv examples that are sampled according to D (independently of we have the following:"
809,810,"Let h be some predictor and assume that the loss function is in of a validation set V of size mv we have set used to construct h and is tighter than the usual bounds that we have seen so far. The reason for the tightness of this bound is that it is in terms of an estimate on a fresh validation set that is independent of the way h was generated. To illustrate this point, suppose that h was obtained by applying an ERM predictor"
810,811,"Therefore, taking mv to be order of m, we obtain an estimate that is more price we pay for using such an estimate is that it requires an additional sample on top of the sample used for training the learner."
811,812,"Sampling a training set and then sampling an independent validation set is equivalent to randomly partitioning our random set of examples into two parts, using one part for training and the other one for validation. For this reason, the validation set is often referred to as a hold out set."
812,813,Validation for Model Selection
813,814,"Validation can be naturally used for model selection as follows. We ﬁrst train diﬀerent algorithms (or the same algorithm with diﬀerent parameters) on the given training set. Let H = {h1, . . . , hr} be the set of all output predictors of the diﬀerent algorithms. For example, in the case of training polynomial regressors, we would have each hr be the output of polynomial regression of degree r. Now, to choose a single predictor from H we sample a fresh validation set and choose the predictor that minimizes the error over the validation set. In other words, we apply ERMH over the validation set."
814,815,"This process is very similar to learning a ﬁnite hypothesis class. The only ing set. However, since the validation set is independent of the training set we get that it is also independent of H and therefore the same technique we used to derive bounds for ﬁnite hypothesis classes holds here as well. In particular,"
815,816,"Let H = {h1, . . . , hr} be an arbitrary set of predictors and"
816,817,Model Selection and Validation
817,818,"This theorem tells us that the error on the validation set approximates the true error as long as H is not too large. However, if we try too many methods (resulting in |H| that is large relative to the size of the validation set) then we’re in danger of overﬁtting."
818,819,"To illustrate how validation is useful for model selection, consider again the example of ﬁtting a one dimensional polynomial as described in the beginning of this chapter. In the following we depict the same training set, with ERM error, and hence it will be chosen as the best model. tion of the complexity of the model considered. For example, for the polynomial ﬁtting problem mentioned previously, the curve will look like:"
819,820,"As can be shown, the training error is monotonically decreasing as we increase the polynomial degree (which is the complexity of the model in our case). On the other hand, the validation error ﬁrst decreases but then starts to increase, which indicates that we are starting to suﬀer from overﬁtting."
820,821,"Plotting such curves can help us understand whether we are searching the correct regime of our parameter space. Often, there may be more than a single parameter to tune, and the possible number of values each parameter can take regularization, in which the parameter of the learning algorithm is a real number."
821,822,"In such cases, we start with a rough grid of values for the parameter(s) and plot in to the correct regime and employ a ﬁner grid to search over. It is important to verify that we are in the relevant regime. For example, in the polynomial ﬁtting and do not employ a ﬁner grid based on the resulting curve, we will end up with a rather poor model."
822,823,"The validation procedure described so far assumes that data is plentiful and that we have the ability to sample a fresh validation set. But in some applications, cross validation technique is designed to give an accurate estimate of the true error without wasting too much data. the algorithm is trained on the union of the other folds and then the error of its output is estimated using the fold. Finally, the average of all these errors is the"
823,824,"Model Selection and Validation estimate of the true error. The special case k = m, where m is the number of and once the best parameter is chosen, the algorithm is retrained using this for model selection is given in the following. The procedure receives as input a training set, S, a set of possible parameter values, Θ, an integer, k, representing the number of folds, and a learning algorithm, A, which receives as input a as the hypothesis trained by this parameter on the entire training set. training set S = (x1, y1), . . . , (xm, ym) set of parameter values Θ learning algorithm A partition S into S1, S2, . . . , Sk"
824,825,"The cross validation method often works very well in practice. However, it orously understanding the exact behavior of cross validation is still an open dure gives a very good estimate of the true error. Other papers show that cross validation works for stable algorithms (we will study stability and its relation to"
825,826,"In most practical applications, we split the available examples into three sets."
826,827,"The ﬁrst set is used for training our algorithm and the second is used as a validation set for model selection. After we select the best model, we test the performance of the output predictor on the third set, which is often called the “test set.” The number obtained is used as an estimator of the true error of the"
827,828,"What to Do If Learning Fails eters. You used a validation set to tune the parameters and tested the learned factory. What went wrong then, and what should you do next?"
828,829,There are many elements that can be “ﬁxed.” The main approaches are listed
829,830,• Get a larger sample
830,831,• Change the hypothesis class by: – Completely changing it – Changing the parameters you consider
831,832,• Change the feature representation of the data
832,833,• Change the optimization algorithm used to apply your learning rule
833,834,"In order to ﬁnd the best remedy, it is essential ﬁrst to understand the cause ror of the learned predictor into approximation error and estimation error. The while the estimation error is deﬁned to be LD(hS) −LD(h⋆), where hS is the learned predictor (which is based on the training set S)."
834,835,"The approximation error of the class does not depend on the sample size or on the algorithm being used. It only depends on the distribution D and on the hypothesis class H. Therefore, if the approximation error is large, it will not help us to enlarge the training set size, and it also does not make sense to reduce the hypothesis class. What can be beneﬁcial in this case is to enlarge the hypothesis class or completely change it (if we have some alternative prior knowledge in the form of a diﬀerent hypothesis class). We can also consider applying the same hypothesis class but on a diﬀerent feature representation of the data (see"
835,836,"The estimation error of the class does depend on the sample size. Therefore, if we have a large estimation error we can make an eﬀort to obtain more training examples. We can also consider reducing the hypothesis class. However, it doesn’t make sense to enlarge the hypothesis class in that case."
836,837,Error Decomposition Using Validation
837,838,"We see that understanding whether our problem is due to approximation error or estimation error is very useful for ﬁnding the best remedy. In the previous section we saw how to estimate LD(hS) using the empirical risk on a validation set. However, it is more diﬃcult to estimate the approximation error of the class."
838,839,Model Selection and Validation
839,840,"Instead, we give a diﬀerent error decomposition, one that can be estimated from the train and validation sets."
840,841,"LD(hS) = (LD(hS) −LV (hS)) + (LV (hS) −LS(hS)) + LS(hS). that our algorithm suﬀers from “overﬁtting” while when the empirical risk term,"
841,842,"LS(hS), is large we say that our algorithm suﬀers from “underﬁtting.” Note that proximation errors. To illustrate this, consider the case in which H is a class of risk, LS(hS), and the approximation error, LD(h⋆), can be signiﬁcantly diﬀerent."
842,843,"Nevertheless, as we show later, the values of LS(hS) and (LV (hS)−LS(hS)) still provide us useful information."
843,844,Consider ﬁrst the case in which LS(hS) is large. We can write
844,845,"LS(hS) = (LS(hS) −LS(h⋆)) + (LS(h⋆) −LD(h⋆)) + LD(h⋆). since h⋆does not depend on S, the term (LS(h⋆)−LD(h⋆)) can be bounded quite that if LS(hS) is large then so is the approximation error, and the remedy to the failure of our algorithm should be tailored accordingly (as discussed previously)."
845,846,"It is possible that the approximation error of our class is small, yet the value of LS(hS) is large. For example, maybe we had a bug in our ERM implementation, and the algorithm returns a hypothesis hS that is not an ERM."
846,847,"It may also be the case that ﬁnding an ERM hypothesis is computationally hard, and our algorithm applies some heuristic trying to ﬁnd an approximate ERM. In some cases, it is hard to know how good hS is relative to an ERM hypothesis. But, sometimes it is possible at least to know whether there are better hypotheses."
847,848,"For example, in the next chapter we will study convex learning problems in which there are optimality conditions that can be checked to verify whether our optimization algorithm converged to an ERM solution. In other cases, the solution may depend on randomness in initializing the algorithm, so we can try diﬀerent randomly selected initial points to see whether better solutions pop out."
848,849,"Next consider the case in which LS(hS) is small. As we argued before, this does not necessarily imply that the approximation error is small. Indeed, consider two scenarios, in both of which we are trying to learn a hypothesis class of training set of m < d examples and the approximation error of the class is high."
849,850,"In the second scenario, we have a training set of m > 2d examples and the scenario in which the number of examples is always smaller than the VC dimension of the class. Right: This learning curve corresponds to the scenario in which the approximation error is zero and the number of examples is larger than the VC dimension of the class. distinguish between the two cases?"
850,851,"One possible way to distinguish between the two cases is by plotting learning curves. To produce a learning curve we train the algorithm on preﬁxes of the data of increasing sizes. For example, we can ﬁrst train the algorithm on the calculate the training error (on the preﬁx the algorithm is being trained on) and the validation error (on a predeﬁned validation set). Such learning curves can help us distinguish between the two aforementioned scenarios. In the ﬁrst as we didn’t really learn anything. In the second scenario the validation error will start as a constant but then should start decreasing (it must start decreasing"
851,852,"In general, as long as the approximation error is greater than zero we expect the training error to grow with the sample size, as a larger amount of data points makes it harder to provide an explanation for all of them. On the other hand, the validation error tends to decrease with the increase in sample size. If the train errors converge to the approximation error. Therefore, by extrapolating imation error, or at least to get a rough estimate on an interval in which the approximation error resides."
852,853,"Getting back to the problem of ﬁnding the best remedy for the failure of our algorithm, if we observe that LS(hS) is small while the validation error is large, then in any case we know that the size of our training set is not suﬃcient for learning the class H. We can then plot a learning curve. If we see that the"
853,854,"Model Selection and Validation validation error is starting to decrease then the best solution is to increase the number of examples (if we can aﬀord to enlarge the data). Another reasonable solution is to decrease the complexity of the hypothesis class. On the other hand, that the approximation error of H is good. It may be the case that increasing the training set size will not help us at all. Obtaining more data can still help us, as at some point we can see whether the validation error starts to decrease or whether the training error starts to increase. But, if more data is expensive, it may be better ﬁrst to try to reduce the complexity of the hypothesis class."
854,855,"To summarize the discussion, the following steps should be applied: completely change it, or change the feature representation of the data. whether the problem is estimation error or approximation error."
855,856,"If this is not possible, consider reducing the complexity of the hypothesis class. pothesis class or the feature representation of the data completely."
856,857,Model selection is the task of selecting an appropriate model for the learning task based on the data itself. We have shown how this can be done using the
857,858,"SRM learning paradigm or using the more practical approach of validation. If our learning algorithm fails, a decomposition of the algorithm’s error should be performed using learning curves, so as to ﬁnd the best remedy. examples and you would like to learn the class H = ∪k alternative approaches:"
858,859,"• Learn H on the m examples using the ERM rule α)m training examples using the ERM rule with respect to Hi, and let ˆh1, . . . , ˆhk be the resulting hypotheses. Second, apply the ERM rule with respect to the ﬁnite class {ˆh1, . . . , ˆhk} on the αm validation examples."
859,860,Describe scenarios in which the ﬁrst method is better than the second and
860,861,Convex Learning Problems
861,862,"In this chapter we introduce convex learning problems. Convex learning comprises an important family of learning problems, mainly because most of what we can learn eﬃciently falls into it. We have already encountered linear regression with the squared loss and logistic regression, which are convex problems, and indeed they can be learned eﬃciently. We have also seen nonconvex problems, such as in the unrealizable case."
862,863,"In general, a convex learning problem is a problem whose hypothesis class is a gin the chapter with some required deﬁnitions of convexity. Besides convexity, we will deﬁne Lipschitzness and smoothness, which are additional properties of the loss function that facilitate successful learning. We next turn to deﬁning convex learning problems and demonstrate the necessity for further constraints such as"
863,864,"Boundedness and Lipschitzness or Smoothness. We deﬁne these more restricted problems are learnable. These claims will be proven in the next two chapters, in which we will present two learning paradigms that successfully learn all problems by minimizing “surrogate” loss functions that are convex (instead of the original nonconvex loss function). Surrogate convex loss functions give rise to eﬃcient solutions but might increase the risk of the learned predictor."
864,865,"Convexity, Lipschitzness, and Smoothness"
865,866,"A set C in a vector space is convex if for any two vectors u, v in C, the line segment between u and v is contained in C. That"
866,867,"Examples of convex and nonconvex sets in R2 are given in the following. For the nonconvex sets, we depict two points in the set such that the line between the two points is not contained in the set."
867,868,Personal use only. Not for distribution. Do not post.
868,869,Let C be a convex set. A function f :
869,870,"In words, f is convex if for any u, v, the graph of f between u and v lies below the line segment joining f(u) and f(v). An illustration of a convex function,"
870,871,"The epigraph of a function f is the set epigraph(f) = {(x, β) : f(x) ≤β}."
871,872,"It is easy to verify that a function f is convex if and only if its epigraph is a epigraph, is given in the following."
872,873,Convex Learning Problems
873,874,"An important property of convex functions is that every local minimum of the function is also a global minimum. Formally, let B(u, r) = {v : ∥v −u∥≤r} be a ball of radius r centered around u. We say that f(u) is a local minimum of f such that u + α(v −u) ∈B(u, r) and therefore f(u) ≤f(u + α(v −u)) ."
874,875,"If f is convex, we also have that"
875,876,"Combining these two equations and rearranging terms, we conclude that f(u) ≤ f(v). Since this holds for every v, it follows that f(u) is also a global minimum"
876,877,"Another important property of convex functions is that for every w we can construct a tangent to f at w that lies below f everywhere. If f is diﬀerentiable, this tangent is the linear function l(u) = f(w) + ⟨∇f(w), u −w⟩, where ∇f(w) is the gradient of f at w, namely, the vector of partial derivatives of f, ∇f(w) = . That is, for convex diﬀerentiable functions, f(u) ≥f(w) + ⟨∇f(w), u −w⟩. f(w) + ⟨u −w, ∇f(w)⟩"
877,878,"If f is a scalar diﬀerentiable function, there is an easy way to check if it is f ′, f ′′ be its ﬁrst and second derivatives, respectively. Then, the following are"
878,879,"• The scalar function f(x) = x2 is convex. To see this, note that f ′(x) = 2x since the exponent function is a monotonically increasing function."
879,880,The following claim shows that the composition of a convex scalar function where the last inequality follows from the convexity of g.
880,881,"Convex Learning Problems linear function, and hence f is a convex function."
881,882,"Finally, the following lemma shows that the maximum of convex functions is convex and that a weighted sum of convex functions, with nonnegative weights, following functions from Rd to R are also convex."
882,883,The ﬁrst claim follows by
883,884,For the second claim
884,885,"The function g(x) = |x| is convex. To see this, note that g(x) = max{x, −x} and that both the function f1(x) = x and f2(x) = −x are convex."
885,886,The deﬁnition of Lipschitzness below is with respect to the Euclidean norm over
886,887,"Rd. However, it is possible to deﬁne Lipschitzness with respect to any norm. is diﬀerentiable, then by the mean value theorem we have f(w1) −f(w2) = f ′(u)(w1 −w2) , where u is some point between w1 and w2. It follows that if the derivative of f is everywhere bounded (in absolute value) by ρ, then the function is ρ-Lipschitz. inequality: For every x1, x2, |x1| −|x2| = |x1 −x2 + x2| −|x2| ≤|x1 −x2| + |x2| −|x2| = |x1 −x2|."
887,888,"Since this holds for both x1, x2 and x2, x1, we obtain that ||x1| −|x2|| ≤"
888,889,"• The function f(x) = x2 is not ρ-Lipschitz over R for any ρ. To see this, take |f(w1) −f(w2)| = |⟨v, w1 −w2⟩| ≤∥v∥∥w1 −w2∥."
889,890,The following claim shows that composition of Lipschitz functions preserves
890,891,"Lipschitz. Then, f is (ρ1ρ2)-Lipschitz. In particular, if g2 is the linear function, |f(w1) −f(w2)| = |g1(g2(w1)) −g1(g2(w2))|"
891,892,Convex Learning Problems
892,893,"The deﬁnition of a smooth function relies on the notion of gradient. Recall that vector of partial derivatives of f, namely, ∇f(w) = smooth if its gradient is β-Lipschitz; namely, for all v, w we have ∥∇f(v) −"
893,894,"It is possible to show that smoothness implies that for all v, w we have f(v) ≤f(w) + ⟨∇f(w), v −w⟩+ β"
894,895,"Recall that convexity of f implies that f(v) ≥f(w)+⟨∇f(w), v−w⟩. Therefore, when a function is both convex and smooth, we have both upper and lower bounds on the diﬀerence between the function and its ﬁrst order approximation. ranging terms, we obtain"
895,896,The following claim shows that a composition of a smooth scalar function over a linear function preserves smoothness.
896,897,"By the chain rule we have that ∇f(w) = g′(⟨w, x⟩+ b)x, where g′ is the ≤g(⟨w, x⟩+ b) + g′(⟨w, x⟩+ b)⟨v −w, x⟩+ β ≤g(⟨w, x⟩+ b) + g′(⟨w, x⟩+ b)⟨v −w, x⟩+ β"
897,898,"Convex Learning Problems instance space and a target space, Z = X ×Y, and H being a set of functions from"
898,899,"X to Y. However, H can be an arbitrary set. Indeed, throughout this chapter, we consider hypothesis classes H that are subsets of the Euclidean space Rd. a hypothesis in H by w. Now we can ﬁnally deﬁne convex learning problems:"
899,900,"A learning problem, (H, Z, ℓ), loss function, ℓ(·, z), is a convex function (where, for any z, ℓ(·, z) denotes the regression is a tool for modeling the relationship between some “explanatory” is a subset of Rd, for some d, and the label set Y is the set of real numbers. equivalently model the learning problem as a convex learning problem as follows."
900,901,Convex Learning Problems
901,902,"H to be the set of all such parameters, namely, H = Rd. The set of examples is"
902,903,"Clearly, the set H is a convex set. The loss function is also convex with respect"
903,904,"If ℓis a convex loss function and the class H is convex, then the tion problem (that is, a problem of minimizing a convex function over a convex"
904,905,Recall that the ERMH problem is deﬁned by
905,906,"Since, for a sample S = z1, . . . , zm, for every w, LS(w) = is a problem of minimizing a convex function subject to the constraint that the solution should be in a convex set."
906,907,"Under mild conditions, such problems can be solved eﬃciently using generic simple algorithm for minimizing convex functions."
907,908,Learnability of Convex Learning Problems
908,909,"We have argued that for many cases, implementing the ERM rule for convex learning problems can be done eﬃciently. But is convexity a suﬃcient condition for the learnability of a problem?"
909,910,"To make the quesion more speciﬁc: In VC theory, we saw that halfspaces in using the “discretization trick” that if the problem is of d parameters, it is learnable with a sample complexity being a function of d. That is, for a constant d, the problem should be learnable. So, maybe all convex learning problems over all convex learning problems over Rd are learnable. There is no contradiction to VC theory since VC theory only deals with binary classiﬁcation while here we consider a wide family of problems. There is also no contradiction to the “discretization trick” as there we assumed that the loss function is bounded and also assumed that a representation of each parameter using a ﬁnite number of bits suﬃces. As we will show later, under some additional restricting conditions that hold in many practical scenarios, convex problems are learnable. contradiction, that A is a successful PAC learner for this problem. That is, there deﬁne two distributions, and will show that A is likely to fail on at least one supported entirely on z2."
910,911,"Observe that for both distributions, the probability that all examples of the"
911,912,"D2, whereas for D1, the probability of this event is"
912,913,"Since we assume that A is a deterministic algorithm, upon receiving a training"
913,914,"A there exists a distribution on which A fails, which implies that the problem is esis class. In addition to the convexity requirement, we require that H will be bounded; namely, we assume that for some predeﬁned scalar B, every hypothesis"
914,915,"Boundedness and convexity alone are still not suﬃcient for ensuring that the problem is learnable, as the following example demonstrates. simplicity. A slightly more involved argument will show that nondeterministic algorithms will also fail to learn the problem."
915,916,Convex Learning Problems hypothesis class. It is easy to verify that H is convex. The argument will be distribution to be D1 and have that
916,917,"This example shows that we need additional assumptions on the learning problem, and this time the solution is in Lipschitzness or smoothness of the loss function. This motivates a deﬁnition of two families of learning problems, the following holds: that the instances are in a ball of radius ρ and we restrict the hypotheses to be homogenous linear functions deﬁned by a vector w whose norm is bounded by the following holds:"
917,918,"Note that we also required that the loss function is nonnegative. This is needed corresponds to a regression problem with the squared loss, where we assume that homogenous linear functions deﬁned by a vector w whose norm is bounded by B."
918,919,"We claim that these two families of learning problems are learnable. That is, the properties of convexity, boundedness, and Lipschitzness or smoothness of the loss function are suﬃcient for learnability. We will prove this claim in the next chapters by introducing algorithms that learn these problems successfully."
919,920,Surrogate Loss Functions
920,921,"As mentioned, and as we will see in the next chapters, convex problems can be learned eﬀﬁciently. However, in many cases, the natural loss function is not convex and, in particular, implementing the ERM rule is hard."
921,922,"This loss function is not convex with respect to w and indeed, when trying to minimize the empirical risk with respect to this loss function we might encounter"
922,923,"To circumvent the hardness result, one popular approach is to upper bound the nonconvex loss function by a convex surrogate loss function. As its name indicates, the requirements from a convex surrogate loss are as follows:"
923,924,Convex Learning Problems
924,925,"Once we have deﬁned the surrogate convex loss, we can learn the problem with respect to it. The generalization requirement from a hinge loss learner will have (w) = E(x,y)∼D[ℓhinge(w, (x, y))]. Using the surrogate property, we (A(S)), which yields"
925,926,"We can further rewrite the upper bound as follows: (w), which measures how"
926,927,• Estimation error: This is the error that results from the fact that we only receive a training set and do not observe the distribution D. We already
927,928,"• Optimization error: This is the term that measures the diﬀerence between the approximation error with respect inal loss. The optimization error is a result of our inability to minimize the training loss with respect to the original loss. The size of this error depends on the speciﬁc distribution of the data and on the speciﬁc surrogate loss learning algorithms for these families. We also introduced the notion of convex surrogate loss function, which enables us also to utilize the convex machinery for"
928,929,Bibliographic Remarks
929,930,There are several excellent books on convex analysis and optimization (Boyd & text of PAC learning. not a global minimum of LS.
930,931,"In the next chapter we show that from the statistical nostic PAC model). However, our main motivation to learn such problems resulted from the computational perspective – convex optimization is often eﬃciently solvable. Yet the goal of this exercise is to show that convexity"
931,932,Convex Learning Problems the set of all Turing machines. Deﬁne the loss function as follows. For every
932,933,"Regularization and Stability learning problems in these two families are learnable. For some learning problems of this type it is possible to show that uniform convergence holds; hence they are learnable using the ERM rule. However, this is not true for all learning problems of this type. Yet, we will introduce another learning rule and will show"
933,934,"The new learning paradigm we introduce in this chapter is called Regularized pirical risk and a regularization function. Intuitively, the regularization function ularization function is the structural risk minimization paradigm we discussed algorithm. An algorithm is considered stable if a slight change of its input does not change its output much. We will formally deﬁne the notion of stability (what put”) and prove its close relation to learnability. Finally, we will show that using rule for these families of learning problems."
934,935,"Regularized Loss Minimization imize the empirical risk and a regularization function. Formally, a regularization outputs a hypothesis in"
935,936,Regularized loss minimization shares similarities with minimum description length
936,937,Personal use only. Not for distribution. Do not post.
937,938,"Regularization and Stability tion, and the algorithm balances between low empirical risk and “simpler,” or “less complex,” hypotheses."
938,939,"There are many possible regularization functions one can use, reﬂecting some prior belief about the problem (similarly to the description language in Minimum"
939,940,"Description Length). Throughout this section we will focus on one of the most norm is the ℓ2 norm, ∥w∥= i . This yields the learning rule:"
940,941,"This type of regularization function is often called Tikhonov regularization. risk minimization, where the norm of w is a measure of its “complexity.” Recall that in the previous chapter we introduced the notion of bounded hypothesis classes. Therefore, we can deﬁne a sequence of hypothesis classes, H1 ⊂H2 ⊂ on i then the RLM rule is similar to the SRM rule for this sequence of nested"
941,942,"A diﬀerent interpretation of regularization is as a stabilizer. In the next section we deﬁne the notion of stability and prove that stable learning rules do not overﬁt. But ﬁrst, let us demonstrate the RLM rule for linear regression with the"
942,943,"Applying the RLM rule with Tikhonov regularization to linear regression with the squared loss, we obtain the following learning rule: obtain the set of linear equations to ridge regression becomes rithm and prevents overﬁtting. In particular, the analysis presented in the next"
943,944,The preceding theorem tells us how many examples are needed to guarantee that the expected value of the risk of the learned predictor will be of agnostic PAC learning we require that the risk of the learned predictor will algorithm with a bounded expected risk can be used to construct an agnostic
944,945,Stable Rules Do Not Overﬁt
945,946,"Intuitively, a learning algorithm is stable if a small change of the input to the algorithm does not change the output of the algorithm much. Of course, there are many ways to deﬁne what we mean by “a small change of the input” and what we mean by “does not change the output much”. In this section we deﬁne a speciﬁc notion of stability and prove that under this deﬁnition, stable rules do"
946,947,"Let A be a learning algorithm, let S = (z1, . . . , zm) be a training set of m examples, and let A(S) denote the output of A. The algorithm A suﬀers from overﬁtting if the diﬀerence between the true risk of its output, LD(A(S)), and the throughout this chapter we focus on the expectation (with respect to the choice of S) of this quantity, namely, ES[LD(A(S)) −LS(A(S))]. ditional example z′, let S(i) be the training set obtained by replacing the i’th nition of stability, “a small change of the input” means that we feed A with S(i) instead of with S. That is, we only replace one training example. We measure the eﬀect of this small change of the input on the output of A, by comparing the loss of the hypothesis A(S) on zi to the loss of the hypothesis A(S(i)) on zi. since in the ﬁrst term the learning algorithm does not observe the example zi while in the second term zi is indeed observed. If the preceding diﬀerence is very large we suspect that the learning algorithm might overﬁt. This is because the"
947,948,"Regularization and Stability learning algorithm drastically changes its prediction on zi if it observes it in the training set. This is formalized in the following theorem. quence of examples and let z′ be another i.i.d. example. Let U(m) be the uniform distribution over [m]. Then, for any learning algorithm,"
948,949,S∼Dm[LD(A(S)) −LS(A(S))] =
949,950,"Since S and z′ are both drawn i.i.d. from D, we have that for every i,"
950,951,"S,z′[ℓ(A(S), z′)] = E"
951,952,"S,z′[ℓ(A(S(i)), zi)]."
952,953,"On the other hand, we can write"
953,954,"Combining the two equations we conclude our proof. algorithm – changing a single example in the training set does not lead to a signiﬁcant change. Formally, not overﬁt is not necessarily a good learning algorithm – take, for example, an algorithm A that always outputs the same hypothesis. A useful algorithm should ﬁnd a hypothesis that on one hand ﬁts the training set (i.e., has a low empirical algorithm should both ﬁt the training set and at the same time be stable. As we set and being stable."
954,955,Tikhonov Regularization as a Stabilizer
955,956,In the previous section we saw that stable rules do not overﬁt. In this section we a stable algorithm. We will assume that the loss function is convex and that it is either Lipschitz or smooth.
956,957,"The main property of the Tikhonov regularization that we rely on is that it makes the objective of RLM strongly convex, as deﬁned in the following. convexity is given in the following ﬁgure. vex. In addition, it underscores an important property of strong convexity."
957,958,"The ﬁrst two points follow directly from the deﬁnition. To prove the last point, we divide the deﬁnition of strong convexity by α and rearrange terms to f(u + α(w −u)) −f(u)"
958,959,"We now turn to prove that RLM is stable. Let S = (z1, . . . , zm) be a training"
959,960,"Let A be the RLM rule, namely,"
960,961,Regularization and Stability
961,962,"On the other hand, for any v and u, and for all i, we have + ℓ(v, zi) −ℓ(u, zi) + ℓ(u, z′) −ℓ(v, z′) fS(A(S(i)))−fS(A(S)) ≤ℓ(A(S(i)), zi) −ℓ(A(S), zi) +ℓ(A(S), z′) −ℓ(A(S(i)), z′) + ℓ(A(S), z′) −ℓ(A(S(i)), z′) schitz or smooth loss functions. For both families of loss functions we show that"
962,963,RLM is stable and therefore it does not overﬁt.
963,964,"If the loss function, ℓ(·, zi), is ρ-Lipschitz, then by the deﬁnition of Lipschitzness, ℓ(A(S(i)), zi) −ℓ(A(S), zi) ≤ρ ∥A(S(i)) −A(S)∥. ℓ(A(S), z′) −ℓ(A(S(i)), z′) ≤ρ ∥A(S(i)) −A(S)∥."
964,965,"Since this holds for any S, z′, i we immediately obtain:"
965,966,Assume that the loss function is convex and ρ-Lipschitz.
966,967,"Smooth and Nonnegative Loss smoothness assumption we have that ℓ(A(S(i)), zi)−ℓ(A(S), zi) ≤⟨∇ℓ(A(S), zi), A(S(i))−A(S)⟩+β ℓ(A(S(i)), zi) −ℓ(A(S), zi) ≤∥∇ℓ(A(S), zi)∥∥A(S(i)) −A(S)∥+ β"
967,968,"2βℓ(A(S), zi) ∥A(S(i)) −A(S)∥+ β"
968,969,"By a symmetric argument it holds that, ℓ(A(S), z′) −ℓ(A(S(i)), z′)"
969,970,"2βℓ(A(S(i)), z′) ∥A(S(i)) −A(S)∥+ β"
970,971,"Regularization and Stability ℓ(A(S(i)), zi) −ℓ(A(S), zi)"
971,972,"2βℓ(A(S), zi) ∥A(S(i)) −A(S)∥+ β ℓ(A(S), zi) + ℓ(A(S(i)), z′) tion with respect to S, z′, i and noting that E[ℓ(A(S), zi)] = E[ℓ(A(S(i)), z′)] ="
972,973,"E[LS(A(S))], we conclude that:"
973,974,"Assume that the loss function is β-smooth and nonnegative. ℓ(A(S(i)), zi) −ℓ(A(S), zi) ℓ(A(S(i)), zi) −ℓ(A(S), zi)"
974,975,We can rewrite the expected risk of a learning algorithm as
975,976,S[LD(A(S)) −LS(A(S))].
976,977,"The ﬁrst term reﬂects how well A(S) ﬁts the training set while the second term reﬂects the diﬀerence between the true and empirical risks of A(S). As we have our goal is to minimize the risk of the algorithm, we need that the sum of both terms will be small."
977,978,"In the previous section we have bounded the stability term. We have shown that the stability term decreases as the regularization parameter, λ, increases. complexity tradeoﬀwe discussed previously in the book."
978,979,We now derive bounds on the empirical risk term for the RLM rule. Recall that the RLM rule is deﬁned as A(S) = argminw arbitrary vector w∗. We have
979,980,Taking expectation of both sides with respect to S and noting that ES[LS(w∗)] =
980,981,"LD(w∗), we obtain that"
981,982,S[LD(A(S)) −LS(A(S))].
982,983,"Assume that the loss function is convex and ρ-Lipschitz. pothesis with low risk, the bound tells us how many examples are needed so that"
983,984,"A(S) will be almost as good as w∗, had we known the norm of w∗. In practice, then for every distribution D,"
984,985,The preceding corollary holds for Lipschitz loss functions. If instead the loss derive an agnostic PAC learning guarantee.
985,986,Regularization and Stability
986,987,"Assume that the loss function is convex, β-smooth, and m , satisﬁes the following for all w∗: we obtain from the preceding that the expected true risk of A(S) is at most twice the expected empirical risk of A(S)."
987,988,"We introduced stability and showed that if an algorithm is stable then it does not problems, the RLM rule with Tikhonov regularization leads to a stable learning bounded problems are learnable using the RLM rule. The RLM paradigm is the basis for many popular learning algorithms, including ridge regression (which we discussed in this chapter) and support vector machines (which will be discussed"
988,989,"In the next chapter we will present Stochastic Gradient Descent, which gives us"
989,990,Bibliographic Remarks
990,991,"In the context of modern learning theory, the use of stability can be traced back ity of a learning algorithm with regard to small changes in the sample controls cal methods have been developed to introduce stability into learning algorithms, problems uniform convergence does not hold in a strong sense. distribution D it holds that nonnegative and rely on Markov’s inequality."
991,992,Suggest a procedure that agnostic PAC learns the problem with sample using A. On the basis of the previous question argue that the probability
992,993,Regularization and Stability
993,994,"This problem corresponds to an unsupervised learning task, meaning that we do not try to predict the label of x. Instead, what we try to do is to ﬁnd the “center of mass” of the distribution over B. However, there is a twist, modeled by the vectors α. Each example is a pair (x, α), where x is the instance x and α indicates which features of x are “active” and which are “turned oﬀ.” A hypothesis is a vector w representing the center of mass of the distribution, and the loss function is the squared Euclidean distance between x and w, but only with respect to the “active” elements of x."
994,995,• Show that this problem is learnable using the RLM rule with a sample complexity that does not depend on d.
995,996,"• Consider a distribution D over Z as follows: x is ﬁxed to be some x0, and"
996,997,Show that the rate of uniform convergence of this problem grows with
997,998,"Hint: Let m be a training set size. Show that if d ≫2m, then there is a high probability of sampling a set of examples such that there exists sample complexity of uniform convergence must grow with log(d)."
998,999,• Conclude that if we take d to inﬁnity we obtain a problem that is learnable but for which the uniform convergence property does not hold. Compare to the fundamental theorem of statistical learning.
999,1000,We say that a learning rule A is an AERM (Asymptotic Empirical Risk distribution D it holds that
1000,1001,Prove the following:
1001,1002,"Throughout the section we used the ℓ2 norm. In this exercise we generalize some of the results to general norms. Let ∥·∥be some arbitrary norm, and let f norm ∥· ∥. Let A be an RLM rule with respect to R, namely,"
1002,1003,"Assume that for every z, the loss function ℓ(·, z) is ρ-Lipschitz with respect to the same norm, namely, ℓ(w, z) −ℓ(v, z) ≤ρ ∥w −v∥."
1003,1004,-strongly convex with respect to the ℓ1 norm over Rd.
1004,1005,Stochastic Gradient Descent
1005,1006,"Recall that the goal of learning is to minimize the risk function, LD(h) ="
1006,1007,"Ez∼D[ℓ(h, z)]. We cannot directly minimize the risk function since it depends on the unknown distribution D. So far in the book, we have discussed learning methods that depend on the empirical risk. That is, we ﬁrst sample a training set S and deﬁne the empirical risk function LS(h). Then, the learner picks a hypothesis based on the value of LS(h). For example, the ERM rule tells us to pick the hypothesis that minimizes LS(h) over the hypothesis class, H. Or, in the previous chapter, we discussed regularized risk minimization, in which we pick a hypothesis that jointly minimizes LS(h) and a regularization function over h."
1007,1008,"In this chapter we describe and analyze a rather diﬀerent learning approach, focus on the important family of convex learning problems, and following the notation in that chapter, we will refer to hypotheses as vectors w that come from a convex hypothesis class, H. In SGD, we try to minimize the risk function LD(w) directly using a gradient descent procedure. Gradient descent is an iterative optimization procedure in which at each step we improve the solution by taking a step along the negative of the gradient of the function to be minimized at the current point. Of course, in our case, we are minimizing the risk function, and since we do not know D we also do not know the gradient of LD(w). SGD circumvents this problem by allowing the optimization procedure to take a step along a random direction, as long as the expected value of the direction is the negative of the gradient. And, as we shall see, ﬁnding a random direction whose expected value corresponds to the gradient is rather simple even though we do not know the underlying distribution D."
1008,1009,"The advantage of SGD, in the context of convex learning problems, over the regularized risk minimization learning rule is that SGD is an eﬃcient algorithm that can be implemented in a few lines of code, yet still enjoys the same sample complexity as the regularized risk minimization rule. The simplicity of SGD also allows us to use it in situations when it is not possible to apply methods that are based on the empirical risk, but this is beyond the scope of this book."
1009,1010,We start this chapter with the basic gradient descent algorithm and analyze its subgradient and show that gradient descent can be applied for nondiﬀerentiable
1010,1011,"Personal use only. Not for distribution. Do not post. the Stochastic Gradient Descent algorithm, along with several useful variants."
1011,1012,"We show that SGD enjoys an expected convergence rate similar to the rate of gradient descent. Finally, we turn to the applicability of SGD to learning"
1012,1013,"Before we describe the stochastic gradient descent method, we would like to describe the standard gradient descent approach for minimizing a diﬀerentiable convex function f(w). is the vector of partial derivatives of f, namely, ∇f(w) ="
1013,1014,"Gradient descent is an iterative algorithm. We start with an initial value of w negative of the gradient at the current point. That is, the update step is ent points in the direction of the greatest rate of increase of f around w(t), the algorithm makes a small step in the opposite direction, thus decreasing the value of the function. Eventually, after T iterations, the algorithm outputs the averaged vector, ¯w = turns out to be rather useful, especially when we generalize gradient descent to nondiﬀerentiable functions and to the stochastic case. tion. The gradient of f at w yields the ﬁrst order Taylor approximation of f mation lower bounds f, that is, f(u) ≥f(w) + ⟨u −w, ∇f(w)⟩."
1014,1015,"Therefore, for w close to w(t) we have that f(w) ≈f(w(t))+⟨w−w(t), ∇f(w(t))⟩."
1015,1016,"Hence we can minimize the approximation of f(w). However, the approximation might become loose for w, which is far away from w(t). Therefore, we would like to minimize jointly the distance between w and w(t) and the approximation of f around w(t). If the parameter η controls the tradeoﬀbetween the two terms, we obtain the update rule f(w(t)) + ⟨w −w(t), ∇f(w(t))⟩"
1016,1017,Solving the preceding by taking the derivative with respect to w and comparing
1017,1018,Stochastic Gradient Descent
1018,1019,"To analyze the convergence rate of the GD algorithm, we limit ourselves to themselves easily to this setting). Let w⋆be any vector and let B be an upper bound on ∥w⋆∥. It is convenient to think of w⋆as the minimizer of f(w), but the analysis that follows holds for every w⋆."
1019,1020,"We would like to obtain an upper bound on the suboptimality of our solution deﬁnition of ¯w, and using Jensen’s inequality, we have that"
1020,1021,"For every t, because of the convexity of f, we have that f(w(t)) −f(w⋆) ≤⟨w(t) −w⋆, ∇f(w(t))⟩."
1021,1022,"Combining the preceding we obtain ⟨w(t) −w⋆, ∇f(w(t))⟩."
1022,1023,"Let v1, . . . , vT be an arbitrary sequence of vectors. Any algorithm ρ2 T , then for every w⋆with ∥w⋆∥≤B we have"
1023,1024,"Using algebraic manipulations (completing the square), we obtain: where the last equality follows from the deﬁnition of the update rule. Summing the equality over t, we have ∥w⋆∥by B, ∥vt∥by ρ, dividing by T, and plugging in the value of η."
1024,1025,Stochastic Gradient Descent satisfy the lemma’s conditions and achieve the following corollary:
1025,1026,"Let f be a convex, ρ-Lipschitz function, and let w⋆∈argmin{w:∥w∥≤B} f(w)."
1026,1027,"If we run the GD algorithm on f for T steps with η = ρ2 T , then the output"
1027,1028,"GD algorithm for a number of iterations that satisﬁes alize the discussion beyond diﬀerentiable functions. We will show that the GD gradient of f(w) at w(t), instead of the gradient."
1028,1029,"To motivate the deﬁnition of subgradients, recall that for a convex function f, the gradient at w deﬁnes the slope of a tangent that lies below f, that is, f(u) ≥f(w) + ⟨u −w, ∇f(w)⟩."
1029,1030,"The existence of a tangent that lies below f is an important property of convex functions, which is in fact an alternative characterization of convexity. f(u) ≥f(w) + ⟨u −w, v⟩."
1030,1031,"The proof of this lemma can be found in many convex analysis textbooks (e.g., called a subgradient of f at w. The set of subgradients of f at w is called the diﬀerential set and denoted ∂f(w)."
1031,1032,"For scalar functions, a subgradient of a convex function f at w is a slope of a line that touches f at w and is not above f elsewhere. f(w) + ⟨u −w, ∇f(w)⟩ a convex function, the tangent lower bounds f. Right: Illustration of several subgradients of a nondiﬀerentiable convex function."
1032,1033,Calculating Subgradients
1033,1034,"How do we construct subgradients of a given convex function? If a function is diﬀerentiable at a point w, then the diﬀerential set is trivial, as the following"
1034,1035,"If f is diﬀerentiable at w then ∂f(w) contains a single element – the gradient of f at w, ∇f(w). the diﬀerential set for the diﬀerentiable parts of f, and the only point that dients at a given point, as one member of this set would suﬃce. The following"
1035,1036,"Since gj is convex we have that for all u gj(u) ≥gj(w) + ⟨u −w, ∇gj(w)⟩."
1036,1037,"Since g(w) = gj(w) and g(u) ≥gj(u) we obtain that g(u) ≥g(w) + ⟨u −w, ∇gj(w)⟩, which concludes our proof."
1037,1038,Stochastic Gradient Descent
1038,1039,Recall the hinge loss function
1039,1040,To calculate a subgradient of the hinge loss at some w we rely on the preceding claim and obtain that the vector v deﬁned in the following is a subgradient of the hinge loss at w:
1040,1041,Subgradients of Lipschitz Functions |f(u) −f(v)| ≤ρ ∥u −v∥.
1041,1042,"The following lemma gives an equivalent deﬁnition using norms of subgradients. f(w) −f(u) ≤⟨v, w −u⟩. f(w) −f(u) ≤⟨v, w −u⟩≤∥v∥∥w −u∥≤ρ ∥w −u∥."
1042,1043,An analogous argument can show that f(u) −f(w) ≤ρ ∥w −u∥. Hence f is
1043,1044,"On the other hand, from the Lipschitzness of f we have"
1044,1045,Combining the two inequalities we conclude that ∥v∥≤ρ.
1045,1046,"The gradient descent algorithm can be generalized to nondiﬀerentiable functions by using a subgradient of f(w) at w(t), instead of the gradient. The analysis of true for subgradients as well. gradient descent algorithm (right). The function to be minimized is"
1046,1047,Stochastic Gradient Descent (SGD)
1047,1048,"In stochastic gradient descent we do not require the update direction to be based exactly on the gradient. Instead, we allow the direction to be a random vector and only require that its expected value at each iteration will equal the gradient direction. Or, more generally, we require that the expected value of the random vector will be a subgradient of the function at the current vector."
1048,1049,Stochastic Gradient Descent (SGD) for minimizing choose vt at random from a distribution such that E[vt | w(t)] ∈∂f(w(t))
1049,1050,"An illustration of stochastic gradient descent versus gradient descent is given it is easy to ﬁnd a random vector whose expectation is a subgradient of the risk stochastic case, in which only the expectation of vt is in ∂f(w(t)), we cannot"
1050,1051,"Stochastic Gradient Descent subgradient of f at w(t), we can still derive a similar bound on the expected output of stochastic gradient descent. This is formalized in the following theorem."
1051,1052,Assume that SGD is run for T iterations with η = ρ2 T . Assume also that for
1052,1053,E [f( ¯w)] −f(w⋆) ≤B ρ
1053,1054,SGD algorithm for a number of iterations that satisﬁes
1054,1055,"Let us introduce the notation v1:t to denote the sequence v1, . . . , vt. v1:T[f( ¯w) −f(w⋆)] ≤E"
1055,1056,By taking expectation of the bound in the lemma we have
1056,1057,It is left to show that which we will hereby prove.
1057,1058,"Using the linearity of the expectation we have v1:T[⟨w(t) −w⋆, vt⟩]."
1058,1059,"Next, we recall the law of total expectation: For every two random variables α, β, v1:T[⟨w(t) −w⋆, vt⟩] = E v1:t[⟨w(t) −w⋆, vt⟩]"
1059,1060,"Overall, we have shown that v1:T[⟨w(t) −w⋆, vt⟩] ≥ v1:T[f(w(t)) −f(w⋆)] ."
1060,1061,"Summing over t, dividing by T, and using the linearity of expectation, we get"
1061,1062,In this section we describe several variants of Stochastic Gradient Descent.
1062,1063,Adding a Projection Step
1063,1064,"In the previous analyses of the GD and SGD algorithms, we required that the norm of w⋆will be at most B, which is equivalent to requiring that w⋆is in the set H = {w : ∥w∥≤B}. In terms of learning, this means restricting ourselves to the gradient (or its expected direction) might result in stepping out of this bound, and there is even no guarantee that ¯w satisﬁes it. We show in the following how to overcome this problem while maintaining the same convergence rate. update rule, where we ﬁrst subtract a subgradient from the current value of w and then project the resulting vector onto H. Formally,"
1064,1065,The projection step replaces the current value of w by the vector in H closest
1065,1066,"Clearly, the projection step guarantees that w(t) ∈H for all t. Since H is of SGD with projections remains the same. This is based on the following lemma."
1066,1067,"Let H be a closed convex set and let v be the projection of w onto H, namely,"
1067,1068,Stochastic Gradient Descent
1068,1069,"Therefore, from the optimality of v we obtain"
1069,1070,"Rearranging, we obtain"
1070,1071,"Equipped with the preceding lemma, we can easily adapt the analysis of SGD to the case in which we add projection steps on a closed and convex set. Simply note that for every t, of the analysis follows directly."
1071,1072,"Another variant of SGD is decreasing the step size as a function of t. That is, rather than updating with a constant η, we use ηt. For instance, we can set we are closer to the minimum of the function, we take our steps more carefully, so as not to “overshoot” the minimum."
1072,1073,Other Averaging Techniques
1073,1074,"We have set the output vector to be ¯w = a weighted average of the last few iterates. These more sophisticated averaging schemes can improve the convergence speed in some situations, such as in the case of strongly convex functions deﬁned in the following."
1074,1075,"In this section we show a variant of SGD that enjoys a faster convergence rate for of strong convexity in the previous chapter). We rely on the following claim, ⟨w −u, v⟩≥f(w) −f(u) + λ"
1075,1076,Choose a random vector vt s.t. E[vt|w(t)] ∈∂f(w(t))
1076,1077,"Let ∇(t) = E[vt|w(t)]. Since f is strongly convex and ∇(t) is in the subgradient set of f at w(t) we have that ⟨w(t) −w⋆, ∇(t)⟩≥f(w(t)) −f(w⋆) + λ"
1077,1078,"Stochastic Gradient Descent = 2ηt⟨w(t) −w⋆, vt⟩−η2 summing over t we obtain (E[f(w(t))] −f(w⋆)) ≤ρ2"
1078,1079,The theorem follows from the preceding by dividing by T and using Jensen’s in which the log(T) term is eliminated for a variant of the algorithm in which
1079,1080,We have so far introduced and analyzed the SGD algorithm for general convex functions. Now we shall consider its applicability to learning tasks.
1080,1081,SGD for Risk Minimization
1081,1082,Recall that in learning we face the problem of minimizing the risk function
1082,1083,"We have seen the method of empirical risk minimization, where we minimize the empirical risk, LS(w), as an estimate to minimizing LD(w). SGD allows us to take a diﬀerent approach and minimize LD(w) directly. Since we do not know"
1083,1084,"D, we cannot simply calculate ∇LD(w(t)) and minimize it with the GD method."
1084,1085,"With SGD, however, all we need is to ﬁnd an unbiased estimate of the gradient of"
1085,1086,"LD(w), that is, a random vector whose conditional expected value is ∇LD(w(t))."
1086,1087,We shall now see how such an estimate can be easily constructed.
1087,1088,"For simplicity, let us ﬁrst consider the case of diﬀerentiable loss functions."
1088,1089,"Hence the risk function LD is also diﬀerentiable. The construction of the random vector vt will be as follows: First, sample z ∼D. Then, deﬁne vt to be the gradient of the function ℓ(w, z) with respect to w, at the point w(t). Then, by the linearity of the gradient we have z∼D[∇ℓ(w(t), z)] = ∇E z∼D[ℓ(w(t), z)] = ∇LD(w(t))."
1089,1090,"The gradient of the loss function ℓ(w, z) at w(t) is therefore an unbiased estimate of the gradient of the risk function LD(w(t)) and is easily constructed by sampling a single fresh example z ∼D at each iteration t."
1090,1091,"The same argument holds for nondiﬀerentiable loss functions. We simply let vt be a subgradient of ℓ(w, z) at w(t). Then, for every u we have ℓ(u, z) −ℓ(w(t), z) ≥⟨u −w(t), vt⟩."
1091,1092,Taking expectation on both sides with respect to z ∼D and conditioned on the value of w(t) we obtain
1092,1093,"LD(u) −LD(w(t)) = E[ℓ(u, z) −ℓ(w(t), z)|w(t)] ≥E[⟨u −w(t), vt⟩|w(t)] = ⟨u −w(t), E[vt|w(t)]⟩."
1093,1094,It follows that E[vt|w(t)] is a subgradient of LD(w) at w(t).
1094,1095,"To summarize, the stochastic gradient descent framework for minimizing the"
1095,1096,Stochastic Gradient Descent (SGD) for minimizing
1096,1097,Stochastic Gradient Descent
1097,1098,"LD(w) with a number of iterations (i.e., number of examples) ρ2 T , then the output of SGD satisﬁes"
1098,1099,"It is interesting to note that the required sample complexity is of the same order of magnitude as the sample complexity guarantee we derived for regularized loss minimization. In fact, the sample complexity of SGD is even better than what"
1099,1100,In the previous chapter we saw that the regularized loss minimization rule also the SGD algorithm can be also used for such problems.
1100,1101,"Assume that for all z, the loss function ℓ(·, z) is convex, β- smooth, and nonnegative. Then, if we run the SGD algorithm for minimizing"
1101,1102,"LD(w) we have that for every w⋆, samples of the SGD algorithm, let ft(·) = ℓ(·, zt), and note that vt = ∇ft(w(t))."
1102,1103,"For all t, ft is a convex function and therefore ft(w(t))−ft(w⋆) ≤⟨vt, w(t)−w⋆⟩. (ft(w(t)) −ft(w⋆)) ≤"
1103,1104,"Dividing by T and rearranging, we obtain"
1104,1105,"Next, we take expectation of the two sides of the preceding equation with respect to z1, . . . , zT . Clearly, E[ft(w⋆)] = LD(w⋆). In addition, using the same argument"
1105,1106,Combining all we conclude our proof.
1106,1107,As a direct corollary we obtain:
1107,1108,"SGD for Regularized Loss Minimization as regularized loss minimization. However, on some distributions, regularized loss minimization may yield a better solution. Therefore, in some cases we may want to solve the optimization problem associated with regularized loss minimization,"
1108,1109,"Since we are dealing with convex learning problems in which the loss function is convex, the preceding problem is also a convex optimization problem that can be solved using SGD as well, as we shall see in this section."
1109,1110,"To apply this algorithm, we only need to ﬁnd a way to construct an unbiased estimate of a subgradient of f at w(t). This is easily done by noting that if"
1110,1111,"To analyze the resulting algorithm, we ﬁrst rewrite the update rule (assuming"
1111,1112,Stochastic Gradient Descent that H = Rd and therefore the projection step does not matter) as follows
1112,1113,"If we assume that the loss function is ρ-Lipschitz, it follows that for all t we have ∥vt∥≤ρ and therefore ∥λw(t)∥≤ρ, which yields"
1113,1114,"E[f( ¯w)] −f(w⋆) ≤4ρ2 rithms, along with several of their variants. We have analyzed their convergence rate and calculated the number of iterations that would guarantee an expected shown that by using SGD we can directly minimize the risk function. We do so by sampling a point i.i.d from D and using a subgradient of the loss of the current hypothesis w(t) at this point as an unbiased estimate of the gradient (or a subgradient) of the risk function. This implies that a bound on the number of iterations also yields a sample complexity bound. Finally, we have also shown how to apply the SGD method to the problem of regularized risk minimization."
1114,1115,In future chapters we show how this yields extremely simple solvers to some optimization problems associated with regularized risk minimization.
1115,1116,"Bibliographic Remarks in the context of stochastic optimization. See, for example, (Nemirovski & Yudin"
1116,1117,"The bound we have derived for strongly convex function is due to Hazan, among all vectors that satisfy the preceding requirement. Let R = maxi ∥xi∥."
1117,1118,"• Show how to calculate a subgradient of f. pare the algorithm and the analysis to the Batch Perceptron algorithm variable step size, ηt ="
1118,1119,Support Vector Machines
1119,1120,In this chapter and the next we discuss a very useful machine learning tool: the support vector machine paradigm (SVM) for learning linear predictors in high dimensional feature spaces. The high dimensionality of the feature space raises both sample complexity and computational complexity challenges.
1120,1121,"The SVM algorithmic paradigm tackles the sample complexity challenge by searching for “large margin” separators. Roughly speaking, a halfspace separates a training set with a large margin if all the examples are not only on the correct side of the separating hyperplane but also far away from it. Restricting the algorithm to output a large margin separator can yield a small sample complexity even if the dimensionality of the feature space is high (and even inﬁnite). We introduce the concept of margin and relate it to the regularized loss minimization paradigm as well as to the convergence rate of the Perceptron algorithm."
1121,1122,"In the next chapter we will tackle the computational complexity challenge using the idea of kernels. a halfspace, (w, b), such that yi = sign(⟨w, xi⟩+ b) for all i. Alternatively, this condition can be rewritten as error is zero, which is the minimum possible error). For any separable training sample, there are many ERM halfspaces. Which one of them should the learner"
1122,1123,"Consider, for example, the training set described in the picture that follows."
1123,1124,"Personal use only. Not for distribution. Do not post. amples, our intuition would probably lead us to prefer the black hyperplane over the green one. One way to formalize this intuition is using the concept of margin."
1124,1125,"The margin of a hyperplane with respect to a training set is deﬁned to be the minimal distance between a point in the training set and the hyperplane. If a hyperplane has a large margin, then it will still separate the training set even if we slightly perturb each instance."
1125,1126,"We will see later on that the true error of a halfspace can be bounded in terms of the margin it has over the training sample (the larger the margin, the smaller the error), regardless of the Euclidean dimension in which this halfspace resides. formally, we ﬁrst express the distance between a point x to a hyperplane using the parameters deﬁning the halfspace."
1126,1127,The distance between a point x and the hyperplane deﬁned by
1127,1128,The distance between a point x and the hyperplane is deﬁned as
1128,1129,"Taking v = x −(⟨w, x⟩+ b)w we have that ∥x −v∥= |⟨w, x⟩+ b| ∥w∥= |⟨w, x⟩+ b|."
1129,1130,"Hence, the distance is at most |⟨w, x⟩+ b|. Next, take any other point u on the where the last equality is because ⟨w, v⟩= ⟨w, u⟩= −b. Hence, the distance"
1130,1131,"Support Vector Machines between x and u is at least the distance between x and v, which concludes our"
1131,1132,"On the basis of the preceding claim, the closest point in the training set to the input: (x1, y1), . . . , (xm, ym) for w of minimal norm among all the vectors that separate the data and for but now the units in which we measure the margin scale with the norm of w."
1132,1133,"Therefore, ﬁnding the largest margin halfspace boils down to ﬁnding w whose norm is minimal. Formally: yi(⟨w⋆, xi⟩+ b⋆) ≥γ⋆"
1133,1134,"Hence, the pair ( w⋆ γ⋆) satisﬁes the conditions of the quadratic optimization convex quadratic function and the constraints are linear inequalities. ∥w0∥yi(⟨w0, xi⟩+ b0) ≥"
1134,1135,"It is often more convenient to consider homogenous halfspaces, namely, halfspaces that pass through the origin and are thus deﬁned by sign(⟨w, x⟩), where the bias halfspaces to the problem of learning homogenous halfspaces by adding one more of the weight vector) as well. However, regularizing b usually does not make a signiﬁcant diﬀerence to the sample complexity. sample complexity of learning halfspaces grows with the dimensionality of the problem. Furthermore, the fundamental theorem of learning tells us that if the"
1135,1136,"To overcome this problem, we will make an additional assumption on the underlying data distribution. In particular, we will deﬁne a “separability with margin γ” assumption and will show that if the data is separable with margin follows that even if the dimensionality is very large (or even inﬁnite), as long as the data adheres to the separability with margin assumption we can still have a small sample complexity. There is no contradiction to the lower bound given in the fundamental theorem of learning because we are now making an additional assumption on the underlying data distribution."
1136,1137,"Before we formally deﬁne the separability with margin assumption, there is a scaling issue we need to resolve. Suppose that a training set S = (x1, y1), . . . , (xm, ym)"
1137,1138,"Support Vector Machines ple scaling of the data can make it separable with an arbitrarily large margin. It follows that in order to give a meaningful deﬁnition of margin we must take into account the scale of the examples as well. One way to formalize this is using the deﬁnition that follows. b⋆) ≥γ and ∥x∥≤ρ. Similarly, we say that D is separable with a (γ, ρ)-margin using a homogenous halfspace if the preceding holds with a halfspace of the form separability with margin assumption using a homogenous halfspace. Then, with and analyzed the Perceptron algorithm for ﬁnding an ERM hypothesis with the number of updates the Perceptron might make on a given training set. It is the radius of examples and γ is the margin. violated for some of the examples in the training set. This can be modeled by introducing nonnegative slack variables, ξ1, . . . , ξm, and replacing each constraint minimizes the norm of w (corresponding to the margin) and the average of ξi (corresponding to the violations of the constraints). The tradeoﬀbetween the two input: (x1, y1), . . . , (xm, ym)"
1138,1139,Recall the deﬁnition of the hinge loss:
1139,1140,"Given (w, b) and a training set S, the averaged hinge loss on S is denoted by ((w, b)). Now, consider the regularized loss minimization problem: ξi = ℓhinge((w, b), (xi, yi)) for all i, and the claim follows. only for training errors but also for large norm. halfspace, where the bias term b is set to be zero, which yields the following optimization problem:"
1140,1141,Support Vector Machines a generalization bound for the regularized loss minimization framework assuming that the loss function is convex and Lipschitz. We have already shown that the hinge loss is convex so it is only left to analyze the Lipschitzness of the hinge
1141,1142,"It is easy to verify that any subgradient of f at w is of the form αx where space as a function of the norm of that halfspace, independently of the Euclidean dimension of the space over which the halfspace is deﬁned. This becomes highly signiﬁcant when we learn via embeddings into high dimensional feature spaces, as we will consider in the next chapter."
1142,1143,"The condition that X will contain vectors with a bounded norm follows from the requirement that the loss function will be Lipschitz. This is not just a technicality. As we discussed before, separation with large margin deed, without a constraint on the scale, we can always enlarge the margin by multiplying all instances by a large scalar. dimension of the instance space. Instead, the bounds depend on the norm of the examples, ρ, the norm of the halfspace B (or equivalently the margin parameter γ) and, in the nonseparable case, the bounds also depend on the minimum hinge homogenous halfspaces is d, which implies that the error of an ERM hypothesis"
1143,1144,"Consider the problem of learning to classify a short text document according to its topic, say, whether the document is about sports or not. We ﬁrst need to dimension d to be the number of words in the dictionary. Given a document, problem, the value of ρ2 will be the maximal number of distinct words in a given"
1144,1145,"A halfspace for this problem assigns weights to words. It is natural to assume that by assigning positive and negative weights to a few dozen words we will be able to determine whether a given document is about sports or not with reasonable accuracy. Therefore, for this problem, the value of B2 can be set to therefore shown a problem in which there can be an order of magnitude diﬀerence between learning a halfspace with the SVM rule and learning a halfspace using the vanilla ERM rule."
1145,1146,"Of course, it is possible to construct problems in which the SVM bound will be worse than the VC bound. When we use SVM, we in fact introduce another tive bias can signiﬁcantly decrease our estimation error, it can also enlarge the approximation error. we minimize the hinge loss. As we have shown in the previous subsection, the with respect to the hinge loss while the approximation error in VC bounds is that of the hinge loss."
1146,1147,It is not possible to derive bounds that involve the estimation error term
1147,1148,"Support Vector Machines insensitive, and therefore there is no meaning to the norm of w or its margin function that on one hand it is scale sensitive and thus enjoys the estimation option is the ramp loss, deﬁned as penalize examples that are separated with margin. The diﬀerence between the classiﬁed but not with a signiﬁcant margin. Generalization bounds for the ramp"
1148,1149,"The reason SVM relies on the hinge loss and not on the ramp loss is that the hinge loss is convex and, therefore, from the computational point of view, minimizing the hinge loss can be performed eﬃciently. In contrast, the problem of minimizing the ramp loss is computationally intractable."
1149,1150,Optimality Conditions and “Support Vectors”*
1150,1151,"The name “Support Vector Machine” stems from the fact that the solution of therefore called support vectors. To see this, we rely on Fritz John optimality where f, g1, . . . , gm are diﬀerentiable. Then, there exists α ∈Rm such that"
1151,1152,"Historically, many of the properties of SVM have been obtained by considering"
1152,1153,We start by rewriting the problem in an equivalent form as follows. Consider
1153,1154,Now suppose that we ﬂip the order of min and max in the above equation. This
1154,1155,"The preceding inequality is called weak duality. It turns out that in our case, strong duality also holds; namely, the inequality holds with equality. Therefore,"
1155,1156,"We can simplify the dual problem by noting that once α is ﬁxed, the optimization"
1156,1157,"Support Vector Machines problem with respect to w is unconstrained and the objective is diﬀerentiable; thus, at the optimum, the gradient equals zero:"
1157,1158,"This shows us that the solution must be in the linear span of the examples, a fact we will use later to derive SVM with kernels. Plugging the preceding into"
1158,1159,Rearranging yields the dual problem
1159,1160,"Note that the dual problem only involves inner products between instances and erty is important when implementing SVM with kernels, as we will discuss in"
1160,1161,"In this section we describe a very simple algorithm for solving the optimization where vj is a subgradient of the loss function at w(j) on the random example chosen at iteration j. For the hinge loss, given an example (x, y), we can choose vj"
1161,1162,"Choose i uniformly at random from [m] not assume separability of the data and allows the constraints to be violated to some extent. The sample complexity for both types of SVM is diﬀerent from the sample complexity of straightforward halfspace learning, as it does not depend on the dimension of the domain but rather on parameters such as the maximal in the next chapter, where we will discuss the embedding of the given domain into some high dimensional feature space as means for enriching our hypothesis class. Such a procedure raises computational and sample complexity problems."
1162,1163,"The latter is solved by using SVM, whereas the former can be solved by using"
1163,1164,"SVM with kernels, as we will see in the next chapter."
1164,1165,Bibliographic Remarks
1165,1166,Support Vector Machines is equivalent to the following formulation: arable with a margin γ and such that all the instances are within a ball of radius ρ. Prove that the maximal number of updates the Batch Perceptron
1166,1167,"In the previous chapter we described the SVM paradigm for learning halfspaces in high dimensional feature spaces. This enables us to enrich the expressive power of halfspaces by ﬁrst mapping the data into a high dimensional feature space, and then learning a linear predictor in that space. This is similar to the potheses. While this approach greatly extends the expressiveness of halfspace lenges. In the previous chapter we tackled the sample complexity issue using the concept of margin. In this chapter we tackle the computational complexity challenge using the method of kernels."
1167,1168,"We start the chapter by describing the idea of embedding the data into a high dimensional feature space. We then introduce the idea of kernels. A kernel is a type of a similarity measure between instances. The special property of kernel similarities is that they can be viewed as inner products in some Hilbert space tually embedded. We introduce the “kernel trick” that enables computationally sional representation of the domain instances. Kernel based learning algorithms, tools. Their success may be attributed both to being ﬂexible for accommodating domain speciﬁc prior knowledge and to having a well developed set of eﬃcient implementation algorithms."
1168,1169,Embeddings into Feature Spaces ing training set is not separable by a halfspace.
1169,1170,"To make the class of halfspaces more expressive, we can ﬁrst map the original instance space into another space (possibly of a higher dimension) and then learn a halfspace in that space. For example, consider the example mentioned previously. Instead of learning a halfspace in the original representation let us"
1170,1171,Personal use only. Not for distribution. Do not post.
1171,1172,"We use the term feature space to denote the range of ψ. After applying ψ the data can be easily explained using the halfspace h(x) = sign(⟨w, ψ(x)⟩−b),"
1172,1173,"The basic paradigm is as follows: for some feature space F, that will usually be Rn for some n (however, the range of such a mapping can be any Hilbert space, including such spaces of inﬁnite dimension, as we will show later). image sequence ˆS = (ψ(x1), y1), . . . , (ψ(xm), ym)."
1173,1174,"Note that, for every probability distribution D over X × Y, we can readily deﬁne its image probability distribution Dψ over F × Y by setting, for every over the feature space, LDψ(h) = LD(h ◦ψ), where h ◦ψ is the composition of h"
1174,1175,"The success of this learning paradigm depends on choosing a good ψ for a given learning task: that is, a ψ that will make the image of the data distribution (close to being) linearly separable in the feature space, thus making the resulting algorithm a good learner for a given task. Picking such an embedding requires prior knowledge about that task. However, often some generic mappings that enable us to enrich the class of halfspaces and extend its expressiveness are used."
1175,1176,"One notable example is polynomial mappings, which are a generalization of the ψ we have seen in the previous example."
1176,1177,Recall that the prediction of a standard halfspace classiﬁer on an instance x
1177,1178,"In that case, p(x) = Pk of the polynomial we need to learn. We can rewrite p(x) = ⟨w, ψ(x)⟩where learning a k degree polynomial over R can be done by learning a linear mapping than halfspaces. We have seen at the beginning of this chapter an example in which the training set, in its original domain (X = R), cannot be separable"
1178,1179,"So, while the classiﬁer is always linear in the feature space, it can have highly nonlinear behavior on the original space from which instances were sampled. any ﬁnite d. But there are also inﬁnite dimensional Hilbert spaces (as we shall see later on in this chapter)."
1179,1180,"The bottom line of this discussion is that we can enrich the class of halfspaces by ﬁrst applying a nonlinear mapping, ψ, that maps the instance space into some feature space, and then learning a halfspace in that feature space. However, if large, we need many more samples in order to learn a halfspace in the range of ψ. Second, from the computational point of view, performing calculations in the high dimensional space might be too costly. In fact, even the representation of the vector w in the feature space can be unrealistic. The ﬁrst issue can be tackled using the paradigm of large margin (or low norm predictors), as we already discussed in the previous chapter in the context of the SVM algorithm."
1180,1181,In the following section we address the computational issue.
1181,1182,"We have seen that embedding the input space into some high dimensional feature space makes halfspace learning more expressive. However, the computational complexity of such learning may still pose a serious hurdle – computing linear separators over very high dimensional data may be computationally expensive."
1182,1183,"The common solution to this concern is kernel based learning. The term “kernels” is used in this context to describe inner products in the feature space. Given an embedding ψ of some domain space X into some Hilbert space, we deﬁne the kernel function K(x, x′) = ⟨ψ(x), ψ(x′)⟩. One can think of K as specifying similarity between instances and of the embedding ψ as mapping the domain set complete if all Cauchy sequences in the space converge."
1183,1184,"In our case, the norm ∥w∥is deﬁned by the inner product ⟨w, w⟩. The reason we require the range of ψ to be in a Hilbert space is that projections in a Hilbert space are well deﬁned. In particular, if M is a linear subspace of a Hilbert space, then every x in the"
1184,1185,X into a space where these similarities are realized as inner products. It turns out that many learning algorithms for halfspaces can be carried out just on the basis of the values of the kernel function over pairs of domain points. The main advantage of such algorithms is that they implement linear separators in high dimensional feature spaces without having to specify points in that space or expressing the embedding ψ explicitly. The remainder of this section is devoted to constructing such algorithms.
1185,1186,"In the previous chapter we saw that regularizing the norm of w yields a small estingly, as we show later, regularizing the norm of w is also helpful in overcoming timization problem we have derived in the previous chapter are instances of the following general problem: w (f (⟨w, ψ(x1)⟩, . . . , ⟨w, ψ(xm)⟩) + R(∥w∥)), for all i, and f(a1, . . . , am) = ∞otherwise."
1186,1187,"Assume that ψ is a mapping from X to a Hilbert space. Then, there exists a vector α ∈Rm such that w = Pm element of a Hilbert space, we can rewrite w⋆as thus ∥w∥≤∥w⋆∥. Since R is nondecreasing we obtain that R(∥w∥) ≤R(∥w⋆∥)."
1187,1188,"Additionally, for all i we have that ⟨w, ψ(xi)⟩= ⟨w⋆−u, ψ(xi)⟩= ⟨w⋆, ψ(xi)⟩, f (⟨w, ψ(x1)⟩, . . . , ⟨w, ψ(xm)⟩) = f (⟨w⋆, ψ(x1)⟩, . . . , ⟨w⋆, ψ(xm)⟩) . than the objective at w⋆and therefore w is also an optimal solution. Since respect to the coeﬃcients α instead of the coeﬃcients w as follows. Writing"
1188,1189,"Let K(x, x′) = ⟨ψ(x), ψ(x′)⟩be a function that implements the kernel function the equivalent problem αjK(xj, x1), . . . , direct access to elements in the feature space. The only thing we should know is how to calculate inner products in the feature space, or equivalently, to calculate value of the m × m matrix G s.t. Gi,j = K(xi, xj), which is often called the where (Gα)i is the i’th element of the vector obtained by multiplying the Gram programming and hence can be solved eﬃciently. In the next section we describe"
1189,1190,Once we learn the coeﬃcients α we can calculate the prediction on a new
1190,1191,The advantage of working with kernels rather than directly optimizing w in the feature space is that in some situations the dimension of the feature space is extremely large while implementing the kernel function is very simple. A few examples are given in the following.
1191,1192,The k degree polynomial kernel is deﬁned
1192,1193,"Now we will show that this is indeed a kernel function. That is, we will show that there exists a mapping ψ from the original space to some higher dimensional space for which K(x, x′) = ⟨ψ(x), ψ(x′)⟩. For simplicity, denote x0 = x′ element of ψ(x) that equals Qk"
1193,1194,"K(x, x′) = ⟨ψ(x), ψ(x′)⟩."
1194,1195,"Since ψ contains all the monomials up to degree k, a halfspace over the range of ψ corresponds to a polynomial predictor of degree k over the original space."
1195,1196,"Hence, learning a halfspace with a k degree polynomial kernel enables us to learn polynomial predictors of degree k over the original space."
1196,1197,Note that here the complexity of implementing K is O(n) while the dimension of the feature space is on the order of nk.
1197,1198,Let the original instance space be R and an element ψ(x)n that equals
1198,1199,Here the feature space is of inﬁnite dimension while evaluating the kernel is very
1199,1200,"Intuitively, the Gaussian kernel sets the inner product in the feature space between x, x′ to be close to zero if the instances are far away from each other controls the scale determining what we mean by “close.” It is easy to verify that"
1200,1201,K implements an inner product in a space in which for any n and any monomial of order k there exists an element of ψ(x) that equals
1201,1202,"Hence, we can learn any polynomial predictor over the original space by using a required to learn with Gaussian kernels depends on the margin in the feature space, which will be large if we are lucky, but can in general be arbitrarily small."
1202,1203,Kernels as a Way to Express Prior Knowledge
1203,1204,"As we discussed previously, a feature mapping, ψ, may be viewed as expanding the class of linear classiﬁers to a richer class (corresponding to linear classiﬁers over the feature space). However, as discussed in the book so far, the suitability of any hypothesis class to a given learning task depends on the nature of that task. One can therefore think of an embedding ψ as a way to express and utilize prior knowledge about the problem at hand. For example, if we believe that positive examples can be distinguished by some ellipse, we can deﬁne ψ to be all"
1204,1205,"As a more realistic example, consider the task of learning to ﬁnd a sequence of characters (“signature”) in a ﬁle that indicates whether it contains a virus or not."
1205,1206,"Formally, let Xd be the set of all strings of length at most d over some alphabet"
1206,1207,Let us show how using an appropriate embedding this class can be realized by linear classiﬁers over the resulting feature space. Consider a mapping ψ to a space
1207,1208,"Rs where s = |Xd|, so that each coordinate of ψ(x) corresponds to some string v in d. It is not hard to see that every member of the class H can be realized by composing a linear classiﬁer over ψ(x), and, moreover, by such a halfspace whose complexity that is polynomial in d. However, the dimension of the feature space is exponential in d so a direct implementation of SVM over the feature space is problematic. Luckily, it is easy to calculate the inner product in the feature space (i.e., the kernel function) without explicitly mapping instances into the feature space. Indeed, K(x, x′) is simply the number of common substrings of x and x′, which can be easily calculated in time polynomial in d."
1208,1209,This example also demonstrates how feature mapping enables us to use halfspaces for nonvectorial domains.
1209,1210,"As we have discussed in the previous section, we can think of the speciﬁcation of the kernel matrix as a way to express prior knowledge. Consider a given similarity it represent an inner product between ψ(x) and ψ(x′) for some feature mapping ψ? The following lemma gives a suﬃcient and necessary condition. product in some Hilbert space if and only if it is positive semideﬁnite; namely, for all x1, . . . , xm, the Gram matrix, Gi,j = K(xi, xj), is a positive semideﬁnite"
1210,1211,"It is trivial to see that if K implements an inner product in some Hilbert space then the Gram matrix is positive semideﬁnite. For the other direction, combinations of elements of the form K(·, x). Deﬁne an inner product on this"
1211,1212,"This is a valid inner product since it is symmetric (because K is symmetric), it is with equality only for ψ(x) being the zero function). Clearly, ⟨ψ(x), ψ(x′)⟩= ⟨K(·, x), K(·, x′)⟩= K(x, x′), which concludes our proof. while only using kernel evaluations. The basic observation is that the vector w(t) the linear span of {ψ(x1), . . . , ψ(xm)}. Therefore, rather than maintaining w(t) we can maintain the corresponding coeﬃcients α."
1212,1213,"Formally, let K be the kernel function, namely, for all x, x′, K(x, x′) = ⟨ψ(x), ψ(x′)⟩. We shall maintain two vectors in Rm, corresponding to two vectors and α(t) be such that"
1213,1214,The vectors β and α are updated according to the following procedure.
1214,1215,Choose i uniformly at random from [m]
1215,1216,The following lemma shows that the preceding implementation is equivalent the output of applying SGD with kernels. Then ¯w = ˆw. space. By the deﬁnition of α(t) = which concludes our proof.
1216,1217,"Mappings from the given domain to some higher dimensional space, on which a halfspace predictor is used, can be highly powerful. We beneﬁt from a rich and complex hypothesis class, yet need to solve the problems of high sample and rithm, which faces these challenges by using a weak learner: Even though we’re in a very high dimensional space, we have an “oracle” that bestows on us a duced a diﬀerent approach, the kernel trick. The idea is that in order to ﬁnd a halfspace predictor in the high dimensional space, we do not need to know the representation of instances in that space, but rather the values of inner products between the mapped instances. Calculating inner products between instances in the high dimensional space without using their representation in that space is done using kernel functions. We have also shown how the SGD algorithm can be implemented using kernels."
1217,1218,The ideas of feature mapping and the kernel trick allow us to use the framework of halfspaces and linear predictors for nonvectorial data. We demonstrated how kernels can be used to learn predictors over the domain of strings.
1218,1219,"We presented the applicability of the kernel trick in SVM. However, the kernel trick can be applied in many other algorithms. A few examples are given as lems. The next two chapters deal with completely diﬀerent types of hypothesis"
1219,1220,"Bibliographic Remarks of conditions due to Mercer. Many useful kernel functions have been introduced in the literature for various applications. We refer the reader to Sch¨olkopf & only accessing the instances via the kernel function. Hint: The derivation is similar to the derivation of implementing SGD with kernels. mapping ψ, is the problem of ﬁnding a vector w that minimizes the function and then returning the predictor"
1220,1221,Show how to implement the ridge regression algorithm with kernels.
1221,1222,Hint: The representer theorem tells us that there exists a vector α ∈Rm i ψ(xi) is a minimizer of f.
1222,1223,"K(x, x′) = min{x, x′}. where H is some Hilbert space, such that on the basis of their shopping carts. Speciﬁcally, he sampled i.i.d. customers, customer has a baby. As prior knowledge, the manager knows that there are at least one of these k items. Of course, the identity of these k items is not known (otherwise, there was nothing to learn). In addition, according to the store regulation, each customer can buy at most s items. Help the manager to design a learning algorithm such that both its time complexity and its sample implements inner products in the feature space V ."
1223,1224,"Consider the binary classiﬁcation algorithm that predicts the label of an unseen instance according to the class with the closest average. Formally, the algorithm outputs the following decision rule: h(x) = sign(⟨w, ψ(x)⟩+ b). accessing individual entries of ψ(x) or w."
1224,1225,"Multiclass, Ranking, and Complex"
1225,1226,Multiclass categorization is the problem of classifying instances into one of several rizing documents according to topic (X is the set of documents and Y is the set of possible topics) or determining which object appears in a given image (X is the set of images and Y is the set of possible objects).
1226,1227,The centrality of the multiclass learning problem has spurred the development of various approaches for tackling the task. Perhaps the most straightforward approach is a reduction from multiclass classiﬁcation to binary classiﬁcation. In drawback of the reduction approach.
1227,1228,We then turn to describe a family of linear predictors for multiclass problems.
1228,1229,"Relying on the RLM and SGD frameworks from previous chapters, we describe several practical algorithms for multiclass prediction. diction problems in which Y can be extremely large but has some structure on strate this approach for the task of recognizing handwritten words, in which Y is the set of all possible strings of some bounded length (hence, the size of Y is exponential in the maximal length of a word). ical application is ordering results of a search engine according to their relevance to the query. We describe several performance measures that are adequate for assessing the performance of ranking predictors and describe how to learn linear predictors for ranking problems eﬃciently."
1229,1230,The simplest approach to tackle multiclass prediction problems is by reduction to binary classiﬁcation. Recall that in multiclass prediction we would like to learn
1230,1231,Personal use only. Not for distribution. Do not post.
1231,1232,"Multiclass, Ranking, and Complex Prediction Problems siﬁers, each of which discriminates between one class and the rest of the classes."
1232,1233,"That is, given a training set S = (x1, y1), . . . , (xm, ym), where every yi is in Y, we given h1, . . . , hk, we construct a multiclass predictor using the rule which class to predict (e.g., we can arbitrarily decide to break ties by taking the minimal index in argmaxi hi(x)). A better approach can be applied whenever each hi hides additional information, which can be interpreted as the conﬁdence in the prediction y = i. For example, this is the case in halfspaces, where the actual prediction is sign(⟨w, x⟩), but we can interpret ⟨w, x⟩as the conﬁdence approach is given in the following. training set S = (x1, y1), . . . , (xm, ym) algorithm for binary classiﬁcation A classes are compared to each other. Formally, given a training set S = (x1, y1), . . . , (xm, ym), sequence, Si,j, containing all examples from S whose label is either i or j. For classiﬁcation algorithm based on every Si,j to get hi,j. Finally, we construct a multiclass classiﬁer by predicting the class that had the highest number of training set S = (x1, y1), . . . , (xm, ym) algorithm for binary classiﬁcation A initialize Si,j to be the empty sequence the multiclass hypothesis deﬁned by simple and easy to construct from existing algorithms, their simplicity has a price. The binary learner is not aware of the fact that we are going to use its output hypotheses for constructing a multiclass predictor, and this might lead to suboptimal results, as illustrated in the following example. merical value of the class label). In contrast, if we choose hi(x) = ⟨wi, x⟩, ﬁer deﬁned by h(x) = argmaxi hi(x) perfectly predicts all the examples. We see"
1233,1234,"Multiclass, Ranking, and Complex Prediction Problems that even though the approximation error of the class of predictors of the form good predictor from this class."
1234,1235,Linear Multiclass Predictors
1235,1236,"In light of the inadequacy of reduction methods, in this section we study a more direct approach for learning multiclass predictors. We describe the family of linear multiclass predictors. To motivate the construction of this family, recall that a linear predictor for binary classiﬁcation (i.e., a halfspace) takes the form h(x) = sign(⟨w, x⟩)."
1236,1237,An equivalent way to express the prediction is as follows: where yx is the vector obtained by multiplying each element of x by y.
1237,1238,This representation leads to a natural generalization of halfspaces to multiclass
1238,1239,"That is, Ψ takes as input a pair (x, y) and maps it into a d dimensional feature vector. Intuitively, we can think of the elements of Ψ(x, y) as score functions that assess how well the label y ﬁts the instance x. We will elaborate on Ψ later on."
1239,1240,"That is, the prediction of h for the input x is the label that achieves the highest weighted score, where weighting is according to the vector w."
1240,1241,"Of course, the immediate question, which we discuss in the sequel, is how to"
1241,1242,"W = Rd, then HΨ,W becomes the hypothesis class of homogeneous halfspace predictors for binary classiﬁcation."
1242,1243,"As mentioned before, we can think of the elements of Ψ(x, y) as score functions that assess how well the label y ﬁts the instance x. Naturally, designing a good Ψ is similar to the problem of designing a good feature mapping (as we discussed in of useful constructions are given in the following."
1243,1244,The Multivector Construction:
1244,1245,"That is, Ψ(x, y) is composed of k vectors, each of which is of dimension n, where we set all the vectors to be the all zeros vector except the y’th vector, which is tor construction. By the construction we have that ⟨w, Ψ(x, y)⟩= ⟨wy, x⟩, and therefore the multiclass prediction becomes"
1245,1246,A geometric illustration of the multiclass prediction over X = R2 is given in the
1246,1247,"The previous deﬁnition of Ψ(x, y) does not incorporate any prior knowledge about the problem. We next describe an example of a feature function Ψ that does incorporate prior knowledge. Let X be a set of text documents and Y be a set of possible topics. Let d be a size of a dictionary of words. For each word in the dictionary, whose corresponding index is j, let TF(j, x) be the number of times the word corresponding to j appears in the document x. This quantity is called corresponding to j appears in documents in our training set that are not about"
1247,1248,"Ψj(x, y) = TF(j, x) log"
1248,1249,"Multiclass, Ranking, and Complex Prediction Problems pears a lot in the document x but does not appear at all in documents that are not on topic y. If this is the case, we tend to believe that the document x is on topic y. Note that unlike the multivector construction described previously, in the current construction the dimension of Ψ does not depend on the number of topics (i.e., the size of Y). of loss for diﬀerent mistakes. For example, in object recognition tasks, it is less severe to predict that an image of a tiger contains a cat than predicting that the image contains a whale. This can be modeled by specifying a loss function, the label y′ when the correct label is y is deﬁned to be ∆(y′, y). We assume"
1249,1250,"We have deﬁned the hypothesis class HΨ,W and speciﬁed a loss function ∆. To learn the class with respect to the loss function, we can apply the ERM rule with parameterized by a vector w, that minimizes the empirical risk with respect to"
1250,1251,"We now show that when W = Rd and we are in the realizable case, then it is possible to solve the ERM problem eﬃciently using linear programming. Indeed,"
1251,1252,"Equivalently, we need that w will satisfy the following set of linear inequalities ⟨w, Ψ(xi, yi)⟩> ⟨w, Ψ(xi, y)⟩."
1252,1253,Finding w that satisﬁes the preceding set of linear equations amounts to solving
1253,1254,"As in the case of binary classiﬁcation, it is also possible to use a generalization tionally hard. We tackle this diﬃculty using the method of convex surrogate multiclass problems."
1254,1255,"Generalized Hinge Loss y⟨w, x⟩}. We now generalize the hinge loss to multiclass predictors of the form"
1255,1256,"Recall that a surrogate convex loss should upper bound the original nonconvex loss, which in our case is ∆(hw(x), y). To derive an upper bound on ∆(hw(x), y) we ﬁrst note that the deﬁnition of hw(x) implies that ⟨w, Ψ(x, y)⟩≤⟨w, Ψ(x, hw(x))⟩. ∆(hw(x), y) ≤∆(hw(x), y) + ⟨w, Ψ(x, hw(x)) −Ψ(x, y)⟩. y′∈Y (∆(y′, y) + ⟨w, Ψ(x, y′) −Ψ(x, y)⟩)"
1256,1257,"We use the term “generalized hinge loss” to denote the preceding expression. As ever the score of the correct label is larger than the score of any other label, y′, by at least ∆(y′, y), namely, ⟨w, Ψ(x, y)⟩≥⟨w, Ψ(x, y′)⟩+ ∆(y′, y)."
1257,1258,"It is also immediate to see that ℓ(w, (x, y)) is a convex function with respect to w and that ℓ(w, (x, y)) is ρ-Lipschitz with ρ = maxy′∈Y ∥Ψ(x, y′) −Ψ(x, y)∥."
1258,1259,"We use the name “generalized hinge loss” since in the binary becomes the vanilla hinge loss for binary classiﬁcation,"
1259,1260,Geometric Intuition:
1260,1261,"The value of ℓ(w, (x, y)) will be zero if there exists a direction w such that when projecting the |Y| vectors onto this direction we obtain that each vector is represented by the scalar ⟨w, Ψ(x, y)⟩, and we can rank the diﬀerent points on the basis of these scalars so that"
1261,1262,"Multiclass, Ranking, and Complex Prediction Problems"
1262,1263,"• For each y′ ̸= y, the diﬀerence between ⟨w, Ψ(x, y)⟩and ⟨w, Ψ(x, y′)⟩is larger than the loss of predicting y′ instead of y. The diﬀerence ⟨w, Ψ(x, y)⟩−"
1263,1264,This is illustrated in the following ﬁgure:
1264,1265,"Multiclass SVM and SGD multiclass SVM rule: input: (x1, y1), . . . , (xm, ym) y′∈Y (∆(y′, yi) + ⟨w, Ψ(xi, y′) −Ψ(xi, yi)⟩) ing generic convex optimization algorithms (or using the method described in generalized hinge loss upper bounds the ∆loss, we immediately obtain an analog"
1265,1266,"B2m on a training set S ∼Dm and let hw be the output of Multiclass SVM. Then,"
1266,1267,"D(h) = E(x,y)∼D[∆(h(x), y)] and Lg−hinge (w) = E(x,y)∼D[ℓ(w, (x, y))]"
1267,1268,"We can also apply the SGD learning framework for minimizing Lg−hinge functions. In light of this claim, in order to ﬁnd a subgradient of the generalized deﬁnition of the generalized hinge loss. This yields the following algorithm:"
1268,1269,"SGD for Multiclass Learning  ∆(y′, y) + ⟨w(t), Ψ(x, y′) −Ψ(x, y)⟩ set vt = Ψ(x, ˆy) −Ψ(x, y) iterations (i.e., number of examples) ρ2 T , then the output of SGD satisﬁes pend implicitly on the size of Y via the norm of Ψ(x, y) and the fact that the bounds are meaningful only when there exists some vector u, ∥u∥≤B, for which (u) is not excessively large."
1269,1270,"Multiclass, Ranking, and Complex Prediction Problems"
1270,1271,Structured Output Prediction
1271,1272,"Structured output prediction problems are multiclass problems in which Y is very large but is endowed with a predeﬁned structure. The structure plays a key role in constructing eﬃcient algorithms. To motivate structured learning problems, consider the problem of optical character recognition (OCR). Suppose we receive an image of some handwritten word and would like to predict which word is written in the image. To simplify the setting, suppose we know how to segment the image into a sequence of images, each of which contains a patch of the image corresponding to a single letter. Therefore, X is the set of sequences of images and Y is the set of sequences of letters. Note that the size of Y grows exponentially with the maximal length of a word. An example of an image x corresponding to the label y = “workable” is given in the following."
1272,1273,"To tackle structure prediction we can rely on the family of linear predictors described in the previous section. In particular, we need to deﬁne a reasonable"
1273,1274,"Ψ. By “good” we mean a feature mapping that will lead to a low approximation error for the class of linear predictors with respect to Ψ and ∆. Once we do this, we can rely, for example, on the SGD learning algorithm deﬁned in the previous"
1274,1275,"However, the huge size of Y poses several challenges: over Y. How can we predict eﬃciently when Y is so large? need to solve a maximization problem over Y."
1275,1276,"In the previous section we have already shown that the sample complexity of learning a linear multiclass predictor does not depend explicitly on the number of classes. We just need to make sure that the norm of the range of Ψ is not too large. This will take care of the overﬁtting problem. To tackle the computational challenges we rely on the structure of the problem, and deﬁne the functions Ψ and ∆so that calculating the maximization problems in the deﬁnition of hw and in the SGD algorithm can be performed eﬃciently. In the following we demonstrate one way to achieve these goals for the OCR task mentioned previously."
1276,1277,"To simplify the presentation, let us assume that all the words in Y are of length r and that the number of diﬀerent letters in our alphabet is q. Let y and y′ be two words (i.e., sequences of letters) in Y. We deﬁne the function ∆(y′, y) to be the nient to think about x as a matrix of size n × r, where n is the number of pixels in each image, and r is the number of images in the sequence. The j’th column of x corresponds to the j’th image in the sequence (encoded as a vector of gray level values of pixels). The dimension of the range of Ψ is set to be d = n q + q2."
1277,1278,"That is, we sum the value of the i’th pixel only over the images for which y whose gray level values are indicative of a certain letter. The second type of features take the form"
1278,1279,"That is, we sum the number of times the letter i follows the letter j. Intuitively, these features can capture rules like “It is likely to see the pair ‘qu’ in a word” or “It is unlikely to see the pair ‘rz’ in a word.” Of course, some of these features will not be very useful, so the goal of the learning process is to assign weights to features by learning the vector w, so that the weighted score will give us a good"
1279,1280,"It is left to show how to solve the optimization problem in the deﬁnition of hw(x) eﬃciently, as well as how to solve the optimization problem in the deﬁnition of ˆy in the SGD algorithm. We can do this by applying a dynamic programming procedure. We describe the procedure for solving the maximization in the deﬁnition of hw and leave as an exercise the maximization problem in the deﬁnition of ˆy in the SGD algorithm."
1280,1281,"To derive the dynamic programming procedure, let us ﬁrst observe that we"
1281,1282,"Multiclass, Ranking, and Complex Prediction Problems"
1282,1283,"Therefore, the prediction can be written as"
1283,1284,In the following we derive a dynamic programming procedure that solves every
1284,1285,"Clearly, the maximum of ⟨w, Ψ(x, y)⟩equals maxs Ms,r. Furthermore, we can calculate M in a recursive manner:"
1285,1286,This yields the following procedure:
1286,1287,"Dynamic Programming for Calculating hw(x) as Given set yt = argmaxs Ms,r output: y = (y1, . . . , yr) vance.” A typical application is ordering results of a search engine according to tronic transactions and should alert for possible fraudulent transactions. Such a system should order transactions according to how suspicious they are."
1287,1288,"Formally, let X ∗= S∞"
1288,1289,"X of arbitrary length. A ranking hypothesis, h, is a function that receives a sequence of instances ¯x = (x1, . . . , xr) ∈X ∗, and returns a permutation of [r]. sorting the elements of y we obtain the permutation over [r]. We denote by"
1289,1290,"In the notation of our PAC learning model, the examples domain is Z ="
1290,1291,"We next turn to describe loss functions for ranking. There are many possible ways to deﬁne such loss functions, and here we list a few examples. In all the examples we deﬁne ℓ(h, (¯x, y)) = ∆(h(¯x), y), for some function ∆: S∞ a loss function is almost never used in practice as it does not distinguish between the case in which π(y′) is almost equal to π(y) and the case in which π(y′) is completely diﬀerent from π(y). order in the two permutations. This can be written as similarity between the two rankings. phasizes the correctness at the top of the list by using a monotonically cumulative gain measure:"
1291,1292,"In words, if we interpret yi as a score of the “true relevance” of item i, then we take a weighted sum of the relevance of the elements, while the weight of yi is determined on the basis of the position of i in π(y′). Assuming that"
1292,1293,"G(y, y). We can therefore deﬁne a normalized discounted cumulative gain by the ratio G(y′, y)/G(y, y), and the corresponding loss function would (D(π(y)i) −D(π(y′)i)) yi."
1293,1294,"Multiclass, Ranking, and Complex Prediction Problems"
1294,1295,"A typical way to deﬁne the discount function is by where k < r is a parameter. This means that we care more about elements that are ranked higher, and we completely ignore elements that are not at the performance of search engines since in such applications it makes sense completely to ignore elements that are not at the top of the ranking."
1295,1296,"Once we have a hypothesis class and a ranking loss function, we can learn a ranking function using the ERM rule. However, from the computational point of view, the resulting optimization problem might be hard to solve. We next discuss how to learn linear predictors for ranking."
1296,1297,Linear Predictors for Ranking
1297,1298,"A natural way to deﬁne a ranking function is by projecting the instances onto some vector w and then outputting the resulting scalars as our representation deﬁne a ranking function hw((x1, . . . , xr)) = (⟨w, x1⟩, . . . , ⟨w, xr⟩). instances into some feature space and then takes the inner products with w in the"
1298,1299,"Given some W ⊂Rd, we can now deﬁne the hypothesis class HW = {hw : loss function, we can apply the ERM rule as follows: Given a training set, S ="
1299,1300,"As in the case of binary classiﬁcation, for many loss functions this problem is computationally hard, and we therefore turn to describe convex surrogate loss functions. We describe the surrogates for the Kendall tau loss and for the NDCG"
1300,1301,A Hinge Loss for the Kendall Tau Loss Function:
1301,1302,"In particular, for every (i, j) we can rewrite j = ⟨w, xi −xj⟩. It follows that we can use the hinge loss upper"
1302,1303,"Taking the average over the pairs we obtain the following surrogate convex loss for the Kendall tau loss function: tau loss. It is also a ρ-Lipschitz function with parameter ρ ≤maxi,j ∥xi −xj∥."
1303,1304,A Hinge Loss for the NDCG Loss Function:
1304,1305,The NDCG loss function depends on the predicted ranking vector y′ ∈Rr via the permutation it induces. To derive a surrogate loss function we ﬁrst make the following observation. Let V be the set of all permutations of [r] encoded as
1305,1306,"Let us denote Ψ(¯x, v) = Pr sensitive multiclass classiﬁcation as a surrogate loss function for the NDCG loss ∆(hw(¯x), y) ≤∆(hw(¯x), y) + ⟨w, Ψ(¯x, π(hw(¯x)))⟩−⟨w, Ψ(¯x, π(y))⟩ [∆(v, y) + ⟨w, Ψ(¯x, v)⟩−⟨w, Ψ(¯x, π(y))⟩]"
1306,1307,"Multiclass, Ranking, and Complex Prediction Problems equivalent to solving the problem"
1307,1308,"Ai,j = jαi + D(j) βi."
1308,1309,"Now, let us think about each j as a “worker,” each i as a “task,” and Ai,j as the cost of assigning task i to worker j. With this view, the problem of ﬁnding v becomes the problem of ﬁnding an assignment of the tasks to workers of minimal cost. This problem is called “the assignment problem” and can be solved"
1309,1310,"Another way to solve the assignment problem is using linear programming. To do so, let us ﬁrst write the assignment problem as"
1310,1311,"A matrix B that satisﬁes the constraints in the preceding optimization problem is called a permutation matrix. This is because the constraints guarantee that redundant – if we solve the optimization problem while simply omitting the combinatorial constraint, then we are still guaranteed that there is an optimal solution that will satisfy this constraint. This is formalized later. mizing ⟨A, B⟩such that B is a permutation matrix."
1311,1312,The following claim states that every doubly stochastic matrix is a convex combination of permutation matrices.
1312,1313,"The set of doubly stochastic matrices in Rr,r is the convex hull of the set of permutation matrices in Rr,r."
1313,1314,"On the basis of the claim, we easily obtain the following: ⟨A, B⟩≤⟨A, Ci⟩for every i. We claim that there is some i for which ⟨A, B⟩= ⟨A, Ci⟩. This must be true since otherwise, if for every i ⟨A, B⟩< ⟨A, Ci⟩, we which cannot hold. We have thus shown that some permutation matrix, Ci, satisﬁes ⟨A, B⟩= ⟨A, Ci⟩. But, since for every other permutation matrix C we"
1314,1315,Bipartite Ranking and Multivariate Performance Measures
1315,1316,"In the previous section we described the problem of ranking. We used a vector are diﬀerent from each other, then y speciﬁes a full order over [r]. However, if two elements of y attain the same value, yi = yj for i ̸= j, then y can only specify a partial order over [r]. In such a case, we say that xi and xj are of equal relevance either relevant or nonrelevant. This setting is often called “bipartite ranking.” For example, in the fraud detection application mentioned in the previous section,"
1316,1317,"Seemingly, we can solve the bipartite ranking problem by learning a binary classiﬁer, applying it on each instance, and putting the positive ones at the top of the ranked list. However, this may lead to poor results as the goal of a binary goal of a ranker might be signiﬁcantly diﬀerent. To illustrate this, consider again the problem of fraud detection. Usually, most of the transactions are benign (say predictor is meaningless for the fraud detection application. The crux of the"
1317,1318,"Multiclass, Ranking, and Complex Prediction Problems interested in. A more adequate performance measure should take into account the predictions over the entire set of instances. For example, in the previous section we have deﬁned the NDCG loss, which emphasizes the correctness of the speciﬁcally adequate for bipartite ranking problems."
1318,1319,"As in the previous section, we are given a sequence of instances, ¯x = (x1, . . . , xr), threshold transforms the vector y′ ∈Rr into the vector (sign(y′"
1319,1320,"The recall (a.k.a. sensitivity) of a prediction vector is the fraction of true positives y′ “catches,” namely, predictions among the positive labels we predict, namely, is the fraction of true negatives that our predictor “catches,” namely, various techniques for combining both the precision and recall."
1320,1321,"• Averaging sensitivity and speciﬁcity: This measure is the average of the . This is also the accuracy on positive examples averaged with the accuracy on negative examples. the numbers a, b, c as follows; F1 ="
1321,1322,"• Fβ-score: It is like F1 score, but we attach β2 times more importance to recall than to precision, that is,"
1322,1323,Recall . It can also be written as
1323,1324,"• Recall at k: We measure the recall while the prediction must contain at most nient, for example, in the application of a fraud detection system, where a bank employee can only handle a small number of suspicious transactions."
1324,1325,• Precision at k: We measure the precision while the prediction must contain mance measures. Note that these measures are highly diﬀerent from the average
1325,1326,Linear Predictors for Bipartite Ranking
1326,1327,"We next describe how to train linear predictors for bipartite ranking. As in the previous section, a linear predictor for ranking is deﬁned to be hw(¯x) = (⟨w, x1⟩, . . . , ⟨w, xr⟩)."
1327,1328,"The corresponding loss function is one of the multivariate performance measures described before. The loss function depends on y′ = hw(¯x) via the binary vector it induces, which we denote by"
1328,1329,"As in the previous section, to facilitate an eﬃcient algorithm we derive a convex surrogate loss function on ∆. The derivation is similar to the derivation of the generalized hinge loss for the NDCG ranking loss, as described in the previous"
1329,1330,"Multiclass, Ranking, and Complex Prediction Problems ∆(hw(¯x), y) = ∆(b(hw(¯x)), y) (bi(hw(¯x)) −yi)⟨w, xi⟩"
1330,1331,"In the following we describe how to ﬁnd this maximizer eﬃciently for any performance measure that can be written as a function of the numbers a, b, c, d for which the values of a, b satisfy some constraints. For example, for “recall at within each ¯Ya,b that has a nonempty intersection with V separately, and then take the optimal value. The key observation is that once we are searching only within ¯Ya,b, the value of ∆is ﬁxed so we only need to maximize the expression"
1331,1332,"Suppose the examples are sorted so that ⟨w, x1⟩≥· · · ≥⟨w, xr⟩. Then, it is easy to verify that we would like to set vi to be positive for the smallest indices yields the following procedure. (x1, . . . , xr), (y1, . . . , yr), w, V, ∆ ∆is a function of a, b, c, d µ = (⟨w, x1⟩, . . . , ⟨w, xr⟩), α⋆= −∞ let i1, . . . , iP be the (sorted) indices of the positive examples let j1, . . . , jN be the (sorted) indices of the negative examples calculate ∆using a, b, c, d"
1332,1333,"Many real world supervised learning problems can be cast as learning a multiclass predictor. We started the chapter by introducing reductions of multiclass learning to binary learning. We then described and analyzed the family of linear predictors for multiclass learning. We have shown how this family can be used even if the number of classes is extremely large, as long as we have an adequate structure study the sample complexity of multiclass learning in more detail."
1333,1334,Bibliographic Remarks der the framework of Error Correction Output Codes (ECOC) (Dietterich &
1334,1335,"Multiclass, Ranking, and Complex Prediction Problems in which we analyze the sample complexity of multiclass learning."
1335,1336,"Structured output SVM has been studied in (Weston, Chapelle, Vapnik, Elisseeﬀ"
1336,1337,"The dynamic procedure we have presented for calculating the prediction hw(x) calculated by the Viterbi procedure in HMMs (see, for instance, (Rabiner & output is closely related to the problem of inference in graphical models (see, for served that the maximization problem in the deﬁnition of the generalized hinge loss is equivalent to the assignment problem. ranking with multivariate performance measures. such that every example (x, y) ∈S falls within a ball centered at µy whose multivector construction, namely,"
1337,1338,"Hint: Observe that for every example (x, y) ∈S we can write x = µy + v for"
1338,1339,Multiclass Batch Perceptron
1339,1340,"A training set (x1, y1), . . . , (xm, ym)"
1340,1341,"If (∃i and y ̸= yi s.t. ⟨w(t), Ψ(xi, yi)⟩≤⟨w(t), Ψ(xi, y)⟩) then"
1341,1342,Prove the following:
1342,1343,"Assume that there exists w⋆such that for all i and for all dure for multiclass prediction. You can assume that ∆(y′, y) = Pr an instance x by traveling from a root node of a tree to a leaf. For simplicity trees can be applied for other prediction problems as well. At each node on the input space. Usually, the splitting is based on one of the features of x or on a predeﬁned set of splitting rules. A leaf contains a speciﬁc label. An example of gives slightly to palm pressure pale green to pale yellow"
1343,1344,"To check if a given papaya is tasty or not, the decision tree ﬁrst examines the color of the Papaya. If this color is not in the range pale green to pale yellow, then the tree immediately predicts that the papaya is not tasty without additional tests. Otherwise, the tree turns to examine the softness of the papaya."
1344,1345,"If the softness level of the papaya is such that it gives slightly to palm pressure, the decision tree predicts that the papaya is tasty. Otherwise, the prediction is decision trees – the resulting classiﬁer is very simple to understand and interpret."
1345,1346,Personal use only. Not for distribution. Do not post.
1346,1347,"A popular splitting rule at internal nodes of the tree is based on thresholding the value of a single feature. That is, we move to the right or left child of the node on is the threshold. In such cases, we can think of a decision tree as a splitting of the instance space, X = Rd, into cells, where each leaf of the tree corresponds to one cell. It follows that a tree with k leaves can shatter a set of k instances."
1347,1348,"Hence, if we allow decision trees of arbitrary size, we obtain a hypothesis class of inﬁnite VC dimension. Such an approach can easily lead to overﬁtting."
1348,1349,"To avoid overﬁtting, we can rely on the minimum description length (MDL) hand ﬁts the data well while on the other hand is not too large. is a vector of d bits. In that case, thresholding the value of a single feature we can model the “papaya decision tree” earlier by assuming that a papaya is represents whether the color is pale green to pale yellow or not, and the bit x2 represents whether the softness is gives slightly to palm pressure or not. With algorithms and analysis we provide in the following can be extended to more"
1349,1350,"With the aforementioned simplifying assumption, the hypothesis class becomes the number of examples we need to PAC learn the hypothesis class grows with"
1350,1351,"2d. Unless d is very small, this is a huge number of examples."
1351,1352,"The underlying prior knowledge is that we should prefer smaller trees over larger trees. To formalize this intuition, we ﬁrst need to deﬁne a description language for decision trees, which is preﬁx free and requires fewer bits for smaller decision code. Each block indicates whether the current node is:"
1352,1353,"This bound performs a tradeoﬀ: on the one hand, we expect larger, more complex decision trees to have a smaller training risk, LS(h), but the respective value of n will be larger. On the other hand, smaller decision trees will have a smaller value of n, but LS(h) might be larger. Our hope (or prior knowledge) is that we can ﬁnd a decision tree with both low empirical risk, LS(h), and a number of nodes n not too high. Our bound indicates that such a tree will have low true"
1353,1354,Decision Tree Algorithms
1354,1355,"Consequently, practical decision tree learning algorithms are based on heuristics such as a greedy approach, where the tree is constructed gradually, and locally optimal decisions are made at the construction of each node. Such algorithms cannot guarantee to return the globally optimal decision tree but tend to work reasonably well in practice."
1355,1356,"A general framework for growing a decision tree is as follows. We start with a tree with a single leaf (the root) and assign this leaf a label according to a majority vote among all labels over the training set. We now perform a series of iterations. On each iteration, we examine the eﬀect of splitting a single leaf. We deﬁne some “gain” measure that quantiﬁes the improvement due to this split."
1356,1357,"Then, among all possible splits, we either choose the one that maximizes the gain and perform it, or choose not to split the leaf at all."
1357,1358,"In the following we provide a possible implementation. It is based on a popular child, we can replace the node by its child without aﬀecting the predictions of the decision"
1358,1359,"The algorithm works by recursive calls, with the initial call being ID3(S, [d]), and returns a decision tree. In the pseudocode that follows, we use a call to a procedure Gain(S, i), which receives a training set S and an index i and evaluates the gain of a split of the tree according to the ith feature. We describe several if A = ∅, return a leaf whose value = majority of labels in S if all examples in S have the same label"
1359,1360,Return a leaf whose value = majority of labels in S
1360,1361,Implementations of the Gain Measure
1361,1362,"Diﬀerent algorithms use diﬀerent implementations of Gain(S, i). Here we present three. We use the notation PS[F] to denote the probability that an event holds with respect to the uniform distribution over S."
1362,1363,Train Error: The simplest deﬁnition of gain is the decrease in training error. the error after splitting on feature i is
1363,1364,"Therefore, we can deﬁne Gain to be the diﬀerence between the two, namely,"
1364,1365,"Information Gain: Another popular gain measure that is used in the ID3 gain is the diﬀerence between the entropy of the label before and after the split, and is achieved by replacing the function C in the previous expression by the"
1365,1366,"Yet another deﬁnition of a gain, which is used by the CART"
1366,1367,Both the information gain and the Gini index are smooth and concave upper
1367,1368,"The ID3 algorithm described previously still suﬀers from a big problem: The returned tree will usually be very large. Such trees may have low empirical risk, but their true risk will tend to be high – both according to our theoretical analysis, and in practice. One solution is to limit the number of iterations of ID3, leading to a tree with a bounded number of nodes. Another common solution is to prune the tree after it is built, hoping to reduce it to a much smaller tree, but still with a similar empirical error. Theoretically, according to the bound in much, we are likely to get a decision tree with a smaller true risk. might be replaced with one of its subtrees or with a leaf, based on some bound of a common template is given in the following."
1368,1369,"Generic Tree Pruning Procedure of a decision tree T, based on a sample of size m), ﬁnd T ′ which minimizes f(T ′, m), where T ′ is any of the following: the current tree after replacing node j with its left subtree. the current tree after replacing node j with its right subtree."
1369,1370,In the previous section we have described an algorithm for growing a decision tree assuming that the features are binary and the splitting rules are of the
1370,1371,"The basic idea is to reduce the problem to the case of binary features as procedure described in the previous section. It is easy to verify that for any features there exists a decision tree over the constructed binary features with the same training error and the same number of nodes. is m, then the number of constructed binary features becomes dm. Calculating the Gain of each feature might therefore take O(dm2) operations. However, using a more clever implementation, the runtime can be reduced to O(dm log(m)). The idea is similar to the implementation of ERM for decision stumps as described"
1371,1372,"As mentioned before, the class of decision trees of arbitrary size has inﬁnite VC dimension. We therefore restricted the size of the decision tree. Another way to reduce the danger of overﬁtting is by constructing an ensemble of trees. In particular, in the following we describe the method of random forests, introduced"
1372,1373,"A random forest is a classiﬁer consisting of a collection of decision trees, where each tree is constructed by applying an algorithm A on the training set S and"
1373,1374,The prediction of the random forest is obtained by a majority vote over the predictions of the individual trees.
1374,1375,"To specify a particular random forest, we need to deﬁne the algorithm A and from S with replacements; namely, we sample a new training set S′ of size m′ using the uniform distribution over S. Second, we construct a sequence I1, I2, . . ., where each It is a subset of [d] of size k, which is generated by sampling uniformly the algorithm A grows a decision tree (e.g., using the ID3 algorithm) based on the sample S′, where at each splitting stage of the algorithm, the algorithm is restricted to choosing a feature that maximizes Gain from the set It. Intuitively, if k is small, this restriction may prevent overﬁtting."
1375,1376,"Decision trees are very intuitive predictors. Typically, if a human programmer creates a predictor it will look like a decision tree. We have shown that the VC dimension of decision trees with k leaves is k and proposed the MDL paradigm for learning decision trees. The main problem with decision trees is that they cedures for training them."
1376,1377,Bibliographic Remarks
1377,1378,The proof of the hardness of training decision trees is given in Hyaﬁl & Rivest
1378,1379,"Suppose we wish to use this training set in order to build a decision tree of node and its children according to the algorithm, but instead of keeping on with the recursion, we stop and pick leaves according to the majority label in each subtree). Assume that the subroutine used to measure the quality of each feature is based on the entropy function (so we measure the information gain), and that if two features get the same score, one of them is picked arbitrarily. Show that the training error of the resulting decision"
1379,1380,Nearest Neighbor algorithms are among the simplest of all machine learning algorithms. The idea is to memorize the training set and then to predict the label of any new instance on the basis of the labels of its closest neighbors in the training set. The rationale behind such a method is based on the assumption that the features that are used to describe the domain points are relevant to
1380,1381,"Furthermore, in some situations, even when the training set is immense, ﬁnding a nearest neighbor can be done extremely fast (for example, when the training set is the entire Web and distances are based on links)."
1381,1382,"Note that, in contrast with the algorithmic paradigms that we have discussed so far, like ERM, SRM, MDL, or RLM, that are determined by some hypothesis class, H, the Nearest Neighbor method ﬁgures out a label on any test point without searching for a predictor within some predeﬁned class of functions."
1382,1383,"In this chapter we describe Nearest Neighbor methods for classiﬁcation and regression problems. We analyze their performance for the simple case of binary classiﬁcation and discuss the eﬃciency of implementing these methods. the distance between any two elements of X. For example, if X = Rd then ρ can be the Euclidean distance, ρ(x, x′) = ∥x −x′∥="
1383,1384,"Let S = (x1, y1), . . . , (xm, ym) be a sequence of training examples. For each distance to x, ρ(x, xi). That is, for all i < m,"
1384,1385,"Personal use only. Not for distribution. Do not post. depicted are the sample points, and the predicted label of any new point will be the label of the sample point in the center of the cell it belongs to. These cells are called a"
1385,1386,"Voronoi Tessellation of the space. input: a training sample S = (x1, y1), . . . , (xm, ym)"
1386,1387,"For regression problems, namely, Y = R, one can deﬁne the prediction to be  (xπ1(x), yπ1(x)), . . . , (xπk(x), yπk(x))"
1387,1388,"It is easy to verify that we can cast the prediction by majority of labels (for an appropriate choice of φ. The generality can lead to other rules; for example, if"
1388,1389,"Y = R, we can take a weighted average of the targets according to the distance sistency results, analyzing the performance of NN rules when the sample size, m, eralization performance as a function of the size of such ﬁnite training sets and of m and how it depends on properties of the distribution. We will also explain particular, the analysis speciﬁes the number of examples required to achieve a that the labeling rule is “well behaved” (in a sense we will deﬁne later)."
1389,1390,"We start by introducing some notation. Let D be a distribution over X × Y. the conditional probability1 over the labels, that is,"
1390,1391,"Recall that the Bayes optimal rule (that is, the hypothesis that minimizes LD(h) over all functions) is assumption means that if two vectors are close to each other then their labels are likely to be the same."
1391,1392,The following lemma applies the Lipschitzness of the conditional probability expected distance between each test instance and its nearest neighbor in the
1392,1393,"S = (x1, y1), . . . , (xm, ym) be an i.i.d. sample and let hS be its corresponding"
1393,1394,"S∼Dm,x∼D[∥x −xπ1(x)∥]. probability to sample a training set S and an additional example (x, y), such that the label of π1(x) is diﬀerent from y. In other words, we can ﬁrst sample m unlabeled examples, Sx = (x1, . . . , xm), according to DX , and an additional unlabeled example, x ∼DX , then ﬁnd π1(x) to be the nearest neighbor of x in"
1394,1395,"Sx, and ﬁnally sample y ∼η(x) and yπ1(x) ∼η(π1(x)). It follows that y∼η(x),y′∼η(π1(x))[y ̸= y′]"
1395,1396,"We next upper bound Py∼η(x),y′∼η(x′)[y ̸= y′] for any two domain points x, x′: the probability is at most:"
1396,1397,"Finally, the error of the Bayes optimal classiﬁer is"
1397,1398,Combining the preceding two inequalities concludes our proof.
1398,1399,"The next step is to bound the expected distance between a random x and its closest element in S. We ﬁrst need the following general probability lemma. The lemma bounds the probability weight of subsets that are not hit by a random sample, as a function of the size of that sample."
1399,1400,"Let C1, . . . , Cr be a collection of subsets of some domain set, X."
1400,1401,"Let S be a sequence of m points sampled i.i.d. according to some probability distribution, D over X. Then,"
1401,1402,"From the linearity of expectation, we can rewrite:"
1402,1403,"Next, for each i we have"
1403,1404,Combining the preceding two equations we get me and this concludes the proof.
1404,1405,Equipped with the preceding lemmas we are now ready to state and prove the
1405,1406,"For each x, x′ in the same box we have ∥x−x′∥≤ we conclude our proof."
1406,1407,"Bayes error. The analysis can be generalized to larger values of k, showing that"
1407,1408,"The “Curse of Dimensionality” of η) and with d, the Euclidean dimension of the domain set X. In fact, it is easy increase exponentially with the dimension. The following theorem tells us that this is not just an artifact of our upper bound, but, for some distributions, this amount of examples is indeed necessary for learning with the NN rule."
1408,1409,"Fix any values of c and d. Let Gd contains the set of all binary valued functions over that domain. We can therefore needed sample sizes for learning that class. The number of points on the grid is a predeﬁned set of hypotheses, it still relies on some prior knowledge – its success depends on the assumption that the dimension and the Lipschitz constant of the underlying distribution, η, are not too high. entire training data set to be stored, and at test time, we need to scan the entire data set in order to ﬁnd the neighbors. The time of applying the NN rule is therefore Θ(d m). This leads to expensive computation at test time."
1409,1410,"When d is small, several results from the ﬁeld of computational geometry have"
1410,1411,"However, the space required by these data structures is roughly mO(d), which makes these methods impractical for larger values of d."
1411,1412,"To overcome this problem, it was suggested to improve the search method by guaranteed to retrieve a point within distance of at most r times the distance to the nearest neighbor. Three popular approximate algorithms for NN are the that “things that look alike must be alike.” We formalized this intuition using risk of the Bayes optimal rule. We have also derived a lower bound that shows nentially with the dimension. As a result, NN is usually performed in practice after a dimensionality reduction preprocessing step. We discuss dimensionality"
1412,1413,Bibliographic Remarks verges to twice the Bayes optimal error under mild conditions. Following a lemma is consistent (with respect to the hypothesis class of all functions from Rd to another ﬁnite sample bound for NN that is more similar to VC bounds.
1413,1414,"Let h⋆be the Bayes optimal hypothesis. Then,"
1414,1415,"Let C1, . . . , Cr be a collection of subsets of some domain set,"
1415,1416,X. Let S be a sequence of m points sampled i.i.d. according to some probability me to show that for such i we have with expected value p.” Prove the following lemma:
1416,1417,"• To conclude the proof of the lemma, you can rely on the following inequality y∼p′[y ̸= y′] + |p −p′|. box we have ∥x −x′∥≤"
1417,1418,"• To bound the second summand, let us ﬁx S|x and x such that all the k assume that the k NN are x1, . . . , xk. Denote pi = η(xi) and let p = y∼η(x)[hS(x) ̸= y] ≤ y∼p[hS(x) ̸= y] + |p −η(x)|. to conclude the proof."
1418,1419,"An artiﬁcial neural network is a model of computation inspired by the structure of neural networks in the brain. In simpliﬁed models of the brain, it consists of a large number of basic computing devices (neurons) that are connected to each other in a complex communication network, through which the brain is able to carry out highly complex computations. Artiﬁcial neural networks are formal computation constructs that are modeled after this computation paradigm. edge performance on several learning tasks."
1419,1420,A neural network can be described as a directed graph whose nodes correspond to neurons and edges correspond to links between them. Each neuron receives as input a weighted sum of the outputs of the neurons connected to its incoming edges. We focus on feedforward networks in which the underlying graph does not
1420,1421,"In the context of learning, we can deﬁne a hypothesis class consisting of neural ture of the network and diﬀer in the weights over edges. As we will show in the size of the network is the number of nodes in it. It follows that the family of hypothesis classes of neural networks of polynomial size can suﬃce for all practical learning tasks, in which our goal is to learn predictors which can be ple complexity of learning such hypothesis classes is also bounded in terms of the size of the network. Hence, it seems that this is the ultimate learning paradigm plexity and has the minimal approximation error among all hypothesis classes consisting of eﬃciently implementable predictors. learner if the loss function is convex. In neural networks, the loss function is highly nonconvex. Nevertheless, we can still implement the SGD algorithm and"
1421,1422,"Personal use only. Not for distribution. Do not post. hope it will ﬁnd a reasonable solution (as happens to be the case in several networks. In particular, the most complicated operation is the calculation of the gradient of the loss function with respect to the parameters of the network. We present the backpropagation algorithm that eﬃciently calculates the gradient."
1422,1423,Feedforward Neural Networks
1423,1424,The idea behind neural networks is that many neurons can be joined together by communication links to carry out complex computations. It is common to describe the structure of a neural network as a graph whose nodes are the neurons and each (directed) edge in the graph links the output of some neuron to the input of another neuron. We will restrict our attention to feedforward network structures in which the underlying graph does not contain cycles.
1424,1425,"A feedforward neural network is described by a directed acyclic graph, G = threshold function. We call σ the “activation” function of the neuron. Each edge in the graph links the output of some neuron to the input of another neuron."
1425,1426,"The input of a neuron is obtained by taking a weighted sum of the outputs of all the neurons connected to it, where the weighting is according to w."
1426,1427,"To simplify the description of the calculation performed by the network, we further assume that the network is organized in layers. That is, the set of nodes can be decomposed into a union of (nonempty) disjoint subsets, V = ·∪T output of neuron i in V0 is simply xi. The last neuron in V0 is the “constant” and by ot,i(x) the output of vt,i when the network is fed with the input vector x."
1427,1428,"We now proceed with the calculation in a layer by layer manner. Suppose we have calculated the outputs of the neurons at layer t. Then, we can calculate the output layer. In simple prediction problems the output layer contains a single neuron whose output is the output of the network."
1428,1429,"We refer to T as the number of layers in the network (excluding V0), or the “depth” of the network. The size of the network is |V |. The “width” of the network is maxt |Vt|. An illustration of a layered feedforward neural network of neuron in the hidden layer that has no incoming edges. This neuron will output"
1429,1430,Learning Neural Networks
1430,1431,"Once we have speciﬁed a neural network by (V, E, σ, w), we obtain a function tors by ﬁxing the graph (V, E) as well as the activation function σ and letting"
1431,1432,"The triplet (V, E, σ) is often called the architecture of the network. We denote the hypothesis class by"
1432,1433,"HV,E,σ = {hV,E,σ,w : w is a mapping from E to R}."
1433,1434,"That is, the parameters specifying a hypothesis in the hypothesis class are the weights over the edges of the network."
1434,1435,"We can now study the approximation error, estimation error, and optimization error of HV,E,σ by studying what type of functions hypotheses in HV,E,σ can study the estimation error of HV,E,σ, for the case of binary classiﬁcation (i.e., commonly used heuristic for training HV,E,σ."
1435,1436,The Expressive Power of Neural Networks
1436,1437,"In this section we study the expressive power of neural networks, namely, what type of functions can be implemented using a neural network. More concretely, we will ﬁx some architecture, V, E, σ, and will study what functions hypotheses in HV,E,σ can implement, as a function of the size of V ."
1437,1438,"We start the discussion with studying which type of Boolean functions (i.e., for every computer in which real numbers are stored using b bits, whenever we be implemented by HV,E,sign can tell us which functions can be implemented on a computer that stores real numbers using b bits."
1438,1439,"We begin with a simple claim, showing that without restricting the size of the network, every Boolean function can be implemented using a neural network of be some Boolean function. We need to show that we can adjust the weights so implements the function gi(x). Next, we observe that f(x) is the disjunction of the functions gi(x), and therefore can be written as which concludes our proof."
1439,1440,"The preceding claim shows that neural networks can implement any Boolean function. However, this is a very weak property, as the size of the resulting network might be exponentially large. In the construction given at the proof of is not an artifact of our proof, as stated in the following theorem."
1440,1441,"For every n, let s(n) be the minimal integer such that there exists a graph (V, E) with |V | = s(n) such that the hypothesis class HV,E,sign"
1441,1442,"Similar results hold for HV,E,σ where σ is the sigmoid function."
1442,1443,"Suppose that for some (V, E) we have that HV,E,sign contains all functions proof for the case of networks with the sign activation function. The proof for the sigmoid case is analogous."
1443,1444,"It is possible to derive a similar theorem for HV,E,σ for any σ, as long as we restrict the weights so that it is possible to express every weight using sider hypothesis classes where diﬀerent neurons can employ diﬀerent activation functions, as long as the number of allowed activation functions is also ﬁnite. ceding claim tells us that it is impossible to express all Boolean functions using a network of polynomial size. On the positive side, in the following we show that all Boolean functions that can be calculated in time O(T(n)) can also be that can be implemented using a Turing machine using runtime of at most T(n)."
1444,1445,"The proof of this theorem relies on the relation between the time complexity nutshell, a Boolean circuit is a type of network in which the individual neurons plexity measures the size of Boolean circuits required to calculate functions. The relation between time complexity and circuit complexity can be seen intuitively as follows. We can model each step of the execution of a computer program as a simple operation on its memory state. Therefore, the neurons at each layer of the network will reﬂect the memory state of the computer at the corresponding time, and the translation to the next layer of the network involves a simple calculation that can be carried out by the network. To relate Boolean circuits to networks with the sign activation function, we need to show that we can implement the operations of conjunction, disjunction, and negation, using the sign activation tion function. The following lemma shows that the sign activation function can also implement conjunctions and disjunctions of its inputs."
1445,1446,"Suppose that a neuron v, that implements the sign activation function, has k incoming edges, connecting it to neurons whose outputs are in by adjusting the weights on the edges to v, the output of v can implement the conjunction or the disjunction of its inputs. tion, f(x) = ∧ixi, then it can be written as f(x) = sign"
1446,1447,"Similarly, the disjunction function, f(x) = ∨ixi, can be written as f(x) = functions, the size of the network here again cannot be polynomial in n. This is such that there exists a graph (V, E) with |V | = s(n) such that the hypothesis class"
1447,1448,"HV,E,σ, with σ being the sigmoid function, can approximate, to within precision and show how to express them using a neural network with the sign activation layer. Each neuron in the hidden layer implements a halfspace predictor. Then, the single neuron at the output layer applies a halfspace on top of the binary outputs of the neurons in the hidden layer. As we have shown before, a halfspace can implement the conjunction function. Therefore, such networks contain all of neurons in the hidden layer; namely, they can express all convex polytopes"
1448,1449,"We have shown that a neuron in layer V2 can implement a function that indicates whether x is in some convex polytope. By adding one more layer, and letting the neuron in the output layer implement the disjunction of its inputs, we get a network that computes the union of polytopes. An illustration of such a function is given in the following."
1449,1450,The Sample Complexity of Neural Networks
1450,1451,"Next we discuss the sample complexity of learning the class HV,E,σ. Recall that fore, we focus on calculating the VC dimension of hypothesis classes of the form"
1451,1452,"HV,E,σ, where the output layer of the graph contains a single neuron."
1452,1453,"We start with the sign activation function, namely, with HV,E,sign. What is the VC dimension of this class? Intuitively, since we learn |E| parameters, the"
1453,1454,"VC dimension should be order of |E|. This is indeed the case, as formalized by the following theorem."
1454,1455,"The VC dimension of HV,E,sign is O(|E| log(|E|)). pothesis class by H. Recall the deﬁnition of the growth function, τH(m), from nition for a set of functions from X to some ﬁnite set Y, by letting HC be the restriction of H to functions from C to Y, and keeping the deﬁnition of τH(m)"
1455,1456,"Our neural network is deﬁned by a layered graph. Let V0, . . . , VT be the layers that the growth function of a composition of hypothesis classes is bounded by the products of the growth functions of the individual classes. Therefore,"
1456,1457,"In addition, each H(t) can be written as a product of function classes, H(t) = classes, and this yields"
1457,1458,"Let dt,i be the number of edges that are headed to the ith neuron of layer t."
1458,1459,"Since the neuron is a homogenous halfspace hypothesis and the VC dimension of homogenous halfspaces is the dimension of their input, we have by Sauer’s"
1459,1460,"Overall, we obtained that"
1460,1461,"Now, assume that there are m shattered points. Then, we must have τH(m) ="
1461,1462,"2m, from which we obtain"
1462,1463,"Next, we consider HV,E,σ, where σ is the sigmoid function. Surprisingly, it but the proof is beyond the scope of this book. In any case, since in practice we only consider networks in which the weights have a short representation as obtain that such networks have a VC dimension of O(|E|), even if we use the sigmoid activation function."
1463,1464,The Runtime of Learning Neural Networks
1464,1465,"In the previous sections we have shown that the class of neural networks with an mented eﬃciently, and that the sample complexity has a favorable dependence on the size of the network. In this section we turn to the analysis of the time complexity of training neural networks."
1465,1466,"We ﬁrst show that it is NP hard to implement the ERM rule with respect to in the hidden layer. constant neuron, and a single output node. Then, it is NP hard to implement the"
1466,1467,"ERM rule with respect to HV,E,sign."
1467,1468,One way around the preceding hardness result could be that for the purpose
1468,1469,"One may also wonder whether it may be possible to change the architecture of the network so as to circumvent the hardness result. That is, maybe ERM with respect to the original network structure is computationally hard but ERM with respect to some other, larger, network may be implemented eﬃciently (see vation functions (such as sigmoids, or any other type of eﬃciently computable activation functions). There is a strong indication that all of such approaches are doomed to fail. Indeed, under some cryptographic assumption, the problem implies that, under the same cryptographic assumption, any hypothesis class which contains intersections of halfspaces cannot be learned eﬃciently. learner if the loss function is convex. In neural networks, the loss function is highly nonconvex. Nevertheless, we can still implement the SGD algorithm and hope it will ﬁnd a reasonable solution (as happens to be the case in several"
1469,1470,SGD and Backpropagation
1470,1471,"The problem of ﬁnding a hypothesis in HV,E,σ with a low risk amounts to the problem of tuning the weights over the edges. In this section we show how to apply a heuristic search for good weights using the SGD algorithm. Throughout the derivation holds for any diﬀerentiable scalar function."
1471,1472,"Suppose the network has n input neurons and k output neurons, and denote by deﬁned by w. Let us denote by ∆(hw(x), y) the loss of predicting hw(x) when every diﬀerentiable function. Finally, given a distribution D over the examples domain, Rn × Rk, let LD(w) be the risk of the network, namely, (x,y)∼D [∆(hw(x), y)] ."
1472,1473,"Recall the SGD algorithm for minimizing the risk function LD(w). We repeat to the neural network application because of the nonconvexity of the objective we initialize w to be a randomly chosen vector with values close to zero. This is because an initialization with the zero vector will lead all hidden neurons to have the same weights (if the network is a full layered network). In addition, the hope is that if we repeat the SGD procedure several times, where each time we initialize the process with a new random vector, one of the runs will lead to a good local minimum. Second, while a ﬁxed step size, η, is guaranteed to be good enough for convex problems, here we utilize a variable step size, ηt, as choice of the sequence ηt is more signiﬁcant, and it is tuned in practice by a trial and error manner. Third, we output the best performing vector on a validation set. In addition, it is sometimes helpful to add regularization on the weights, gradient does not have a closed form solution. Instead, it is implemented using the backpropagation algorithm, which will be described in the sequel."
1473,1474,"SGD for Neural Networks number of iterations τ step size sequence η1, η2, . . . , ητ layered graph (V, E) calculate gradient vi = backpropagation(x, y, w, (V, E), σ) ¯w is the best performing w(i) on a validation set example (x, y), weight vector w, layered graph (V, E),"
1474,1475,Explaining How Backpropagation Calculates the Gradient:
1475,1476,"We next explain how the backpropagation algorithm calculates the gradient of the loss function on an example (x, y) with respect to the vector w. Let us ﬁrst recall a few deﬁnitions from vector calculus. Each element of the gradient is the partial derivative with respect to the variable in w corresponding to one of the edges of the network. Recall the deﬁnition of a partial derivative. Given a (represented as a row vector). Two examples of Jacobian calculations, which we will later use, are as follows."
1476,1477,"• For every n, we use the notation σ to denote the function from Rn to Rn"
1477,1478,The chain rule for taking the derivative of a composition of functions can be
1478,1479,Jw(f ◦g) = Jg(w)(f)Jw(g).
1479,1480,Jw(σ ◦g) = diag(σ′(Aw)) A.
1480,1481,"To describe the backpropagation algorithm, let us ﬁrst decompose V into the layers of the graph, V = ·∪T"
1481,1482,"Since when calculating the partial derivative with respect to the weight of some edge we ﬁx all other weights, these additional “phantom” edges have no eﬀect on the partial derivative with respect to existing edges. It follows that we can"
1482,1483,"Next, we discuss how to calculate the partial derivatives with respect to the ﬁx all other weights of the network, it follows that the outputs of all the neurons loss function of the subnetwork deﬁned by layers Vt, . . . , VT as a function of the outputs of the neurons in Vt. The input to the neurons of Vt can be written as"
1483,1484,"Therefore, applying the chain rule, we obtain that of ℓt at ot. We calculate this in a recursive manner. First observe that for the last layer we have that ℓT (u) = ∆(u, y), where ∆is the loss function. Since we"
1484,1485,"Therefore, by the chain rule,"
1485,1486,"In summary, we can ﬁrst calculate the vectors {at, ot} from the bottom of the network back to its bottom. Once we have all of these vectors, the partial the pseudocode of backpropagation indeed calculates the gradient."
1486,1487,"Neural networks over graphs of size s(n) can be used to describe hypothesis classes of all predictors that can be implemented in runtime of O( have also shown that their sample complexity depends polynomially on s(n) (speciﬁcally, it depends on the number of edges in the network). Therefore, classes of neural network hypotheses seem to be an excellent choice. Regrettably, the problem of training the network on the basis of training data is computationally hard. We have presented the SGD framework as a heuristic approach for training neural networks and described the backpropagation algorithm which eﬃciently calculates the gradient of the loss function with respect to the weights over the"
1487,1488,Bibliographic Remarks
1488,1489,"Neural networks were extensively studied in the 1980s and early 1990s, but with ments, as well as increasing computational power and data size, has led to a"
1489,1490,"The expressive power of neural networks and the relation to circuit complexity sentation independent learning. This hardness result relies on the cryptographic vector problem. As we have argued, this implies that there cannot be an eﬃcient algorithm for training neural networks, even if we allow larger networks or other activation functions that can be implemented eﬃciently."
1490,1491,"The backpropagation algorithm has been introduced in Rumelhart, Hinton & boxes. Use the Lipschitzness of f to show that it is approximately constant at each box. Finally, show that a neural network can ﬁrst decide which box the input vector belongs to, and then predict the averaged value of f at that"
1491,1492,"X to Yi. Deﬁne H = F1 × F2 to be the Cartesian product class. That is, for"
1492,1493,"Prove that τH(m) ≤τF1(m) τF2(m). to Z and let F2 be a set of functions from Z to Y. Let H = F2 ◦F1 be the such that h(x) = f2(f1(x)). Prove that τH(m) ≤τF2(m)τF1(m). (V, E) such that the VC dimension of the class of neural networks over these the exercise we assume that we can exactly implement the activation function of the network will be x. mension of the resulting hypothesis class is n2."
1493,1494,Additional Learning Models
1494,1495,"In this chapter we describe a diﬀerent model of learning, which is called online learning. Previously, we studied the PAC learning model, in which the learner pothesis, and only when learning is completed uses the learned hypothesis for predicting the label of new examples. In our papayas learning problem, this means that we should ﬁrst buy a bunch of papayas and taste them all. Then, we use all of this information to learn a prediction rule that determines the taste of new papayas. In contrast, in online learning there is no separation between a training phase and a prediction phase. Instead, each time we buy a papaya, it is ﬁrst considered a test example since we should predict whether it is going to taste good. Then, after taking a bite from the papaya, we know the true label, and the same papaya can be used as a training example that can help us improve our prediction mechanism for future papayas."
1495,1496,"Concretely, online learning takes place in a sequence of consecutive rounds."
1496,1497,"On each online round, the learner ﬁrst receives an instance (the learner buys a papaya and knows its shape and color, which form the instance). Then, the learner is required to predict a label (is the papaya tasty?). At the end of the round, the learner obtains the correct label (he tastes the papaya and then knows whether it is tasty or not). Finally, the learner uses this information to improve his future predictions."
1497,1498,"To analyze online learning, we follow a similar route to our study of PAC learning. We start with online binary classiﬁcation problems. We consider both the realizable case, in which we assume, as prior knowledge, that all the labels are generated by some hypothesis from a given hypothesis class, and the unrealizable case, which corresponds to the agnostic PAC learning model. In particular, we learning problems in which the loss function is convex. Finally, we present the"
1498,1499,Perceptron algorithm as an example of the use of surrogate convex loss functions in the online learning model.
1499,1500,Personal use only. Not for distribution. Do not post.
1500,1501,Online Classiﬁcation in the Realizable Case
1501,1502,"Online learning is performed in a sequence of consecutive rounds, where at round t the learner is given an instance, xt, taken from an instance domain X, and is required to provide its label. We denote the predicted label by pt. After predicting goal is to make as few prediction mistakes as possible during this process. The learner tries to deduce information from previous rounds so as to improve its predictions on future rounds."
1502,1503,"Clearly, learning is hopeless if there is no correlation between past and present rounds. Previously in the book, we studied the PAC model in which we assume that past and present examples are sampled i.i.d. from the same distribution ministic, stochastic, or even adversarially adaptive to the learner’s own behavior number of prediction mistakes of our online learning algorithm arbitrarily large."
1503,1504,"For example, the adversary can present the same instance on each online round, wait for the learner’s prediction, and provide the opposite label as the correct izability assumption is one possible natural restriction. In the realizable case, we thermore, h⋆is taken from a hypothesis class H, which is known to the learner. restriction on the sequence, the learner should make as few mistakes as possible, versary. For an online learning algorithm, A, we denote by MA(H) the maximal number of mistakes A might make on a sequence of examples which is labeled by some h⋆∈H. We emphasize again that both h⋆and the sequence of instances we will study how to design algorithms for which MA(H) is minimal. Formally: esis class and let A be an online learning algorithm. Given any sequence S = (x1, h⋆(y1)), . . . , (xT , h⋆(yT )), where T is any integer and h⋆∈H, let MA(S) be the number of mistakes A makes on the sequence S. We denote by MA(H) the supremum of MA(S) over all sequences of the above form. A bound of the form"
1504,1505,MA(H) ≤B < ∞is called a mistake bound. We say that a hypothesis class H is online learnable if there exists an algorithm A for which MA(H) ≤B < ∞.
1505,1506,"Our goal is to study which hypothesis classes are learnable in the online model, and in particular to ﬁnd good learning algorithms for a given hypothesis class. tional aspect of learning, and do not restrict the algorithms to be eﬃcient. In"
1506,1507,"To simplify the presentation, we start with the case of a ﬁnite hypothesis class,"
1507,1508,"In PAC learning, we identiﬁed ERM as a good learning algorithm, in the sense that if H is learnable then it is learnable by the rule ERMH. A natural learning rule for online learning is to use (at any online round) any ERM hypothesis, namely, any hypothesis which is consistent with all past examples. input: A ﬁnite hypothesis class H receive true label yt = h⋆(xt)"
1508,1509,"The Consistent algorithm maintains a set, Vt, of all the hypotheses which"
1509,1510,"Obviously, whenever Consistent makes a prediction mistake, at least one hypothesis is removed from Vt. Therefore, after making M mistakes we have |Vt| ≤|H| −M. Since Vt is always nonempty (by the realizability assumption it"
1510,1511,Let H be a ﬁnite hypothesis class. The Consistent algorithm
1511,1512,It is rather easy to construct a hypothesis class and a sequence of examples on shall see that this algorithm is guaranteed to make exponentially fewer mistakes. input: A ﬁnite hypothesis class H receive true label yt = h⋆(xt)
1512,1513,"Let H be a ﬁnite hypothesis class. The Halving algorithm enjoys the mistake bound MHalving(H) ≤log2(|H|). (hence the name Halving). Therefore, if M is the total number of mistakes, we"
1513,1514,Rearranging this inequality we conclude our proof.
1514,1515,"Of course, Halving’s mistake bound is much better than Consistent’s mistake bound. We already see that online learning is diﬀerent from PAC learning—while in PAC, any ERM hypothesis is good, in online learning choosing an arbitrary"
1515,1516,"ERM hypothesis is far from being optimal. ability. In particular, we target the following question: What is the optimal online learning algorithm for a given hypothesis class H? able mistake bound. This measure was proposed by Nick Littlestone and we therefore refer to it as Ldim(H)."
1516,1517,"To motivate the deﬁnition of Ldim it is convenient to view the online learning process as a game between two players: the learner versus the environment. On round t of the game, the environment picks an instance xt, the learner predicts a"
1517,1518,Suppose that the environment wants to make the learner err on the ﬁrst T rounds should choose the instances xt in such a way that ensures that for some h⋆∈H
1518,1519,"A strategy for an adversarial environment can be formally described as a binary tree, as follows. Each node of the tree is associated with an instance from"
1519,1520,"X. Initially, the environment presents to the learner the instance associated with at each round, the environment will present the instance associated with the"
1520,1521,"Formally, consider a complete binary tree of depth T (we deﬁne the depth of the tree as the number of edges in a path from the root to a leaf). We have x1 = v1. At round t, we set xt = vit where it is the current node. At the end of"
1521,1522,"H = {h1, h2, h3, h4}, where the predictions of each hypothesis in H on the instances"
1522,1523,"The preceding strategy for the environment succeeds only if for every (y1, . . . , yT )"
1523,1524,A shattered tree of depth d is a sequence
1524,1525,"Ldim(H) is the maximal integer T such that there exists a shattered tree of depth T, which is shattered"
1525,1526,No algorithm can have a mistake bound strictly smaller than
1526,1527,"Ldim(H); namely, for every algorithm, A, we have MA(H) ≥Ldim(H). requirements in the deﬁnition of Ldim. If the environment sets xt = vit and"
1527,1528,"Let us now give several examples. tered by H has depth of at most log2(|H|). Therefore, Ldim(H) ≤log2(|H|). large. Therefore, this example shows that Ldim(H) can be signiﬁcantly smaller"
1528,1529,"This tree is shattered by H. And, because of the density of the reals, this tree can be made arbitrarily deep. algorithm. Interestingly, there is a standard algorithm whose mistake bound matches this lower bound. The algorithm is similar to the Halving algorithm."
1529,1530,"Recall that the prediction of Halving is made according to a majority vote of the hypotheses which are consistent with previous examples. We denoted this set by Vt. Put another way, Halving partitions Vt into two sets: V + larger of the two groups. The rationale behind this prediction is that whenever"
1530,1531,"The optimal algorithm we present in the following uses the same idea, but instead of predicting according to the larger class, it predicts according to the class with larger Ldim."
1531,1532,Standard Optimal Algorithm (SOA) input: A hypothesis class H receive true label yt
1532,1533,SOA enjoys the mistake bound MSOA(H) ≤Ldim(H). of pt implies that Ldim(V (r) leads to the desired contradiction.
1533,1534,"Let H be any hypothesis class. Then, the standard optimal algorithm enjoys the mistake bound MSOA(H) = Ldim(H) and no other algorithm can have MA(H) < Ldim(H)."
1534,1535,Comparison to VC Dimension
1535,1536,"In the PAC learning model, learnability is characterized by the VC dimension of the class H. Recall that the VC dimension of a class H is the maximal number d such that there are instances x1, . . . , xd that are shattered by H. That is, for that gives exactly this sequence of labels. The following theorem relates the VC dimension to the Littlestone dimension."
1536,1537,"For any class H, VCdim(H) ≤Ldim(H), and there are classes for which strict inequality holds. Furthermore, the gap can be arbitrarily larger."
1537,1538,"We ﬁrst prove that VCdim(H) ≤Ldim(H). Suppose VCdim(H) = d and let x1, . . . , xd be a shattered set. We now construct a complete binary tree of following illustration:"
1538,1539,"Now, the deﬁnition of a shattered set clearly implies that we got a valid shattered tree of depth d, and we conclude that VCdim(H) ≤Ldim(H). To show that the"
1539,1540,Online Classiﬁcation in the Unrealizable Case
1540,1541,"In the previous section we studied online learnability in the realizable case. We now consider the unrealizable case. Similarly to the agnostic PAC model, we no longer assume that all labels are generated by some h⋆∈H, but we require the learner to be competitive with the best ﬁxed predictor from H. This is captured by the regret of the algorithm, which measures how “sorry” the learner"
1541,1542,"Formally, the regret of an algorithm A relative to h when running on a sequence of T examples is deﬁned as (x1,y1),...,(xT ,yT ) and the regret of the algorithm relative to a hypothesis class H is"
1542,1543,We restate the learner’s goal as having the lowest possible regret relative to H.
1543,1544,"An interesting question is whether we can derive an algorithm with low regret, meaning that RegretA(H, T) grows sublinearly with the number of rounds, T, which implies that the diﬀerence between the error rate of the learner and the best hypothesis in H tends to zero as T goes to inﬁnity."
1544,1545,"We ﬁrst show that this is an impossible mission—no algorithm can obtain a equal to T, by simply waiting for the learner’s prediction and then providing the opposite label as the true label. In contrast, for any sequence of true labels, y1, . . . , yT , let b be the majority of labels in y1, . . . , yT , then the number of"
1545,1546,"To sidestep Cover’s impossibility result, we must further restrict the power of the adversarial environment. We do so by allowing the learner to randomize ity result, since in deriving this result we assumed nothing about the learner’s onment to decide on yt without knowing the random coins ﬂipped by the learner on round t. The adversary can still know the learner’s forecasting strategy and even the random coin ﬂips of previous rounds, but it does not know the actual value of the random coin ﬂips used by the learner on round t. With this (mild) change of game, we analyze the expected number of mistakes of the algorithm, where the expectation is with respect to the learner’s own randomization. That"
1546,1547,"P[ˆyt ̸= yt] = |pt −yt|. ular, we will prove the following theorem."
1547,1548,"For every hypothesis class H, there exists an algorithm for"
1548,1549,"Furthermore, no algorithm can achieve an expected regret bound smaller than"
1549,1550,"We will provide a constructive proof of the upper bound part of the preceding learning with expert advice. This algorithm is important by itself and we dedicate the next subsection to it. vice. In this online learning problem, on round t the learner has to choose the advice of d given experts. We also allow the learner to randomize his choice by tions are randomized, then its loss is deﬁned to be the averaged cost, namely, i vt,i = ⟨w(t), vt⟩. The algorithm assumes that the number of rounds T is input: number of experts, d ; number of rounds, T set w(t) = ˜w(t)/Zt where Zt = P choose expert i at random according to P[i] = w(t) rithm enjoys the bound = −η ⟨w(t), vt⟩+ η2 X"
1550,1551,Summing this inequality over t we get which can be rearranged as follows:
1551,1552,"Plugging the value of η into the equation concludes our proof. a ﬁnite class, and let us write H = {h1, . . . , hd}. In this case, we can refer to each hypothesis, hi, as an expert, whose advice is to predict hi(xt), and whose cost is vt,i = |hi(xt) −yt|. The prediction of the algorithm will therefore be i |hi(xt) −yt|. All in all, we have shown that i |hi(xt) −yt| = ⟨w(t), vt⟩."
1552,1553,"Furthermore, for each i, P t vt,i is exactly the number of mistakes hypothesis hi"
1553,1554,"Let H be a ﬁnite hypothesis class. There exists an algorithm structed an expert for each individual hypothesis. However, if H is inﬁnite this leads to a vacuous bound. The main idea is to construct a set of experts in a more sophisticated way. The challenge is how to deﬁne a set of experts that, on one hand, is not excessively large and, on the other hand, contains experts that give accurate predictions. sequence of instances, x1, x2, . . . , xT , there exists at least one expert in the set the game between SOA (presented in the previous section) and the environment on the sequence of instances x1, x2, . . . , xT assuming that SOA makes a mistake precisely in rounds i1, i2, . . . , iL. The expert is deﬁned by the following algorithm."
1554,1555,"Expert(i1, i2, . . . , iL) input A hypothesis class H ; Indices i1 < i2 < · · · < iL deﬁne ˜yt = argmaxr Ldim"
1555,1556,"Note that each such expert can give us predictions at every round t while only observing the instances x1, . . . , xt. Our generic online learning algorithm is now"
1556,1557,"To analyze the algorithm we ﬁrst note that the number of experts is is at most the number of mistakes of the best expert plus next show that the number of mistakes of the best expert is at most the number of mistakes of the best hypothesis in H. The following key lemma shows that, with the same behavior."
1557,1558,"Let H be any hypothesis class with Ldim(H) < ∞. Let x1, x2, . . . , xT on the sequence x1, x2, . . . , xT , the expert predicts h(xt) on each online round indices i1, i2, . . . , iL. Consider running SOA on the input (x1, h(x1)), (x2, h(x2)), . . ., (xT , h(xT )). SOA makes at most Ldim(H) mistakes on such input. We deﬁne"
1558,1559,"L to be the number of mistakes made by SOA and we deﬁne {i1, i2, . . . , iL} to be the set of rounds in which SOA made the mistakes."
1559,1560,"Now, consider the Expert(i1, i2, . . . , iL) running on the sequence x1, x2, . . . , xT ."
1560,1561,"By construction, the set Vt maintained by Expert(i1, i2, . . . , iL) equals the set Vt maintained by SOA when running on the sequence (x1, h(x1)), . . . , (xT , h(xT ))."
1561,1562,"The predictions of SOA diﬀer from the predictions of h if and only if the round is in {i1, i2, . . . , iL}. Since Expert(i1, i2, . . . , iL) predicts exactly like SOA if t is not in {i1, i2, . . . , iL} and the opposite of SOAs’ predictions if t is in {i1, i2, . . . , iL},"
1562,1563,"The previous lemma holds in particular for the hypothesis in H that makes the least number of mistakes on the sequence of examples, and we therefore obtain"
1563,1564,"Let (x1, y1), (x2, y2), . . . , (xT , yT ) be a sequence of examples mistakes on the sequence of examples."
1564,1565,"Online Convex Optimization results for these problems in the agnostic PAC learning framework. In this section we show that similar learnability results hold for convex problems in the online learning framework. In particular, we consider the following problem."
1565,1566,"Online Convex Optimization learner predicts a vector w(t) ∈H learner suﬀers loss ℓ(w(t), zt)"
1566,1567,"As in the online classiﬁcation problem, we analyze the regret of the algorithm. pothesis, which here will be some vector w⋆∈H, is deﬁned as"
1567,1568,"As before, the regret of the algorithm relative to a set of competing vectors, H, learning problems in the agnostic PAC model. We now show that a very similar algorithm, Online Gradient Descent, solves online convex learning problems."
1568,1569,"Online Gradient Descent receive zt and let ft(·) = ℓ(·, zt)"
1569,1570,"The Online Gradient Descent algorithm enjoys the following regret bound for every w⋆∈H,"
1570,1571,"The analysis is similar to the analysis of Stochastic Gradient Descent deﬁnition of subgradients, we have that for every t, (ft(w(t)) −ft(w⋆)) + η2"
1571,1572,"This proves the ﬁrst bound in the theorem. The second bound follows from the assumption that ft is ρ-Lipschitz, which implies that ∥vt∥≤ρ."
1572,1573,The Online Perceptron Algorithm
1573,1574,The Perceptron is a classic online learning algorithm for binary classiﬁcation with which aims to solve the ERM problem with respect to H. We now present an online version of the Perceptron algorithm.
1574,1575,"The learner maintains a weight vector w(t) ∈Rd and predicts pt = sign(⟨w(t), xt⟩)."
1575,1576,"The goal of the learner is to make as few prediction mistakes as possible. In achievable mistake bound depends on the Littlestone dimension of the class. no hope of making few prediction mistakes. Indeed, consider the tree for which the density of the reals, this tree is shattered by the subset of H which contains"
1576,1577,"To sidestep this impossibility result, the Perceptron algorithm relies on the"
1577,1578,"A weight vector w makes a mistake on an example (x, y) whenever the sign of"
1578,1579,"On rounds on which the algorithm makes a prediction mistake, we shall use the"
1579,1580,• ft is a convex function
1580,1581,"• For all w, ft(w) ≥ℓ(w, (xt, yt)). In particular, this holds for w(t). examples. In the online model, we allow the surrogate to depend on the speciﬁc round. It can even depend on w(t). Our ability to use a round speciﬁc surrogate"
1581,1582,"Let us now run the Online Gradient Descent algorithm on the sequence of functions, f1, . . . , fT , with the hypothesis class being all vectors in Rd (hence, and its update rule is is in ∂ft(w(t)). We therefore obtain the update rule"
1582,1583,"Denote by M the set of rounds in which sign(⟨w(t), xt⟩) ̸= yt. Note that on round t, the prediction of the Perceptron can be rewritten as pt = sign(⟨w(t), xt⟩) = sign"
1583,1584,"This form implies that the predictions of the Perceptron algorithm and the set obtained the Perceptron algorithm: predict pt = sign(⟨w(t), xt⟩) scent given in the previous section. In our case, the subgradient of ft we use"
1584,1585,"Denote R = maxt ∥xt∥; then we obtain |M| and rearranging, we obtain"
1585,1586,This inequality implies
1586,1587,"Suppose that the Perceptron algorithm runs on a sequence (x1, y1), . . . , (xT , yT ) and let R = maxt ∥xt∥. Let M be the rounds on which the last claim can be easily derived by analyzing the roots of the convex parabola xt lies on the correct side of the halfspace, it also guarantees that xt is not too close to the decision boundary. More speciﬁcally, the distance from xt to the"
1587,1588,"When the separability assumption does not hold, the bound involves the term"
1588,1589,"As a last remark we note that there can be cases in which there exists some w⋆that makes zero errors on the sequence but the Perceptron will make many errors. Indeed, this is a direct consequence of the fact that Ldim(H) = ∞. The way we sidestep this impossibility result is by assuming more on the sequence of t ft(w⋆) is not excessively large."
1589,1590,"In this chapter we have studied the online learning model. Many of the results we derived for the PAC learning model have an analog in the online model. First, acterizes online learnability. To show this, we introduced the SOA algorithm (for case). We have also studied online convex optimization and have shown that online gradient descent is a successful online learner whenever the loss function is convex and Lipschitz. Finally, we presented the online Perceptron algorithm as a combination of online gradient descent and the concept of surrogate convex"
1590,1591,Bibliographic Remarks bound of the Halving algorithm is tight.
1591,1592,"Calculate MHalving(H) (i.e., derive lower and upper bounds on MHalving(H), and prove that they are equal). exercise we show how to get rid of this dependence by a simple trick."
1592,1593,"Consider an algorithm that enjoys a regret bound of the form α its parameters require the knowledge of T. The doubling trick, described in the following, enables us to convert such an algorithm into an algorithm that does not need to know the time horizon. The idea is to divide the time into periods of increasing size and run the original algorithm on each period. input: algorithm A whose parameters depend on the time horizon"
1593,1594,Show that if the regret of A on each period of 2m rounds is at most α then the total regret is at most cessful online learning algorithm can be used to derive a successful PAC
1594,1595,"Consider a PAC learning problem for binary classiﬁcation parameterized by an instance domain, X, and a hypothesis class, H. Suppose that there exists an online learning algorithm, A, which enjoys a mistake bound MA(H) < ∞. pled i.i.d. from a distribution D over the instance space X, and are labeled by some h⋆∈H. Suppose that for every round t, the prediction of the algorithm where the expectation is over the random choice of the instances as well as a random choice of r according to the uniform distribution over [T]. ysis. Across all disciplines, from social sciences to biology to computer science, people try to get a ﬁrst intuition about their data by identifying meaningful groups among the data points. For example, computational biologists cluster tailers cluster customers, on the basis of their customer proﬁles, for the purpose of targeted marketing; and astronomers cluster stars on the basis of their spacial tuitively, clustering is the task of grouping a set of objects such that similar ferent groups. Clearly, this description is quite imprecise and possibly ambiguous."
1595,1596,"Quite surprisingly, it is not at all clear how to come up with a more rigorous"
1596,1597,"There are several sources for this diﬃculty. One basic problem is that the two objectives mentioned in the earlier statement may in many cases contradict tive relation, while cluster sharing is an equivalence relation and, in particular, it is a transitive relation. More concretely, it may be the case that there is a long sequence of objects, x1, . . . , xm such that each xi is very similar to its two sure that whenever two elements are similar they share the same cluster, then we must put all of the elements of the sequence in the same cluster. However, in that case, we end up with dissimilar elements (x1 and xm) sharing a cluster, thus violating the second requirement."
1597,1598,"To illustrate this point further, suppose that we would like to cluster the points in the following picture into two clusters. input by separating it horizontally according to the two lines:"
1598,1599,Personal use only. Not for distribution. Do not post.
1599,1600,"Another basic problem is the lack of “ground truth” for clustering, which is a common problem in unsupervised learning. So far in the book, we have mainly dealt with supervised learning (e.g., the problem of learning a classiﬁer from labeled training data). The goal of supervised learning is clear – we wish to learn a classiﬁer which will predict the labels of future examples as accurately as possible. Furthermore, a supervised learner can estimate the success, or the risk, of its hypotheses using the labeled training data by computing the empirical loss. In contrast, clustering is an unsupervised learning problem; namely, there are no labels that we try to predict. Instead, we wish to organize the data in some meaningful way. As a result, there is no clear success evaluation procedure for clustering. In fact, even on the basis of full knowledge of the underlying data distribution, it is not clear what is the “correct” clustering for that data or how to evaluate a proposed clustering."
1600,1601,"Consider, for example, the following set of points in R2: and suppose we are required to cluster them into two clusters. We have two highly justiﬁable solutions:"
1601,1602,"This phenomenon is not just artiﬁcial but occurs in real applications. A given set of objects can be clustered in various diﬀerent meaningful ways. This may be due to having diﬀerent implicit notions of distance (or similarity) between objects, for example, clustering recordings of speech by the accent of the speaker versus clustering them by content, clustering movie reviews by movie topic versus clustering them by the review sentiment, clustering paintings by topic versus clustering them by style, and so on. lutions for a given data set. As a result, there is a wide variety of clustering algorithms that, on some input data, will output very diﬀerent clusterings."
1602,1603,"Clustering tasks can vary in terms of both the type of input they have and the type of outcome they are expected to compute. For concreteness, we shall focus on the following common setup:"
1603,1604,"Input — a set of elements, X, and a distance function over it. That is, a function and often also satisﬁes the triangle inequality. Alternatively, the function algorithms also require an input parameter k (determining the number of required clusters)."
1604,1605,"Output — a partition of the domain set X into subsets. That is, C = (C1, . . . Ck) clustering is “soft,” namely, the partition of X into the diﬀerent clusters is probabilistic where the output is a function assigning to each domain the probability that x belongs to cluster Ci. Another possible output is ing), which is a hierarchical tree of domain subsets, having the singleton sets in its leaves, and the full domain as its root. We shall discuss this formulation in more detail in the following."
1605,1606,"In the following we survey some of the most popular clustering methods. In the last section of this chapter we return to the high level discussion of what is of clustering. These algorithms proceed in a sequence of rounds. They start from ing. Consequently, the number of clusters decreases with each such round. If kept going, such algorithms would eventually result in the trivial clustering in which all of the domain points share one large cluster. Two parameters, then, need to be determined to deﬁne such an algorithm clearly. First, we have to decide how to measure (or deﬁne) the distance between clusters, and, second, we have to determine when to stop merging. Recall that the input to a clustering algorithm a measure of distance between domain subsets (or clusters). The most common by the minimum distance between members of the two clusters, namely, deﬁned to be the average distance between a point in one of the clusters and a point in the other, namely, as the maximum distance between their elements, namely, start from data that is completely fragmented and keep building larger and larger clusters as they proceed. Without employing a stopping rule, the outcome of such an algorithm can be described by a clustering dendrogram: that is, a tree of domain subsets, having the singleton sets in its leaves, and the full domain as its root. For example, if the input is the elements X = {a, b, c, d, e} ⊂R2 with the Euclidean distance as depicted on the left, then the resulting dendrogram is the one depicted on the right:"
1606,1607,"The single linkage algorithm is closely related to Kruskal’s algorithm for ﬁnding a minimal spanning tree on a weighted graph. Indeed, consider the full graph whose vertices are elements of X and the weight of an edge (x, y) is the distance d(x, y). Each merge of two clusters performed by the single linkage algorithm corresponds to a choice of an edge in the aforementioned graph. It is also possible to show that the set of edges the single linkage algorithm chooses along its run forms a minimal spanning tree."
1607,1608,"If one wishes to turn a dendrogram into a partition of the space (a clustering), one needs to employ a stopping criterion. Common stopping criteria include"
1608,1609,"• Fixed number of clusters – ﬁx some parameter, k, and stop merging clusters as soon as the number of clusters is k. criterion is called “scaled distance upper bound.”"
1609,1610,"Another popular approach to clustering starts by deﬁning a cost function over a parameterized set of possible clusterings and the goal of the clustering algorithm is to ﬁnd a partitioning (clustering) of minimal cost. Under this paradigm, the clustering task is turned into an optimization problem. The objective function is a function from pairs of an input, (X, d), and a proposed clustering solution"
1610,1611,"C = (C1, . . . , Ck), to positive real numbers. Given such an objective function, which we denote by G, the goal of a clustering algorithm is deﬁned as ﬁnding, for a given input (X, d), a clustering C so that G((X, d), C) is minimized. In order to reach that goal, one has to apply some appropriate search algorithm. tion algorithm rather than the cost function or the corresponding exact solution of the minimization problem."
1611,1612,"Many common objective functions require the number of clusters, k, as a parameter. In practice, it is often up to the user of the clustering algorithm to choose the parameter k that is most suitable for the given clustering problem."
1612,1613,In the following we describe some of the most common objective functions. where each Ci is represented by a centroid µi. It is assumed that the input the squared distance between each point in X to the centroid of its cluster.
1613,1614,The centroid of Ci is deﬁned to be
1614,1615,"Gk−means((X, d), (C1, . . . , Ck)) ="
1615,1616,This can also be rewritten as
1616,1617,"Gk−means((X, d), (C1, . . . , Ck)) = munication tasks, where the members of X may be viewed as a collection of signals that have to be transmitted. While X may be a very large set of real valued vectors, digital transmission allows transmitting of only a sion under such constraints is to represent each member of X by a “close” member of some ﬁnite set µ1, . . . µk, and replace the transmission of any can be viewed as a measure of the distortion created by such a transmission representation scheme. except that it requires the cluster centroids to be members of the input set. The objective function is deﬁned by"
1617,1618,"GK−medoid((X, d), (C1, . . . , Ck)) = tive, except that the “distortion” between a data point and the centroid of its cluster is measured by distance, rather than by the square of the"
1618,1619,"GK−median((X, d), (C1, . . . , Ck)) ="
1619,1620,"An example where such an objective makes sense is the facility location problem. Consider the task of locating k ﬁre stations in a city. One can model houses as data points and aim to place the stations so as to minimize the average distance between a house and its closest ﬁre station. lution to such a clustering problem is determined by a set of cluster centers,"
1620,1621,"Gf((X, d), (C1, . . . Ck)) = where X ′ is either X or some superset of X. cluster distances (SOD)"
1621,1622,"GSOD((X, d), (C1, . . . Ck)) = approximate to within some constant). As an alternative, the following simple respect to the Euclidean distance function d(x, y) = ∥x −y∥. input: X ⊂Rn ; Number of clusters k initialize: Randomly choose initial centroids µ1, . . . , µk repeat until convergence (break ties in some arbitrary manner)"
1622,1623,"To simplify the notation, let us use the shorthand G(C1, . . . , Ck) for the"
1623,1624,It is convenient to deﬁne µ(Ci) = new partition assigned at iteration t. Using the deﬁnition of the objective as
1624,1625,"In addition, the deﬁnition of the new partition (C(t) minimizes the expression Pk (C1, . . . , Ck). Hence, we obtain that G(C(t) ), which concludes our gorithm needs in order to reach convergence. Furthermore, there is no nontrivial gorithm’s output and the minimum possible value of that objective function. In the procedure several times with diﬀerent randomly chosen initial centroids (e.g., we can choose the initial centroids to be random points from the data)."
1625,1626,"Often, a convenient way to represent the relationships between points in a data set X = {x1, . . . , xm} is by a similarity graph; each vertex represents a data point xi, and every two vertices are connected by an edge whose weight is their"
1626,1627,The clustering problem can now be formulated as follows: We want to ﬁnd a partition of the graph such that the edges between diﬀerent groups have low weights and the edges within a group have high weights.
1627,1628,"In the clustering objectives described previously, the focus was on one side of our intuitive deﬁnition of clustering – making sure that points in the same cluster are similar. We now present objectives that focus on the other requirement – points separated into diﬀerent clusters should be nonsimilar."
1628,1629,"Given a graph represented by a similarity matrix W, the simplest and most direct way to construct a partition of the graph is to solve the mincut problem, which chooses a partition C1, . . . , Ck that minimizes the objective cut(C1, . . . , Ck) = often does not lead to satisfactory partitions. The problem is that in many cases, the solution of mincut simply separates one individual vertex from the rest of the graph. Of course, this is not what we want to achieve in clustering, as clusters should be reasonably large groups of points."
1629,1630,Several solutions to this problem have been suggested. The simplest solution is to normalize the cut and deﬁne the normalized mincut objective as follows:
1630,1631,"RatioCut(C1, . . . , Ck) ="
1631,1632,The preceding objective assumes smaller values if the clusters are not too small.
1632,1633,"Unfortunately, introducing this balancing makes the problem computationally hard to solve. Spectral clustering is a way to relax the problem of minimizing"
1633,1634,Graph Laplacian and Relaxed Graph Cuts
1634,1635,"The main mathematical object for spectral clustering is the graph Laplacian matrix. There are several diﬀerent deﬁnitions of graph Laplacian in the literature, and in the following we describe one particular deﬁnition."
1635,1636,The unnormalized graph
1636,1637,Laplacian is the m × m matrix L = D −W where D is a diagonal matrix with
1637,1638,"Then, the columns of H are orthonormal to each other and"
1638,1639,"RatioCut(C1, . . . , Ck) = trace(H⊤L H)."
1639,1640,"Let h1, . . . , hk be the columns of H. The fact that these vectors are nipulations, it can be shown that trace(H⊤L H) = Pk any vector v we have"
1640,1641,"Therefore, to minimize RatioCut we can search for a matrix H whose columns |Cj|. Unfortunately, this is an integer programming problem which we cannot solve eﬃciently. Instead,"
1641,1642,"Rm,k that minimizes trace(H⊤L H). As we will see in the next chapter about to set U to be the matrix whose columns are the eigenvectors corresponding to the k minimal eigenvalues of L. The resulting algorithm is called Unnormalized"
1642,1643,Spectral Clustering.
1643,1644,Unnormalized Spectral Clustering
1644,1645,Unnormalized Spectral Clustering
1645,1646,Initialize: Compute the unnormalized graph Laplacian L corresponding to the k smallest eigenvalues
1646,1647,"Let v1, . . . , vm be the rows of U"
1647,1648,"The spectral clustering algorithm starts with ﬁnding the matrix H of the k eigenvectors corresponding to the smallest eigenvalues of the graph Laplacian matrix. It then represents points according to the rows of H. It is due to the properties of the graph Laplacians that this change of representation is useful. algorithm to detect the clusters seamlessly. Intuitively, if H is as deﬁned in whose value is nonzero only on the element corresponding to the cluster it belongs"
1648,1649,The information bottleneck method is a clustering technique introduced by
1649,1650,"Tishby, Pereira, and Bialek. It relies on notions from information theory. To illustrate the method, consider the problem of clustering text documents where corresponding to index i appears in the document. Given a set of m documents, probability over a random variable x, indicating the identity of a document (thus taking values in [m]), and a random variable y, indicating the identity of a word in the dictionary (thus taking values in [n])."
1650,1651,"With this interpretation, the information bottleneck refers to the identity of a clustering as another random variable, denoted C, that takes values in [k] (where k will be set by the method as well). Once we have formulated x, y, C as random variables, we can use tools from information theory to express a clustering objective. In particular, the information bottleneck objective is p(C|x) I(x; C) −βI(C; y) , parameter, and the minimization is over all possible probabilistic assignments of points to clusters. Intuitively, we would like to achieve two contradictory goals."
1651,1652,"On one hand, we would like the mutual information between the identity of the document and the identity of the cluster to be as small as possible. This reﬂects the fact that we would like a strong compression of the original data. On the other hand, we would like high mutual information between the clustering variable and the identity of the words, which reﬂects the goal that the “relevant” information about the document (as reﬂected by the words that appear in the document) is retained. This generalizes the classical notion of minimal suﬃcient statistics2 used in parametric statistics to arbitrary distributions."
1652,1653,Solving the optimization problem associated with the information bottleneck principle is hard in the general case. Some of the proposed methods are similar
1653,1654,"A High Level View of Clustering damental questions remain unaddressed. First and foremost, what is clustering?"
1654,1655,What is it that distinguishes a clustering algorithm from any arbitrary function that takes an input space and outputs a partition of that space? Are there any basic properties of clustering that are independent of any speciﬁc algorithm or
1655,1656,One method for addressing such questions is via an axiomatic approach. There have been several attempts to provide an axiomatic deﬁnition of clustering. Let us demonstrate this approach by presenting the attempt made by Kleinberg
1656,1657,"Consider a clustering function, F, that takes as input any ﬁnite domain X with a dissimilarity function d over its pairs and returns a partition of X."
1657,1658,Consider the following three properties of such a function:
1658,1659,"Scale Invariance (SI) For any domain set X, dissimilarity function d, and"
1659,1660,"Richness (Ri) For any ﬁnite X and every partition C = (C1, . . . Ck) of X (into nonempty subsets) there exists some dissimilarity function d over X such , where the sum is over all values x can take and all respect to a statistical model and its associated unknown parameter, meaning that “no other statistic which can be calculated from the same sample provides any additional information as to the value of the parameter.” For example, if we assume that a variable is distributed normally with a unit variance and an unknown expectation, then the average function is a suﬃcient statistic."
1660,1661,"Consistency (Co) If d and d′ are dissimilarity functions over X, such that d′(x, y) ≤d(x, y) and if x, y belong to diﬀerent clusters in F(X, d) then d′(x, y) ≥d(x, y), then F(X, d) = F(X, d′)."
1661,1662,"A moment of reﬂection reveals that the Scale Invariance is a very natural requirement – it would be odd to have the result of a clustering function depend basically states that the outcome of the clustering function is fully controlled by the function d, which is also a very intuitive feature. The third requirement,"
1662,1663,"Consistency, is the only requirement that refers to the basic (informal) deﬁnition of clustering – we wish that similar points will be clustered together and that dissimilar points will be separated to diﬀerent clusters, and therefore, if points that already share a cluster become more similar, and points that are already separated become even less similar to each other, the clustering function should have even stronger “support” of its previous clustering decisions. ties: Scale Invariance, Richness, and Consistency."
1663,1664,"Assume, by way of contradiction, that some F does satisfy all three properties. Pick some domain set X with at least three points. By Richness, some d2 such that F(X, d2) ̸= F(X, d1). αd2. Consider F(X, d3). By the Scale Invariance property of F, we should have diﬀerent clusters w.r.t. F(X, d1), and d3(x, y) ≥d1(x, y), the Consistency of F implies that F(X, d3) = F(X, d1). This is a contradiction, since we chose d1, d2 so that F(X, d2) ̸= F(X, d1)."
1664,1665,"It is important to note that there is no single “bad property” among the three properties. For every pair of the the three axioms, there exist natural clustering functions that satisfy the two properties in that pair (one can even construct such examples just by varying the stopping criteria for the Single Linkage clustering function). On the other hand, Kleinberg shows that any clustering algorithm does satisfy Consistency)."
1665,1666,"The Kleinberg impossibility result can be easily circumvented by varying the properties. For example, if one wishes to discuss clustering functions that have"
1666,1667,"Alternatively, one can relax the Consistency property. For example, say that two clusterings C = (C1, . . . Ck) and C′ = (C′ l) are compatible if for every tained by trimming that dendrogram are compatible). “Reﬁnement Consistency” is the requirement that, under the assumptions of the Consistency property, the new clustering F(X, d′) is compatible with the old clustering F(X, d). Many common clustering functions satisfy this requirement as well as Scale Invariance erties of clustering functions that sound intuitive and desirable and are satisﬁed by some common clustering functions. cating that there is no “ideal” clustering function. Every clustering function will tion for any given task must therefore take into account the speciﬁc properties theorem shows). Clustering, just like classiﬁcation prediction, must take into account some prior knowledge about the speciﬁc task at hand."
1667,1668,"Clustering is an unsupervised learning problem, in which we wish to partition tering, and the information bottleneck. We discussed the diﬃculty of formalizing the intuitive meaning of clustering."
1668,1669,"Bibliographic Remarks spectral clustering we refer the reader to the excellent tutorial by Von Luxburg and suppose we break ties in the deﬁnition of Ci by assigning i to be the smallest value in argminj ∥x −µj∥. a partition of X into C1, . . . , Ck which minimizes the expression"
1669,1670,"Gk−diam((X, d), (C1, . . . , Ck)) = max diam objective. Fortunately, we have a very simple approximation algorithm: is, if we denote its output by ˆC1, . . . , ˆCk, and denote the optimal solution by following inequalities"
1670,1671,"Gk−diam((X, d), ( ˆC1, . . . , ˆCk)) ≤2r is a clustering that minimizes the objective"
1671,1672,"Gf((X, d), (C1, . . . Ck)) = where X ′ is either X or some superset of X. ness, and Consistency. Consider the Single Linkage clustering algorithm."
1672,1673,Fixed Number of Clusters (any ﬁxed nonzero number) stopping rule.
1673,1674,"Distance Upper Bound (any ﬁxed nonzero upper bound) stopping rule. for Single Linkage clustering, under which these two axioms are satisﬁed."
1674,1675,"For any ﬁnite X and every partition C = (C1, . . . Ck) of X (into nonempty subsets) there exists some dissimilarity function d over X such that F(X, d) = C."
1675,1676,"Prove that, for every number k, there exists a clustering function that"
1676,1677,Dimensionality Reduction
1677,1678,"Dimensionality reduction is the process of taking data in a high dimensional space and mapping it into a new space whose dimensionality is much smaller. mation theory. There are several reasons to reduce the dimensionality of the data. First, high dimensional data impose computational challenges. Moreover, ties of the learning algorithm (for example, in Nearest Neighbor classiﬁers the"
1678,1679,"Finally, dimensionality reduction can be used for interpretability of the data, for ﬁnding meaningful structure of the data, and for illustration purposes."
1679,1680,"In this chapter we describe popular methods for dimensionality reduction. In those methods, the reduction is performed by applying a linear transformation to the original data. That is, if the original data is in Rd and we want to embed enable a reasonable recovery of the original x. It is not hard to show that in"
1680,1681,The ﬁrst method we describe is called Principal Component Analysis (PCA). mations and the method ﬁnds the linear transformations for which the diﬀerences between the recovered vectors and the original vectors are minimal in the least
1681,1682,"Next, we describe dimensionality reduction using random matrices W. We which analyzes the distortion caused by such a random dimensionality reduction"
1682,1683,"Last, we show how one can reduce the dimension of all sparse vectors using again a random matrix. This process is known as Compressed Sensing. In this case, the recovery process is nonlinear but can still be implemented eﬃciently using linear programming."
1683,1684,"We conclude by underscoring the underlying “prior assumptions” behind PCA and compressed sensing, which can help us understand the merits and pitfalls of"
1684,1685,Personal use only. Not for distribution. Do not post.
1685,1686,Dimensionality Reduction
1686,1687,"Principal Component Analysis (PCA) mately) recover each original vector x from its compressed version. That is, for a compressed vector y = Wx, where y is in the low dimensional space Rn, we can construct ˜x = Uy, so that ˜x is the recovered version of x and resides in the original high dimensional space Rd."
1687,1688,"In PCA, we ﬁnd the compression matrix W and the recovering matrix U so that the total squared distance between the original and recovered vectors is minimal; namely, we aim at solving the problem"
1688,1689,To solve this problem we ﬁrst show that the optimal solution takes a speciﬁc
1689,1690,"U are orthonormal (namely, U ⊤U is the identity matrix of Rn) and W = U ⊤. namely, the range of V is R and V ⊤V = I. Therefore, each vector in R can be where we used the fact that V ⊤V is the identity matrix of Rn. Minimizing the preceding expression with respect to y by comparing the gradient with respect to y to zero gives that y = V ⊤x. Therefore, for each x we have that"
1690,1691,"In particular this holds for x1, . . . , xm and therefore we can replace U, W by"
1691,1692,"V, V ⊤and by that do not increase the objective"
1692,1693,"Since this holds for every U, W the proof of the lemma follows."
1693,1694,"On the basis of the preceding lemma, we can rewrite the optimization problem"
1694,1695,"We further simplify the optimization problem by using the following elementary where the trace of a matrix is the sum of its diagonal entries. Since the trace is i . The matrix A is symmetric and therefore it can be written using its spectral decomposition as A = VDV ⊤, where D is diagonal and"
1695,1696,"V ⊤V = VV ⊤= I. Here, the elements on the diagonal of D are the eigenvalues of"
1696,1697,A and the columns of V are the corresponding eigenvectors. We assume without the matrix U whose columns are the n eigenvectors of A corresponding to the largest n eigenvalues.
1697,1698,"Let x1, . . . , xm be arbitrary vectors in Rd, let A = Pm and let u1, . . . , un be n eigenvectors of the matrix A corresponding to the largest n eigenvalues of A. Then, the solution to the PCA optimization problem given with orthonormal columns and let B = V ⊤U. Then, VB = VV ⊤U = U. It"
1698,1699,"U ⊤AU = B⊤V ⊤VDV ⊤VB = B⊤DB,"
1699,1700,"Note that B⊤B = U ⊤VV ⊤U = U ⊤U = I. Therefore, the columns of B are also orthonormal, which implies that Pd"
1700,1701,"Rd,d be a matrix such that its ﬁrst n columns are the columns of B and in addition ˜B⊤˜B = I. Then, for every j we have Pd"
1701,1702,Dimensionality Reduction thonormal columns it holds that trace(U ⊤AU) ≤Pn if we set U to be the matrix whose columns are the n leading eigenvectors of A we obtain that trace(U ⊤AU) = Pn
1702,1703,It is a common practice to “center” the examples before applying
1703,1704,"PCA. That is, we ﬁrst calculate µ = vectors (x1 −µ), . . . , (xm −µ). This is also related to the interpretation of PCA"
1704,1705,A More Eﬃcient Solution for the Case d ≫m
1705,1706,In some situations the original dimensionality of the data is much larger than the number of examples m. The computational complexity of calculating the
1706,1707,PCA solution as described previously is O(d3) (for calculating eigenvalues of A) plus O(md2) (for constructing the matrix A). We now show a simple trick that enables us to calculate the PCA solution more eﬃciently when d ≫m.
1707,1708,"Recall that the matrix A is deﬁned to be Pm i . It is convenient to rewrite ⟨xi, xj⟩. Suppose that u is an eigenvector of B: That is, Bu = λu for some λ ∈R. Multiplying the equality by X⊤and using the deﬁnition of B we obtain"
1708,1709,"X⊤XX⊤u = λX⊤u. But, using the deﬁnition of A, we get that A(X⊤u) ="
1709,1710,We can therefore calculate the PCA solution by calculating the eigenvalues of
1710,1711,B instead of A. The complexity is O(m3) (for calculating eigenvalues of B) and m2d (for constructing the matrix B).
1711,1712,The previous discussion also implies that to calculate the PCA solution we only need to know how to calculate inner products between vectors.
1712,1713,"This enables us to calculate PCA implicitly even when d is very large (or even inﬁnite) using kernels, which yields the kernel PCA algorithm."
1713,1714,Implementation and Demonstration
1714,1715,A pseudocode of PCA is given in the following. dimensionality reduction to R1 using PCA (red circles). number of components n
1715,1716,"Let u1, . . . , un be the eigenvectors of A with largest eigenvalues"
1716,1717,"Let v1, . . . , vn be the eigenvectors of B with largest eigenvalues output: u1, . . . , un"
1717,1718,"To illustrate how PCA works, let us generate vectors in R2 that approximately reside on a line, namely, on a one dimensional subspace of R2. For example, suppose that each example is of the form (x, x + y) where x is chosen uniformly eigenvector corresponding to the largest eigenvalue will be close to the vector"
1718,1719,"Next, we demonstrate the eﬀectiveness of PCA on a data set of faces. We extracted images of faces from the Yale data set (Georghiades, Belhumeur & dimensionality is very high."
1719,1720,Dimensionality Reduction reconstruction. Middle row: an enlarged version of one of the images before and after
1720,1721,PCA. Bottom: The images after dimensionality reduction to R2. The diﬀerent marks indicate diﬀerent individuals.
1721,1722,"In this section we show that reducing the dimension by using a random linear transformation leads to a simple compression scheme with a surprisingly low referred to as a random projection. In particular, we provide a variant of a famous lemma due to Johnson and Lindenstrauss, showing that random projections do not distort Euclidean distances too much."
1722,1723,"Let x1, x2 be two vectors in Rd. A matrix W does not distort too much the distance between x1 and x2 if the ratio the transformation are almost the same. To show that ∥Wx1 −Wx2∥is not too far away from ∥x1 −x2∥it suﬃces to show that W does not distort the norm of the diﬀerence vector x = x1 −x2. Therefore, from now on we focus on the ratio"
1723,1724,We start with analyzing the distortion caused by applying a random projection equivalent inequality is
1724,1725,"Let wi be the ith row of W. The random variable ⟨wi, x⟩is a weighted sum of d independent normal random variables and therefore it is normally distributed with zero mean and variance P n distribution. The claim now follows directly from a measure concentration property of χ2 random variables stated in"
1725,1726,Let Q be a ﬁnite set of vectors
1726,1727,"Dimensionality Reduction such that each element of W is distributed normally with zero mean and variance dimension of x. In fact, the bound holds even if x is in an inﬁnite dimensional"
1727,1728,"Compressed sensing is a dimensionality reduction technique which utilizes a prior thermore, this compression is lossless – we can reconstruct x exactly from the s (index,value) pairs. Now, lets take one step forward and assume that x = Uα, ural vectors are (at least approximately) sparse in some representation. In fact, this assumption underlies many modern compression schemes. For example, the are approximately sparse in a wavelet basis."
1728,1729,"Can we still compress x into roughly s numbers? Well, one simple way to do this is to multiply x by U ⊤, which yields the sparse vector α, and then represent α by its s (index,value) pairs. However, this requires us ﬁrst to “sense” x, to store it, and then to multiply it by U ⊤. This raises a very natural question: Why go to so much eﬀort to acquire all the data when most of what we get will be thrown away? Cannot we just directly measure the part that will not end up presses the data. The key result is that a random linear transformation can compress x without losing information. The number of measurements needed is order of s log(d). That is, we roughly acquire only the important information about the signal. As we will see later, the price we pay is a slower reconstruction phase. In some situations, it makes sense to save time in compression even at the price of a slower reconstruction. For example, a security camera should sense and compress a large amount of images while most of the time we do not need to decode the compressed data at all. Furthermore, in many practical applications, formed eﬃciently in hardware. For example, a team led by Baraniuk and Kelly has proposed a camera architecture that employs a digital micromirror array to perform optical calculations of a linear transformation of an image. In this case, obtaining each compressed measurement is as easy as obtaining a single raw measurement. Another important application of compressed sensing is medical imaging, in which requiring fewer measurements translates to less radiation for stricted Isoperimetric Property (RIP). A matrix that satisﬁes this property is guaranteed to have a low distortion of the norm of any sparse representable is greater than an order of s log(d)."
1729,1730,"The ﬁrst theorem establishes that RIP matrices yield a lossless compression scheme for sparse vectors. It also provides a (noneﬃcient) reconstruction scheme. be a reconstructed vector. Then, ˜x = x."
1730,1731,Dimensionality Reduction
1731,1732,"We assume, by way of contradiction, that ˜x ̸= x. Since x satisﬁes the constraints in the optimization problem that deﬁnes ˜x we clearly have that which leads to a contradiction. because we need to minimize a combinatorial objective (the sparsity of v). Quite can be solved eﬃciently. This is stated formally in the following theorem."
1732,1733,"In fact, we will prove a stronger result, which holds even if x is not a sparse arbitrary vector and denote"
1733,1734,"That is, xs is the vector which equals x on the s largest elements of x and equals be the reconstructed vector. Then,"
1734,1735,"Note that in the special case that x = xs we get an exact recovery, x⋆= x, so likely to be RIP. In fact, the theorem shows that multiplying a random matrix by an orthonormal matrix also provides an RIP matrix. This is important for compressing signals of the form x = Uα where x is not sparse but α is sparse."
1735,1736,"In that case, if W is a random matrix and we compress using y = Wx then this is the same as compressing α by y = (WU)α and since WU is also RIP we can reconstruct α (and thus also x) from y."
1736,1737,Let h = x⋆−x. Given a vector v and a set of indices I we denote by vI the the partition as follows. In T0 we put the s indices corresponding to the s largest elements in absolute values of x (ties are broken arbitrarily). Let T c
1737,1738,"Next, T1 will be the s indices corresponding to the s largest elements in absolute the s largest elements in absolute value of hT c"
1738,1739,"To prove the theorem we ﬁrst need the following lemma, which shows that"
1739,1740,RIP also implies approximate orthogonality.
1740,1741,"I, J, both of size at most s, and for any vector u we have that ⟨WuI, WuJ⟩≤"
1741,1742,"We are now ready to prove the theorem. Clearly,"
1742,1743,To prove the theorem we will show the following two claims:
1743,1744,Dimensionality Reduction and this will conclude our proof.
1744,1745,To prove this claim we do not use the RIP condition at all but only use the fact
1745,1746,"Next, we show that ∥hT c"
1746,1747,"For the second claim we use the RIP condition to get that ⟨WhT0 + WhT1, WhTj⟩."
1747,1748,Rearranging the inequality gives which concludes the proof of the second claim. lemma with a simple covering argument.
1748,1749,We start with a covering property of the unit ball.
1749,1750,"Let k be an integer and let ℓ2 ball of Rd. Since the points in Q′ are distributed evenly on the unit ℓ∞ball, the size of Q is the size of Q′ times the ratio between the volumes of the unit ℓ2"
1750,1751,"For simplicity, assume that d is even and therefore"
1751,1752,Dimensionality Reduction where in the last inequality we used Stirling’s approximation. Overall we obtained
1752,1753,Let U be an orthonormal d × d matrix and let I ⊂[d] be a set
1753,1754,"But since U is orthogonal we also have that the condition given in the lemma, the following holds with probability of at least"
1754,1755,This also implies that
1755,1756,Let a be the smallest number such that
1756,1757,But the deﬁnition of a implies that
1757,1758,Dimensionality Reduction
1758,1759,PCA or Compressed Sensing?
1759,1760,"Suppose we would like to apply a dimensionality reduction technique to a given set of examples. Which method should we use, PCA or compressed sensing? In this section we tackle this question, by underscoring the underlying assumptions behind the two methods. fect recovery. PCA guarantees perfect recovery whenever the set of examples is contained in an n dimensional subspace of Rd. Compressed sensing guarantees perfect recovery whenever the set of examples is sparse (in some basis). On the basis of these observations, we can describe cases in which PCA will be better than compressed sensing and vice versa."
1760,1761,"As a ﬁrst example, suppose that the examples are the vectors of the standard will lead to poor performance, since the data is far from being in an n dimensional subspace, as long as n < d. Indeed, it is easy ro verify that in such a case, the"
1761,1762,"We next show a case where PCA is better than compressed sensing. Consider m examples that are exactly on an n dimensional subspace. Clearly, in such a case, PCA will lead to perfect recovery. As to compressed sensing, note that the subspace. Therefore, compressed sensing would also work if we will reduce the dimension to Ω(n log(d)). However, with exactly n dimensions, compressed sensing might fail. PCA has also better resilience to certain types of noise. See mations: PCA and random projections. We have shown that PCA is optimal in tion procedure to be linear as well. However, if we allow nonlinear reconstruction, dom projections can signiﬁcantly outperform PCA. This fact is at the heart of the compressed sensing method."
1762,1763,"Bibliographic Remarks position (SVD). The SVD method is described in Appendix C. SVD dates back sides PCA and SVD, there are additional names that refer to the same idea and"
1763,1764,"Young theorem (after Carl Eckart and Gale Young who analyzed the method in compression scheme is impossible. increase βi, while possibly decreasing βj for some j > i, and obtain a better ing nonlinear dimensionality reduction on the basis of the kernel trick (see"
1764,1765,"Let X be some instance space and let S = {x1, . . . , xm} be a set of points (possibly of inﬁnite dimension). Let K : X × X be a kernel function, that is, k(x, x′) = ⟨ψ(x), ψ(x′)⟩. Kernel PCA is the process of mapping the elements in S into V using ψ, and then applying PCA over {ψ(x1), . . . , ψ(xm)} into"
1765,1766,Rn. The output of this process is the set of reduced elements.
1766,1767,"Show how this process can be done in polynomial time in terms of m stant time. In particular, if your implementation requires multiplication of two matrices A and B, verify that their product can be computed. Similarly,"
1767,1768,"Dimensionality Reduction if an eigenvalue decomposition of some matrix C is required, verify that this decomposition can be computed."
1768,1769,"Let x1, . . . , xm be m vectors in Rd, and let x be a random vector distributed random variable ⟨w, x⟩has maximal variance. That is, we would like to"
1769,1770,"Var[⟨w, x⟩] = argmax"
1770,1771,"Show that the solution of the problem is to set w to be the ﬁrst principle vector of x1, . . . , xm. mizes the variance of ⟨w2, x⟩, but is also uncorrelated to ⟨w1, x⟩. That is, we would like to solve:"
1771,1772,"Show that the solution to this problem is to set w to be the second principal component of x1, . . . , xm."
1772,1773,"E[(⟨w1, x⟩)(⟨w, x⟩)] = w⊤ i . Since w is an eigenvector of A we have that the lemma tells us that a random projection preserves distances between a ﬁnite set of vectors. In this exercise you need to prove that if the set of vectors are within the unit ball, then not only are the distances between any two vectors preserved, but the inner product is also preserved. that these vectors are linearly separable with margin of γ. Assume that"
1773,1774,"We started this book with a distribution free learning framework; namely, we did not impose any assumptions on the underlying distribution over the data."
1774,1775,"Furthermore, we followed a discriminative approach in which our goal is not to learn the underlying distribution but rather to learn an accurate predictor. In this chapter we describe a generative approach, in which it is assumed that the underlying distribution over the data has a speciﬁc parametric form and our goal is to estimate the parameters of the model. This task is called parametric density"
1775,1776,The discriminative approach has the advantage of directly optimizing the ing distribution. This was phrased as follows by Vladimir Vapnik in his principle for solving problems using a restricted amount of information:
1776,1777,"When solving a given problem, try to avoid a more general problem as an intermediate"
1777,1778,"Of course, if we succeed in learning the underlying distribution accurately, we are considered to be “experts” in the sense that we can predict by using the Bayes optimal classiﬁer. The problem is that it is usually more diﬃcult to learn the underlying distribution than to learn an accurate predictor. However, in some situations, it is reasonable to adopt the generative learning approach."
1778,1779,"For example, sometimes it is easier (computationally) to estimate the parameters of the model than to learn a discriminative predictor. Additionally, in some cases we do not have a speciﬁc task at hand but rather would like to model the data either for making predictions at a later time without having to retrain a predictor or for the sake of interpretability of the data."
1779,1780,"We start with a popular statistical method for estimating the parameters of the data, which is called the maximum likelihood principle. Next, we describe two scribe the EM algorithm for calculating the maximum likelihood in the presence of latent variables. We conclude with a brief description of Bayesian reasoning."
1780,1781,Personal use only. Not for distribution. Do not post.
1781,1782,Maximum Likelihood Estimator
1782,1783,"Let us start with a simple example. A drug company developed a new drug to treat some deadly disease. We would like to estimate the probability of survival when using the drug. To do so, the drug company sampled a training set of m people and gave them the drug. Let S = (x1, . . . , xm) denote the training set, the probability of survival. the average of m i.i.d. binary random variables we can use Hoeﬀding’s inequality formally explain now. We ﬁrst write the probability of generating the sample S:"
1783,1784,"P[S = (x1, . . . , xm)] ="
1784,1785,"Maximum Likelihood Estimation for Continuous Random Variables overcome this technical problem we deﬁne the likelihood as log of the density of the probability of X at x. That is, given an i.i.d. training set S = (x1, . . . , xm)"
1785,1786,"As an example, consider a Gaussian random variable, for which the density"
1786,1787,We can rewrite the likelihood as
1787,1788,Solving the preceding equations we obtain the maximum likelihood estimates:
1788,1789,Note that the maximum likelihood estimate is not always an unbiased estimator.
1789,1790,"For example, while ˆµ is unbiased, it is possible to show that the estimate ˆσ of"
1790,1791,Simplifying Notation
1791,1792,"To simplify our notation, we use P[X = x] in this chapter to describe both the probability that X = x (for discrete random variables) and the density of the distribution at x (for continuous variables)."
1792,1793,Maximum Likelihood and Empirical Risk Minimization
1793,1794,The maximum likelihood estimator shares some similarity with the Empirical
1794,1795,"Risk Minimization (ERM) principle, which we studied extensively in previous chapters. Recall that in the ERM principle we have a hypothesis class H and empirical risk. We now show that the maximum likelihood estimator is an ERM for a particular loss function. likelihood principle is equivalent to minimizing the empirical risk with respect tion. The relative entropy is a divergence measure between two probabilities. sumption aﬀects our density estimation, even in the limit of inﬁnite data. It shows that if the underlying distribution is indeed of a parametric form, then by bution. However, if the distribution is not of the assumed parametric form, even the best parameter leads to an inferior model and the suboptimality is measured by the relative entropy divergence."
1795,1796,Generalization Analysis
1796,1797,"How good is the maximum likelihood estimator when we learn from a ﬁnite mated solution of the density estimation problem. Unlike discriminative learning, where there is a clear notion of “loss,” in generative learning there are various ways to deﬁne the loss of a model. On the basis of the previous subsection, one"
1797,1798,"In some situations, it is easy to prove that the maximum likelihood principle guarantees low true risk as well. For example, consider the problem of estimating the mean of a Gaussian variable of unit variance. We saw previously that the i xi. Let µ⋆be the optimal"
1798,1799,"Next, we note that ˆµ is the average of m Gaussian variables and therefore it is also distributed normally with mean µ⋆and variance σ⋆/m. From this fact we"
1799,1800,"In some situations, the maximum likelihood estimator clearly overﬁts. For we saw previously, using Hoeﬀding’s inequality we can easily derive a guarantee"
1800,1801,This simple example shows that we should be careful in applying the maximum likelihood principle. viously in the book. A simple regularization technique is outlined in Exercise sumptions and parameter estimations simplify the learning process. Consider optimal classiﬁer is
1801,1802,"To describe the probability function P[Y = y|X = x] we need 2d parameters,"
1802,1803,This implies that the number of examples we need grows exponentially with the
1803,1804,"In the Naive Bayes approach we make the (rather naive) generative assumption that given the label, the features are independent of each other. That is,"
1804,1805,"With this assumption and using Bayes’ rule, the Bayes optimal classiﬁer can be"
1805,1806,P[Y = y]P[X = x|Y = y]
1806,1807,"Here, the generative assumption we made reduced signiﬁcantly the number of parameters we need to learn. ple, the resulting classiﬁer is called the Naive Bayes classiﬁer."
1807,1808,Linear Discriminant Analysis
1808,1809,"Linear discriminant analysis (LDA) is another demonstration of how generative assumptions simplify the learning process. As in the Naive Bayes classiﬁer we vector of features x = (x1, . . . , xd). But now the generative assumption is as that the conditional probability of X given Y is a Gaussian distribution. Finally, the covariance matrix of the Gaussian distribution is the same for both values the density distribution is given by"
1809,1810,"As we have shown in the previous section, using Bayes’ rule we can write"
1810,1811,P[Y = y]P[X = x|Y = y].
1811,1812,"We can rewrite this as ⟨w, x⟩+ b where tioned generative assumptions, the Bayes optimal classiﬁer is a linear classiﬁer."
1812,1813,"Additionally, one may train the classiﬁer by estimating the parameter µ0, µ1 and Σ from the data, using, for example, the maximum likelihood estimator."
1813,1814,"With those estimators at hand, the values of w and b can be calculated as in"
1814,1815,Latent Variables and the EM Algorithm
1815,1816,"In generative models we assume that the data is generated by sampling from a speciﬁc parametric distribution over our instance space X. Sometimes, it is convenient to express this distribution using latent random variables. A natural example is a mixture of k Gaussian distributions. That is, X = Rd and we assume that each x is generated as follows. First, we choose a random number in"
1816,1817,"P[Y = y] = cy. Second, we choose x on the basis of the value of Y according to a Gaussian distribution"
1817,1818,"Therefore, the density of X can be written as:"
1818,1819,"P[Y = y]P[X = x|Y = y] less, we introduce Y since it helps us describe a simple parametric form of the algorithm, due to Dempster, Laird, and Rubin, is an iterative procedure for global maximum, it often works reasonably well in practice."
1819,1820,"EM is designed for those cases in which, had we known the values of the latent variables Y , then the maximum likelihood optimization problem would have been tractable. More precisely, deﬁne the following function over m × k matrices and"
1820,1821,"If each row of Q deﬁnes a probability over the ith latent variable given X = xi, (x1, y1), . . . , (xm, ym), where the expectation is with respect to the choice of each yi on the basis of the ith row of Q. In the deﬁnition of F, the summation is outside the log, and we assume that this makes the optimization problem with"
1821,1822,"The intuitive idea of EM is that we have a “chicken and egg” problem. On one hand, had we known Q, then by our assumption, the optimization problem of"
1822,1823,• Expectation Step: Set
1823,1824,"By our assumption, it is possible to solve this optimization problem eﬃ- procedure terminates after the improvement in the likelihood value stops being"
1824,1825,EM as an Alternate Maximization Algorithm
1825,1826,"To analyze the EM algorithm, we ﬁrst view it as an alternate maximization algorithm. Deﬁne the following objective function"
1826,1827,The second term is the sum of the entropies of the rows of Q. Let be the set of matrices whose rows deﬁne probabilities over [k]. The following
1827,1828,The EM procedure can be rewritten as:
1828,1829,The preceding lemma immediately implies:
1829,1830,"By the lemma we have simplicity, we assume that Σ1 = Σ2 = · · · = Σk = I, where I is the identity matrix. Specifying the EM algorithm for this case we obtain the following: where Zi is a normalization factor which ensures that P which in our case amounts to maximizing the following expression w.r.t. c ranging terms we obtain:"
1830,1831,"That is, µy is a weighted average of the xi where the weights are according to the probabilities calculated in the E step. To ﬁnd the optimal c we need to be more careful since we must ensure that c is a probability vector. In to a cluster according to the distance ∥xi −µy∥. Then, we update each center µy according to the average of the examples assigned to this cluster. In the EM approach, however, we determine the probability that each example belongs to each cluster. Then, we update the centers on the basis of a weighted sum over"
1831,1832,"The maximum likelihood estimator follows a frequentist approach. This means that we do not know its value. A diﬀerent approach to parameter estimation is called Bayesian reasoning. In the Bayesian approach, our uncertainty about name indicates, the prior distribution should be deﬁned by the learner prior to"
1832,1833,"As an example, let us consider again the drug company which developed a new drug. On the basis of past experience, the statisticians at the drug company believe that whenever a drug has reached the level of clinic experiments on people, it is likely to be eﬀective. They model this prior belief by deﬁning a"
1833,1834,X. This is because we can write the probability over X as a marginal probability where the last equality follows from the deﬁnition of conditional probability. If
1834,1835,"Seemingly, once we know P[X = x], a training set S = (x1, . . . , xm) tells us nothing as we are already experts who know the distribution over a new point"
1835,1836,"X. However, the Bayesian view introduces dependency between S and X. This is since it is just a parameter of the distribution, a new point X and previous points"
1836,1837,S are always independent.
1837,1838,"In the Bayesian framework, since X and S are not independent anymore, what we would like to calculate is the probability of X given S, which by the chain rule can be written as follows:"
1838,1839,The second inequality follows from the assumption that X and S are independent
1839,1840,We therefore obtain the following expression for Bayesian prediction:
1840,1841,"Getting back to our drug company example, we can rewrite P[X = x|S] as"
1841,1842,Solving the preceding integral (using integration by parts) we obtain
1842,1843,"Recall that the prediction according to the maximum likelihood principle in this m . The Bayesian prediction with uniform prior is rather similar to the maximum likelihood prediction, except it adds “pseudoexamples” to the training set, thus biasing the prediction toward the uniform prior."
1843,1844,Maximum A Posteriori
1844,1845,"In many situations, it is diﬃcult to ﬁnd a closed form solution to the integral riori estimator. Once this value is found, we can calculate the probability that"
1845,1846,"X = x given the maximum a posteriori estimator and independently on S. bution over the data. In particular, in parametric density estimation we further ric form and our goal is to estimate the parameters of the model. We have lihood, Bayesian estimation, and maximum a posteriori. We have also described ferent assumptions on the underlying data distribution, in particular, Naive"
1846,1847,Bibliographic Remarks
1847,1848,"The maximum likelihood principle was studied by Ronald Fisher in the beginning of the 20th century. Bayesian statistics follow Bayes’ rule, which is named after the 18th century English mathematician Thomas Bayes."
1848,1849,There are many excellent books on the generative and Bayesian approaches
1849,1850,• Show that the preceding objective is equivalent to the usual empirical error had we added two pseudoexamples to the training set. Conclude that the regularized maximum likelihood estimator would be + is a vector of nonnegative weights. Verify that the M step y νy ν. Show that c⋆is a probability vector.
1850,1851,• Show that the optimization problem is equivalent to the problem:
1851,1852,"• Using properties of the relative entropy, conclude that c⋆is the solution to the optimization problem."
1852,1853,Feature Selection and Generation
1853,1854,"In the beginning of the book, we discussed the abstract model of learning, in which the prior knowledge utilized by the learner is fully encoded by the choice of the hypothesis class. However, there is another modeling choice, which we have so far ignored: How do we represent the instance space X? For example, in the papayas learning problem, we proposed the hypothesis class of rectangles in to represent a papaya as a two dimensional point corresponding to its softness and color. Only after that did we choose the hypothesis class of rectangles as a class of mappings from the plane into the label set. The transformation from the real world object “papaya” into the scalar representing its softness or its color is called a feature function or a feature for short; namely, any measurement of the real world object can be regarded as a feature. If X is a subset of a vector understand that the way we encode real world objects as an instance space X is by itself prior knowledge about the problem. resented as a subset of a vector space, we might still want to change it into a diﬀerent representation and apply a hypothesis class on top of it. That is, we may deﬁne a hypothesis class on X by composing some class H on top of a mapping ψ that maps each original instance in X into some Hilbert space. And, indeed, the choice of ψ is another form of prior knowledge we impose on the"
1854,1855,In this chapter we study several methods for constructing a good feature set.
1855,1856,"We start with the problem of feature selection, in which we have a large pool of features and our goal is to select a small number of features that will be used by our predictor. Next, we discuss feature manipulations and normalization."
1856,1857,"These include simple transformations that we apply on our original features. Such transformations may decrease the sample complexity of our learning algorithm, its bias, or its computational complexity. Last, we discuss several approaches for feature learning. In these methods, we try to automate the process of feature"
1857,1858,Personal use only. Not for distribution. Do not post.
1858,1859,Feature Selection and Generation mate feature learner. Any feature learning algorithm might fail on some problem.
1859,1860,"In other words, the success of each feature learner relies (sometimes implicitly) on some form of prior assumption on the data distribution. Furthermore, the relative quality of features highly depends on the learning algorithm we are later going to apply using these features. This is illustrated in the following example."
1860,1861,"Consider a regression problem in which X = R2, Y = R, and the loss function is the squared loss. Suppose that the underlying distribution is such that an example (x, y) is generated as follows: First, we sample x1 from"
1861,1862,"Finally, the second feature is set to be x2 = y + z, where z is sampled from the feature. Intuitively, the ﬁrst feature should be preferred over the second feature as the target can be perfectly predicted based on the ﬁrst feature alone, while it cannot be perfectly predicted based on the second feature. Indeed, choosing the ﬁrst feature would be the right choice if we are later going to apply polynomial regressor, then we should prefer the second feature over the ﬁrst one, since the optimal linear predictor based on the ﬁrst feature will have a larger risk than the optimal linear predictor based on the second feature. sented as a vector of d features. Our goal is to learn a predictor that only relies on k ≪d features. Predictors that use only a small subset of features require a smaller memory footprint and can be applied faster. Furthermore, in applications such as medical diagnostics, obtaining each possible “feature” (e.g., test result) can be costly; therefore, a predictor that uses only a small number of features is desirable even at the cost of a small degradation in performance, relative to a predictor that uses more features. Finally, constraining the hypothesis class to use a small subset of features can reduce its estimation error and thus prevent"
1862,1863,"Ideally, we could have tried all subsets of k out of d features and choose the subset which leads to the best performing predictor. However, such an exhaustive search is usually computationally intractable. In the following we describe three computationally feasible approaches for feature selection. While these methods cannot guarantee ﬁnding the optimal subset, they often work reasonably well in practice. Some of the methods come with formal guarantees on the quality of the selected subsets under certain assumptions. We do not discuss these guarantees"
1863,1864,"Maybe the simplest approach for feature selection is the ﬁlter method, in which we assess individual features, independently of other features, according to some quality measure. We can then select the k features that achieve the highest score (alternatively, decide also on the number of features to select according to the value of their scores)."
1864,1865,Many quality measures for features have been proposed in the literature. cording to the error rate of a predictor that is trained solely by that feature.
1865,1866,"To illustrate this, consider a linear regression problem with the squared loss."
1866,1867,"Let v = (x1,j, . . . , xm,j) ∈Rm be a vector designating the values of the jth feature on a training set of m examples and let y = (y1, . . . , ym) ∈Rm be the values of the target on the same m examples. The empirical squared loss of an"
1867,1868,"ERM linear predictor that uses only the jth feature would be where the meaning of adding a scalar b to a vector v is adding b to all coordinates of v. To solve this problem, let ¯v = feature and let ¯y = objective we obtain the value"
1868,1869,"Ranking the features according to the minimal loss they achieve is equivalent to ranking them according to the absolute value of the following score (where now a higher score yields a better feature): merator is the empirical estimate of the covariance of the jth feature and the target value, E[(v −E v)(y −E y)], while the denominator is the squared root of with zero empirical risk."
1869,1870,Feature Selection and Generation
1870,1871,"If Pearson’s coeﬃcient equals zero it means that the optimal linear function predicting y. However, this does not mean that v is a bad feature, as it might be the case that together with other features v can perfectly predict y. Indeed, consider a simple example in which the target is generated by the function y = x1 + 2x2. Assume also that x1 is generated from the uniform distribution over"
1871,1872,"2z, where z is also generated i.i.d. from the uniform"
1872,1873,"Therefore, for a large enough training set, the ﬁrst feature is likely to have a"
1873,1874,"Pearson’s correlation coeﬃcient that is close to zero, and hence it will most probably not be selected. However, no function can predict the target value well without knowing the ﬁrst feature."
1874,1875,There are many other score functions that can be used by a ﬁlter method.
1875,1876,Notable examples are estimators of the mutual information or the area under the receiver operating characteristic (ROC) curve. All of these score functions suﬀer from similar problems to the one illustrated previously. We refer the reader
1876,1877,Greedy Selection Approaches
1877,1878,"Greedy selection is another popular approach for feature selection. Unlike ﬁlter methods, greedy selection approaches are coupled with the underlying learning algorithm. The simplest instance of greedy selection is forward greedy selection."
1878,1879,"We start with an empty set of features, and then we gradually add one feature at a time to the set of selected features. Given that our current set of selected features is I, we go over all i /∈I, and apply the learning algorithm on the set choose to add the feature that yields the predictor with the smallest risk (on the training set or on a validation set). This process continues until we either select k features, where k is a predeﬁned budget of allowed features, or achieve an accurate enough predictor."
1879,1880,"To illustrate the forward greedy selection approach, we specify it to the problem of linear regression with be the ith column of X. Given a set I ⊂[d] we denote by XI the matrix whose"
1880,1881,"The forward greedy selection method starts with I0 = ∅. At iteration t, we look for the feature index jt, which is in tion approach for linear regression which is called Orthogonal Matching Pursuit (OMP). The idea is to keep an orthogonal basis of the features aggregated so far. Let Vt be a matrix whose columns form an orthonormal basis of the columns every j, we decompose Xj = vj + uj where vj = Vt−1V ⊤ t−1Xj is the projection"
1881,1882,It follows that we should select the feature
1882,1883,The rest of the update is to set
1883,1884,"The OMP procedure maintains an orthonormal basis of the selected features, where in the preceding description, the orthonormalization property is obtained orthonormal basis in a numerically stable manner."
1884,1885,Feature Selection and Generation
1885,1886,Orthogonal Matching Pursuit (OMP) budget of features T
1886,1887,More Eﬃcient Greedy Selection Criteria
1887,1888,"Let R(w) be the empirical risk of a vector w. At each round of the forward greedy selection method, and for every possible j, we should minimize R(w)"
1888,1889,"A simpler approach is to choose jt that minimizes the weights of the previously chosen coordinates intact and only optimize over the new variable. Therefore, for each j we need to solve an optimization problem over a single variable, which is a much easier task than optimizing over t."
1889,1890,An even simpler approach is to upper bound R(w) using a “simple” function and then choose the feature which leads to the largest decrease in this upper
1890,1891,R(w + ηej) ≤R(w) + η ∂R(w) value into the above yields
1891,1892,This value is minimized if the partial derivative of R(w) with respect to wj is maximal. We can therefore choose jt to be the index of the largest coordinate of the gradient of R(w) at w. selection procedure with respect to the function
1892,1893,Backward Elimination
1893,1894,"Another popular greedy selection approach is backward elimination. Here, we start with the full set of features, and then we gradually remove one feature at a time from the set of features. Given that our current set of selected features is I,"
1894,1895,"Each such application yields a diﬀerent predictor, and we choose to remove the feature i for which the predictor obtained from I \ {i} has the smallest risk (on the training set or on a validation set)."
1895,1896,"Naturally, there are many possible variants of the backward elimination idea."
1896,1897,It is also possible to combine forward and backward greedy steps.
1897,1898,The problem of minimizing the empirical risk subject to a budget of k features
1898,1899,"In other words, we want w to be sparse, which implies that we only need to measure the features corresponding to nonzero elements of w. where k1 is a parameter. Since the ℓ1 norm is a convex function, this problem can be solved eﬃciently as long as the loss function is convex. A related problem is minimizing the sum of LS(w) plus an ℓ1 norm regularization term,"
1899,1900,Feature Selection and Generation are in some sense equivalent.
1900,1901,"The ℓ1 regularization often induces sparse solutions. To illustrate this, let us start with the simple optimization problem thresholding” operator w = sign(x) [|x| −λ]+ ,"
1901,1902,"Next, consider a one dimensional regression problem with respect to the squared"
1902,1903,We can rewrite the problem as
1903,1904,"For simplicity let us assume that then the optimal solution is w = sign(⟨x, y⟩) [|⟨x, y⟩|/m −λ]+ ."
1904,1905,"That is, the solution will be zero unless the correlation between the feature x"
1905,1906,"Unlike the ℓ1 norm, the ℓ2 norm does not induce sparse solutions."
1906,1907,"Indeed, consider the problem above with an ℓ2 regularization, namely,"
1907,1908,"Then, the optimal solution is"
1908,1909,This solution will be nonzero even if the correlation between x and y is very small.
1909,1910,"In contrast, as we have shown before, when using ℓ1 regularization, w will be nonzero only if the correlation between x and y is larger than the regularization"
1910,1911,"Adding ℓ1 regularization to a linear regression problem with the squared loss yields the LASSO algorithm, deﬁned as"
1911,1912,Under some assumptions on the distribution and the regularization parameter and the references therein). Another advantage of the ℓ1 norm is that a vector
1912,1913,Feature Manipulation and Normalization
1913,1914,"Feature manipulations or normalization include simple transformations that we apply on each of our original features. Such transformations may decrease the approximation or estimation errors of our hypothesis class or can yield a faster algorithm. Similarly to the problem of feature selection, here again there are no absolute “good” and “bad” transformations, but rather each transformation that we apply should be related to the learning algorithm we are going to apply on the resulting feature vector as well as to our prior assumptions on the problem."
1914,1915,"To motivate normalization, consider a linear regression problem with the"
1915,1916,"The crux of the preceding example is that the two features have completely diﬀerent scales. Feature normalization can overcome this problem. There are many ways to perform feature normalization, and one of the simplest approaches the preceding example, if we divide each feature by the maximal value it attains"
1916,1917,Feature Selection and Generation ridge regression is quite close to w⋆. ularized loss minimization depend on the norm of the optimal vector w⋆and the instance vector remains roughly the same; hence the normalization greatly improves the estimation error.
1917,1918,"Feature normalization can also improve the runtime of the learning algorithm. tion problem. The number of iterations required by SGD to converge also depends on the norm of w⋆and on the maximal norm of ∥x∥. Therefore, as before, using normalization can greatly decrease the runtime of SGD."
1918,1919,"Next, we demonstrate in the following how a simple transformation on features, a large number, suppose that the target y is chosen uniformly at random from bounded but with a very small probability it gets a very high value. Then, for any w, the expected squared loss of w is"
1919,1920,Solving for w we obtain that w⋆= ample shows that a simple transformation can have a signiﬁcant inﬂuence on the approximation error. formation actually hurts performance and increases the approximation error.
1920,1921,"This is not surprising, as we have already argued that feature transformations"
1921,1922,"For linear predictors and loss functions of the form ℓ(w, (x, y)) = φ(⟨w, x⟩, y), where φ is ample, a prior assumption that may lead us to use the “clipping” transformation is that features that get values larger than a predeﬁned threshold value give us no additional useful information, and therefore we can clip them to the predeﬁned"
1922,1923,Examples of Feature Transformations
1923,1924,"We now list several common techniques for feature transformations. Usually, it is helpful to combine some of these transformations (e.g., centering + scaling)."
1924,1925,"In the following, we denote by f = (f1, . . . , fm) ∈Rm the value of the feature f over the m training examples. Also, we denote by ¯f = mean of the feature over all examples."
1925,1926,"This transformation makes the feature have zero mean, by setting fi ←fi −¯f. fmax = maxi fi and fmin = mini fi. Then, we set fi ← fmax−fmin . Similarly,"
1926,1927,This transformation makes all features have a zero mean and unit variance.
1927,1928,"Then, we set fi ←fi−¯"
1928,1929,"This transformation clips high or low values of the feature. For example, fi ←"
1929,1930,Sigmoidal Transformation:
1930,1931,"As its name indicates, this transformation applies a sigmoid function on the feature. For example, fi ←"
1931,1932,This transformation can be thought of as a “soft” version of clipping: It has a small eﬀect on values close to zero and behaves similarly to clipping on values
1932,1933,Feature Selection and Generation
1933,1934,"Logarithmic Transformation: is widely used when the feature is a “counting” feature. For example, suppose that the feature represents the number of appearances of a certain word in a text document. Then, the diﬀerence between zero occurrences of the word and formed on the basis of the values it obtains on the training set, independently of other features’ values. In some situations we would like to set the parameter of the transformation on the basis of other features as well. A notable example is a transformation in which one applies a scaling to the features so that the"
1934,1935,"So far we have discussed feature selection and manipulations. In these cases, we start with a predeﬁned vector space Rd, representing our features. Then, we select a subset of features (feature selection) or transform individual features (feature transformation). In this section we describe feature learning, in which we start tells us that we must incorporate some prior knowledge on the data distribution in order to build a good feature representation. In this section we present a few feature learning approaches and demonstrate conditions on the underlying data distribution in which these methods can be useful. tions. For example, in the context of polynomial regression, we have mapped the top of the constructed features. Automation of this process would be to learn predictors on top of ψ yields a good hypothesis class for the task at hand. {w1, . . . , wk}, where each wi is a string representing a word in the dictionary, and given a document, (p1, . . . , pd), where each pi is a word in the document, processing tasks that linear predictors are quite powerful when applied on this representation. Intuitively, we can think of each word as a feature that measures uments), a learning algorithm searches for a linear predictor that weights these features so that a right combination of appearances of words is indicative of the tionary, in other applications we do not have such an intuitive representation of an instance. For example, consider the computer vision application of object recognition. Here, the instance is an image and the goal is to recognize which resentation of the image does not yield a good classiﬁer. What we would like image and would output a bag of “visual words,” representing the content of the image. For example, a “visual word” can be “there is an eye in the image.” If we had such representation, we could have applied a linear predictor on top of this representation to train a classiﬁer for, say, face recognition. Our question is, words representation of an image would be helpful for predicting which object appears in the image?"
1935,1936,"A ﬁrst naive approach for dictionary learning relies on a clustering algorithm c(x) is the cluster to which x belongs. Then, we can think of the clusters as “words,” and of instances as “documents,” where a document x is mapped to cluster. Now, it is straightforward to see that applying a linear predictor on ψ(x) is equivalent to assigning the same target value to all instances that belong to the same cluster. Furthermore, if the clustering is based on distances from a constant predictor on x. ﬁnd a pair of functions such that the reconstruction error, P is small. Of course, we can trivially set k = d and both ψ, φ to be the identity mapping, which yields a perfect reconstruction. We therefore must restrict ψ and φ in some way. In PCA, we constrain k < d and further restrict ψ and φ to be ψ and φ rely on k centroids, µ1, . . . , µk, and ψ(x) returns an indicator vector"
1936,1937,"Feature Selection and Generation indicator vector and returns the centroid representing this vector. k to be larger than d, is that ψ maps instances into sparse vectors. In fact, in at most s nonzero elements, where s is a small integer. In particular, let ψ and φ be functions that depend on µ1, . . . , µk. The function ψ maps an instance vector x to a vector ψ(x) ∈Rk, where ψ(x) should have at most s nonzero elements."
1937,1938,"The function φ(v) is deﬁned to be Pk small reconstruction error, and therefore we can deﬁne vector of the centroid closest to x. For larger values of s, the optimization problem in the preceding deﬁnition of ψ becomes computationally diﬃcult. Therefore, in practice, we sometime use ℓ1 regularization instead of the sparsity constraint and the ℓ1 regularization, this is still a computationally hard problem (similar to reasonably good solutions. These algorithms are beyond the scope of this book."
1938,1939,"Many machine learning algorithms take the feature representation of instances feature transformations and demonstrated their usefulness. Last, we discussed ture selection, manipulation, and learning all depend on some prior knowledge"
1939,1940,Bibliographic Remarks many types of ﬁlters. also show that low ℓ1 norm can be used to bound the estimation error of our
1940,1941,Feature learning and dictionary learning have been extensively studied recently tation of AdaBoost as a forward greedy selection algorithm.
1941,1942,"• Given a set of m instances x1, . . . , xm, and a hypothesis class H of ﬁnite"
1942,1943,"VC dimension, show that there exist d and h1, . . . , hd such that for every"
1943,1944,Feature Selection and Generation
1944,1945,Let D be the distribution over [m] deﬁned by where Z is a normalization factor that ensures that D is a probability
1945,1946,"Rademacher Complexities for learnability. In this chapter we study the Rademacher complexity, which measures the rate of uniform convergence. We will provide generalization bounds based on this measure."
1946,1947,"The Rademacher Complexity (w.r.t. domain Z, hypothesis class H, loss function ℓ, and distribution D) if"
1947,1948,"To simplify our notation, let us denote tween the true error of a function f and its empirical error, namely,"
1948,1949,"Now, suppose we would like to estimate the representativeness of S using the refer to S1 as a validation set and to S2 as a training set. We can then estimate the representativeness of S by"
1949,1950,Personal use only. Not for distribution. Do not post.
1950,1951,"Rademacher Complexities pectation of the above with respect to a random choice of σ. Formally, let F ◦S"
1951,1952,"More generally, given a set of vectors, A ⊂Rm, we deﬁne"
1952,1953,The following lemma bounds the expected value of the representativeness of
1953,1954,S by twice the expected Rademacher complexity. of expectation is smaller than expectation of the supremum we obtain
1954,1955,Taking expectation over S on both sides we obtain
1955,1956,"Next, we note that for each j, zj and z′ j are i.i.d. variables. Therefore, we can replace them without aﬀecting the expectation:"
1956,1957,Repeating this for all j we obtain that
1957,1958,"The lemma immediately yields that, in expectation, the ERM rule ﬁnds a hypothesis which is close to the optimal hypothesis in H."
1958,1959,"Furthermore, for any h⋆∈H"
1959,1960,S∼Dm [LD(ERMH(S)) −LD(h⋆)]
1960,1961,"Rademacher Complexities equality follows because for any ﬁxed h⋆,"
1961,1962,The third inequality follows from the previous inequality by relying on Markov’s following bounded diﬀerences concentration inequality.
1962,1963,"Let X1, . . . , Xm be m independent random variables taking values in V . Then, |f(X1, . . . , Xm) −E[f(X1, . . . , Xm)]| ≤c"
1963,1964,On the basis of the McDiarmid inequality we can derive generalization bounds with a better dependence on the conﬁdence parameter.
1964,1965,S′∼Dm R(ℓ◦H ◦S′) + c
1965,1966,"In particular, this holds for h = ERMH(S)."
1966,1967,"In particular, this holds for h = ERMH(S)."
1967,1968,"RepD(F, S) ≤E RepD(F, S) + c"
1968,1969,"The ﬁrst inequality of the theorem follows from the deﬁnition of RepD(F, S)."
1969,1970,"For the second inequality we note that the random variable R(ℓ◦H ◦S) also and the union bound. Finally, for the last inequality, denote hS = ERMH(S) = LD(hS) −LS(hS) + LS(hS) −LS(h⋆) + LS(h⋆) −LD(h⋆) ≤(LD(hS) −LS(hS)) + (LS(h⋆) −LD(h⋆)) . the theorem. For the second summand, we use the fact that h⋆does not depend on S; hence by using Hoeﬀding’s inequality we obtain that with probaility of at"
1970,1971,Combining this with the union bound we conclude our proof.
1971,1972,"The preceding theorem tells us that if the quantity R(ℓ◦H◦S) is small then it is possible to learn the class H using the ERM rule. It is important to emphasize that the last two bounds given in the theorem depend on the speciﬁc training set S. That is, we use S both for learning a hypothesis from H as well as for"
1972,1973,Let us now discuss some properties of the Rademacher complexity measure.
1973,1974,These properties will help us in deriving some simple bounds on R(ℓ◦H ◦S) for speciﬁc cases of interest.
1974,1975,The following lemma is immediate from the deﬁnition.
1975,1976,The following lemma tells us that the convex hull of A has the same complexity
1976,1977,Rademacher Complexities
1977,1978,Let A be a subset of Rm and let A′ = {PN
1978,1979,The main idea follows from the fact that for any vector v we have and we conclude our proof.
1979,1980,"The next lemma, due to Massart, states that the Rademacher complexity of a ﬁnite set grows logarithmically with the size of the set."
1980,1981,"Let A = {a1, . . . , aN} be a ﬁnite set of vectors complexity as follows: // Jensen’s inequality σi eσiai = exp(ai) + exp(−ai) λR(A′) we obtain from the equation that"
1981,1982,The following lemma shows that composing A with a Lipschitz function does not blow up the Rademacher complexity. The proof is due to Kakade and Tewari.
1982,1983,"Lipschitz function, namely for all α, β ∈R we have |φi(α) −φi(β)| ≤ρ |α −β|. for any set A and all i we have R(Ai) ≤R(A). Without loss of generality we will where in the last inequality we used the assumption that φ is Lipschitz. Next, we note that the absolute value on |a1 −a′"
1983,1984,"Rademacher Complexities be omitted since both a and a′ are from the same set A and the rest of the expression in the supremum is not aﬀected by replacing a and a′. Therefore,"
1984,1985,Rademacher Complexity of Linear Classes plify the derivation we ﬁrst deﬁne the following two classes:
1985,1986,"The following lemma bounds the Rademacher complexity of H2. We allow the xi to be vectors in any Hilbert space (even inﬁnite dimensional), and the bound does not depend on the dimensionality of the Hilbert space. This property becomes useful when analyzing kernel methods."
1986,1987,"Let S = (x1, . . . , xm) be vectors in a Hilbert space. Deﬁne: H2 ◦ have ⟨w, v⟩≤∥w∥∥v∥. Therefore,"
1987,1988,"Next, using Jensen’s inequality we have that"
1988,1989,"Finally, since the variables σ1, . . . , σm are independent we have"
1989,1990,Next we bound the Rademacher complexity of H1 ◦S.
1990,1991,"Let S = (x1, . . . , xm) be vectors in Rn. Then,"
1991,1992,"Using Holder’s inequality we know that for any vectors w, v we have which concludes our proof."
1992,1993,Generalization Bounds for SVM
1993,1994,In this section we use Rademacher complexity to derive generalization bounds for generalized linear predictors with Euclidean norm constraint. We will show
1994,1995,"Rademacher Complexities ℓ(w, (x, y)) = φ(⟨w, x⟩, y), absolute loss function, ℓ(w, (x, y)) = |⟨w, x⟩−y|, which can be written as in"
1995,1996,The following theorem bounds the generalization error of all predictors in H using their empirical error.
1996,1997,"Suppose that D is a distribution over X × Y such that with (x,y)∼D[y ̸= sign(⟨wS, x⟩)] ≤LD(wS)."
1997,1998,LD(wS) ≤LS(wS) + 2BR
1998,1999,"The bound in the preceding theorem depends on ∥w⋆∥, which is unknown."
1999,2000,In the following we derive a bound that depends on the norm of the output of
2000,2001,"SVM; hence it can be calculated from the training set itself. The proof is similar to the derivation of bounds for structure risk minimization (SRM). (x,y)∼D[y ̸= sign(⟨wS, x⟩)] ≤4R∥wS∥"
2001,2002,Applying the union bound and using P∞
2002,2003,"In particular, it holds for wS, which concludes our proof."
2003,2004,Rademacher Complexities
2004,2005,"Note that all the bounds we have derived do not depend on the dimension of w. This property is utilized when learning SVM with kernels, where the dimension of w can be extremely large."
2005,2006,Generalization Bounds for Predictors with Low ℓ1 Norm
2006,2007,"In the previous section we derived generalization bounds for linear predictors and let Z = X × Y be the examples domain. Assume that the loss function, being ρ-Lipschitz w.r.t. its ﬁrst argument. The following theorem bounds the generalization error of all predictors in H using their empirical error."
2007,2008,"Suppose that D is a distribution over X × Y such that with both bounds look similar. However, the parameters B, R have diﬀerent meanings (which is stronger than an ℓ2 constraint) while the parameter R captures a low sumption). Therefore, the choice of the constraint should depend on our prior knowledge of the set of instances and on prior assumptions on good predictors."
2008,2009,Bibliographic Remarks
2009,2010,The use of Rademacher complexity for bounding the uniform convergence is
2010,2011,Our proof of the concentration lemma is due to Kakade and Tewari lecture bounds on the Rademacher complexity of linear classes with respect to diﬀerent assumptions on the norms.
2011,2012,"In this chapter we describe another way to measure the complexity of sets, which is called covering numbers."
2012,2013,"Let A ⊂Rm be a set of vectors. We say that A there exists a′ ∈A′ with ∥a −a′∥≤r. We deﬁne by N(r, A) the cardinality of"
2013,2014,"To see this, let v1, . . . , vd be an orthonormal basis of the subspace. Then, any"
2014,2015,The following lemma is immediate from the deﬁnition.
2015,2016,Personal use only. Not for distribution. Do not post.
2016,2017,"Next, we derive a contraction principle."
2017,2018,"N(ρ r, φ ◦A) ≤N(r, A)."
2018,2019,"Hence, B′ is an (ρ r)-cover of B."
2019,2020,From Covering to Rademacher Complexity via Chaining
2020,2021,"The following lemma bounds the Rademacher complexity of A based on the covering numbers N(r, A). This technique is called Chaining and is attributed"
2021,2022,"Let ¯a be a minimizer of the objective function given in the deﬁnition in an arbitrary way, and if a maximizer does not exist, choose a∗such that ⟨σ, a∗⟩is close enough to the supremum). Note that a∗is a function of σ. For each k, let b(k) be the nearest neighbor of a∗in Bk (hence b(k) is also a function of σ). Using the triangle inequality,"
2022,2023,For each k deﬁne the set
2023,2024,"As a corollary we obtain the following: log(N(c2−k, A)) ≤α + βk."
2024,2025,Consider a set A which lies in a d dimensional subspace of Rm
2025,2026,Bibliographic Remarks ing numbers as well as other complexity measures that can be used to bound the
2026,2027,"Proof of the Fundamental Theorem the conditions of the theorem, which will hold throughout this chapter: H is a"
2027,2028,We shall prove the upper bound for both the realizable and agnostic cases and shall prove the lower bound for the agnostic case. The lower bound for the realizable case is left as an exercise.
2028,2029,The Upper Bound for the Agnostic Case
2029,2030,For the upper bound we need to prove that there exists C such that H is agnostic
2030,2031,PAC learnable with sample complexity
2031,2032,We will prove the slightly looser bound:
2032,2033,"The tighter bound in the theorem statement requires a more involved proof, in which a more careful analysis of the Rademacher complexity using a technique called “chaining” should be used. This is beyond the scope of this book."
2033,2034,Shelah lemma tells us that if VCdim(H) = d then
2034,2035,Personal use only. Not for distribution. Do not post.
2035,2036,The Lower Bound for the Agnostic Case
2036,2037,"Here, we prove that there exists C such that H is agnostic PAC learnable with"
2037,2038,"Choose one example that is shattered by H. That is, let c be an example such"
2038,2039,"Proof of the Fundamental Theorem of Learning Theory and the probability of (c, −b) is"
2039,2040,"Let A be an arbitrary algorithm. Any training set sampled from Db has the form S = (c, y1), . . . , (c, ym). Therefore, it is fully characterized by the vector where h is the hypothesis that A outputs upon receiving the training set S = (c, y1), . . . , (c, ym)."
2040,2041,Note that for any hypothesis h we have
2041,2042,"In particular, the Bayes optimal hypothesis is hb and then there exists b such that butions as follows. First, let C = {c1, . . . , cd} be a set of d instances which are"
2042,2043,It is easy to verify that the Bayes optimal predictor for Db is the hypothesis
2043,2044,"Proof of the Fundamental Theorem of Learning Theory theorem, we have that deﬁnition of Db, to sample S ∼Db we can ﬁrst sample (j1, . . . , jm) ∼U([d])m, set"
2044,2045,"We now proceed in two steps. First, we show that among all learning algorithms,"
2045,2046,"S is fully determined. Therefore, we can write A(j, y) instead of A(S). Let us jr = i and let y¬I be the rest of the elements of y. We have"
2046,2047,"The sum within the parentheses is minimized when A(j, y)(ci) is the maximizer peating the same argument for all i we conclude our proof."
2047,2048,"Fix i. For every j, let ni(j) = {|t : jt = i|} be the number of instances in which"
2048,2049,We have thus shown that
2049,2050,"Since the square root function is concave, we can apply Jensen’s inequality to obtain that the above is lower bounded by"
2050,2051,Proof of the Fundamental Theorem of Learning Theory
2051,2052,"In summary, we have shown that if m <"
2052,2053,8ρ2 then for any algorithm there exists a distribution such that
2053,2054,The Upper Bound for the Realizable Case
2054,2055,"Here we prove that there exists C such that H is PAC learnable with sample , H is learnable using the with respect to a distribution D over X if for a single try) and m (number of tries). Chernoﬀ’s inequality implies (x1, . . . , x2m) let A0 = (x1, . . . , xm). Using the deﬁnition of B′ we get that"
2055,2056,"Now, let us deﬁne by HA the eﬀective number of diﬀerent hypotheses on A,"
2056,2057,"Aj = (xj1, . . . , xjm). Since the elements of A are chosen i.i.d., we have that"
2057,2058,Proof of the Fundamental Theorem of Learning Theory
2058,2059,"EA∼D2m[f(A, Aj)]. Since this holds for any j it also holds for the expectation of j chosen at random from J. In particular, it holds for the function f(A, A0) = when choosing m balls from a bag with at least α red balls, we will never choose a red ball. This probability is at most"
2059,2060,We therefore get that
2060,2061,Combining this with the two claims we obtain that
2061,2062,"Rearranging, we obtain the requirement"
2062,2063,A suﬃcient condition for this is that and this concludes our proof.
2063,2064,Let H be a hypothesis class over X with VCdim(H) = d. Let over a choice of m i.i.d. instances from X with labels according to c we have that easy to verify that if some A ⊂X is shattered by H then it is also shattered by Hc concludes our proof.
2064,2065,Multiclass Learnability the main goal of this chapter is to:
2065,2066,"• Quantify the sample complexity of such hypothesis classes. ral to seek a generalization of the VC dimension to multiclass hypothesis classes. and state a generalization of the fundamental theorem based on the Natarajan dimension. Then, we demonstrate how to calculate the Natarajan dimension of several important hypothesis classes."
2066,2067,"Recall that the main message of the fundamental theorem of learning theory is that a hypothesis class of binary classiﬁers is learnable (with respect to the that this equivalence breaks down for a certain convex learning problem. The last section of this chapter is devoted to showing that the equivalence between learnability and uniform convergence breaks down even in multiclass problems construct a hypothesis class which is learnable by a speciﬁc ERM learner, but for which other ERM learners might fail and the uniform convergence property"
2067,2068,The Natarajan Dimension
2068,2069,"In this section we deﬁne the Natarajan dimension, which is a generalization of the VC dimension to classes of multiclass predictors. Throughout this section, function from X to [k]."
2069,2070,Personal use only. Not for distribution. Do not post.
2070,2071,"We say that a set C ⊂X noted Ndim(H), is the maximal size of a shattered set C ⊂X."
2071,2072,"It is not hard to see that in the case that there are exactly two classes,"
2072,2073,"Ndim(H) = VCdim(H). Therefore, the Natarajan dimension generalizes the VC ize the fundamental theorem of statistical learning from binary classiﬁcation to multiclass classiﬁcation."
2073,2074,The Multiclass Fundamental Theorem
2074,2075,"There exist absolute of functions from X to [k], such that the Natarajan dimension of H is d, we have nonstraightforward manner is Sauer’s lemma. It applies only to binary classes and therefore must be replaced. An appropriate substitute is Natarajan’s lemma:"
2075,2076,Multiclass Learnability |H| ≤|X|Ndim(H) · k2Ndim(H).
2076,2077,The proof of Natarajan’s lemma shares the same spirit of the proof of Sauer’s
2077,2078,"Calculating the Natarajan Dimension these calculations indicate, the Natarajan dimension is often proportional to the number of parameters required to deﬁne a hypothesis. gests considering multiclass hypothesis classes of the following form. Let Hbin ⊂"
2078,2079,"If there are two labels that maximize hi(x), we choose the smaller one. Also, let"
2079,2080,"What “should” be the Natarajan dimension of HOvA,k ? Intuitively, to specify a , we need to specify k hypotheses in Hbin. Therefore, kd parameters should suﬃce. The following lemma establishes this intuition."
2080,2081,If d = VCdim(Hbin) then
2081,2082,"On the other hand, each hypothesis in HOvA,k ses from Hbin. Therefore,"
2082,2083,"By Sauer’s lemma, | (Hbin)C | ≤|C|d. We conclude that binary classes, Hbin (e.g., halfspaces), for which Ndim(HOvA,k stance, they predict its label by using some rule that takes into account the"
2083,2084,"Suppose that such a method trains l binary classiﬁers from a binary class Hbin, to the predictions of the binary classiﬁers. The hypothesis class corresponding to this method can be deﬁned as follows. For every ¯h = (h1, . . . , hl) ∈(Hbin)l"
2084,2085,"R(¯h)(x) = r(h1(x), . . . , hl(x))."
2085,2086,If d = VCdim(Hbin) then
2086,2087,Linear Multiclass Predictors
2087,2088,"Rd. Therefore, we would expect that the Natarajan dimension would be upper bounded by d. Indeed:"
2088,2089,"Multiclass Learnability functions that witness the shattering. We need to show that |C| ≤d. For every by the binary hypothesis class of homogeneous linear separators on Rd,"
2089,2090,"Since VCdim(H) = d, it will follow that |C| = |ρ(C)| ≤d, as required."
2090,2091,The theorem is tight in the sense that there are mappings Ψ for which Ndim(HΨ) =
2091,2092,Let X = Rn and let Ψ : X ×[k] →Rnk be the class sensitive
2092,2093,On Good and Bad ERMs
2093,2094,"In this section we present an example of a hypothesis class with the property that not all ERMs for the class are equally successful. Furthermore, if we allow an inﬁnite number of labels, we will also obtain an example of a class that is learnable by some ERM, but other ERMs will fail to learn it. Clearly, this also implies that the class is learnable but it does not have the uniform convergence property. For simplicity, we consider only the realizable case."
2094,2095,The class we consider is deﬁned as follows. The instance space X will be any ﬁnite or countable set. Let Pf(X) be the collection of all ﬁnite and coﬁnite
2095,2096,"Instead of [k], the label set is Y = Pf(X) ∪{∗}, where ∗is some special label."
2096,2097,"Finally, the hypothesis class we take is"
2097,2098,"Let A be some ERM algorithm for H. Assume that A operates on a sample the label A, if A observes the label A, it “knows” that the learned hypothesis is hA, and, as an ERM, must return it (note that in this case the error of the the hypothesis it returns upon receiving a sample of the form"
2098,2099,"S = {(x1, ∗), . . . , (xm, ∗)}."
2099,2100,"We consider two ERMs: The ﬁrst, Agood, is deﬁned by"
2100,2101,"ERM, Abad, is deﬁned by"
2101,2102,"Abad(S) = h{x1,...xm}c."
2102,2103,"The following claim shows that the sample complexity of Abad is about |X|-times larger than the sample complexity of Agood. This establishes a gap between diﬀerent ERMs. If X is inﬁnite, we even obtain a learnable class that is not learnable by every ERM. examples, sampled according to D and labeled by"
2103,2104,Multiclass Learnability
2104,2105,"Let D be a distribution over X and suppose that the correct labeling sample size be m. Clearly, the hypothesis returned by Abad will err on all the"
2105,2106,"The conclusion of the example presented is that in multiclass classiﬁcation, the sample complexity of diﬀerent ERMs may diﬀer. Are there “good” ERMs for every hypothesis class? The following conjecture asserts that the answer is"
2106,2107,The realizable sample complexity of every hypothesis class
2107,2108,"Ndim(H), but no factor of k."
2108,2109,"Bibliographic Remarks eralizations and sharper versions of the Natarajan lemma are studied in Haussler family of notions of dimensions, all of which generalize the VC dimension and may be used to estimate the sample complexity of multiclass classiﬁcation."
2109,2110,"The calculation of the Natarajan dimension, presented here, together with d such that Ndim(HOvA,k"
2110,2111,"X \ {x0} →[k] that can be extended to a function in H both by deﬁning f(x0) = i and by deﬁning f(x0) = j. Show that |H| ≤|HX\{x0}|+P class of Natarajan dimension d, the agnostic sample complexity of H is of Natarajan dimension d, the agnostic sample complexity of H is"
2111,2112,Hint: Deduce it from the binary fundamental theorem.
2112,2113,"Show that Ndim(HOvA,k last coordinate is i and the rest are zeros."
2113,2114,"Throughout the book, we have tried to characterize the notion of learnability gence property of a hypothesis class guarantees successful learning. Later on we introduced the notion of stability and have shown that stable algorithms are guaranteed to be good learners. Yet there are other properties which may be suﬃcient for learning, and in this chapter and its sequel we will introduce two"
2114,2115,"In this chapter we study compression bounds. Roughly speaking, we shall see set of the training set, then the error of the hypothesis on the rest of the examples estimates its true error. In other words, an algorithm that can “compress” its output is a good learner."
2115,2116,"To motivate the results, let us ﬁrst consider the following learning protocol."
2116,2117,"First, we sample a sequence of k examples denoted T. On the basis of these examples, we construct a hypothesis denoted hT . Now we would like to estimate the performance of hT so we sample a fresh sequence of m−k examples, denoted"
2117,2118,"V , and calculate the error of hT on V . Since V and T are independent, we"
2118,2119,"To derive this bound, all we needed was independence between T and V ."
2119,2120,"Therefore, we can redeﬁne the protocol as follows. First, we agree on a sequence of k indices I = (i1, . . . , ik) ∈[m]k. Then, we sample a sequence of m examples"
2120,2121,"S = (z1, . . . , zm). Now, deﬁne T = SI = (zi1, . . . , zik) and deﬁne V to be the rest of the examples in S. Note that this protocol is equivalent to the protocol"
2121,2122,Applying a union bound over the choice of the sequence of indices we obtain the following theorem.
2122,2123,"Personal use only. Not for distribution. Do not post. of size m and returns a hypothesis such that A(S) = B(zi1, . . . , zik) for some (i1, . . . , ik) ∈[m]k. Let V = {zj : j /∈(i1, . . . , ik)} be the set of examples which the choice of S we have"
2123,2124,LD(A(S)) ≤LV (A(S)) +
2124,2125,LD(A(S)) ≤LV (A(S)) + which concludes our proof.
2125,2126,As a direct corollary we obtain:
2126,2127,"These results motivate the following deﬁnition: (Compression Scheme) Let H be a hypothesis class of functions from X to Y and let k be an integer. We say that H has a compression scheme of size k if the following holds: if we feed any training set of the form (x1, h(x1)), . . . , (xm, h(xm)) into A and then feed (xi1, h(xi1)), . . . , (xik, h(xik)) into B, where (i1, . . . , ik) is the output"
2127,2128,It is possible to generalize the deﬁnition for unrealizable sequences as follows. (Compression Scheme for Unrealizable Sequences)
2128,2129,Let H be a hypothesis class of functions from X to Y and let k be an integer.
2129,2130,"We say that H has a compression scheme of size k if the following holds: if we feed any training set of the form (x1, y1), . . . , (xm, ym) into A and then feed (xi1, yi1), . . . , (xik, yik) into B, where (i1, . . . , ik) is the output of A, then the output of B, denoted h′, satisﬁes LS(h′) ≤LS(h)."
2130,2131,The following lemma shows that the existence of a compression scheme for the realizable case also implies the existence of a compression scheme for the
2131,2132,"Let H be a hypothesis class for binary classiﬁcation, and assume it has a compression scheme of size k in the realizable case. Then, it has a compression scheme of size k for the unrealizable case as well."
2132,2133,"Consider the following scheme: First, ﬁnd an ERM hypothesis and denote it by h. Then, discard all the examples on which h errs. Now, apply the realizable compression scheme on the examples that have not been removed. The output of the realizable compression scheme, denoted h′, must be correct on the examples that have not been removed. Since h errs on the removed examples it follows that the error of h′ cannot be larger than the error of h; hence h′ is also an ERM pression scheme, it is necessary to show that there exist A, B, and k for which"
2133,2134,Axis Aligned Rectangles
2134,2135,Note that this is an uncountable inﬁnite class. We show that there is a simple compression scheme. Consider the algorithm A that works as follows: For each sion. Deﬁne B to be the function that returns the minimal enclosing rectangle.
2135,2136,"A Compression Scheme: pression scheme we propose is as follows. First, A ﬁnds the vector w which is in the convex hull of {x1, . . . , xm} and has minimal norm. Then, it represents it as a convex combination of d points in the sample (it will be shown later that this is always possible). The output of A are these d points. The algorithm B receives these d points and set w to be the point in their convex hull of minimal"
2136,2137,"Next we prove that this indeed is a compression sceme. Since the data is linearly separable, the convex hull of {x1, . . . , xm} does not contain the origin."
2137,2138,Consider the point w in this convex hull closest to the origin. (This is a unique point which is the Euclidean projection of the origin onto this convex hull.) We
2138,2139,Then w′ is also in the convex hull and which leads to a contradiction.
2139,2140,"We have thus shown that w is also an ERM. Finally, since w is in the convex hull of the examples, we can apply Caratheodory’s theorem to obtain that w is the minimality of w implies that w must be on a face of the polygon and this implies it can be represented as a convex combination of d points."
2140,2141,"It remains to show that w is also the projection onto the polygon deﬁned by the d points. But this must be true: On one hand, the smaller polygon is a subset of the larger one; hence the projection onto the smaller cannot be smaller in norm."
2141,2142,"On the other hand, w itself is a valid solution. The uniqueness of projection concludes our proof."
2142,2143,Separating Polynomials
2143,2144,"Note that p(x) can be rewritten as ⟨w, ψ(x)⟩where the elements of ψ(x) are all pression scheme for p(x) reduces to the problem of constructing a compression scheme for halfspaces in Rd′ where d′ = O(dr)."
2144,2145,Separation with Margin
2145,2146,"Suppose that a training set is separated with margin γ. The Perceptron algorithm makes no mistakes on the entire training set. Hence, we have a compression"
2146,2147,"Bibliographic Remarks mental theorem of learning that the class has a ﬁnite VC dimension. The other direction, namely, whether every hypothesis class of ﬁnite VC dimension has a"
2147,2148,The Minimum Description Length (MDL) and Occam’s razor principles allow a potentially very large hypothesis class but deﬁne a hierarchy over hypotheses and prefer to choose hypotheses that appear higher in the hierarchy. In this chapter distribution over the hypothesis class.
2148,2149,"As in the MDL paradigm, we deﬁne a hierarchy over hypotheses in our class H."
2149,2150,"Now, the hierarchy takes the form of a prior distribution over H. That is, we and refer to P(h) as the prior score of h. Following the Bayesian reasoning pothesis. Instead, the learning process deﬁnes a posterior probability over H, which we denote by Q. In the context of a supervised learning problem, where"
2150,2151,"H contains functions from X to Y, one can think of Q as deﬁning a randomized prediction rule as follows. Whenever we get a new instance x, we randomly pick"
2151,2152,"By the linearity of expectation, the generalization loss and training loss of Q can"
2152,2153,"The following theorem tells us that the diﬀerence between the generalization loss and the empirical loss of a posterior Q is bounded by an expression that eralization loss of Q, we should jointly minimize both the empirical loss of Q"
2153,2154,Personal use only. Not for distribution. Do not post. later show how in some cases this idea leads to the regularized risk minimization
2154,2155,"Let D be an arbitrary distribution over an example domain Z. to D, for all distributions Q over H (even such that depend on S), we have"
2155,2156,"For any function f(S), using Markov’s inequality:"
2156,2157,"We now turn to bound ES[ef(S)]. The main trick is to upper bound f(S) by using an expression that does not depend on Q but rather depends on the prior probability P. To do so, ﬁx some S and note that from the deﬁnition of D(Q||P) we get that for all Q, where the inequality follows from Jensen’s inequality and the concavity of the log function. Therefore, we can switch the order of expectations (because P is a prior that does not depend on S), which yields"
2157,2158,Hoeﬀding’s inequality tells us that
2158,2159,Rearranging the inequality and using Jensen’s inequality again (the function x2 is convex) we conclude that
2159,2160,"Given a prior P, return a posterior Q that minimizes the function"
2160,2161,"This rule is similar to the regularized risk minimization principle. That is, we “distance” between Q and P."
2161,2162,Bibliographic Remarks
2162,2163,Compare to the bounds we derived using uniform convergence.
2163,2164,"Appendix A Technical Lemmas necessary condition for the inequality x < a log(x) to hold is that x < 2a log(a). ditionally and therefore the claim is trivial. From now on, assume that a > √e."
2164,2165,"Thus, for x > a the derivative is positive and the function increases. In addition, f(2a log(a)) = 2a log(a) −a log(2a log(a))"
2165,2166,Let X be a random variable and x′ ∈R be a scalar and assume we have that E[|X −x′|] is at most P∞
2166,2167,The proof now follows from the inequalities
2167,2168,Let X be a random variable and x′ ∈R be a scalar and assume
2168,2169,Personal use only. Not for distribution. Do not post. log(b)). Since ti is monotonically increasing we have that
2169,2170,Using the assumption in the lemma we have
2170,2171,Combining the preceding inequalities we conclude our proof.
2171,2172,Using Stirling’s approximation we further have that which proves our inductive argument.
2172,2173,Appendix B Measure Concentration
2173,2174,"Let Z1, . . . , Zm be an i.i.d. sequence of random variables and let µ be their mean. average from the expectation when m is ﬁnite."
2174,2175,We start with an inequality which is called Markov’s inequality. Let Z be a nonnegative random variable. The expectation of Z can be written as follows:
2175,2176,Rearranging the inequality yields Markov’s inequality: inequality the following.
2176,2177,Personal use only. Not for distribution. Do not post.
2177,2178,Chebyshev’s Inequality
2178,2179,Chebyshev’s inequality:
2179,2180,Consider the random variable
2180,2181,"Applying Chebyshev’s inequality, we obtain the following:"
2181,2182,"Let Z1, . . . , Zm be a sequence of i.i.d. random variables and assume"
2182,2183,The deviation between the empirical average and the mean given previously decreases polynomially with m. It is possible to obtain a signiﬁcantly faster decrease. In the sections that follow we derive bounds that decrease exponentially
2183,2184,"Measure Concentration monotonicity of the exponent function and Markov’s inequality, we have that for"
2184,2185,"Let Z1, . . . , Zm be independent Bernoulli variables where for every"
2185,2186,"For the other direction, we apply similar calculations:"
2186,2187,Hoeﬀding’s Inequality
2187,2188,"Let Z1, . . . , Zm be a sequence of i.i.d. random variables and let ¯Z ="
2188,2189,Denote Xi = Zi −E[Zi] and ¯X = i Xi. Using the monotonicity of
2189,2190,Using the independence assumption we also have
2190,2191,Measure Concentration
2191,2192,Let X be a random variable that takes values
2192,2193,"Taking the expectation, we obtain that the above can be rewritten as eL(h). Therefore, to conclude our proof it suﬃces"
2193,2194,Bennet’s and Bernstein’s Inequalities
2194,2195,"Bennet’s and Bernsein’s inequalities are similar to Chernoﬀ’s bounds, but they hold for any sequence of independent random variables. We state the inequalities"
2195,2196,"Let Z1, . . . , Zm be i.i.d. random variables"
2196,2197,"Deﬁne random variables α1, . . . , αm s.t. αi = ℓ(h, zi) −LD(h). Note that ≤E[ℓ(h, zi)] = LD(h),"
2197,2198,Measure Concentration
2198,2199,Solving for t yields which proves the ﬁrst inequality. The second part of the lemma follows in a
2199,2200,"Let X be a (m, p) binomial variable. That is, X = Pm bility that a normal variable will be greater than or equal to following lemma follows by standard tail bounds for the normal distribution."
2200,2201,Concentration of χ2 Variables
2201,2202,"Let X1, . . . , Xk be k independent normally distributed random variables. That χ2 (chi square) and the distribution of the random variable Z = X2 k (chi square with k degrees of freedom). Clearly, E[X2"
2202,2203,"E[Z] = k. The following lemma states that X2 k is concentrated around its mean. we use Chernoﬀ’s bounding method. For the ﬁrst inequality, we ﬁrst bound"
2203,2204,"Using the well known equalities, E[X2"
2204,2205,"Now, applying Chernoﬀ’s bounding method we get that"
2205,2206,"For the second inequality, we use a known closed form expression for the moment generating function of a χ2 k distributed random variable:"
2206,2207,On the basis of the equation and using Chernoﬀ’s bounding method we have
2207,2208,"Finally, the last inequality follows from the ﬁrst two inequalities and the union"
2208,2209,Appendix C Linear Algebra
2209,2210,In this chapter we only deal with linear algebra over ﬁnite dimensional Euclidean spaces. We refer to vectors as column vectors.
2210,2211,"The Euclidean norm (a.k.a. the ℓ2 norm) is ∥u∥= ⟨u, u⟩. We also use the ℓ1"
2211,2212,"A subspace of Rd is a subset of Rd which is closed under addition and scalar multiplication. The span of a set of vectors u1, . . . , uk is the subspace containing all vectors of the form"
2212,2213,"A set of vectors U = {u1, . . . , uk} is independent if for every i, ui is not in the span of the vectors in U. We say that U is a basis of V if it is both independent and spans V. The dimension of V is the size of a basis of V (and it can be veriﬁed that all bases of V have the same size). We say that U is an orthogonal set if for is the dimension of its range."
2213,2214,"The transpose of a matrix A, denoted A⊤, is the matrix whose (i, j) entry equals the (j, i) entry of A. We say that A is symmetric if A = A⊤."
2214,2215,Personal use only. Not for distribution. Do not post.
2215,2216,"Eigenvalues and Eigenvectors rank k, then there exists an orthonormal basis of Rd, u1, . . . , ud, such that each ui is an eigenvector of A. Furthermore, A can be written as A = Pd be written equivalently as A = UDU ⊤, where the columns of U are the vectors u1, . . . , ud, and D is a diagonal matrix with Di,i = λi and for i ̸= j, Di,j = eigenvectors which correspond to the nonzero eigenvalues span the range of A, and the eigenvectors which correspond to zero eigenvalues span the null space of"
2216,2217,Positive deﬁnite matrices
2217,2218,A is positive semideﬁnite if all its eigenvalues are nonnegative. equivalent deﬁnitions of positive semideﬁniteness of A:
2218,2219,• All the eigenvalues of A are nonnegative.
2219,2220,• There exists a matrix B such that A = BB⊤.
2220,2221,"Singular Value Decomposition (SVD) of A, which is called Singular Value Decomposition, or SVD for short."
2221,2222,"We ﬁrst show that if we can ﬁnd r orthonormal singular vectors with positive singular values, then we can decompose A = UDV ⊤, with the columns of U and"
2222,2223,"V containing the left and right singular vectors, and D being a diagonal r × r matrix with the singular values on its diagonal. orthonormal set of right singular vectors of A, u1, . . . , ur is an orthonormal set of corresponding left singular vectors of A, and σ1, . . . , σr are the corresponding singular values. Then,"
2223,2224,"It follows that if U is a matrix whose columns are the ui’s, V is a matrix whose columns are the vi’s, and D is a diagonal matrix with Di,i = σi, then"
2224,2225,"Any right singular vector of A must be in the range of A⊤(otherwise, the singular value will have to be zero). Therefore, v1, . . . , vr is an orthonormal basis of the range of A. Let us complete it to an orthonormal basis of Rn by i . It suﬃces to prove where the last equality follows from the deﬁnition."
2225,2226,"The next lemma relates the singular values of A to the eigenvalues of A⊤A v, u are right and left singular vectors of A with singular value σ iﬀv is an eigenvector of A⊤A with corresponding eigenvalue σ2 and u = σ−1Av is an eigenvector of AA⊤with corresponding eigenvalue σ2. sponding right singular vector. Then,"
2226,2227,"Finally, we show that if A has rank r then it has r orthonormal singular"
2227,2228,"Then, v1, . . . , vr is an orthonormal set of right singular vectors of A."
2228,2229,"First note that since the rank of A is r, the range of A is a subspace of sition of A⊤A, namely, A⊤A = WDW ⊤, with D being a diagonal matrix with that these are also right singular vectors of A. The proof is by induction. For the basis of the induction, note that any unit vector v can be written as v = Wx, with all the ﬁrst t elements of x being zero. It follows that"
2229,2230,UDV ⊤where D is an r × r matrix with nonzero singular values of A and the responding eigenvector of A⊤A and the ith column of U is the corresponding
2230,2231,"Personal use only. Not for distribution. Do not post. minimax lower bounds for online convex games, in ‘Proceedings of the Nineteenth"
2231,2232,"Annual Conference on Computational Learning Theory’. of axioms for clustering, in ‘Proceedings of Neural Information Processing Systems of the potential function method in pattern recognition learning’, Automation and"
2232,2233,"Cambridge University Press. the restricted isometry property for random matrices’, Constructive Approximation"
2233,2234,"Personal use only. Not for distribution. Do not post. ference on Learning Theory (COLT)’. ﬁlter trees’, Preprint, June . plied to the Analysis of Learning Algorithms, PhD thesis, Ecole Polytechnique. tics Department, University of California at Berkeley."
2234,2235,"Regression Trees, Wadsworth & Brooks. sures, in ‘NIPS Workshop: Machine Learning for Web Search’. and experiments with perceptron algorithms, in ‘Conference on Empirical Methods in Natural Language Processing’. ing: deep neural networks with multitask learning, in ‘International Conference on"
2235,2236,Machine Learning (ICML)’.
2236,2237,"Prague Conf. Information Theory Statistical Decision Functions, Random Processes"
2237,2238,"Cambridge University Press. ability and the erm principle, in ‘Conference on Learning Theory (COLT)’. theoretical comparison with implications, in ‘NIPS’. sophical Transactions of the Royal Society of London. Series A, Containing Papers and an application to boosting, in ‘European Conference on Computational Learning tion cone models for face recognition under variable lighting and pose’, IEEE Trans. in ‘Proceedings of the International Conference on Machine Learning (ICML)’."
2238,2239,"Risk bounds, margin bounds, and regularization, in ‘NIPS’. learning algorithms, in ‘ACM Symposium on the Theory of Computing (STOC)’. tions of halfspaces, in ‘FOCS’. ization error, in ‘Proceedings of the 18th Conference in Uncertainty in Artiﬁcial models for segmenting and labeling sequence data, in ‘International Conference on"
2239,2240,"Le, Q. V., Ranzato, M.-A., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J. & in ‘International Conference on Machine Learning (ICML)’. for scalable unsupervised learning of hierarchical representations, in ‘International"
2240,2241,Conference on Machine Learning (ICML)’.
2241,2242,"Unpublished manuscript. schemes, in ‘Conference on Learning Theory (COLT)’."
2242,2243,"Cambridge university press. ometry, The MIT Press. suﬃcient for generalization and necessary and suﬃcient for consistency of empirical"
2243,2244,"Networks. Cambridge University Press, Cambridge, UK . proximation approach to stochastic programming’, SIAM Journal on Optimization mization, Nauka Publishers, Moscow. sity of Louvain (UCL)."
2244,2245,"The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science ‘Conference on Uncertainty in Artiﬁcial Intelligence (UAI)’. strongly convex stochastic optimization, in ‘International Conference on Machine combinatorial parameters, and learnability, in ‘NIPS’. invariant feature hierarchies with applications to object recognition, in ‘Computer lel Distributed Processing – Explorations in the Microstructure of Cognition’, MIT senter theorem, in ‘NeuroCOLT’."
2245,2246,"Regularization, Optimization and Beyond, MIT Press. and vision: theory and practice, MIT Press."
2246,2247,"PhD thesis, The Hebrew University. stability and uniform convergence’, The Journal of Machine Learning Research optimization, in ‘Conference on Learning Theory (COLT)’. linear separability: New relaxations and eﬃcient boosting algorithms, in ‘Proceedings of the Nineteenth Annual Conference on Computational Learning Theory’."
2247,2248,"GrAdient SOlver for SVM, in ‘International Conference on Machine Learning’, in optimization problems with sparsity constraints’, Siam Journal on Optimization ence on Machine Learning (ICML)’. machine learning for interdependent and structured output spaces, in ‘Proceedings"
2248,2249,"S. J. Hanson & R. P. Lippmann, eds, ‘Advances in Neural Information Processing frequencies of events to their probabilities’, Theory of Probability and its applications"
2249,2250,"Moscow. (In Russian). ‘Algorithmic Learning Theory (ALT)’. that maximize the margin, in ‘Proceedings of the 23rd international conference on recognition, in ‘Proceedings of the Seventh European Symposium on Artiﬁcial Neural on Machine Learning’. ent ascent, in ‘International Conference on Machine Learning’."
2250,2251,"Empirical Risk Minimization, see ERM"
2251,2252,Personal use only. Not for distribution. Do not post.
2252,2253,"GD, see gradient descent improper, see representation independent inductive bias, see bias linear discriminant analysis, see LDA"
2253,2254,"Littlestone dimension, see Ldim"
2254,2255,"Minimum Description Length, see MDL"
2255,2256,"Normalized Discounted Cumulative Gain, orthogonal matching pursuit, see OMP"
2256,2257,"Principal Component Analysis, see PCA"
2257,2258,"Probably Approximately Correct, see PAC regularized loss minimization, see RLM"
2258,2259,"Singular Value Decomposition, see SVD"
2259,2260,"Stochastic Gradient Descent, see SGD"
2260,2261,"Structural Risk Minimization, see SRM"
2261,2262,"Support Vector Machines, see SVM"
