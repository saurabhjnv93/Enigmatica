{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction and cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean extracted text to exclude lines that resemble mathematical expressions,\n",
    "    index entries, or table of contents entries (e.g., lines with ellipses or numbers).dd\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to identify unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"\\.\\.\\.+\\s*\\d+\",            # Lines with ellipses followed by numbers\n",
    "        r\"^\\d+\\.\\d+\\s\",              # Lines starting with a decimal number (e.g., \"1.1 \")\n",
    "        r\"\\b\\d+\\b\",                  # Lines containing isolated numbers (page numbers, indexes)\n",
    "        r\"[A-Za-z0-9]+[\\^\\+\\-*/=<>]\", # Lines with mathematical operators\n",
    "        r\"[A-Za-z0-9]+\\s*[∈∀∃∅⊆∪∩≈∑∏∫θμϵλϕδΩ→≤≥]\", # Lines with symbols commonly in math\n",
    "        r\"[θμϵδ]+\",                  # Greek symbols or Greek-like variables\n",
    "        r\"[<>≤≥=]{2,}\",              # Comparison operators often found in math expressions\n",
    "        r\"^\\d+$\",                    # Lines with only numbers\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns into one\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    # Split the text into lines, filter them, and rejoin\n",
    "    filtered_lines = [\n",
    "        line for line in text.splitlines()\n",
    "        if not combined_pattern.search(line)\n",
    "    ]\n",
    "    \n",
    "    return \"\\n\".join(filtered_lines)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, clean it using clean_text function, and save it to a text file.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Iterate through pages and extract text\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        full_text += page.get_text()\n",
    "\n",
    "    # Clean the extracted text\n",
    "    cleaned_text = clean_text(full_text)\n",
    "    \n",
    "    # Save cleaned text to output file\n",
    "    with open(output_txt_path, \"w\") as txt_file:\n",
    "        txt_file.write(cleaned_text)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf_document.close()\n",
    "\n",
    "# Usage\n",
    "pdf_path = \"../INIGMETICA/AI_Russell_Norvig.pdf\"         # Replace with your PDF file path\n",
    "output_txt_path = \"output_text_russell.txt\"    # Replace with desired output text file path\n",
    "extract_text_from_pdf(pdf_path, output_txt_path)\n",
    "\n",
    "print(\"Text extraction and cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_file(input_txt_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Reads a text file, removes lines with specific matrix, summation, or symbol patterns,\n",
    "    and removes lines with fewer than 20 characters. Writes the cleaned content to a new file.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to identify unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"=\\s*m\\s*X\\s*xi\\s*x⊤\\s*i\",  # Pattern for \"= m X xi x⊤ i\"\n",
    "        r\"b\\s*=\\s*m\\s*X\\s*yixi\",     # Pattern for \"b = m X yixi\"\n",
    "        r\"A\\s*=\\s*\\(.*\\)\",           # Matrix-like form pattern\n",
    "        r\"b\\s*=\\s*\\(\"                # Pattern for matrix b with open parentheses\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns into one\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    with open(input_txt_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter lines by patterns and length\n",
    "    filtered_lines = [\n",
    "        line for line in lines\n",
    "        if not combined_pattern.search(line) and len(line.strip()) >= 20\n",
    "    ]\n",
    "\n",
    "    # Write the cleaned lines to the output file\n",
    "    with open(output_txt_path, \"w\") as file:\n",
    "        file.writelines(filtered_lines)\n",
    "\n",
    "# Usage\n",
    "input_txt_path = \"output_text_russell.txt\"      # Replace with the path to your input text file\n",
    "output_txt_path = \"cleaned_text_russell.txt\"   # Replace with the desired output file path\n",
    "clean_text_file(input_txt_path, output_txt_path)\n",
    "\n",
    "print(\"Text cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_and_process_text_file(file_path):\n",
    "    try:\n",
    "        # Read the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        # Split text into paragraphs using a more specific pattern\n",
    "        # This pattern looks for:\n",
    "        # 1. Paragraphs starting with capital letters after line breaks\n",
    "        # 2. Sections separated by blank lines\n",
    "        # 3. Numbered or bulleted sections\n",
    "        paragraphs = []\n",
    "        current_para = []\n",
    "        \n",
    "        # Split into lines first\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            # Remove leading/trailing whitespace\n",
    "            line = line.strip()\n",
    "            \n",
    "            # If line is empty and we have collected some text\n",
    "            if not line and current_para:\n",
    "                # Join the collected lines and add to paragraphs\n",
    "                paragraphs.append(' '.join(current_para))\n",
    "                current_para = []\n",
    "            # If line starts with bullet points, numbers, or is a new section\n",
    "            elif line and (line[0].isupper() or \n",
    "                         line[0].isdigit() or \n",
    "                         line.startswith('•') or \n",
    "                         line.startswith('-')):\n",
    "                # If we have a previous paragraph, save it\n",
    "                if current_para:\n",
    "                    paragraphs.append(' '.join(current_para))\n",
    "                    current_para = []\n",
    "                current_para.append(line)\n",
    "            # If it's a continuation line with content\n",
    "            elif line:\n",
    "                current_para.append(line)\n",
    "        \n",
    "        # Add the last paragraph if exists\n",
    "        if current_para:\n",
    "            paragraphs.append(' '.join(current_para))\n",
    "        \n",
    "        # Clean paragraphs\n",
    "        cleaned_paragraphs = []\n",
    "        for para in paragraphs:\n",
    "            # Clean up extra spaces and join lines\n",
    "            cleaned = ' '.join(para.split())\n",
    "            if cleaned and len(cleaned) > 10:  # Minimum length to filter out very short segments\n",
    "                cleaned_paragraphs.append(cleaned)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(cleaned_paragraphs, columns=['paragraph'])\n",
    "        \n",
    "        # Add paragraph number\n",
    "        df['paragraph_number'] = range(1, len(df) + 1)\n",
    "        \n",
    "        # Reorder columns\n",
    "        df = df[['paragraph_number', 'paragraph']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage example\n",
    "file_path = 'cleaned_text.txt'  # Replace with your text file path\n",
    "df = read_and_process_text_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   paragraph_number                     paragraph\n",
      "0                 1  [Your document content here]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_text_to_df(text):\n",
    "    # Split text into paragraphs\n",
    "    # Looking for double newlines or combinations of newlines and spaces\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    \n",
    "    # Clean paragraphs\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        # Remove extra whitespace and newlines\n",
    "        cleaned = ' '.join(para.split())\n",
    "        if cleaned:  # Only add non-empty paragraphs\n",
    "            cleaned_paragraphs.append(cleaned)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(cleaned_paragraphs, columns=['paragraph'])\n",
    "    \n",
    "    # Add paragraph number\n",
    "    df['paragraph_number'] = range(1, len(df) + 1)\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df[['paragraph_number', 'paragraph']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract the document content\n",
    "text = \"\"\"[Your document content here]\"\"\"  # Replace with actual document content\n",
    "\n",
    "# Create DataFrame\n",
    "df = process_text_to_df(text)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction and cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean extracted text to exclude lines that resemble mathematical expressions,\n",
    "    index entries, or table of contents entries (e.g., lines with ellipses or numbers).dd\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to identify unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"\\.\\.\\.+\\s*\\d+\",            # Lines with ellipses followed by numbers\n",
    "        r\"^\\d+\\.\\d+\\s\",              # Lines starting with a decimal number (e.g., \"1.1 \")\n",
    "        r\"\\b\\d+\\b\",                  # Lines containing isolated numbers (page numbers, indexes)\n",
    "        r\"[A-Za-z0-9]+[\\^\\+\\-*/=<>]\", # Lines with mathematical operators\n",
    "        r\"[A-Za-z0-9]+\\s*[∈∀∃∅⊆∪∩≈∑∏∫θμϵλϕδΩ→≤≥]\", # Lines with symbols commonly in math\n",
    "        r\"[θμϵδ]+\",                  # Greek symbols or Greek-like variables\n",
    "        r\"[<>≤≥=]{2,}\",              # Comparison operators often found in math expressions\n",
    "        r\"^\\d+$\",                    # Lines with only numbers\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns into one\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    # Split the text into lines, filter them, and rejoin\n",
    "    filtered_lines = [\n",
    "        line for line in text.splitlines()\n",
    "        if not combined_pattern.search(line)\n",
    "    ]\n",
    "    \n",
    "    return \"\\n\".join(filtered_lines)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, clean it using clean_text function, and save it to a text file.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Iterate through pages and extract text\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        full_text += page.get_text()\n",
    "\n",
    "    # Clean the extracted text\n",
    "    cleaned_text = clean_text(full_text)\n",
    "    \n",
    "    # Save cleaned text to output file\n",
    "    with open(output_txt_path, \"w\") as txt_file:\n",
    "        txt_file.write(cleaned_text)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf_document.close()\n",
    "\n",
    "# Usage\n",
    "pdf_path = \"Understanding_Machine_Learning_From_Theory_to_Algorithms.pdf\"         # Replace with your PDF file path\n",
    "output_txt_path = \"output_text.txt\"    # Replace with desired output text file path\n",
    "extract_text_from_pdf(pdf_path, output_txt_path)\n",
    "\n",
    "print(\"Text extraction and cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Paragraph]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean extracted text to exclude lines with only numbers, table of contents-like entries,\n",
    "    and obvious mathematical symbols. Keep filtering minimal to avoid removing too much text.\n",
    "    \"\"\"\n",
    "    # Minimal patterns to exclude unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"\\.\\.\\.+\\s*\\d+\",           # Lines with ellipses followed by numbers (e.g., table of contents)\n",
    "        r\"^\\d+$\",                   # Lines with only numbers (indices or page numbers)\n",
    "        r\"[≥≤∑∫∂∈∀→⇔]+\",            # Lines with certain mathematical symbols\n",
    "    ]\n",
    "    \n",
    "    # Combine patterns into a single regex\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    # Split text by paragraphs, filter, and rejoin\n",
    "    filtered_paragraphs = [\n",
    "        paragraph.strip() for paragraph in text.split('\\n\\n')\n",
    "        if not combined_pattern.search(paragraph.strip()) and len(paragraph.strip()) > 0\n",
    "    ]\n",
    "    \n",
    "    return filtered_paragraphs\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, clean it using clean_text function, and return it as a list of paragraphs.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Iterate through pages and extract text\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        full_text += page.get_text()\n",
    "\n",
    "    # Clean and split the extracted text into paragraphs\n",
    "    cleaned_paragraphs = clean_text(full_text)\n",
    "    pdf_document.close()\n",
    "    return cleaned_paragraphs\n",
    "\n",
    "def save_paragraphs_to_dataframe(paragraphs):\n",
    "    \"\"\"\n",
    "    Save paragraphs to a DataFrame where each row is a paragraph.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(paragraphs, columns=[\"Paragraph\"])\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "pdf_path = \"Understanding_Machine_Learning_From_Theory_to_Algorithms.pdf\"   # Replace with your PDF file path\n",
    "paragraphs = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Check cleaned paragraphs before saving to DataFrame\n",
    "for i, paragraph in enumerate(paragraphs[:5]):\n",
    "    print(f\"Paragraph {i+1}:\", paragraph)\n",
    "\n",
    "df = save_paragraphs_to_dataframe(paragraphs)\n",
    "print(\"DataFrame:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_paragraphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcleaned_paragraphs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_paragraphs' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Paragraph]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction and cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean extracted text to exclude lines that resemble mathematical expressions,\n",
    "    index entries, or table of contents entries (e.g., lines with ellipses or numbers).\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to identify unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"\\.\\.\\.+\\s*\\d+\",       # Lines with ellipses followed by numbers\n",
    "        r\"^\\d+\\.\\d+\\s\",         # Lines starting with a decimal number (e.g., \"1.1 \")\n",
    "        r\"\\b\\d+\\b\",             # Lines containing isolated numbers (page numbers, indexes)\n",
    "        r\"\\b[0-9]+\\b\",          # Isolated numbers\n",
    "        r\"^\\d+$\",               # Lines with only numbers\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns into one\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    # Split the text into lines, filter them, and rejoin\n",
    "    filtered_lines = [\n",
    "        line for line in text.splitlines()\n",
    "        if not combined_pattern.search(line)\n",
    "    ]\n",
    "    \n",
    "    return \"\\n\".join(filtered_lines)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, clean it using clean_text function, and save it to a text file.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Iterate through pages and extract text\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        full_text += page.get_text()\n",
    "\n",
    "    # Clean the extracted text\n",
    "    cleaned_text = clean_text(full_text)\n",
    "    \n",
    "    # Save cleaned text to output file\n",
    "    with open(output_txt_path, \"w\") as txt_file:\n",
    "        txt_file.write(cleaned_text)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf_document.close()\n",
    "\n",
    "# Usage\n",
    "pdf_path = \"Understanding_Machine_Learning_From_Theory_to_Algorithms.pdf\"         # Replace with your PDF file path\n",
    "output_txt_path = \"output_text.txt\"    # Replace with desired output text file path\n",
    "extract_text_from_pdf(pdf_path, output_txt_path)\n",
    "\n",
    "print(\"Text extraction and cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "def scrap_pdf(pdf_path):\n",
    "    # Path to save the extracted text file\n",
    "    output_txt_path = \"Extracted_Text.txt\"\n",
    "\n",
    "    # Open the PDF file\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        # Open the output text file in write mode\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            # Loop through each page\n",
    "            for page_num in range(pdf_document.page_count):\n",
    "                # Get the page\n",
    "                page = pdf_document[page_num]\n",
    "                # Extract text from the page\n",
    "                text = page.get_text(\"text\")\n",
    "                # Write text to the file\n",
    "                text_file.write(f\"Page {page_num + 1}\\n\")\n",
    "                text_file.write(text)\n",
    "                text_file.write(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator between pages\n",
    "\n",
    "    print(f\"Text extraction complete. The text has been saved to {output_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction complete. The text has been saved to Extracted_Text.txt\n"
     ]
    }
   ],
   "source": [
    "scrap_pdf(\"Understanding_Machine_Learning_From_Theory_to_Algorithms.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction complete. The text has been saved to Extracted_Text.txt\n"
     ]
    }
   ],
   "source": [
    "scrap_pdf(\"AI_Russell_Norvig.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Paragraph\n",
      "0   Page 1\\nUnderstanding Machine Learning:\\nFrom ...\n",
      "1                                 import pandas as pd\n",
      "2   # Your complete text, divided by double newlin...\n",
      "3   # Split the text by double newline to separate...\n",
      "4   # Create a DataFrame where each row is a parag...\n",
      "5                  # Display the DataFrame\\nprint(df)\n",
      "6   Page 53\\n3.5 Exercises\\n53\\nof the two trainin...\n",
      "7                                   m\\nX\\ni=1\\nαiyixi\n",
      "8   2\\n+\\nm\\nX\\ni=1\\nαi\\n\\n1 −yi\\n*X\\nj\\nαjyjxj,...\n",
      "9                                   \\nm\\nX\\ni=1\\nσixi\n",
      "10                          2\\n#\\n= E\\nσ\\n\\n\\n\\n\n",
      "11                                    m\\nX\\ni=1\\nσixi\n",
      "12              2\\n2\\n\\n\\n1/2\\n≤\\n\\nE\\nσ\\n\\n\n",
      "13                                    m\\nX\\ni=1\\nσixi\n",
      "14                  2\\n2\\n\\n\\n\\n\\n1/2\\n.\\n(26.16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read text from a .txt file\n",
    "with open('Extracted_Text.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split the text into paragraphs by double newlines\n",
    "paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "# Remove paragraphs that are only \"=====\" or similar patterns\n",
    "paragraphs = [p for p in paragraphs if not p.strip().startswith('=') and len(p.strip()) > 0]\n",
    "\n",
    "# Create a DataFrame where each row is a paragraph\n",
    "df = pd.DataFrame(paragraphs, columns=['Paragraph'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Paragraph\n",
      "0    Page 1\\nUnderstanding Machine Learning:\\nFrom ...\n",
      "1    ==============================================...\n",
      "2    ==============================================...\n",
      "3    ==============================================...\n",
      "4    ==============================================...\n",
      "..                                                 ...\n",
      "523  ==============================================...\n",
      "524  ==============================================...\n",
      "525  ==============================================...\n",
      "526  ==============================================...\n",
      "527  ==============================================...\n",
      "\n",
      "[528 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the text file\n",
    "with open(\"Extracted_Text.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split the text by paragraphs (assuming paragraphs are separated by blank lines)\n",
    "paragraphs = re.split(r'\\n\\s*\\n', text.strip())\n",
    "\n",
    "# Create a DataFrame with each paragraph as a row\n",
    "df = pd.DataFrame(paragraphs, columns=[\"Paragraph\"])\n",
    "\n",
    "# Display or save the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save to a CSV file\n",
    "df.to_csv(\"paragraphs_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Paragraph\n",
      "0   Page 1\\nUnderstanding Machine Learning:\\nFrom ...\n",
      "1                                 import pandas as pd\n",
      "2   # Your complete text, divided by double newlin...\n",
      "3   # Split the text by double newline to separate...\n",
      "4   # Create a DataFrame where each row is a parag...\n",
      "5                  # Display the DataFrame\\nprint(df)\n",
      "6   Page 53\\n3.5 Exercises\\n53\\nof the two trainin...\n",
      "7                                                    \n",
      "8                                                    \n",
      "9                                   m\\nX\\ni=1\\nαiyixi\n",
      "10                                                   \n",
      "11                                                   \n",
      "12  2\\n+\\nm\\nX\\ni=1\\nαi\\n\\n1 −yi\\n*X\\nj\\nαjyjxj,...\n",
      "13                                                   \n",
      "14                                  \\nm\\nX\\ni=1\\nσixi\n",
      "15                                                   \n",
      "16                                                   \n",
      "17                          2\\n#\\n= E\\nσ\\n\\n\\n\\n\n",
      "18                                                   \n",
      "19                                                   \n",
      "20                                    m\\nX\\ni=1\\nσixi\n",
      "21                                                   \n",
      "22                                                   \n",
      "23              2\\n2\\n\\n\\n1/2\\n≤\\n\\nE\\nσ\\n\\n\n",
      "24                                                   \n",
      "25                                                   \n",
      "26                                    m\\nX\\ni=1\\nσixi\n",
      "27                                                   \n",
      "28                                                   \n",
      "29                  2\\n2\\n\\n\\n\\n\\n1/2\\n.\\n(26.16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read text from a .txt file\n",
    "with open('Extracted_Text.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split the text by double newline to separate paragraphs\n",
    "paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "# Filter out lines that only contain \"=====\" or similar patterns\n",
    "paragraphs = [p for p in paragraphs if not p.strip().startswith('=')]\n",
    "\n",
    "# Create a DataFrame where each row is a paragraph\n",
    "df = pd.DataFrame(paragraphs, columns=['Paragraph'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_file.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Read text from a .txt file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myour_file.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Split the text by double newline to separate paragraphs\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_file.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read text from a .txt file\n",
    "with open('Extracted_Text.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split the text by double newline to separate paragraphs\n",
    "paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "# Create a DataFrame where each row is a paragraph\n",
    "df = pd.DataFrame(paragraphs, columns=['Paragraph'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Page 1\\nUnderstanding Machine Learning:\\nFrom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>==============================================...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>==============================================...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>==============================================...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>==============================================...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Paragraph\n",
       "0  Page 1\\nUnderstanding Machine Learning:\\nFrom ...\n",
       "1  ==============================================...\n",
       "2  ==============================================...\n",
       "3  ==============================================...\n",
       "4  ==============================================..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
