{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction and cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean extracted text to exclude lines that resemble mathematical expressions,\n",
    "    index entries, or table of contents entries (e.g., lines with ellipses or numbers).dd\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to identify unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"\\.\\.\\.+\\s*\\d+\",            # Lines with ellipses followed by numbers\n",
    "        r\"^\\d+\\.\\d+\\s\",              # Lines starting with a decimal number (e.g., \"1.1 \")\n",
    "        r\"\\b\\d+\\b\",                  # Lines containing isolated numbers (page numbers, indexes)\n",
    "        # Cleaning mathematical formule and expression..\n",
    "        r\"[A-Za-z0-9]+[\\^\\+\\-*/=<>]\", # Lines with mathematical operators\n",
    "        r\"[A-Za-z0-9]+\\s*[∈∀∃∅⊆∪∩≈∑∏∫θμϵλϕδΩ→≤≥]\", # Lines with symbols commonly in math\n",
    "        r\"[θμϵδ]+\",                  # Greek symbols or Greek-like variables\n",
    "        r\"[<>≤≥=]{2,}\",              # Comparison operators often found in math expressions\n",
    "        r\"^\\d+$\",                    # Lines with only numbers\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns into one\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    # Split the text into lines, filter them, and rejoin\n",
    "    filtered_lines = [\n",
    "        line for line in text.splitlines()\n",
    "        if not combined_pattern.search(line)\n",
    "    ]\n",
    "    \n",
    "    return \"\\n\".join(filtered_lines)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, clean it using clean_text function, and save it to a text file.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Iterate through pages and extract text\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        full_text += page.get_text()\n",
    "\n",
    "    # Clean the extracted text\n",
    "    cleaned_text = clean_text(full_text)\n",
    "    \n",
    "    # Save cleaned text to output file\n",
    "    with open(output_txt_path, \"w\") as txt_file:\n",
    "        txt_file.write(cleaned_text)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf_document.close()\n",
    "\n",
    "# Usage\n",
    "pdf_path = \"../books/AI_Russell_Norvig.pdf\"         # Replace with your PDF file path\n",
    "output_txt_path = \"output_text_russell.txt\"    # Replace with desired output text file path\n",
    "extract_text_from_pdf(pdf_path, output_txt_path)\n",
    "\n",
    "print(\"Text extraction and cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page 14: Contains unwanted content\n",
      "Skipping page 15: Contains unwanted content\n",
      "Skipping page 16: Contains unwanted content\n",
      "Skipping page 17: Contains unwanted content\n",
      "Skipping page 18: Contains unwanted content\n",
      "Skipping page 19: Contains unwanted content\n",
      "Skipping page 1082: Contains unwanted content\n",
      "Skipping page 1083: Contains unwanted content\n",
      "Skipping page 1084: Contains unwanted content\n",
      "Skipping page 1085: Contains unwanted content\n",
      "Skipping page 1086: Contains unwanted content\n",
      "Skipping page 1087: Contains unwanted content\n",
      "Skipping page 1088: Contains unwanted content\n",
      "Skipping page 1089: Contains unwanted content\n",
      "Skipping page 1090: Contains unwanted content\n",
      "Skipping page 1091: Contains unwanted content\n",
      "Skipping page 1092: Contains unwanted content\n",
      "Skipping page 1093: Contains unwanted content\n",
      "Skipping page 1094: Contains unwanted content\n",
      "Skipping page 1095: Contains unwanted content\n",
      "Skipping page 1096: Contains unwanted content\n",
      "Skipping page 1097: Contains unwanted content\n",
      "Skipping page 1098: Contains unwanted content\n",
      "Skipping page 1099: Contains unwanted content\n",
      "Skipping page 1100: Contains unwanted content\n",
      "Skipping page 1101: Contains unwanted content\n",
      "Skipping page 1102: Contains unwanted content\n",
      "Skipping page 1103: Contains unwanted content\n",
      "Skipping page 1104: Contains unwanted content\n",
      "Skipping page 1105: Contains unwanted content\n",
      "Skipping page 1106: Contains unwanted content\n",
      "Skipping page 1107: Contains unwanted content\n",
      "Skipping page 1108: Contains unwanted content\n",
      "Skipping page 1109: Contains unwanted content\n",
      "Skipping page 1110: Contains unwanted content\n",
      "Skipping page 1111: Contains unwanted content\n",
      "Skipping page 1112: Contains unwanted content\n",
      "Skipping page 1114: Contains unwanted content\n",
      "Skipping page 1115: Contains unwanted content\n",
      "Skipping page 1116: Contains unwanted content\n",
      "Skipping page 1117: Contains unwanted content\n",
      "Skipping page 1118: Contains unwanted content\n",
      "Skipping page 1119: Contains unwanted content\n",
      "Skipping page 1120: Contains unwanted content\n",
      "Skipping page 1121: Contains unwanted content\n",
      "Skipping page 1122: Contains unwanted content\n",
      "Skipping page 1123: Contains unwanted content\n",
      "Skipping page 1124: Contains unwanted content\n",
      "Skipping page 1125: Contains unwanted content\n",
      "Skipping page 1126: Contains unwanted content\n",
      "Skipping page 1127: Contains unwanted content\n",
      "Skipping page 1128: Contains unwanted content\n",
      "Skipping page 1129: Contains unwanted content\n",
      "Skipping page 1130: Contains unwanted content\n",
      "Skipping page 1131: Contains unwanted content\n",
      "Skipping page 1132: Contains unwanted content\n",
      "Skipping page 1133: Contains unwanted content\n",
      "Skipping page 1134: Contains unwanted content\n",
      "Skipping page 1135: Contains unwanted content\n",
      "Skipping page 1136: Contains unwanted content\n",
      "Skipping page 1137: Contains unwanted content\n",
      "Skipping page 1138: Contains unwanted content\n",
      "Skipping page 1139: Contains unwanted content\n",
      "Skipping page 1140: Contains unwanted content\n",
      "Skipping page 1141: Contains unwanted content\n",
      "Skipping page 1142: Contains unwanted content\n",
      "Skipping page 1143: Contains unwanted content\n",
      "Skipping page 1144: Contains unwanted content\n",
      "Skipping page 1145: Contains unwanted content\n",
      "Skipping page 1146: Contains unwanted content\n",
      "Skipping page 1147: Contains unwanted content\n",
      "Skipping page 1148: Contains unwanted content\n",
      "Skipping page 1149: Contains unwanted content\n",
      "Skipping page 1150: Contains unwanted content\n",
      "Skipping page 1151: Contains unwanted content\n",
      "Text extraction and cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean extracted text to exclude lines that resemble mathematical expressions,\n",
    "    index entries, or table of contents entries (e.g., lines with ellipses or numbers).\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to identify unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"\\.\\.\\.+\\s*\\d+\",            # Lines with ellipses followed by numbers\n",
    "        r\"^\\d+\\.\\d+\\s\",              # Lines starting with a decimal number (e.g., \"1.1 \")\n",
    "        r\"\\b\\d+\\b\",                  # Lines containing isolated numbers (page numbers, indexes)\n",
    "        # Cleaning mathematical formule and expression..\n",
    "        r\"[A-Za-z0-9]+[\\^\\+\\-*/=<>]\", # Lines with mathematical operators\n",
    "        r\"[A-Za-z0-9]+\\s*[∈∀∃∅⊆∪∩≈∑∏∫θμϵλϕδΩ→≤≥]\", # Lines with symbols commonly in math\n",
    "        r\"[θμϵδ]+\",                  # Greek symbols or Greek-like variables\n",
    "        r\"[<>≤≥=]{2,}\",              # Comparison operators often found in math expressions\n",
    "        r\"^\\d+$\",                    # Lines with only numbers\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns into one\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    # Split the text into lines, filter them, and rejoin\n",
    "    filtered_lines = [\n",
    "        line for line in text.splitlines()\n",
    "        if not combined_pattern.search(line)\n",
    "    ]\n",
    "    \n",
    "    return \"\\n\".join(filtered_lines)\n",
    "\n",
    "def is_page_unwanted(page_text):\n",
    "    \"\"\"\n",
    "    Determine if a page should be skipped based on its content.\n",
    "    \n",
    "    Skips pages starting with \"Contents\", \"Index\", or pages with mostly short-form text and numbers.\n",
    "    \n",
    "    Parameters:\n",
    "        page_text (str): The extracted text from the page.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the page is unwanted, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the page starts with \"Contents\" or \"Index\"\n",
    "    # if re.match(r\"^(Contents|Index)|bibliography\", page_text, re.IGNORECASE):\n",
    "    #     return True\n",
    "    # Split text into lines\n",
    "    lines = page_text.splitlines()\n",
    "    \n",
    "    # Check the first line or second line (if it exists)\n",
    "    if len(lines) > 0 and re.match(r\"^(Contents|Index|Bibliography)\", lines[0], re.IGNORECASE):\n",
    "        return True\n",
    "    if len(lines) > 1 and re.match(r\"^(Contents|Index|Bibliography)\", lines[1], re.IGNORECASE):\n",
    "        return True\n",
    "\n",
    "\n",
    "    \n",
    "    # Count the number of lines that are just short-form text (numbers, symbols, etc.)\n",
    "    \n",
    "    # short_form_pattern = r\"^[A-Za-z0-9\\s\\+\\-\\*/=<>≤≥\\.,]*$\"  # Matches simple short-form lines\n",
    "    # short_form_lines = [line for line in page_text.splitlines() if re.match(short_form_pattern, line)]\n",
    "    \n",
    "    # If more than 70% of the lines are short-form, consider the page unwanted\n",
    "    # if len(page_text.splitlines()):\n",
    "    #     if len(short_form_lines) / len(page_text.splitlines()) > 0.8:\n",
    "    #         return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, clean it using clean_text function, and save it to a text file.\n",
    "    Skips pages starting with \"Contents\", \"Index\" or pages with high short-form text.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Iterate through pages and extract text\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        page_text = page.get_text()\n",
    "\n",
    "        # Skip page if it starts with \"Contents\", \"Index\", or is mostly short-form text\n",
    "        if is_page_unwanted(page_text):\n",
    "            print(f\"Skipping page {page_num + 1}: Contains unwanted content\")\n",
    "            continue\n",
    "\n",
    "        # Clean the extracted text\n",
    "        cleaned_text = clean_text(page_text)\n",
    "        full_text += cleaned_text\n",
    "    \n",
    "    # Save cleaned text to output file\n",
    "    with open(output_txt_path, \"w\") as txt_file:\n",
    "        txt_file.write(full_text)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf_document.close()\n",
    "\n",
    "# Usage\n",
    "pdf_path = \"../books/AI_Russell_Norvig.pdf\"         # Replace with your PDF file path\n",
    "output_txt_path = \"output_text_russell.txt\"    # Replace with desired output text file path\n",
    "extract_text_from_pdf(pdf_path, output_txt_path)\n",
    "\n",
    "print(\"Text extraction and cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_file(input_txt_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Reads a text file, removes lines with specific matrix, summation, or symbol patterns,\n",
    "    and removes lines with fewer than 20 characters. Writes the cleaned content to a new file.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to identify unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"=\\s*m\\s*X\\s*xi\\s*x⊤\\s*i\",  # Pattern for \"= m X xi x⊤ i\"\n",
    "        r\"b\\s*=\\s*m\\s*X\\s*yixi\",     # Pattern for \"b = m X yixi\"\n",
    "        r\"A\\s*=\\s*\\(.*\\)\",           # Matrix-like form pattern\n",
    "        r\"b\\s*=\\s*\\(\"                # Pattern for matrix b with open parentheses\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns into one\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    with open(input_txt_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter lines by patterns and length\n",
    "    filtered_lines = [\n",
    "        line for line in lines\n",
    "        if not combined_pattern.search(line) and len(line.strip()) >= 20\n",
    "    ]\n",
    "\n",
    "    # Write the cleaned lines to the output file\n",
    "    with open(output_txt_path, \"w\") as file:\n",
    "        file.writelines(filtered_lines)\n",
    "\n",
    "# Usage\n",
    "input_txt_path = \"output_text_russell.txt\"      # Replace with the path to your input text file\n",
    "output_txt_path = \"cleaned_text_russell.txt\"   # Replace with the desired output file path\n",
    "clean_text_file(input_txt_path, output_txt_path)\n",
    "\n",
    "print(\"Text cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_file(input_txt_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Reads a text file, removes lines with specific unwanted patterns, and filters out lines\n",
    "    with more than 50% short forms or fewer than 20 characters. Writes the cleaned content to a new file.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to identify unwanted lines\n",
    "    unwanted_patterns = [\n",
    "        r\"=\\s*m\\s*X\\s*xi\\s*x⊤\\s*i\",  # Pattern for \"= m X xi x⊤ i\"\n",
    "        r\"b\\s*=\\s*m\\s*X\\s*yixi\",     # Pattern for \"b = m X yixi\"\n",
    "        r\"A\\s*=\\s*\\(.*\\)\",           # Matrix-like form pattern\n",
    "        r\"b\\s*=\\s*\\(\"                # Pattern for matrix b with open parentheses\n",
    "    ]\n",
    "    \n",
    "    # Regular expression to identify short forms (numbers, symbols, or very short words)\n",
    "    short_form_pattern = re.compile(r\"^[A-Za-z0-9\\s\\+\\-\\*/=<>≤≥\\.,!?]*$\")\n",
    "    \n",
    "    # Combine all unwanted patterns into one\n",
    "    combined_pattern = re.compile(\"|\".join(unwanted_patterns))\n",
    "    \n",
    "    with open(input_txt_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter lines based on unwanted patterns, length, and short-form proportion\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        \n",
    "        # Skip lines matching unwanted patterns\n",
    "        if combined_pattern.search(line):\n",
    "            continue\n",
    "        \n",
    "        # Skip lines shorter than 20 characters\n",
    "        if len(line) < 20:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the proportion of short-form elements\n",
    "        words = line.split()\n",
    "        short_form_count = sum(1 for word in words if short_form_pattern.match(word) and len(word) <= 3)\n",
    "        if short_form_count / len(words) > 0.5:\n",
    "            continue  # Skip if more than 50% of the line is short forms\n",
    "        \n",
    "        filtered_lines.append(line + \"\\n\")\n",
    "\n",
    "    # Write the cleaned lines to the output file\n",
    "    with open(output_txt_path, \"w\") as file:\n",
    "        file.writelines(filtered_lines)\n",
    "\n",
    "# Usage\n",
    "input_txt_path = \"output_text_russell.txt\"      # Replace with the path to your input text file\n",
    "output_txt_path = \"cleaned_text_russell.txt\"   # Replace with the desired output file path\n",
    "clean_text_file(input_txt_path, output_txt_path)\n",
    "\n",
    "print(\"Text cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_and_process_text_file(file_path):\n",
    "    try:\n",
    "        # Read the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        # Split text into paragraphs using a more specific pattern\n",
    "        # This pattern looks for:\n",
    "        # 1. Paragraphs starting with capital letters after line breaks\n",
    "        # 2. Sections separated by blank lines\n",
    "        # 3. Numbered or bulleted sections\n",
    "        paragraphs = []\n",
    "        current_para = []\n",
    "        \n",
    "        # Split into lines first\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            # Remove leading/trailing whitespace\n",
    "            line = line.strip()\n",
    "            \n",
    "            # If line is empty and we have collected some text\n",
    "            if not line and current_para:\n",
    "                # Join the collected lines and add to paragraphs\n",
    "                paragraphs.append(' '.join(current_para))\n",
    "                current_para = []\n",
    "            # If line starts with bullet points, numbers, or is a new section\n",
    "            elif line and (line[0].isupper() or \n",
    "                         line[0].isdigit() or \n",
    "                         line.startswith('•') or \n",
    "                         line.startswith('-')):\n",
    "                # If we have a previous paragraph, save it\n",
    "                if current_para:\n",
    "                    paragraphs.append(' '.join(current_para))\n",
    "                    current_para = []\n",
    "                current_para.append(line)\n",
    "            # If it's a continuation line with content\n",
    "            elif line:\n",
    "                current_para.append(line)\n",
    "        \n",
    "        # Add the last paragraph if exists\n",
    "        if current_para:\n",
    "            paragraphs.append(' '.join(current_para))\n",
    "        \n",
    "        # Clean paragraphs\n",
    "        cleaned_paragraphs = []\n",
    "        for para in paragraphs:\n",
    "            # Clean up extra spaces and join lines\n",
    "            cleaned = ' '.join(para.split())\n",
    "            if cleaned and len(cleaned) > 10:  # Minimum length to filter out very short segments\n",
    "                cleaned_paragraphs.append(cleaned)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(cleaned_paragraphs, columns=['paragraph'])\n",
    "        \n",
    "        # Add paragraph number\n",
    "        df['paragraph_number'] = range(1, len(df) + 1)\n",
    "        \n",
    "        # Reorder columns\n",
    "        df = df[['paragraph_number', 'paragraph']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage example\n",
    "file_path = '/home/saurabh/Desktop/Enigmatica /Enigmatica/cleaned_text_russell.txt'  # Replace with your text file path\n",
    "df = read_and_process_text_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_number</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Artiﬁcial Intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Third EditionPRENTICE HALL SERIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>IN ARTIFICIAL INTELLIGENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Stuart Russell and Peter Norvig, Editors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Computer Vision: A Modern Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5313</th>\n",
       "      <td>5314</td>\n",
       "      <td>Example of a generator function and its invoca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>5315</td>\n",
       "      <td>• Lists: [x, y, z] denotes a list of three ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>5316</td>\n",
       "      <td>• Sets: {x, y, z} denotes a set of three eleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>5317</td>\n",
       "      <td>Most of the algorithms in the book have been i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>5318</td>\n",
       "      <td>The same Web site includes instructions for se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5318 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paragraph_number                                          paragraph\n",
       "0                    1                             Artiﬁcial Intelligence\n",
       "1                    2                  Third EditionPRENTICE HALL SERIES\n",
       "2                    3                         IN ARTIFICIAL INTELLIGENCE\n",
       "3                    4           Stuart Russell and Peter Norvig, Editors\n",
       "4                    5                 Computer Vision: A Modern Approach\n",
       "...                ...                                                ...\n",
       "5313              5314  Example of a generator function and its invoca...\n",
       "5314              5315  • Lists: [x, y, z] denotes a list of three ele...\n",
       "5315              5316  • Sets: {x, y, z} denotes a set of three eleme...\n",
       "5316              5317  Most of the algorithms in the book have been i...\n",
       "5317              5318  The same Web site includes instructions for se...\n",
       "\n",
       "[5318 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs saved to /home/saurabh/Desktop/paragraphs.csv\n",
      "   paragraph_number                                          paragraph\n",
      "0                 1  Artiﬁcial Intelligence\\nThird EditionPRENTICE ...\n",
      "1                 2  Russell and Peter Norvig\\nContributing writers...\n",
      "2                 3  Manufactured in the United States of America. ...\n",
      "3                 4  To obtain permission(s) to use materials from ...\n",
      "4                 5  The author and publisher shall\\nof, the furnis...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_and_process_text_file(file_path):\n",
    "    try:\n",
    "        # Read the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Use regex to split text by sentences ending with a period\n",
    "        # This considers sentences that end with '.', '!', or '?'\n",
    "        sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s+'\n",
    "        sentences = re.split(sentence_pattern, text)\n",
    "        \n",
    "        # Combine sentences into paragraphs with a minimum length of 10 characters\n",
    "        paragraphs = []\n",
    "        current_para = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 10:  # Avoid very short fragments\n",
    "                current_para.append(sentence)\n",
    "                # If the current paragraph exceeds a reasonable length, create a new paragraph\n",
    "                if len(' '.join(current_para)) > 300:  # Example threshold\n",
    "                    paragraphs.append(' '.join(current_para))\n",
    "                    current_para = []\n",
    "        \n",
    "        # Add the last paragraph if it exists\n",
    "        if current_para:\n",
    "            paragraphs.append(' '.join(current_para))\n",
    "        \n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(paragraphs, columns=['paragraph'])\n",
    "        \n",
    "        # Add paragraph numbers\n",
    "        df['paragraph_number'] = range(1, len(df) + 1)\n",
    "        \n",
    "        # Reorder columns\n",
    "        df = df[['paragraph_number', 'paragraph']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage example\n",
    "file_path = '/home/saurabh/Desktop/Enigmatica /Enigmatica/cleaned_text_russell.txt'  # Replace with your text file path\n",
    "df = read_and_process_text_file(file_path)\n",
    "\n",
    "# Save DataFrame to a CSV file (optional)\n",
    "if df is not None:\n",
    "    output_path = '/home/saurabh/Desktop/paragraphs.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Paragraphs saved to {output_path}\")\n",
    "\n",
    "# Print a preview\n",
    "if df is not None:\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def add_sentence_embeddings(df, text_column='paragraph', embedding_column='sent_embd', model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Adds sentence embeddings to a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame containing text data.\n",
    "        text_column (str): The column name in the DataFrame containing text data.\n",
    "        embedding_column (str): The name of the new column to store embeddings.\n",
    "        model_name (str): Name of the pre-trained sentence transformer model.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with an additional column for embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the pre-trained sentence transformer model\n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Check if the specified text column exists in the DataFrame\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "        \n",
    "        # Generate embeddings and add them to a new column\n",
    "        df[embedding_column] = df[text_column].apply(lambda text: model.encode(text))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_df = add_sentence_embeddings(df, text_column='paragraph', embedding_column='sent_embd', model_name='all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_number</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>sent_embd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Artiﬁcial Intelligence\\nThird EditionPRENTICE ...</td>\n",
       "      <td>[-0.053213518, -0.062415596, 0.046281844, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Russell and Peter Norvig\\nContributing writers...</td>\n",
       "      <td>[-0.023173211, 0.036421303, 0.010537555, 0.074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Manufactured in the United States of America. ...</td>\n",
       "      <td>[-0.019787418, -0.0119463345, -0.04162405, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>To obtain permission(s) to use materials from ...</td>\n",
       "      <td>[-0.07215656, 0.07856877, 0.011184857, 0.00322...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The author and publisher shall\\nof, the furnis...</td>\n",
       "      <td>[0.0009798234, -0.06381129, 0.013659894, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4060</th>\n",
       "      <td>4061</td>\n",
       "      <td>Thus,\\npersistent variables are like global va...</td>\n",
       "      <td>[0.022720046, -0.0023750374, -0.09216548, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>4062</td>\n",
       "      <td>• Functions as values: Functions and procedure...</td>\n",
       "      <td>[-0.028717004, 0.017587923, -0.03225768, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>4063</td>\n",
       "      <td>and Visual Basic (which use end). swap two var...</td>\n",
       "      <td>[-0.0754576, 0.027873253, -0.06630628, 0.01281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063</th>\n",
       "      <td>4064</td>\n",
       "      <td>returns a generator, which in turn yields one ...</td>\n",
       "      <td>[-0.051368438, 0.054829177, -0.10170182, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>4065</td>\n",
       "      <td>• Sets: {x, y, z} denotes a set of three eleme...</td>\n",
       "      <td>[-0.016395349, 0.056864247, -0.043834805, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4065 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paragraph_number                                          paragraph  \\\n",
       "0                    1  Artiﬁcial Intelligence\\nThird EditionPRENTICE ...   \n",
       "1                    2  Russell and Peter Norvig\\nContributing writers...   \n",
       "2                    3  Manufactured in the United States of America. ...   \n",
       "3                    4  To obtain permission(s) to use materials from ...   \n",
       "4                    5  The author and publisher shall\\nof, the furnis...   \n",
       "...                ...                                                ...   \n",
       "4060              4061  Thus,\\npersistent variables are like global va...   \n",
       "4061              4062  • Functions as values: Functions and procedure...   \n",
       "4062              4063  and Visual Basic (which use end). swap two var...   \n",
       "4063              4064  returns a generator, which in turn yields one ...   \n",
       "4064              4065  • Sets: {x, y, z} denotes a set of three eleme...   \n",
       "\n",
       "                                              sent_embd  \n",
       "0     [-0.053213518, -0.062415596, 0.046281844, -0.0...  \n",
       "1     [-0.023173211, 0.036421303, 0.010537555, 0.074...  \n",
       "2     [-0.019787418, -0.0119463345, -0.04162405, -0....  \n",
       "3     [-0.07215656, 0.07856877, 0.011184857, 0.00322...  \n",
       "4     [0.0009798234, -0.06381129, 0.013659894, -0.03...  \n",
       "...                                                 ...  \n",
       "4060  [0.022720046, -0.0023750374, -0.09216548, 0.07...  \n",
       "4061  [-0.028717004, 0.017587923, -0.03225768, -0.01...  \n",
       "4062  [-0.0754576, 0.027873253, -0.06630628, 0.01281...  \n",
       "4063  [-0.051368438, 0.054829177, -0.10170182, -0.01...  \n",
       "4064  [-0.016395349, 0.056864247, -0.043834805, -0.0...  \n",
       "\n",
       "[4065 rows x 3 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_top_matches(sentence, df, text_column='paragraph', embedding_column='sent_embd', top_n=5, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Find the most similar paragraphs to a given sentence based on vector embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): The input sentence to compare.\n",
    "        df (pd.DataFrame): DataFrame containing text data and embeddings.\n",
    "        text_column (str): The column name in the DataFrame containing text data.\n",
    "        embedding_column (str): The column name containing vector embeddings.\n",
    "        top_n (int): Number of top matches to return.\n",
    "        model_name (str): Name of the pre-trained sentence transformer model.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: List of top matches as (paragraph, similarity_score).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the pre-trained model\n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Encode the input sentence to get its embedding\n",
    "        sentence_embedding = model.encode(sentence)\n",
    "        \n",
    "        # Ensure embeddings in DataFrame are numpy arrays\n",
    "        df[embedding_column] = df[embedding_column].apply(lambda x: np.array(x) if isinstance(x, list) else x)\n",
    "        \n",
    "        # Compute cosine similarity between the input sentence and all embeddings in the DataFrame\n",
    "        similarities = cosine_similarity([sentence_embedding], list(df[embedding_column]))\n",
    "        \n",
    "        # Get the top N matches\n",
    "        top_indices = np.argsort(similarities[0])[::-1][:top_n]\n",
    "        top_matches = [(df.iloc[i][text_column], similarities[0][i]) for i in top_indices]\n",
    "        \n",
    "        return top_matches\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_ai_questions = [\n",
    "    \"What is the definition of Artificial Intelligence according to ?\",\n",
    "    \"What are the main approaches to defining Artificial Intelligence?\",\n",
    "    \"What is the Turing Test, and how does it evaluate AI systems?\",\n",
    "    \"What are the different types of agents in AI?\",\n",
    "    \"Can you explain the PEAS (Performance measure, Environment, Actuators, Sensors) framework for designing agents?\",\n",
    "    \"What is the difference between rational agents and intelligent agents?\",\n",
    "    \"What are the main categories of AI environments?\",\n",
    "    \"What are the characteristics of fully observable vs. partially observable environments?\",\n",
    "    \"What is the role of problem-solving in AI?\",\n",
    "    \"Can you explain uninformed search strategies like BFS and DFS?\",\n",
    "    \"What are heuristic search techniques in AI?\",\n",
    "    \"What is the A* algorithm, and why is it considered optimal and complete?\",\n",
    "    \"What are adversarial search strategies, and where are they used?\",\n",
    "    \"Can you explain the minimax algorithm and alpha-beta pruning?\",\n",
    "    \"What are constraint satisfaction problems (CSPs) in AI?\",\n",
    "    \"How are logical agents different from other types of agents?\",\n",
    "    \"What is propositional logic, and how is it used in AI?\",\n",
    "    \"Can you explain first-order logic and its role in AI knowledge representation?\",\n",
    "    \"What is the difference between forward chaining and backward chaining in logical reasoning?\",\n",
    "    \"How are Bayesian networks used in probabilistic reasoning?\",\n",
    "    \"What is the Markov Decision Process (MDP), and how does it apply to AI?\",\n",
    "    \"Can you explain the concept of reinforcement learning in AI?\",\n",
    "    \"What is Q-learning, and how does it differ from traditional reinforcement learning?\",\n",
    "    \"What is the role of deep learning in modern AI systems?\",\n",
    "    \"Can you explain the structure of neural networks and their use in AI?\",\n",
    "    \"What is the difference between supervised, unsupervised, and reinforcement learning?\",\n",
    "    \"What is natural language processing, and what are its main applications?\",\n",
    "    \"What is the importance of computer vision in AI?\",\n",
    "    \"How do AI systems handle uncertainty in decision-making?\",\n",
    "    \"What ethical challenges are associated with modern AI systems?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Matches:\n",
      "Paragraph: Some systems attempt to analyze the source language text all the way\n",
      "guage from that representation. This is difﬁcult because it involves three unsolved problems:\n",
      "creating a complete knowledge representation of everything; parsing into that representation;\n",
      "and generating sentences from that representation.\n",
      "Similarity Score: 0.5691\n",
      "\n",
      "Paragraph: free grammar) are useful tools for dealing with some aspects of natural language. TheBibliographical and Historical Notes\n",
      "such as the CYK algorithm, which requires grammar rules to be in Chomsky Normal\n",
      "• A treebank can be used to learn a grammar. It is also possible to learn a grammar from\n",
      "an unparsed corpus of sentences, but this is less successful.\n",
      "Similarity Score: 0.5620\n",
      "\n",
      "Paragraph: So in a trigram model (Markov chain of\n",
      "We can deﬁne the probability of a sequence of characters P(c1:N) under the trigram model\n",
      "by ﬁrst factoring with the chain rule and then using the Markov assumption:\n",
      "entries, and can be accurately estimated by counting character sequences in a body of text of\n",
      "Latin word for body.Chapter\n",
      "Natural Language Processing\n",
      "is language identiﬁcation: given a text, determine what natural language it is written in.\n",
      "Similarity Score: 0.5589\n",
      "\n",
      "Paragraph: Modern linguistics and AI, then, were “born” at about the same time, and grew up\n",
      "together, intersecting in a hybrid ﬁeld called computational linguistics or natural language\n",
      "processing. The problem of understanding language soon turned out to be considerably more\n",
      "subject matter and context, not just an understanding of the structure of sentences.\n",
      "Similarity Score: 0.5471\n",
      "\n",
      "Paragraph: In practice, these systems work on all relations in parallel,\n",
      "extraction system that is targeted at a few relations and more like a human reader who learns\n",
      "from the text itself; because of this the ﬁeld has been called machine reading. TEXTRUNNER uses cotraining to boost its performance, but it needs something to bootstrap\n",
      "ships are expressed in English.\n",
      "Similarity Score: 0.5369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example input sentence\n",
    "input_sentence =      \"What is natural language processing, and what are its main applications?\"\n",
    "\n",
    "# Get the top 3 matches\n",
    "top_matches = get_top_matches(input_sentence, modified_df, text_column='paragraph', embedding_column='sent_embd', top_n=5)\n",
    "\n",
    "# Print results\n",
    "print(\"Top Matches:\")\n",
    "for match, score in top_matches:\n",
    "    print(f\"Paragraph: {match}\\nSimilarity Score: {score:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
