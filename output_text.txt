Machine Learning
A Probabilistic Perspective
Kevin P. MurphyMachine Learning: A Probabilistic PerspectiveMachine Learning
A Probabilistic Perspective
Kevin P. Murphy
The MIT Press
Cambridge, Massachusetts
London, EnglandAll rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical
means (including photocopying, recording, or information storage and retrieval) without permission in
writing from the publisher.
For information about special quantity discounts, please email special_sales@mitpress.mit.edu
This book was set in the LATEX programming language by the author. Printed and bound in the United
States of America.
Murphy, Kevin P.
Machine learning : a probabilistic perspective / Kevin P. Murphy.
p. cm. — (Adaptive computation and machine learning series)
Includes bibliographical references and index.This book is dedicated to Alessandro, Michael and Stefano,
and to the memory of Gerard Joseph Murphy.Preface
Introduction
With the ever increasing amounts of data in electronic form, the need for automated methods
for data analysis continues to grow. The goal of machine learning is to develop methods that
can automatically detect patterns in data, and then to use the uncovered patterns to predict
future data or other outcomes of interest. Machine learning is thus closely related to the ﬁelds
of statistics and data mining, but differs slightly in terms of its emphasis and terminology. This
book provides a detailed introduction to the ﬁeld, and includes worked examples drawn from
application domains such as molecular biology, text processing, computer vision, and robotics.
Target audience
computer science, statistics, electrical engineering, econometrics, or any one else who has the
appropriate mathematical background. Speciﬁcally, the reader is assumed to already be familiar
with basic multivariate calculus, probability, linear algebra, and computer programming. Prior
exposure to statistics is helpful but not necessary.
A probabilistic approach
This books adopts the view that the best way to make machines that can learn from data is to
use the tools of probability theory, which has been the mainstay of statistics and engineering for
centuries. Probability theory can be applied to any problem involving uncertainty. In machine
learning, uncertainty comes in many forms: what is the best prediction (or decision) given some
data? what is the best model given some data? what measurement should I perform next? etc.
The systematic application of probabilistic reasoning to all inferential problems, including
inferring parameters of statistical models, is sometimes called a Bayesian approach. However,
this term tends to elicit very strong reactions (either positive or negative, depending on who
you ask), so we prefer the more neutral term “probabilistic approach”. Besides, we will often
use techniques such as maximum likelihood estimation, which are not Bayesian methods, but
certainly fall within the probabilistic paradigm.xxviii
Preface
can often be applied.
Conversely, any given algorithm can often be applied to a variety of
models. This kind of modularity, where we distinguish model from algorithm, is good pedagogy
and good engineering.
We will often use the language of graphical models to specify our models in a concise and
intuitive way.
In addition to aiding comprehension, the graph structure aids in developing
efficient algorithms, as we will see. However, this book is not primarily about graphical models;
it is about probabilistic modeling in general.
A practical approach
Nearly all of the methods described in this book have been implemented in a MATLAB software
package called PMTK, which stands for probabilistic modeling toolkit. This is freely available
one used in this version of the book). There are also a variety of supporting ﬁles, written by other
people, available at pmtksupport.googlecode.com. These will be downloaded automatically,
if you follow the setup instructions described on the PMTK website.
requires the Statistics toolbox, which needs to be purchased separately. There is also a free
supports most of the functionality of MATLAB. Some (but not all) of the code in this book also
works in Octave. See the PMTK website for details.
PMTK was used to generate many of the ﬁgures in this book; the source code for these ﬁgures
is included on the PMTK website, allowing the reader to easily see the effects of changing the
data or algorithm or parameter settings. The book refers to ﬁles by name, e.g., naiveBayesFit.
In order to ﬁnd the corresponding ﬁle, you can use two methods: within Matlab you can type
which naiveBayesFit and it will return the full path to the ﬁle; or, if you do not have Matlab
but want to read the source code anyway, you can use your favorite search engine, which should
return the corresponding ﬁle from the pmtk3.googlecode.com website.
Details on how to use PMTK can be found on the website, which will be udpated over time.
Details on the underlying theory behind these methods can be found in this book.
Acknowledgments
A book this large is obviously a team effort. I would especially like to thank the following people:
my wife Margaret, for keeping the home ﬁres burning as I toiled away in my office for the last six
years; Matt Dunham, who created many of the ﬁgures in this book, and who wrote much of the
code in PMTK; Baback Moghaddam, who gave extremely detailed feedback on every page of an
earlier draft of the book; Chris Williams, who also gave very detailed feedback; Cody Severinski
comments on earlier drafts; Daphne Koller, Nir Friedman, and Chris Manning, for letting me use
their latex style ﬁles; Stanford University, Google Research and Skyline College for hosting me
during part of my sabbatical; and various Canadian funding agencies (NSERC, CRC and CIFAR)
who have supported me ﬁnancially over the years.
In addition, I would like to thank the following people for giving me helpful feedback onPreface
xxix
Blei, Hannes Bretschneider, Greg Corrado, Arnaud Doucet, Mario Figueiredo, Nando de Freitas,
Mark Girolami, Gabriel Goh, Tom Griffiths, Katherine Heller, Geoff Hinton, Aapo Hyvarinen,
Tommi Jaakkola, Mike Jordan, Charles Kemp, Emtiyaz Khan, Bonnie Kirkpatrick, Daphne Koller,
Zico Kolter, Honglak Lee, Julien Mairal, Andrew McPherson, Tom Minka, Ian Nabney, Arthur
Pope, Carl Rassmussen, Ryan Rifkin, Ruslan Salakhutdinov, Mark Schmidt, Daniel Selsam, David
Sontag, Erik Sudderth, Josh Tenenbaum, Kai Yu, Martin Wainwright, Yair Weiss.
Kevin Patrick Murphy
Palo Alto, CaliforniaIntroduction
Machine learning: what and why?
We are drowning in information and starving for knowledge. — John Naisbitt.
been sequenced by various labs; Walmart handles more than 1M transactions per hour and has
on.
This deluge of data calls for automated methods of data analysis, which is what machine
learning provides.
In particular, we deﬁne machine learning as a set of methods that can
automatically detect patterns in data, and then use the uncovered patterns to predict future
data, or to perform other kinds of decision making under uncertainty (such as planning how to
collect more data!).
This books adopts the view that the best way to solve such problems is to use the tools
of probability theory. Probability theory can be applied to any problem involving uncertainty.
In machine learning, uncertainty comes in many forms: what is the best prediction about the
future given some past data? what is the best model to explain some data? what measurement
should I perform next? etc. The probabilistic approach to machine learning is closely related to
the ﬁeld of statistics, but differs slightly in terms of its emphasis and terminology3.
We will describe a wide variety of probabilistic models, suitable for a wide variety of data and
tasks. We will also describe a wide variety of algorithms for learning and using such models.
The goal is not to develop a cook book of ad hoc techiques, but instead to present a uniﬁed
view of the ﬁeld through the lens of probabilistic modeling and inference. Although we will pay
attention to computational efficiency, details on how to scale these methods to truly massiveIt should be noted, however, that even when one has an apparently massive data set, the
effective number of data points for certain cases of interest might be quite small. In fact, data
across a variety of domains exhibits a property known as the long tail, which means that a
means that the core statistical issues that we discuss in this book, concerning generalizing from
relatively small samples sizes, are still very relevant even in the big data era.
Types of machine learning
Machine learning is usually divided into two main types.
In the predictive or supervised
learning approach, the goal is to learn a mapping from inputs x to outputs y, given a labeled
number of training examples.
resenting, say, the height and weight of a person.
These are called features, attributes or
covariates. In general, however, xi could be a complex structured object, such as an image, a
sentence, an email message, a time series, a molecular shape, a graph, etc.
Similarly the form of the output or response variable can in principle be anything, but
most methods assume that yi is a categorical or nominal variable from some ﬁnite set,
level). When yi is categorical, the problem is known as classiﬁcation or pattern recognition,
ordinal regression, occurs where label space Y has some natural ordering, such as grades A–F.
The second main type of machine learning is the descriptive or unsupervised learning
approach. Here we are only given inputs, D = {xi}N
patterns” in the data. This is sometimes called knowledge discovery. This is a much less
obvious error metric to use (unlike supervised learning, where we can compare our prediction
of y for a given x to the observed value).
There is a third type of machine learning, known as reinforcement learning, which is
somewhat less commonly used. This is useful for learning how to act or behave when given
occasional reward or punishment signals. (For example, consider how a baby learns to walk.)
Unfortunately, RL is beyond the scope of this book, although we do discuss decision theory
on RL.(a)
(b)
Right: Representing the training data as an N × D design matrix. Row i represents the feature vector xi.
Supervised learning
We begin our investigation of machine learning by discussing supervised learning, which is the
form of ML most widely used in practice.
Classiﬁcation
In this section, we discuss classiﬁcation. Here the goal is to learn a mapping from inputs x
multiclass classiﬁcation. If the class labels are not mutually exclusive (e.g., somebody may be
use the term “classiﬁcation”, we will mean multiclass classiﬁcation with a single output, unless
we state otherwise.
One way to formalize the problem is as function approximation. We assume y = f(x) for
some unknown function f, and the goal of learning is to estimate the function f given a labeled
training set, and then to make predictions using ˆy = ˆf(x). (We use the hat symbol to denote
an estimate.) Our main goal is to make predictions on novel inputs, meaning ones that we have
not seen before (this is called generalization), since predicting the response on the training set
is easy (we can just look up the answer).
Example
These have been described by a set of D features or attributes, which are stored in an N × D
combination of the two. In addition to the inputs, we have a vector of training labels y.
these have been seen before. Thus we are required to generalize beyond the training set. Ait is not clear what the right label should be in the case of the yellow circle. Similarly, the correct
label for the blue arrow is unclear.
The need for probabilistic predictions
To handle ambiguous cases, such as the yellow circle above, it is desirable to return a probability.
The reader is assumed to already have some familiarity with basic concepts in probability. If
We will denote the probability distribution over possible labels, given the input vector x and
training set D by p(y|x, D). In general, this represents a vector of length C. (If there are just two
input x, as well as the training set D, by putting these terms on the right hand side of the
conditioning bar |. We are also implicitly conditioning on the form of model that we use to make
predictions. When choosing between different models, we will make this assumption explicit by
writing p(y|x, D, M), where M denotes the model. However, if the model is clear from context,
we will drop M from our notation for brevity.
Given a probabilistic output, we can always compute our “best guess” as to the “true label”
using
ˆy = ˆf(x) =
C
argmax
p(y = c|x, D)
This corresponds to the most probable class label, and is called the mode of the distribution
p(y|x, D); it is also known as a MAP estimate (MAP stands for maximum a posteriori). Using
the most probable label makes intuitive sense, but we will give a more formal justiﬁcation for
case we are not very conﬁdent of our answer, so it might be better to say “I don’t know” instead
of returning an answer that we don’t really trust. This is particularly important in domains
Another application where it is important to assess risk is when playing TV game shows, such
as Jeopardy. In this game, contestants have to solve various word puzzles and answer a variety
computer system called Watson which beat the top human Jeopardy champion. Watson uses a
purposes is that it contains a module that estimates how conﬁdent it is of its answer. The system
only chooses to “buzz in” its answer if sufficiently conﬁdent it is correct. Similarly, Google has a
system known as SmartASS (ad selection system) that predicts the probability you will click on
proﬁt. We will discuss some of the basic principles behind systems such as SmartASS later in
this book.words
documents
The red
USENET groups). We can see that there are subsets of words whose presence or absence is indicative
newsgroupsVisualize.
Classiﬁcation is probably the most widely used form of machine learning, and has been used
some important applciations. We give a few more examples below.
Document classiﬁcation and email spam ﬁltering
In document classiﬁcation, the goal is to classify a document, such as a web page or email
tation of the text. A special case of this is email spam ﬁltering, where the classes are spam
Most classiﬁers assume that the input vector x has a ﬁxed size. A common way to represent
occurs in document i. If we apply this transformation to every document in our data set, we get
document classiﬁcation problem has been reduced to one that looks for subtle changes in the
pattern of bits. For example, we may notice that most spam messages have a high probability of(a)
(b)
(c)
Three types of iris ﬂowers: setosa, versicolor and virginica.
sepal length
sepal length
sepal width
petal length
petal width
sepal width
petal length
petal width
Visualization of the Iris data as a pairwise scatter plot.
The diagonal plots the marginal
circle = setosa, green diamond = versicolor, blue star = virginica. Figure generated by fisheririsDemo.
Classifying ﬂowers
is to learn to distinguish three different kinds of iris ﬂower, called setosa, versicolor and virginica.
features or characteristics: sepal length and width, and petal length and width. (Such feature
extraction is an important, but difficult, task. Most machine learning methods use features
chosen by some human. Later we will discuss some methods that can learn good features from
distinguish setosas (red circles) from the other two classes by just checking if their petal length(a)
(b)
randomly. Classiﬁcation performance is identical on both versions of the data (assuming the training data
is permuted in an identical way). Figure generated by shuffledDigitsDemo.
or width is below some threshold. However, distinguishing versicolor from virginica is slightly
harder; any decision will need to be based on at least two features. (It is always a good idea
to perform exploratory data analysis, such as plotting the data, before applying a machine
learning method.)
Image classiﬁcation and handwriting recognition
processed the data. We might want to classify the image as a whole, e.g., is it an indoors or
outdoors scene? is it a horizontal or vertical photo? does it contain a dog or not? This is called
image classiﬁcation.
In the special case that the images consist of isolated handwritten letters and digits, for
example, in a postal or ZIP code on a letter, we can use classiﬁcation to perform handwriting
recognition. A standard dataset used in this area is known as MNIST, which stands for “Modiﬁed
preprocessed to ensure the digits are mostly in the center of the image.) This dataset contains
some example images.
Many generic classiﬁcation methods ignore any structure in the input features, such as spatial
is the same data except we have randomly permuted the order of all the features. (You will
purpose) and a curse (since the methods ignore an obviously useful source of information). We
will discuss methods for exploiting structure in the input features later in the book.(a)
(b)
to a dense set of overlapping patches in the test image. Only the patches whose probability of containing
a face was sufficiently high were returned. Used with kind permission of Pittpatt.com
Face detection and recognition
A harder problem is to ﬁnd objects within an image; this is called object detection or object
localization. An important special case of this is face detection. One approach to this problem
is to divide the image into many small overlapping patches at different locations, scales and
not. This is called a sliding window detector. The system then returns those locations where
out faces in Google’s StreetView system.
Having found the faces, one can then proceed to perform face recognition, which means
might be very large. Also, the features one should use are likely to be different than in the face
detection problem: for recognition, subtle differences between faces such as hairstyle may be
important for determining identity, but for detection, it is important to be invariant to such
Regression
(We explain how to ﬁt such models below.) Various extensions of this basic problem can arise,
ways to handle such problems later in the book.(a)
(b)
Figure generated by linregPolyVsDegree.
•
Predict tomorrow’s stock market price given current market conditions and other possible
side information.
•
Predict the age of a viewer watching a given video on YouTube.
•
Predict the location in 3d space of a robot arm end effector, given control signals (torques)
sent to its various motors.
•
Predict the amount of prostate speciﬁc antigen (PSA) in the body as a function of a number
of different clinical measurements.
•
Predict the temperature at any location inside a building using weather data, time, door
sensors, etc.
Unsupervised learning
We now consider unsupervised learning, where we are just given output data, without any
inputs.
The goal is to discover “interesting structure” in the data; this is sometimes called
knowledge discovery. Unlike supervised learning, we are not told what the desired output is
for each input. Instead, we will formalize our task as one of density estimation, that is, we
want to build models of the form p(xi|θ). There are two differences from the supervised case.
First, we have written p(xi|θ) instead of p(yi|xi, θ); that is, supervised learning is conditional
density estimation, whereas unsupervised learning is unconditional density estimation. Second,
xi is a vector of features, so we need to create multivariate probability models. By contrast,
in supervised learning, yi is usually just a single variable that we are trying to predict. This
means that for most supervised learning problems, we can use univariate probability models
probability models.)
Unsupervised learning is arguably more typical of human and animal learning. It is also
more widely applicable than supervised learning, since it does not require a human expert toheight
weight
(a)
height
weight
(b)
Figure generated by kmeansHeightWeight.
manually label the data. Labeled data is not only expensive to acquire6, but it also contains
relatively little information, certainly not enough to reliably estimate the parameters of complex
models. Geoff Hinton, who is a famous professor of ML at the University of Toronto, has said:
When we’re learning to see, nobody’s telling us what the right answers are — we just
look. Every so often, your mother says “that’s a dog”, but that’s very little information.
You’d be lucky if you got a few bits of information — even one bit per second — that
second. And there’s only one place you can get that much information: from the input
Below we describe some canonical examples of unsupervised learning.
Discovering clusters
As a canonical example of unsupervised learning, consider the problem of clustering data into
it is not clear how many. Let K denote the number of clusters. Our ﬁrst goal is to estimate
the distribution over the number of clusters, p(K|D); this tells us if there are subpopulations
within the data. For simplicity, we often approximate the distribution p(K|D) by its mode,
K∗= arg maxK p(K|D). In the supervised case, we were told that there are two classes (male
and female), but in the unsupervised case, we are free to choose as many or few clusters as we
like. Picking a model of the “right” complexity is called model selection, and will be discussed
in detail below.
represent the cluster to which data point i is assigned.
(zi is an example of a hidden or
which outsource data processing tasks to humans all over the world, has reduced the cost of labeling data. Nevertheless,
the amount of unlabeled data is still orders of magnitude larger than the amount of labeled data.(a)
(b)
(a) A set of points that live on a 2d linear subspace embedded in 3d. The solid red line is the
ﬁrst principal component direction. The dotted black line is the second PC direction. (b) 2D representation
of the data. Figure generated by pcaDemo3d.
latent variable, since it is never observed in the training set.) We can infer which cluster each
data point belongs to by computing z∗
i = argmaxk p(zi = k|xi, D).
This is illustrated in
In this book, we focus on model based clustering, which means we ﬁt a probabilistic model
approach are that one can compare different kinds of models in an objective way (in terms of
the likelihood they assign to the data), we can combine them together into larger systems, etc.
Here are some real world applications of clustering.
•
based on clustering astrophysical measurements.
•
•
Discovering latent factors
When dealing with high dimensional data, it is often useful to reduce the dimensionality by
projecting the data to a lower dimensional subspace which captures the “essence” of the data.
project some 3d data down to a 2d plane. The 2d approximation is quite good, since most points
lie close to this subspace. Reducing to 1d would involve projecting points onto the red line in
The motivation behind this technique is that although the data may appear high dimensional,
there may only be a small number of degrees of variability, corresponding to latent factors. For
example, when modeling the appearance of face images, there may only be a few underlying
latent factors which describe most of the variability, such as lighting, pose, identity, etc, as(a)
(b)
and the ﬁrst three principal component basis vectors (eigenfaces). Figure generated by pcaImageDemo.
When used as input to other statistical models, such low dimensional representations often
result in better predictive accuracy, because they focus on the “essence” of the object, ﬁltering
out inessential features.
Also, low dimensional representations are useful for enabling fast
nearest neighbor searches and two dimensional projections are very useful for visualizing high
dimensional data.
The most common approach to dimensionality reduction is called principal components
Dimensionality reduction, and PCA in particular, has been applied in many different areas.
Some examples include the following:
•
In biology, it is common to use PCA to interpret gene microarray data, to account for the
fact that each measurement is usually the result of many genes which are correlated in their
behavior by the fact that they belong to different biological pathways.
•
In natural language processing, it is common to use a variant of PCA called latent semantic
•
In signal processing (e.g., of acoustic or neural signals), it is common to use ICA (which is a
•
In computer graphics, it is common to project motion capture data to a low dimensionalDiscovering graph structure
Sometimes we measure a set of correlated variables, and we would like to discover which ones
are most correlated with which others. This can be represented by a graph G, in which nodes
represent variables, and edges represent direct dependence between variables (we will make
structure from data, i.e., we compute ˆG = argmax p(G|D).
As with unsupervised learning in general, there are two main applications for learning sparse
graphs: to discover new knowledge, and to get better joint probability density estimators. We
now give somes example of each.
•
Much of the motivation for learning sparse graphical models comes from the systems biology
community. For example, suppose we measure the phosphorylation status of some proteins
connectivity of this part of the bird brain.
•
In some cases, we are not interested in interpreting the graph structure, we just want to
use it to model correlations and to make predictions. One example of this is in ﬁnancial
portfolio management, where accurate models of the covariance between large numbers of
and then using this as the basis of a trading strategy, it is possible to outperform (i.e., make
more money than) methods that do not exploit sparse graphs. Another example is predicting
for predicting traffic ﬂow in the Seattle area; predictions are made using a graphical model
whose structure was learned from data.(a)
(b)
(a) A noisy image with an occluder. (b) An estimate of the underlying pixel intensities, based
on a pairwise MRF model.
Used with kind
permission of Pedro Felzenszwalb.
Matrix completion
Sometimes we have missing data, that is, variables whose values are unknown. For example, we
might have conducted a survey, and some people might not have answered certain questions.
Or we might have various sensors, some of which fail. The corresponding design matrix will
then have “holes” in it; these missing entries are often represented by NaN, which stands for
“not a number”. The goal of imputation is to infer plausible values for the missing entries. This
is sometimes called matrix completion. Below we give some example applications.
Image inpainting
to “ﬁll in” holes (e.g., due to scratches or occlusions) in an image with realistic texture. This is
the occlusion. This can be tackled by building a joint probability model of the pixels, given a
set of clean images, and then inferring the unknown variables (pixels) given the known variables
Collaborative ﬁltering
common example of this concerns predicting which movies people will want to watch based
on how they, and other people, have rated movies which they have already seen. The key idea
is that the prediction is not based on features of the movie or user (although it could be), but
merely on a ratings matrix. More precisely, we have a matrix X where X(m, u) is the ratingXVHUV
PRYLHV


"


"
"










"
are unknown.
that most of the entries in X will be missing or unknown, since most users will not have rated
most movies. Hence we only observe a tiny subset of the X matrix, and we want to predict
a different subset. In particular, for any given user u, we might want to predict which of the
In
of the entries are observed, so the matrix is extremely sparse. A subset of these are used for
some of their methodology. Further details on the teams and their methods can be found at
Market basket analysis
In commercial data mining, there is much interest in a task called market basket analysis. The
data consists of a (typically very large but sparse) binary matrix, where each column represents
purchased on the i’th transaction. Many items are purchased together (e.g., bread and butter),
so there will be correlations amongst the bits.
Given a new partially observed bit vector,
representing a subset of items that the consumer has bought, the goal is to predict which other
bits are likely to turn on, representing other items the consumer might be likely to buy. (Unlike
collaborative ﬁltering, we often assume there is no missing data in the training data, since we
know the past shopping behavior of each customer.)
This task arises in other domains besides modeling purchasing patterns. For example, similar
techniques can be used to model dependencies between ﬁles in complex software systems. In
this case, the task is to predict, given a subset of ﬁles that have been changed, which other ones
It is common to solve such tasks using frequent itemset mining, which create association
approach, and ﬁt a joint density model p(x1, . . . , xD) to the bit vectors, see e.g., (Hu et al.(a)
(b)
generated by knnVoronoi.
may be less interpretible. This is typical of the difference between data mining and machine
learning: in data mining, there is more emphasis on interpretable models, whereas in machine
learning, there is more emphasis on accurate models.
Some basic concepts in machine learning
In this Section, we provide an introduction to some key ideas in machine learning. We will
expand on these concepts later in the book, but we introduce them brieﬂy here, to give a ﬂavor
of things to come.
In this book, we will be focussing on probabilistic models of the form p(y|x) or p(x), depending
on whether we are interested in supervised or unsupervised learning respectively. There are
many ways to deﬁne such models, but the most important distinction is this: does the model
have a ﬁxed number of parameters, or does the number of parameters grow with the amount
parametric model. Parametric models have the advantage of often being faster to use, but the
parametric models are more ﬂexible, but often computationally intractable for large datasets.
We will give examples of both kinds of models in the sections below. We focus on supervised
learning for simplicity, although much of our discussion also applies to unsupervised learning.
This simply “looks at” the K points in the training set that are nearest to the test input x,train
(a)
 
 
(b)
 
 
(c)
 
 
c1
c2
c3
(d)
counts how many members of each class are in this set, and returns that empirical fraction as
K

I(yi = c)
where NK(x, D) are the (indices of the) K nearest points to x in D and I(e) is the indicator
function deﬁned as follows:
I(e) =
if e is true
if e is falses
(a)
Fraction of data in neighborhood
Edge length of cube
(b)
Illustration of the curse of dimensionality. (a) We embed a small cube of side s inside a larger
unit cube. (b) We plot the edge length of a cube needed to cover a given volume of the unit cube as a
curseDimensionality.
distance metric to use is Euclidean distance (which limits the applicability of the technique to
plots the MAP estimate ˆy(x) = argmaxc(y = c|x, D).
This is a partition of space which associates a region V (xi) with each point xi in such a way
that all points in V (xi) are closer to xi than to any other point. Within each cell, the predicted
label is the label of the corresponding training point.
The curse of dimensionality
The KNN classiﬁer is simple and can work quite well, provided it is given a good distance metric
and has enough labeled training data. In fact, it can be shown that the KNN classiﬁer can come
However, the main problem with KNN classiﬁers is that they do not work well with high
dimensional inputs. The poor performance in high dimensional settings is due to the curse of
dimensionality.
cube. Suppose we estimate the density of class labels around a test point x by “growing” aPDF
(a)
(b)
Visualization of the conditional density model p(y|x, θ) = N(y|w0 + w1x, σ2). The density falls off
exponentially fast as we move away from the regression line. Figure generated by linregWedgeDemo2.
very local, despite the name “nearest neighbor”. The trouble with looking at neighbors that are
function at a given point.
Parametric models for classiﬁcation and regression
The main way to combat the curse of dimensionality is to make some assumptions about
the nature of the data distribution (either p(y|x) for a supervised problem or p(x) for an
unsupervised problem). These assumptions, known as inductive bias, are often embodied in
the form of a parametric model, which is a statistical model with a ﬁxed number of parameters.
Below we brieﬂy describe two widely used examples; we will revisit these and other models in
much greater depth later in the book.
Linear regression
One of the most widely used models for regression is known as linear regression. This asserts
that the response is a linear function of the inputs. This can be written as follows:
y(x) = wT x + ϵ =
D

wjxj + ϵ
where wT x represents the inner or scalar product between the input vector x and the model’s
response.(a)
(b)
linregPolyVsDegree.
To make the connection between linear regression and Gaussians more explicit, we can rewrite
the model in the following form:
p(y|x, θ) = N(y|μ(x), σ2(x))
This makes it clear that the model is a conditional probability density. In the simplest case, we
this case, θ = (w, σ2) are the parameters of the model.
follows:
μ(x) = w0 + w1x = wT x
where w0 is the intercept or bias term, w1 is the slope, and where we have deﬁned the vector
allows us to combine the intercept term with the other terms in the model.) If w1 is positive,
it means we expect the output to increase as the input increases. This is illustrated in 1d in
p(y|x, θ) = N(y|wT φ(x), σ2)
We will consider other kinds of basis functions later in the book.
In fact, many popular
machine learning methods — such as support vector machines, neural networks, classiﬁcation
and regression trees, etc. — can be seen as just different ways of estimating basis functions(a)
(b)
(a) The sigmoid or logistic function.
are the data. The open red circles are the predicted probabilities. The green crosses denote two students
the SAT feature. Figure generated by logregSATdemo.
Logistic regression
We can generalize linear regression to the (binary) classiﬁcation setting by making two changes.
First we replace the Gaussian distribution for y with a Bernoulli distribution9,which is more
p(y|x, w) = Ber(y|μ(x))
μ(x) = sigm(wT x)
where sigm(η) refers to the sigmoid function, also known as the logistic or logit function.
This is deﬁned as
sigm(η) ≜
eη
interpreted as a probability.
Putting these two steps together we get
p(y|x, w) = Ber(y|sigm(wT x))
This is called logistic regression due to its similarity to linear regression (although it is a form
of classiﬁcation, not regression!).where xi is the SAT10 score of student i and yi is whether they passed or failed a class. The
the parameters estimated from the training data (we discuss how to compute these estimates in
imagine drawing a vertical line at x = x∗; this is known as a decision boundary. Everything to
is because the data is not linearly separable, i.e., there is no straight line we can draw to
of this later in the book.
Overﬁtting
When we ﬁt highly ﬂexible models, we need to be careful that we do not overﬁt the data, that
is, we should avoid trying to model every minor variation in the input, since this is more likely
degree polynomial results in a curve that is very “wiggly”. It is unlikely that the true function
has such extreme oscillations. Thus using such a model might result in accurate predictions of
future outputs.
As another example, consider the KNN classiﬁer. The value of K can have a large effect on
we just return the labels of the original training points), but the resulting prediction surface is
data.
because we are averaging over a larger neighborhood. As K increases, the predictions becomes
smoother until, in the limit of K = N, we end up predicting the majority label of the whole
data set. Below we discuss how to pick the “right” value of K.
Model selection
When we have a variety of models of different complexity (e.g., linear or logistic regression
models with different degree polynomials, or KNN classiﬁers with different values of K), how
should we pick the right one? A natural approach is to compute the misclassiﬁcation rate on 
 
c1
c2
c3
(a)
 
 
c1
c2
c3
(b)
knnClassifyDemo.
the training set for each method. This is deﬁned as follows:
N
N

I(f(xi) ̸= yi)
(dotted blue line). We see that increasing K increases our error rate on the training set, because
However, what we care about is generalization error, which is the expected value of the
approximated by computing the misclassiﬁcation rate on a large independent test set, not used
simple models (big K), the method underﬁts. Therefore, an obvious way to pick K is to pick
Unfortunately, when training the model, we don’t have access to the test set (by assumption),
a test set by partitioning the training set into two: the part used for training the model, and a
second part, called the validation set, used for selecting the model complexity. We then ﬁt all
the models on the training set, and evaluate their performance on the validation set, and pick
the best. Once we have picked the best, we can reﬁt it to all the available data. If we have a
separate test set, we can evaluate performance on this, in order to estimate the accuracy of our
if the number of training cases is small, this technique runs into problems, because the model
selection, otherwise we will get an unrealistically optimistic estimate of performance of our method. This is one of the
“golden rules” of machine learning research.K
misclassification rate
train
test
(a)
(b)
small, the model is complex and hence we overﬁt. On the right, where K is large, the model is simple
won’t have enough data to train on, and we won’t have enough data to make a reliable estimate
of the future performance.
A simple but popular solution to this is to use cross validation (CV). The idea is simple: we
We then compute the error averaged over all the folds, and use this as a proxy for the test error.
Choosing K for a KNN classiﬁer is a special case of a more general problem known as model
validation is widely used for solving such problems, although we will discuss other approaches
later in the book.
No free lunch theorem
Much of machine learning is concerned with devising different models, and different algorithms
to ﬁt them. We can use methods such as cross validation to empirically choose the best method
for our particular problem. However, there is no universally best model — this is sometimes
that works well in one domain may work poorly in another.As a consequence of the no free lunch theorem, we need to develop many different types of
models, to cover the wide variety of data that occurs in the real world. And for each model,
there may be many different algorithms we can use to train the model, which make different
we will be studying in the subsequent chapters.
Exercises
so that you ﬁrst randomly permute the features (columns of the training and test design matrices), as in
shuffledDigitsDemo, and then apply the classiﬁer. Verify that the error rate is not changed.
form approximate nearest neighbor search, and combine it with mnist1NNdemo to classify the MNIST data
set. How much speedup do you get, and what is the drop (if any) in accuracy?
Use knnClassifyDemo to plot the CV estimate of the misclassiﬁcation rate on the test set. Compare thisProbability
Introduction
Probability theory is nothing but common sense reduced to calculation. — Pierre Laplace,
In the previous chapter, we saw how probability can play a useful role in machine learning. In
this chapter, we discuss probability theory in more detail. We do not have to space to go into
great detail — for that, you are better off consulting some of the excellent textbooks available
brieﬂy review many of the key ideas you will need in later chapters.
Before we start with the more technical material, let us pause and ask: what is probability?
does this mean? There are actually at least two different interpretations of probability. One is
called the frequentist interpretation. In this view, probabilities represent long run frequencies
of events. For example, the above statement means that, if we ﬂip the coin many times, we
The other interpretation is called the Bayesian interpretation of probability. In this view,
probability is used to quantify our uncertainty about something; hence it is fundamentally
statement means we believe the coin is equally likely to land heads or tails on the next toss.
tainty about events that do not have long term frequencies. For example, we might want to
or one times, but cannot happen repeatedly. Nevertheless, we ought to be able to quantify our
uncertainty about this event; based on how probable we think this event is, we will (hopefully!)
uncertainty). To give some more machine learning oriented examples, we might have received
a speciﬁc email message, and want to compute the probability it is spam. Or we might have
observed a “blip” on our radar screen, and want to compute the probability distribution over
the location of the corresponding target (be it a bird, plane, or missile). In all these cases, the
idea of repeated trials does not make sense, but the Bayesian interpretation is valid and indeed(a)
(b)
quite natural. We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the
basic rules of probability theory are the same, no matter which interpretation is adopted.
A brief review of probability theory
This section is a very brief review of the basics of probability theory, and is merely meant as
a refresher for readers who may be “rusty”. Readers who are already familiar with these basics
may safely skip this section.
Discrete random variables
The expression p(A) denotes the probability that the event A is true. For example, A might
be the logical expression “it will rain tomorrow”.
will happen. We write p(A) to denote the probability of the event not A; this is deﬁned to
the event A is false.
We can extend the notion of binary events by deﬁning a discrete random variable X, which
can take on any value from a ﬁnite or countably inﬁnite set X. We denote the probability of
the event that X = x by p(X = x), or just p(x) for short. Here p() is called a probability
Fundamental rules
In this section, we review the basic rules of probability.Probability of a union of two events
Given two events, A and B, we deﬁne the probability of A or B as follows:
p(A ∨B)
=
p(A) + p(B) −p(A ∧B)
=
p(A) + p(B) if A and B are mutually exclusive
Joint probabilities
We deﬁne the probability of the joint event A and B as follows:
p(A, B) = p(A ∧B) = p(A|B)p(B)
This is sometimes called the product rule. Given a joint distribution on two events p(A, B),
we deﬁne the marginal distribution as follows:
p(A) =

b
p(A, B) =

b
p(A|B = b)p(B = b)
where we are summing over all possible states of B. We can deﬁne p(B) similarly. This is
sometimes called the sum rule or the rule of total probability.
The product rule can be applied multiple times to yield the chain rule of probability:
Conditional probability
We deﬁne the conditional probability of event A, given that event B is true, as follows:
p(A|B) = p(A, B)
p(B)
Bayes rule
Combining the deﬁnition of conditional probability with the product and sum rules yields Bayes
rule, also called Bayes Theorem2:
p(X = x|Y = y) = p(X = x, Y = y)
p(Y = y)
=
p(X = x)p(Y = y|X = x)

x′ p(X = x′)p(Y = y|X = x′)
Example: medical diagnosis
As an example of how to use this rule, consider the following medical diagonsis problem.
Suppose you are a woman in your 40s, and you decide to have a medical test for breast cancer
called a mammogram. If the test is positive, what is the probability you have cancer? That
obviously depends on how reliable the test is. Suppose you are told the test has a sensitivitywords,
ignores the prior probability of having breast cancer, which fortunately is quite low:
Ignoring this prior is called the base rate fallacy. We also need to take into account the fact
that the test may be a false positive or false alarm. Unfortunately, such false positives are
quite likely (with current screening technology):
Combining these three terms using Bayes rule, we can compute the correct answer as follows:
=
=
Example: Generative classiﬁers
We can generalize the medical diagonosis example to classify feature vectors x of arbitrary type
as follows:
p(y = c|x, θ) =
p(y = c|θ)p(x|y = c, θ)

c′ p(y = c′|θ)p(x|y = c′, θ)
conditional density p(x|y = c) and the class prior p(y = c). We discuss such models in detail
this is known as a discriminative classiﬁer. We discuss the pros and cons of the two approaches
Independence and conditional independence
We say X and Y are unconditionally independent or marginally independent, denoted
X ⊥Y ⇐⇒p(X, Y ) = p(X)p(Y )



Computing p(x, y) = p(x)p(y), where X ⊥Y . Here X and Y are discrete random variables;
to deﬁne p(x, y).
In general, we say a set of variables is mutually independent if the joint can be written as a
product of marginals.
Unfortunately, unconditional independence is rare, because most variables can inﬂuence most
other variables. However, usually this inﬂuence is mediated via other variables rather than being
direct. We therefore say X and Y are conditionally independent (CI) given Z iff the conditional
joint can be written as a product of conditional marginals:
X ⊥Y |Z ⇐⇒p(X, Y |Z) = p(X|Z)p(Y |Z)
as a graph X −Z −Y , which captures the intuition that all the dependencies between X and Y
are mediated via Z. For example, the probability it will rain tomorrow (event X) is independent
of whether the ground is wet today (event Y ), given knowledge of whether it is raining today
(event Z). Intuitively, this is because Z “causes” both X and Y , so if we know Z, we do not
need to know about Y in order to predict X or vice versa. We shall expand on this concept in
Another characterization of CI is this:
p(x, y|z) = g(x, z)h(y, z)
annual mammogram screening to women in their 40s: the number of false alarms would cause needless worry and
for the optimal way to trade off risk reverse reward in the face of uncertainty.CI assumptions allow us to build large probabilistic models from small pieces. We will see
models; all of these models heavily exploit CI properties.
Continuous random variables
So far, we have only considered reasoning about uncertain discrete quantities. We will now show
quantities.
Suppose X is some uncertain continuous quantity. The probability that X lies in any interval
sum rules gives
p(B) = p(A) + p(W)
and hence
p(W) = p(B) −p(A)
example. Using this notation we have
Now deﬁne f(x) =
d
dxF(x) (we assume this derivative exists); this is called the probability
probability of a continuous variable being in a ﬁnite interval as follows:
 b
a
f(x)dx
As the size of the interval gets smaller, we can write
Unif(x|a, b) =CDF
(a)
(b)
quantileDemo.
Quantiles
Since the cdf F is a monotonically increasing function, it has an inverse; let us denote this by
are the lower and upper quartiles.
We can also use the inverse cdf to compute tail area probabilities. For example, if Φ is
Mean and variance
discrete rv’s, it is deﬁned as E [X] ≜
E [X] ≜

X x p(x)dx. If this integral is not ﬁnite, the mean is not deﬁned (we will see some
examples of this later).
The variance is a measure of the “spread” of a distribution, denoted by σ2. This is deﬁnedas follows:
var [X]
≜
E

=

(x −μ)2p(x)dx
=

x2p(x)dx + μ2


xp(x)dx = E

X2
−μ2
from which we derive the useful result
E

X2
= μ2 + σ2
The standard deviation is deﬁned as
std [X] ≜

var [X]
This is useful since it has the same units as X itself.
Some common discrete distributions
In this section, we review some commonly used parametric distributions deﬁned on discrete
state spaces, both ﬁnite and countably inﬁnite.
The binomial and Bernoulli distributions
is given by
Bin(k|n, θ) ≜
	
n
k

where
	n
k

≜
n!
(n −k)!k!
is the number of ways to choose k items from n (this is known as the binomial coefficient,
This distribution has the following mean and variance:
mean = θ,
written as X ∼Ber(θ), where the pmf is deﬁned as
In other words,
Ber(x|θ) =


θ(a)
(b)
by binomDistPlot.
The multinomial and multinoulli distributions
The binomial distribution can be used to model the outcomes of coin tosses. To model the
follows: let x = (x1, . . . , xK) be a random vector, where xj is the number of times side j of
the die occurs. Then x has the following pmf:
Mu(x|n, θ) ≜
	
n
x1 . . . xK

 K


θxj
j
	
n
x1 . . . xK

≜
n!
x1!x2! · · · xK!
is the multinomial coefficient (the number of ways to divide a set of size n = K
subsets with sizes x1 up to xK).
and 1s (a bit vector), in which only one bit can be turned on. Speciﬁcally, if the dice shows
up as face k, then the k’th bit will be on. In this case, we can think of x as being a scalar
categorical random variable with K states (values), and x is its dummy encoding, that is,
only one of the K “wires” is “hot” or on. In this case, the pmf becomes
K


j
or discrete distribution. (Gustavo Lacerda suggested we call it the multinoulli distribution, byName
n
K
x
Multinomial
-
-
Multinoulli
-
Binomial
-
Bernoulli
Summary of the multinomial and related distributions.
a t a g c c g g t a c g g c a 
t t a g c t g c a a c c g c a 
t c a g c c a c t a g a g c a 
a t a a c c g c g a c c g c a 
t t a g c c g c t a a g g t a 
t a a g c c t c g t a c g t a 
t t a g c c g t t a c g g c c 
a t a t c c g g t a c a g t a 
a t a g c a g g t a c c g a a 
a c a t c c g t g a c g g a a
(a)
Sequence Position
Bits
(b)
(a) Some aligned DNA sequences. (b) The corresponding sequence logo. Figure generated by
seqlogoDemo.
will use the following notation for this case:
Application: DNA sequence motifs
An interesting application of multinomial models arises in biosequence analysis.
Suppose
served by evolution (e.g., because they are part of a gene coding region), since the corresponding
plot the letters A, C, G and T with a fontsize proportional to their empirical probability, and with
the most probable letter on the top. The empirical probability distribution at location t, ˆθt, is
Nt
=

 N

N

N

N


ˆθt
=
This distribution is known as a motif. We can also compute the most probable letter in each
location; this is called the consensus sequence.(a)
(b)
poissonPlotDemo.
The Poisson distribution
X ∼Poi(λ), if its pmf is
Poi(x|λ) = e−λ λx
x!
The Poisson distribution is often used as a model for counts of rare events like radioactive
The empirical distribution
Given a set of data, D = {x1, . . . , xN}, we deﬁne the empirical distribution, also called the
empirical measure, as follows:
N
N

δxi(A)
δx(A) =

if x ̸∈A
In general, we can associate “weights” with each sample:
p(x) =
N

“spikes” at the data points xi, where wi determines the height of spike i. This distributionSome common continuous distributions
ability distributions.
Gaussian (normal) distribution
The most widely used distribution in statistics and machine learning is the Gaussian or normal
distribution. Its pdf is given by
N(x|μ, σ2)
≜
√
2πσ2 e−
√
2πσ2 is the
sometimes called the bell curve.
We will often talk about the precision of a Gaussian, by which we mean the inverse variance:
density at its center, x = μ. We have N(μ|μ, σ2) = (σ
√
√
2π, we have
The cumulative distribution function or cdf of the Gaussian is deﬁned as
Φ(x; μ, σ2) ≜
 x
−∞
N(z|μ, σ2)dz
expression, but is built in to most software packages. In particular, we can compute it in terms
of the error function (erf):
√
where z = (x −μ)/σ and
erf(x) ≜
√π
 x
e−t2dt
The Gaussian distribution is the most widely used distribution in statistics. There are several
reasons for this. First, it has two parameters which are easy to interpret, and which capture
some of the most basic properties of a distribution, namely its mean and variance. Second,
have an approximately Gaussian distribution, making it a good choice for modeling residual
errors or “noise”. Third, the Gaussian distribution makes the least number of assumptions (has
The intended meaning should be clear from context.maximum entropy), subject to the constraint of having a speciﬁed mean and variance, as we
mathematical form, which results in easy to implement, but often highly effective, methods, as
widely used.
Degenerate pdf
lim
δ(x) =


∞
such that
 ∞
−∞
A useful property of delta functions is the sifting property, which selects out a single term
from a sum or integral:
 ∞
−∞
f(x)δ(x −μ)dx = f(μ)
probability only decays quadratically with distance from the center. A more robust distribution
is the Student t distribution5 Its pdf is as follows:
T (x|μ, σ2, ν)
∝

ν
	x −μ
σ

)
the following properties:
mean = μ, mode = μ, var =
νσ2
Guinness brewery in Dublin. Since his employer would not allow him to use his own name, he called it the “Student”
distribution. The origin of the term t seems to have arisen in the context of Tables of the Student distribution, used by
historical details. 
 
Gauss
Student
Laplace
(a)
 
 
Gauss
Student
Laplace
(b)
√
Figure generated by studentLaplacePdfPlot.
 
 
gaussian
student T
laplace
(a)
 
 
gaussian
student T
laplace
(b)
Illustration of the effect of outliers on ﬁtting Gaussian, Student and Laplace distributions. (a)
No outliers (the Gaussian and Student curves are on top of each other). (b) With outliers. We see that the
(Bishop 2006a). Figure generated by robustDemo.
left, we show a Gaussian and a Student ﬁt to some data with no outliers. On the right, we
add some outliers. We see that the Gaussian is affected a lot, whereas the Student distribution
for having such heavy tails that the integral that deﬁnes the mean does not converge.
rapidly approaches a Gaussian distribution and loses its robustness properties.Gamma distributions
 
 
(a)
(b)
we increase the rate b, we reduce the horizontal scale, thus squeezing everything leftwards and upwards.
Figure generated by gammaPlotDemo. (b) An empirical pdf of some rainfall data, with a ﬁtted Gamma
distribution superimposed. Figure generated by gammaRainfallDemo.
The Laplace distribution
Another distribution with heavy tails is the Laplace distribution6, also known as the double
sided exponential distribution. This has the following pdf:
Lap(x|μ, b)
≜
2b exp
	
−|x −μ|
b

distribution has the following properties:
mean = μ, mode = μ, var = 2b2
than the Gaussian. This property is a useful way to encourage sparsity in a model, as we will
The gamma distribution
Ga(T|shape = a, rate = b)
≜
ba
Γ(a)T a−1e−T b
statistics.
unless otherwise speciﬁed.where Γ(a) is the gamma function:
Γ(x) ≜
 ∞
ux−1e−udu
properties:
mean = a
b
, var = a
b2
There are several distributions which are just special cases of the Gamma, which we discuss
below.
•
parameter. This distribution describes the times between events in a Poisson process, i.e. a
•
Erlang distribution This is the same as the Gamma distribution where a is an integer. It
•
S = ν
i , then S ∼χ2
ν.
that
X ∼IG(a, b), where IG is the inverse gamma distribution deﬁned by
IG(x|shape = a, scale = b)
≜
ba
The distribution has these properties
mean =
b
b
b2
We will see applications of these distributions later on.
The beta distribution
Beta(x|a, b) =
Here B(p, q) is the beta function,
B(a, b) ≜Γ(a)Γ(b)
Γ(a + b)beta distributions
 
 
Some beta distributions. Figure generated by betaPlotDemo.
mean =
a
a + b, mode =
ab
Pareto distribution
The Pareto distribution is used to model the distribution of quantities that exhibit long tails,
also called heavy tails. For example, it has been observed that the most frequent word in
English (“the”) occurs approximately twice as often as the second most frequent word (“of”),
which occurs twice as often as the fourth most frequent word, etc. If we plot the frequency of
words vs their rank, we will get a power law; this is known as Zipf’s law. Wealth has a similarly
The Pareto pdf is deﬁned as follow:
This density asserts that x must be greater than some constant m, but not too much greater,
illustration (this is known as a power law). This distribution has the following properties
mean =
km
m2k
the
USA,
Americans
have
more
wealth
than
half
of
all
Americans
combined.
(Source:
extreme distribution of income has arisen in a democratic country.Pareto distribution
 
 
(a)
 
 
(b)
generated by paretoPlot.
Joint probability distributions
So far, we have been mostly focusing on modeling univariate probability distributions. In this
section, we start our discussion of the more challenging problem of building joint probability
distributions on multiple related random variables; this will be a central topic in this book.
and models the (stochastic) relationships between the variables. If all the variables are discrete,
dimension. However, the number of parameters needed to deﬁne such a model is O(KD),
where K is the number of states for each variable.
distributions, an alternative approach is to restrict the form of the pdf to certain functional
forms, some of which we will examine below.
Covariance and correlation
The covariance between two rv’s X and Y measures the degree to which X and Y are (linearly)
related. Covariance is deﬁned as
cov [X, Y ]
≜
E [(X −E [X])(Y −E [Y ])] = E [XY ] −E [X] E [Y ]Several sets of (x, y) points, with the correlation coefficient of x and y for each set. Note
that the correlation reﬂects the noisiness and direction of a linear relationship (top row), but not the slope
of that relationship (middle), nor many aspects of nonlinear relationships (bottom). N.B.: the ﬁgure in the
is zero.
symmetric, positive deﬁnite matrix:
cov [x]
≜
E

(x −E [x])(x −E [x])T 
=
⎛
⎜⎜⎜⎝
var [X1]
cov [X1, X2]
· · ·
cov [X1, Xd]
cov [X2, X1]
var [X2]
· · ·
cov [X2, Xd]
...
...
...
...
cov [Xd, X1]
cov [Xd, X2]
· · ·
var [Xd]
⎞
⎟⎟⎟⎠
normalized measure, with a ﬁnite upper bound. The (Pearson) correlation coefficient between
X and Y is deﬁned as
corr [X, Y ] ≜
cov [X, Y ]

var [X] var [Y ]
A correlation matrix has the form
R =
⎛
⎜⎝
corr [X1, X1]
corr [X1, X2]
· · ·
corr [X1, Xd]
...
...
...
...
corr [Xd, X1]
corr [Xd, X2]
· · ·
corr [Xd, Xd]
⎞
⎟⎠might expect the correlation coefficient to be related to the slope of the regression line, i.e., the
regression coefficient is in fact given by a = cov [X, Y ] /var [X]. A better way to think of the
Y = X2. Clearly Y is dependent on X (in fact, Y is uniquely determined by X), yet one
are independent.
The multivariate Gaussian
we just give some deﬁnitions and plots.
The pdf of the MVN in D dimensions is deﬁned by the following:
N(x|μ, Σ)
≜


matrix. Sometimes we will work in terms of the precision matrix or concentration matrix
or isotropic covariance, Σ = σ2ID, has one free parameter.
Multivariate Student t distribution
A more robust alternative to the MVN is the multivariate Student t distribution, whose pdf is
given by
T (x|μ, Σ, ν)
=

−( ν+D
)
=

−( ν+D
where Σ is called the scale matrix (since it is not exactly the covariance matrix) and V = νΣ.
This has fatter tails than a Gaussian. The smaller ν is, the fatter the tails. As ν →∞, thefull
(a)
diagonal
(b)
spherical
(c)
spherical
(d)
We show the level sets for 2d Gaussians. (a) A full covariance matrix has elliptical contours.
(b) A diagonal covariance matrix is an axis aligned ellipse. (c) A spherical covariance matrix has a circular
shape. (d) Surface plot for the spherical Gaussian in (c). Figure generated by gaussPlot2Ddemo.
distribution tends towards a Gaussian. The distribution has the following properties
mean = μ, mode = μ, Cov =
ν
ν −2Σ
Dirichlet distribution
A multivariate generalization of the beta distribution is the Dirichlet distribution9, which has
support over the probability simplex, deﬁned by
K

The pdf is deﬁned as follows:
Dir(x|α)
≜
B(α)
K


k(a)
(b)
(c)
p
(d)
Figure generated by
a plotting artifact.) Figure generated by dirichlet3dPlot.
(a)
(b)
results in more uniform (and dense) distributions. Figure generated by dirichletHistogramDemo.where B(α1, . . . , αK) is the natural generalization of the beta function to K variables:
B(α) ≜
K
Γ(α0)
where α0 ≜K
probability vectors. We see that α0 = K
corner of the simplex.
For future reference, the distribution has these properties
E [xk] = αk
α0
α0 −K , var [xk] = αk(α0 −αk)
α2
where α0 = 
k αk. Often we use a symmetric Dirichlet prior of the form αk = α/K. In this
increases the precision (decreases the variance) of the distribution.
Transformations of random variables
If x ∼p() is some random variable, and y = f(x), what is the distribution of y? This is the
question we address in this section.
Linear transformations
Suppose f() is a linear function:
y = f(x) = Ax + b
In this case, we can easily derive the mean and covariance of y as follows. First, for the mean,
we have
f(x) = aT x + b, the corresponding result is
E

aT x + b

For the covariance, we have
cov [y] = cov [Ax + b] = AΣAT
where Σ = cov [x]. We leave the proof of this as an exercise. If f() is scalar valued, the result
becomes
var [y] = var

aT x + b

= aT ΣaWe will use both of these results extensively in later chapters. Note, however, that the mean
and covariance only completely deﬁne the distribution of y if x is Gaussian. In general we must
use the techniques described below to derive the full distribution of y, as opposed to just its
ﬁrst two moments.
General transformations
If X is a discrete rv, we can derive the pmf for y by simply summing up the probability mass
for all the x’s such that f(x) = y:
py(y) =

x:f(x)=y
px(x)
cannot sum up densities. Instead, we work with cdf’s, and write
We can derive the pdf of y by differentiating the cdf.
In the case of monotonic and hence invertible functions, we can write
Taking derivatives we get
py(y) ≜d
dy Py(y) = d
dy
d
dxPx(x) = dx
dy px(x)
measures volume in y space. Thus dx
dy measures the change in volume. Since the sign of this
change is not important, we take the absolute value to get the general expression:
py(y) = px(x)


dx
dy



This is called change of variables formula. We can understand this result more intuitively as
Multivariate change of variables *
We can extend the previous results to multivariate distributions as follows. Let f be a function
that maps Rn to Rn, and let y = f(x). Then its Jacobian matrix J is given by
∂(x1, . . . , xn) ≜
⎛
⎜⎝
∂y1
∂x1
· · ·
∂y1
∂xn
...
...
...
∂yn
∂x1
· · ·
∂yn
∂xn
⎞
⎟⎠| det J| measures how much a unit cube changes in volume when we apply f.
If f is an invertible mapping, we can deﬁne the pdf of the transformed variables using the
py(y) = px(x)


 det
	∂x
∂y

 

Gaussian.
As a simple example, consider transforming a density from Cartesian coordinates x = (x1, x2)
=
	 ∂x1
∂r
∂x1
∂θ
∂x2
∂r
∂x2
∂θ

=
	

and
Hence
py(y)
=
px(x)| det J|
pr,θ(r, θ)
=
=
In the limit, this is equal to the density at the center of the patch, p(r, θ), times the size of the
=
Central limit theorem
Now consider N random variables with pdf’s (not necessarily Gaussian) p(xi), each with mean
μ and variance σ2.
We assume each variable is independent and identically distributed
or iid for short. Let SN = N
used transformation of rv’s. One can show that, as N increases, the distribution of this sum
approaches
p(SN = s) =
√
2πNσ2 exp
	
2Nσ2

Hence the distribution of the quantity
σ
√
N
= X −μ
σ/
√
N
converges to the standard normal, where X =
N
N
distribution. We see that the sampling distribution of the mean value rapidly converges to a
Gaussian distribution.(a)
(b)
The central limit theorem in pictures. We plot a histogram of
N
N
Monte Carlo approximation
In general, computing the distribution of a function of an rv using the change of variables
formula can be difficult. One simple but powerful alternative is as follows. First we generate
S samples from the distribution, call them x1, . . . , xS. (There are many ways to generate such
samples; one popular method, for high dimensional distributions, is called Markov chain Monte
the distribution of f(X) by using the empirical distribution of {f(xs)}S
Monte Carlo approximation, named after a city in Europe known for its plush gambling casinos.
Monte Carlo techniques were ﬁrst developed in the area of statistical physics — in particular,
during development of the atomic bomb — but are now widely used in statistics and machine
learning as well.
We can use Monte Carlo to approximate the expected value of any function of a randomComputing the distribution of y = x2, where p(x) is uniform (left). The analytic result is
shown in the middle, and the Monte Carlo approximation is shown on the right. Figure generated by
changeOfVarsDemo1d.
variable.
We simply draw samples, and then compute the arithmetic mean of the function
applied to the samples. This can be written as follows:
E [f(X)] =

S
S

f(xs)
where xs ∼p(X). This is called Monte Carlo integration, and has the advantage over numerical
integration (which is based on evaluating the function at a ﬁxed grid of points) that the function
By varying the function f(), we can approximate many quantities of interest, such as
•
S
S
•
S
S
•
•
median{x1, . . . , xS} →median(X)
We give some examples below, and will see many more in later chapters.
Example: change of variables, the MC way
random variable, y = f(x). A much simpler approach is to use a Monte Carlo approximation.
many samples from p(x), squaring them, and computing the resulting empirical distribution.Estimating π by Monte Carlo integration. Blue points are inside the circle, red crosses are
outside. Figure generated by mcEstimatePi.
Example: estimating π by Monte Carlo integration
MC approximation can be used for many applications, not just statistical ones. Suppose we want
to estimate π. We know that the area of a circle with radius r is πr2, but it is also equal to the
following deﬁnite integral:
I =
 r
−r
 r
−r
Let us approximate this by Monte Carlo integration.
Let f(x, y) =
I
=
(2r)(2r)
 
f(x, y)p(x)p(y)dxdy
=
4r2
 
f(x, y)p(x)p(y)dxdy
≈
S
S

f(xs, ys)
Accuracy of Monte Carlo approximation
the bottom line, we plot a smoothed version of these samples, created using a kernel density(a)
(b)
(c)
(d)
Solid red
line is true pdf.
Top line: histogram of samples.
Bottom line: kernel density estimate derived from
mcAccuracyDemo.
and plotted. Note that this smoothing is just for the purposes of plotting, it is not used for the
Monte Carlo estimate itself.
show that, with independent samples,
S )
where
σ2 = var [f(X)] = E

expression, but it can also be estimated by MC:
ˆσ2
=
S
S

Then we have
P


√
S
√
S

The term


ˆσ2
S is called the (numerical or empirical) standard error, and is an estimate of our

ϵ2 .
Information theory
information theory is concerned with representing data in a compact fashion (a task known as
data compression or source coding), as well as with transmitting and storing it in a way that
is robust to errors (a task known as error correction or channel coding). At ﬁrst, this seems
far removed from the concerns of probability theory and machine learning, but in fact there is
an intimate connection. To see this, note that compactly representing data requires allocating
short codewords to highly probable bit strings, and reserving longer codewords to less probable
bit strings. This is similar to the situation in natural language, where common words (such as
“a”, “the”, “and”) are generally much shorter than rare words. Also, decoding messages sent over
noisy channels requires having a good probability model of the kinds of messages that people
tend to send. In both cases, we need a model that can predict which kinds of data are likely
more details on the connection between information theory and machine learning).
Obviously we cannot go into the details of information theory here (see e.g., (Cover and
concepts that we will need later in the book.
Entropy
The entropy of a random variable X with distribution p, denoted by H (X) or sometimes
H (p), is a measure of its uncertainty. In particular, for a discrete variable with K states, it is
deﬁned by
H (X) ≜−
K

p(X = k) log2 p(X = k)
distribution.H(X)
The maximum entropy is
H (X)
=
=
KL divergence
One way to measure the dissimilarity of two probability distributions, p and q, is known as the
KL (p||q) ≜
K

pk log pk
qk
KL (p||q) =

k
pk log pk −

k
pk log qk = −H (p) + H (p, q)
where H (p, q) is called the cross entropy,
H (p, q) ≜−

k
pk log qk
needed to encode data coming from a source with distribution p when we use model q tothe expected number of bits if we use the true model, so the KL divergence is the difference
between these. In other words, the KL divergence is the average number of extra bits needed to
encode the data, due to the fact that we used distribution q to encode the data instead of the
true distribution p.
the KL is only equal to zero iff q = p. We now give a proof of this important result.
Proof. To prove the theorem, we need to use Jensen’s inequality. This states that, for any
convex function f, we have that
f

 n

λixi

≤
n

λif(xi)
−KL (p||q)
=
−

p(x) log p(x)
q(x) =

p(x) log q(x)
p(x)
≤
log

p(x)q(x)
p(x) = log

q(x)
≤
log

where the ﬁrst inequality follows from Jensen’s. Since log(x) is a strictly concave function, we
iff 
for all x.
One important consequence of this result is that the discrete distribution with the maximum
entropy is the uniform distribution. More precisely, H (X) ≤log |X|, where |X| is the number
≤
KL (p||u) =

x
p(x) log p(x)
u(x)
=

x
p(x) log p(x) −

x
p(x) log u(x) = −H (X) + log |X|
This is a formulation of Laplace’s principle of insufficient reason, which argues in favor of
using uniform distributions when there are no other reasons to favor one distribution over
ﬁrst and second moment constraints, but otherwise has maximum entropy.)Mutual information
Consider two random variables, X and Y . Suppose we want to know how much knowing one
variable tells us about the other. We could compute the correlation coefficient, but this is only
joint distribution p(X, Y ) is to the factored distribution p(X)p(Y ). This is called the mutual
information or MI, and is deﬁned as follows:
I (X; Y ) ≜KL (p(X, Y )||p(X)p(Y )) =

x

y
p(x, y) log p(x, y)
p(x)p(y)
variables are independent.
I (X; Y ) = H (X) −H (X|Y ) = H (Y ) −H (Y |X)
where H (Y |X) is the conditional entropy, deﬁned as H (Y |X) = 
x p(x)H (Y |X = x).
Thus we can interpret the MI between X and Y as the reduction in uncertainty about X after
observing Y , or, by symmetry, the reduction in uncertainty about Y after observing X. We will
connection between MI and correlation coefficients.
A quantity which is closely related to MI is the pointwise mutual information or PMI. For
two events (not random variables) x and y, this is deﬁned as
PMI(x, y) ≜log p(x, y)
p(x)p(y) = log p(x|y)
p(x)
= log p(y|x)
p(y)
This measures the discrepancy between these events occuring together compared to what would
be expected by chance. Clearly the MI of X and Y is just the expected value of the PMI.
Interestingly, we can rewrite the PMI as follows:
PMI(x, y) = log p(x|y)
p(x)
= log p(y|x)
p(y)
lently, updating the prior p(y) into the posterior p(y|x).
Mutual information for continuous random variables *
The above formula for MI is deﬁned for discrete random variables. For continuous random
variables, it is common to ﬁrst discretize or quantize them, by dividing the ranges of each
can then easily compute the MI using the formula above (see mutualInfoAllPairsMixed for
some code, and miMixedDemo for a demo).
Unfortunately, the number of bins used, and the location of the bin boundaries, can have
a signiﬁcant effect on the results. One way around this is to try to estimate the MI directly,







"







	








	



 !
"#$%



%#&

'()*
+




,
	,
	,
'
%-#.,/)"


%-
  /
%


















		




"




#

/	
/


/

$
%
&'
(
)
&*


$ 
)"


*&
 %-
<!



	


	

#

"
= %


.,
$%
%
	







>%?-
$)"


'
%-#.,/)"



	



	

Used with kind permission of David Reshef and the American Association for the Advancement of Science.
many different bin sizes and locations, and to compute the maximum MI achieved.
This
statistic, appropriately normalized, is known as the maximal information coefficient (MIC)
log min(x, y)
where G(x, y) is the set of 2d grids of size x×y, and X(G), Y (G) represents a discretization of
the variables onto this grid. (The maximization over bin locations can be performed efficiently
MIC ≜
max
measuring a variety of social, economic, health and political indicators, collected by the World
Health Organization (WHO). On the left of the ﬁgure, we see the correlation coefficient (CC)
plots for particular pairs of variables, which we now discuss:
•
The point marked C has a low CC and a low MIC. The corresponding scatter plot makes itclear that there is no relationship between these two variables (percentage of lives lost to
injury and density of dentists in the population).
•
The points marked D and H have high CC (in absolute value) and high MIC, because they
represent nearly linear relationships.
•
The points marked E, F, and G have low CC but high MIC. This is because they correspond
relationships between the variables.
In summary, we see that statistics (such as MIC) based on mutual information can be used
to discover interesting relationships between variables in a way that simpler measures, such as
correlation coefficients, cannot. For this reason, the MIC has been called “a correlation for the
Exercises
(Source: Minka.) My neighbor has two children. Assuming that the gender of a child is like a coin ﬂip,
a. Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is
a girl?
b. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability
that the other child is a girl?
(Source: Peter Lee.) Suppose a crime has been committed. Blood is found at the scene for which there is
What is wrong with this argument?
the defendant is guilty, and thus has no relevance.” This is known as the defender’s fallacy. What is
wrong with this argument?
Show that the variance of a sum is var [X + Y ] = var [X] + var [Y ] + 2cov [X, Y ] , where cov [X, Y ]
is the covariance between X and Y
(Source: Koller.) After your yearly checkup, the doctor has bad news and good news. The bad news is that
the chances that you actually have the disease? (Show your calculations as well as giving the ﬁnal result.)(Source: Mackay.) On a game show, a contestant is told the rules as follows:
get to select one door. Initially your chosen door will not be opened. Instead, the gameshow host
will open one of the other two doors, and he will do so in such a way as not to reveal the prize. For
that he will choose which one to open so that the prize will not be revealed.
At this point, you will be given a fresh choice of door: you can either stick with your ﬁrst choice,
or you can switch to the other closed door. All the doors will then be opened and you will receive
whatever is behind your ﬁnal choice of door.
does it make no difference? You may assume that initially, the prize is equally likely to be behind any of
(Source: Koller.)
other random variables E1 and E2. Suppose we wish to calculate the vector
Which of the following sets of numbers are sufficient for the calculation?
i. P(e1, e2), P(H), P(e1|H), P(e2|H)
ii. P(e1, e2), P(H), P(e1, e2|H)
iii. P(e1|H), P(e2|H), P(H)
b. Now suppose we now assume E1 ⊥E2|H (i.e., E1 and E2 are conditionally independent given H).
Show your calculations as well as giving the ﬁnal result. Hint: use Bayes rule.
We say that two random variables are pairwise independent if
p(X2|X1) = p(X2)
and hence
p(X2, X1) = p(X1)p(X2|X1) = p(X1)p(X2)
We say that n random variables are mutually independent if
p(Xi|XS) = p(Xi)
and hence
p(X1:n) =
n

p(Xi)
pendence. It suffices to give a counter example.In the text we said X ⊥Y |Z iff
p(x, y|z) = p(x|z)p(y|z)
function g and h such that
p(x, y|z) = g(x, z)h(y, z)
(Source: Koller.) Are the following properties true? Prove or disprove. Note that we are not restricting
attention to distributions that can be represented by a graphical model.
a. True or false? (X ⊥W|Z, Y ) ∧(X ⊥Y |Z) ⇒(X ⊥Y, W|Z)
b. True or false? (X ⊥Y |Z) ∧(X ⊥Y |W) ⇒(X ⊥Y |Z, W)
Let X ∼Ga(a, b), i.e.
Ga(x|a, b)
=
ba
Γ(a)xa−1e−xb
IG(x|shape = a, scale = b)
=
ba
Hint: use the change of variables formula.
Z =
 b
a
exp

−x2
2σ2

dx
where a = −∞and b = ∞. To compute this, consider its square
Z2 =
 b
a
 b
a
exp

−x2 + y2
2σ2

dxdy
Z2 =
 2π
 ∞
r exp

−r2
2σ2

Evaluate this integral and hence show Z = σ


u′(r)dr = u(r)).Show that
I(X, Y ) = H(X) −H(X|Y ) = H(Y ) −H(Y |X)
normal distribution:
X1
X2

∼N

 σ2
ρσ2
ρσ2
σ2

	
(2πe)d det Σ

In the 1d case, this becomes


2πeσ2

distributed (so H(X) = H(Y )) but not necessarily independent. Deﬁne
H(X)
a. Show r = I(X,Y )
H(X)
Let pemp(x) be the empirical distribution, and let q(x|θ) be some model. Show that argminq KL (pemp||q)
What is the expected location of the left most point?Generative models for discrete data
Introduction
generative classiﬁer of the form
p(y = c|x, θ) ∝p(x|y = c, θ)p(y = c|θ)
p(x|y = c, θ), which deﬁnes what kind of data we expect to see in each class. In this chapter,
we focus on the case where the observed data are discrete symbols. We also discuss how to
Bayesian concept learning
Consider how a child learns to understand the meaning of a word, such as “dog”. Presumably
the child’s parents point out positive examples of this concept, saying such things as, “look at
the cute dog!”, or “mind the doggy”, etc. However, it is very unlikely that they provide negative
an active learning process — the child says “look at the dog” and the parent says “that’s a cat,
dear, not a dog” — but psychological research has shown that people can learn concepts from
We can think of learning the meaning of a word as equivalent to concept learning, which in
just deﬁnes which elements are in the set C. By allowing for uncertainty about the deﬁnition
of f, or equivalently the elements of C, we can emulate fuzzy set theory, but using standard
probability calculus. Note that standard binary classiﬁcation techniques require positive and
negative examples. By contrast, we will devise a way to learn from positive examples alone.
For pedagogical purposes, we will consider a very simple example of concept learning called
proceeds as follows. I choose some simple arithmetical concept C, such as “prime number” or
D = {x1, . . . , xN} drawn from C, and ask you whether some new test case ˜x belongs to C,
i.e., I ask you to classify ˜x.Examples
This illustrates diffuse similarity.
Third row: after
Used with kind permission of Josh Tenenbaum.
way?
some numbers are more likely than others. We can represent this as a probability distribution,
of people derived from a lab experiment. We see that people predict numbers that are similar
the hidden concept is “powers of two”. This is an example of induction. Given this hypothesis,
How can we explain this behavior and emulate it in a machine? The classic approach to
induction is to suppose we have a hypothesis space of concepts, H, such as: odd numbers,As we see more examples, the version space shrinks and we become increasingly certain about
Also, after seeing D =
now provide a Bayesian explanation for this.
Likelihood
We must explain why we chose htwo ≜“powers of two”, and not, say, heven ≜“even numbers”
The key intuition is that we want to avoid suspicious coincidences. If the true concept was
even numbers, how come we only saw numbers that happened to be powers of two?
To formalize this, let us assume that examples are sampled uniformly at random from the
extension of a concept. (The extension of a concept is just the set of numbers that belong
assumption, the probability of independently sampling N items (with replacement) from h is
given by
p(D|h) =

size(h)
N
=
|h|
N
This crucial equation embodies what Tenenbaum calls the size principle, which means the
model favors the simplest (smallest) hypothesis consistent with the data. This is more commonly
So the
Prior
is missing from the set of examples.
can capture such intution by assigning low prior probability to unnatural concepts. Of course,
your prior might be different than mine. This subjective aspect of Bayesian reasoning is a
source of much controversy, since it means, for example, that a child and a math professorwill reach different answers. In fact, they presumably not only have different priors, but also
different hypothesis spaces. However, we can ﬁnesse that by deﬁning the hypothesis space of
the child and the math professor to be the same, and then setting the child’s prior weight to be
zero on certain “advanced” concepts. Thus there is no sharp distinction between the prior and
the hypothesis space.
Although the subjectivity of the prior is controversial, it is actually quite useful. If you are
see that the prior is the mechanism by which background knowledge can be brought to bear on
a problem. Without this, rapid learning (i.e., from small samples sizes) is impossible.
So, what prior should we use? For illustration purposes, let us use a simple prior which
make the concepts even and odd more likely apriori. We also include two “unnatural” concepts,
Posterior
The posterior is simply the likelihood times the prior, normalized. In this context we have
p(h|D)
=
p(D|h)p(h)

h′∈H p(D, h′) =

posterior is a combination of prior and likelihood. In the case of most of the concepts, the prior
is uniform, so the posterior is proportional to the likelihood. However, the “unnatural” concepts
high likelihood, due to the low prior. Conversely, the concept of odd numbers has low posterior
support, despite having a high prior, due to the low likelihood.
likelihood is much more peaked on the powers of two concept, so this dominates the posterior.
Essentially the learner has an aha moment, and ﬁgures out the true concept. (Here we see the
need for the low prior on the unnatural concepts, otherwise we would have overﬁt the data and
In general, when we have enough data, the posterior p(h|D) becomes peaked on a single
concept, namely the MAP estimate, i.e.,
p(h|D) →δˆhMAP (h)
deﬁned by
δx(A) =

if x ̸∈Aall
squares
odd
even
prior
lik
post
by numbersGame.
Note that the MAP estimate can be written as
ˆhMAP = argmax
h
p(D|h)p(h) = argmax
h
[log p(D|h) + log p(h)]
Since the likelihood term depends exponentially on N, and the prior stays constant, as we get
more and more data, the MAP estimate converges towards the maximum likelihood estimate
or MLE:
ˆhmle ≜argmax
h
p(D|h) = argmax
h
log p(D|h)
In other words, if we have enough data, we see that the data overwhelms the prior. In thisall
squares
odd
even
prior
lik
post
generated by numbersGame.
case, the MAP estimate converges towards the MLE.
upon this hypothesis. Thus we say that Bayesian inference (and ML estimation) are consistent
the limit, meaning we can recover the truth in the limit of inﬁnite data. If our hypothesis class
is not rich enough to represent the “truth” (which will usually be the case), we will converge
on the hypothesis that is as close as possible to the truth. However, formalizing this notion of
“closeness” is beyond the scope of this chapter.all
even
squares
Posterior over hypotheses and the corresponding predictive distribution after seeing one
Posterior predictive distribution
The posterior is our internal belief state about the world. The way to test if our beliefs are
justiﬁed is to use them to predict objectively observable quantities (this is the basis of the
scientiﬁc method). Speciﬁcally, the posterior predictive distribution in this context is given by

h
This is just a weighted average of the predictions of each individual hypothesis and is called
bottom show the predictions from each hypothesis; the vertical curve on the right shows the
weight associated with each hypothesis. If we multiply each row by its weight and add up, we
get the distribution at the top.
induces a broad predictive distribution. However, once we have “ﬁgured things out”, the posterior
becomes a delta function centered at the MAP estimate. In this case, the predictive distributionbecomes

h
p(˜x|h)δˆh(h) = p(˜x|ˆh)
will not be as “smooth” as when using BMA. We will see more examples of this later in the book.
For
If we use the simple prior above, the minimal
the same) as we see more data: it starts narrow, but is forced to broaden as it seems more data.
In contrast, in the Bayesian approach, we start broad and then narrow down as we learn more,
approach are quite different in the small sample regime, although they converge to the same
answer as we see more data.
A more complex prior
rived by analysing some experimental data of how people measure similarity between numbers;
these hypotheses are not mutually exclusive.) Thus the prior is a mixture of two priors, one
over arithmetical rules, and one over intervals:
The only free parameter in the model is the relative weight, π0, given to these two parts of the
that people are more likely to think of concepts deﬁned by rules. The predictive distribution
data (modulo the choice of hypothesis space).
The number game involved inferring a distribution over a discrete variable drawn from a ﬁnite
particularly simple: we just needed to sum, multiply and divide. However, in many applications,
the unknown parameters are continuous, so the hypothesis space is (some subset) of RK, whereExamples
The predictions of the Bayesian model are only plotted for those values of ˜x for which human data is
Used with kind permission of Josh Tenenbaum.
K is the number of parameters. This complicates the mathematics, since we have to replace
sums with integrals. However, the basic ideas are the same.
We will illustrate this by considering the problem of inferring the probability that a coin shows
up heads, given a series of observed coin tosses. Although this might seem trivial, it turns out
that this model forms the basis of many of the methods we will consider later in this book,
including naive Bayes classiﬁers, Markov models, etc. It is historically important, since it was the
for further historical details.)
posterior and posterior predictive.
Likelihood
formwhere we have N1 = N
are called the sufficient statistics of the data, since this is all we need to know about D to
More formally, we say s(D) is a sufficient statistic for data D if p(θ|D) = p(θ|s(data)). If
we use a uniform prior, this is equivalent to saying p(D|θ ∝p(s(D)|θ). Consequently, if we
Now suppose the data consists of the count of the number of heads N1 observed in a ﬁxed
number N = N1 + N0 of trials. In this case, we have N1 ∼Bin(N, θ), where Bin represents
the binomial distribution, which has the following pmf:
Bin(k|n, θ) ≜

n
k

Since

n
k

same whether we observe the counts, D = (N1, N), or a sequence of trials, D = {x1, . . . , xN}.
Prior
convenient if the prior had the same form as the likelihood, i.e., if the prior looked like
for some prior parameters γ1 and γ2. If this were the case, then we could easily evaluate the
posterior by simply adding up the exponents:
When the prior and the posterior have the same form, we say that the prior is a conjugate
prior for the corresponding likelihood. Conjugate priors are widely used because they simplify
computation, and are easy to interpret, as we see below.
In the case of the Bernoulli, the conjugate prior is the beta distribution, which we encountered 
 
(a)
 
 
(b)
binomialBetaPosteriorDemo.
Posterior
p(θ|D)
∝
Bin(N1|θ, N0 + N1)Beta(θ|a, b)Beta(θ|N1 + a, N0 + b)
prior, also known as the effective sample size of the prior, is the sum of the pseudo counts,
a + b; this plays a role analogous to the data set size, N1 + N0 = N.
function, corresponding to a large sample size; we see that the posterior is essentially identical
posterior is a “compromise” between the prior and likelihood.
Note that updating the posterior sequentially is equivalent to updating in a single batch.
To see this, suppose we have two data sets Da and Db with sufficient statistics N a
N b
datasets. In batch mode we have
p(θ|Da, Db)
∝
Bin(N1|θ, N1 + N0)Beta(θ|a, b) ∝Beta(θ|N1 + a, N0 + b)
In sequential mode, we have
p(θ|Da, Db)
∝
p(Db|θ)p(θ|Da)
∝
Bin(N b
∝
Beta(θ| N aPosterior mean and mode
ˆθMAP
=
If we use a uniform prior, then the MAP estimate reduces to the MLE, which is just the empirical
fraction of heads:
ˆθMLE = N1
N
This makes intuitive sense, but it can also be derived by applying elementary calculus to
By contrast, the posterior mean is given by,
θ
=
a + N1
a + b + N
This difference between the mode and the mean will prove important later.
We will now show that the posterior mean is convex combination of the prior mean and the
MLE, which captures the notion that the posterior is a compromise between what we previously
believed and what the data is telling us.
Let α0 = a + b be the equivalent sample size of the prior, which controls its strength, and
E [θ|D]
=
α0m1 + N1
N + α0
=
α0
N + α0
m1 +
N
N + α0
N1
α0
similarly that the posterior mode is a convex combination of the prior mode and the MLE, and
that it too converges to the MLE.
Posterior variance
The mean and mode are point estimates, but it is useful to know how much we can trust them.
The variance of the posterior is one way to measure this. The variance of the Beta posterior is
given by
var [θ|D] =
(a + N1)(b + N0)
We can simplify this formidable expression in the case that N ≫a, b, to get
var [θ|D] ≈N1N0
NNN =
N
where ˆθ is the MLE. Hence the “error bar” in our estimate (i.e., the posterior standard deviation),
is given by
σ =
	
var [θ|D] ≈

N√
N. Note, however, that the uncertainty
it is easier to be sure that a coin is biased than to be sure that it is fair.
Posterior predictive distribution
So far, we have been focusing on inference of the unknown parameter(s). Let us now turn our
attention to prediction of future observable data.
rior. We have
=

=

a
a + b
Thus we see that the mean of the posterior predictive distribution is equivalent (in this case) to
plugging in the posterior mean parameters: p(˜x|D) = Ber(˜x|E [θ|D]).
Overﬁtting and the black swan paradox
this approximation can perform quite poorly when the sample size is small.
For example,
observed data as probable as possible. However, using this estimate, we predict that heads are
impossible. This is called the zero count problem or the sparse data problem, and frequently
occurs when estimating counts from small amounts of data. One might think that in the era
of “big data”, such concerns are irrelevant, but note that once we partition the data based on
certain criteria — such as the number of times a speciﬁc person has engaged in a speciﬁc activity
— the sample sizes can become much smaller. This problem arises, for example, when trying
to perform personalized recommendation of web pages. Thus Bayesian methods are still useful,
paradox.
This is based on the ancient Western conception that all swans were white.
In
that context, a black swan was a metaphor for something that could not exist. (Black swans
were discovered in Australia by European explorers in the 17th Century.) The term “black swan
paradox” was ﬁrst coined by the famous philosopher of science Karl Popper; the term has also
the problem of induction, which is the problem of how to draw general conclusions about the
future from speciﬁc observations from the past.
Let us now derive a simple Bayesian solution to the problem. We will use a uniform prior, soparameters would not have this smoothing effect, since the mode has the form ˆθ =
Predicting the outcome of multiple future trials
Suppose now we were interested in predicting the number of heads, x, in M future trials. This
is given by
p(x|D, M)
=

=

M
x

B(a, b)

Hence

Thus we ﬁnd that the posterior predictive is given by the following, known as the (compound)
Bb(x|a, b, M)
≜

M
x
 B(x + a, M −x + b)
B(a, b)
This distribution has the following mean and variance
E [x] = M
a
a + b, var [x] =
Mab
(a + b + M)
a
tails, spreading its probablity mass more widely, and is therefore less prone to overﬁtting and
blackswan type paradoxes.
In the previous section, we discussed how to infer the probability that a coin comes up heads.
In this section, we generalize these results to infer the probability that a dice with K sides
comes up as face k. This might seem like another toy exercise, but the methods we will study
are widely used to analyse text data, biosequence data, etc., as we will see later.posterior predictive
(a)
plugin predictive
(b)
Figure generated by betaBinomPostPredDemo.
Likelihood
the data is iid, the likelihood has the form
p(D|θ)
=
K


θNk
k
where Nk = N
statistics for this model). The likelihood for the multinomial model has the same form, up to an
irrelevant constant factor.
Prior
has support over this simplex. Ideally it would also be conjugate. Fortunately, the Dirichlet
Dir(θ|α)
=
B(α)
K


k
Posterior
Multiplying the likelihood by the prior, we ﬁnd that the posterior is also Dirichlet:
p(θ|D)
∝
p(D|θ)p(θ)
∝
K


θNk
k
=
K


k
=
Dir(θ|α1 + N1, . . . , αK + NK)to the empirical counts Nk.
We can derive the mode of this posterior (i.e., the MAP estimate) by using calculus. However,
we must enforce the constraint that 
We can do this by using a Lagrange
multiplier. The constrained objective function, or Lagrangian, is given by the log likelihood
plus log prior plus the constraint:
ℓ(θ, λ)
=

k

k


k
θk

To simplify notation, we deﬁne N ′
the original constraint:
∂ℓ
∂λ
=


k
θk

∂ℓ
∂θk
=
N ′
k
θk
N ′
k
=
λθk

k
N ′
k
=
λ

k
θk
N + α0 −K
=
λ
where α0 ≜K
given by
N + α0 −K
This is just the empirical fraction of times face k shows up.Posterior predictive
The posterior predictive distribution for a single multinoulli trial is given by the following
expression:
p(X = j|D)
=


=


p(X = j|θj)



=


αj + Nj

k(αk + Nk) = αj + Nj
α0 + N
fact, this form of Bayesian smoothing is even more important in the multinomial case than the
binary case, since the likelihood of data sparsity increases once we start partitioning the data
into many categories.
Worked example: language models using bag of words
modeling, which means predicting which words might occur next in a sequence.
Here we
sampled independently from all the other words using a Cat(θ) distribution. This is called the
bag of words model. Given a past sequence of words, how can we predict which one is likely
to come next?
For example, suppose we observe the following sequence (part of a children’s nursery rhyme):
Mary had a little lamb, little lamb, little lamb,
Mary had a little lamb, its fleece as white as snow
Furthermore, suppose our vocabulary consists of the following words:
mary lamb little big fleece white black snow rain unk
Here unk stands for unknown, and represents all other words that do not appear elsewhere on
the list. To encode each line of the nursery rhyme, we ﬁrst strip off punctuation, and remove
any stop words such as “a”, “as”, “the”, etc. We can also perform stemming, which means
reducing words to their base form, such as stripping off the ﬁnal s in plural words, or the ing
from verbs (e.g., running becomes run). In this example, no words need stemming. Finally, we
replace each word by its index into the vocabulary to get:
We now ignore the word order, and count how often each word occurred, resulting in a
histogram of word counts:Token
Word
mary
lamb
little
big
ﬂeece
white
black
snow
rain
unk
Count
just
p( ˜X = j|D) = E[θj|D] =
αj + Nj

even though they have never been seen before. Later on we will see more sophisticated language
models.
Naive Bayes classiﬁers
where K is the number of values for each feature, and D is the number of features. We will use
a generative approach. This requires us to specify the class conditional distribution, p(x|y = c).
The simplest approach is to assume the features are conditionally independent given the class
label. This allows us to write the class conditional density as a product of one dimensional
densities:
p(x|y = c, θ) =
D


p(xj|y = c, θjc)
The resulting model is called a naive Bayes classiﬁer (NBC).
The model is called “naive” since we do not expect the features to be independent, even
conditional on the class label. However, even if the naive Bayes assumption is not true, it often
model is quite simple (it only has O(CD) parameters, for C classes and D features), and hence
it is relatively immune to overﬁtting.
possibilities below:
•
D
jc is its
variance.
•
c, θ) = D
This is sometimes called the multivariate Bernoulli naive Bayes model. We will see an
application of this below.•
distribution: p(x|y = c, θ) = D
possible values for xj in class c.
Obviously we can handle other kinds of features, or use different distributional assumptions.
Also, it is easy to mix and match features of different types.
Model ﬁtting
We now discuss how to “train” a naive Bayes classiﬁer. This usually means computing the MLE
or the MAP estimate for the parameters. However, we will also discuss how to compute the full
posterior, p(θ|D).
MLE for NBC
The probability for a single data case is given by
p(xi, yi|θ) = p(yi|π)


j
p(xij|θj) =


c
c


j


c
log p(D|θ)
=
C

Nc log πc +
D

C


log p(xij|θjc)
We see that this expression decomposes into a series of terms, one concerning π, and DC
ˆπc
=
Nc
N
where Nc ≜
i I(yi = c) is the number of examples in class c.
The MLE for the likelihood depends on the type of distribution we choose to use for each
feature. For simplicity, let us suppose all features are binary, so xj|y = c ∼Ber(θjc). In this
case, the MLE becomes
ˆθjc = Njc
Nc
This algorithm obviously takes
O(ND) time. The method is easily generalized to handle features of mixed type. This simplicity
is one reason the method is so widely used.
features.)c = yi // Class label of i’th example;
N , ˆθjc = Njc
N
(a)
(b)
windows” and “MS windows”. Figure generated by naiveBayesBowDemo.
Bayesian naive Bayes
The trouble with maximum likelihood is that it can overﬁt. For example, consider the example
does not have this word in it?
Our algorithm will crash and burn, since we will ﬁnd that
A simple solution to overﬁtting is to be Bayesian. For simplicity, we will use a factored prior:
p(θ)
=
p(π)
D


C


p(θjc)following factored posterior:
p(θ|D)
=
p(π|D)
D


C


p(θjc|D)
p(π|D)
=
Dir(N1 + α1 . . . , NC + αC)
p(θjc|D)
=
Beta((Nc −Njc) + β0, Njc + β1)
In other words, to compute the posterior, we just update the prior counts with the empirical
model “ﬁtting”.
Using the model for prediction
At test time, the goal is to compute
p(y = c|x, D)
∝
p(y = c|D)
D


p(xj|y = c, D)
The correct Bayesian procedure is to integrate out the unknown parameters:
p(y = c|x, D)
∝


Cat(y = c|π)p(π|D)dπ

D




Ber(xj|y = c, θjc)p(θjc|D)

p(y = c|x, D)
∝
πc
D


θjk
=
Njc + β1
Nc + β0 + β1
πc
=
Nc + αc
N + α0
where α0 = 
c αc.
If we have approximated the posterior by a single point, p(θ|D) ≈δˆθ(θ), where ˆθ may be
the ML or MAP estimate, then the posterior predictive density is obtained by simply plugging in
the parameters, to yield a virtually identical rule:
p(y = c|x, D)
∝
ˆπc
D

However, this small difference can be important in practice, since the posterior mean will result
We now discuss one important practical detail that arises when using generative classiﬁers of any
that 
small. The obvious solution is to take logs when applying Bayes rule, as follows:
log p(y = c|x)
=
bc −log
 C

ebc′

bc
≜
log p(x|y = c) + log p(y = c)
However, this requires evaluating the following expression
log[

c′
ebc′ ] = log

c′
p(y = c′, x) = log p(x)
and we can’t add up in the log domain. Fortunately, we can factor out the largest term, and just
represent the remaining numbers relative to that. For example,


In general, we have
log

c
ebc
=
log

(

c
ebc−B)eB

=

log(

c
ebc−B)

+ B
logsumexp for an implementation.)
p(yi|xi, ˆθ). See naiveBayesPredict for the Matlab code. Note that we do not need the
quantity log p(yi = c) + log p(xi|y = c).
Feature selection using mutual information
Since an NBC is ﬁtting a joint distribution over potentially many features, it can suffer from
One common approach to tackling both of these problems is to perform feature selection, to
remove “irrelevant” features that do not help much with the classiﬁcation problem. The simplest
approach to feature selection is to evaluate the relevance of each feature separately, and thenLic = log ˆπc;
pic = exp(Lic −logsumexp(Li,:));
ˆyi = argmaxc pic;
take the top K, where K is chosen based on some tradeoff between accuracy and complexity.
This approach is known as variable ranking, ﬁltering, or screening.
Xj and the class label Y :
I(X, Y ) =

xj

y
p(xj, y) log p(xj, y)
p(xj)p(y)
The mutual information can be thought of as the reduction in entropy on the label distribution
that the MI can be computed as follows
Ij
=

c

θj

than the words which are most probable. For example, the most probable word in both classes
is “subject”, which always occurs because this is newsgroup data, which always has a subject
line. But obviously this is not very discriminative. The words with highest MI with the class
label are (in decreasing order) “windows”, “microsoft”, “DOS” and “motif”, which makes sense,
since the classes correspond to Microsoft Windows and X Windows.
Classifying documents using bag of words
Document classiﬁcation is the problem of classifying text documents into different categories.
One simple approach is to represent each document as a binary vector, which records whether
We can then use the following class conditional density:
p(xi|yi = c, θ) =
D


Ber(xij|θjc) =
D


θI(xij)
jcprob
prob
highest MI
MI
subject
subject
windows
this
windows
microsoft
with
this
dos
but
with
motif
you
but
window
This is called the Bernoulli product model, or the binary independence model.
A more accurate representation counts the number
of occurrences of each word.
Speciﬁcally, let xi be a vector of counts for document i, so
the class conditional densities, we can use a multinomial distribution:
p(xi|yi = c, θ) = Mu(xi|Ni, θc) =
Ni!
D
D


θxij
jc
where we have implicitly assumed that the document length Ni is independent of the class.
the constraint that D
Although the multinomial classiﬁer is easy to train and easy to use at test time, it does not
work particularly well for document classiﬁcation. One reason for this is that it does not take
into account the burstiness of word usage. This refers to the phenomenon that most words
never appear in any given document, but if they do appear once, they are likely to appear more
than once, i.e., words occur in bursts.
The multinomial model cannot capture the burstiness phenomenon. To see why, note that
unlikely to generate many of them. For more frequent words, the decay rate is not as fast. To
see why intuitively, note that the most frequent words are function words which are not speciﬁc
to the class, such as “and”, “the”, and “but”; the chance of the word “and” occuring is pretty
much the same no matter how many time it has previously occurred (modulo document length),
so the independence assumption is more reasonable for common words. However, since rare
words are the ones that matter most for classiﬁcation purposes, these are the ones we want to
model the most carefully.
Various ad hoc heuristics have been proposed to improve the performance of the multinomial
that performs as well as these ad hoc methods, yet is probabilistically sound (Madsen et al.
technically the features xij are not independent, because of the constraint 
j xij = Ni.Suppose we simply replace the multinomial class conditional density with the Dirichlet
Compound Multinomial or DCM density, deﬁned as follows:
p(xi|yi = c, α)
=


Ni!
D
B(xi + αc)
B(αc)
to capture the burstiness phenomenon. The intuitive reason for this is as follows: After seeing
independent. The multinomial model corresponds to drawing a ball from an urn with K colors
of ball, recording its color, and then replacing it. By contrast, the DCM model corresponds to
drawing a ball, recording its color, and then replacing it with one additional copy; this is called
the Polya urn.
Using the DCM as the class conditional density gives much better results than using the
multinomial, and has performance comparable to state of the art methods, as described in
Exercises
p(D)
=
Z(α1 + N1, α0 + N0)
Z(α1, α0)
= Γ(α1 + N1)Γ(α0 + N0)
Γ(α1 + α0 + N)
Γ(α1 + α0)
Γ(α1)Γ(α0)
We will now derive an alternative derivation of this fact. By the chain rule of probability,
p(X = k|D1:N) =
Nk + αk

i Ni + αi ≜Nk + αk
N + α
Then
p(D)
=
α1
α ·
α0
=
=p(x|n, D)
=
Bb(x|α′
=
B(x + α′
B(α′
n
x

Prove that this reduces to
α′
α′
=
α′
α′
Hint: use the fact that
Let
φ = logit(θ) = log
θ
The Poisson pmf is deﬁned as Poi(x|λ) = e−λ λx
parameter. Derive the MLE.
conjugate Bayesian analysis.
a. Derive the posterior p(λ|D) assuming a conjugate prior p(λ) = Ga(λ|a, b) ∝λa−1e−λb. Hint: the
posterior is also a Gamma distribution.
given bya. Given a data set x1, . . . , xn, what is the maximum likelihood estimate of a (call it ˆa)?
c. Do you see any problem with the above approach? Brieﬂy suggest (in words) a better approach.
outside the training data. In this exercise, we will perform a Bayesian analysis of the uniform distribution
(following (Minka 2001a)). The conjugate prior is the Pareto distribution, p(θ) = Pareto(θ|b, K), deﬁned in
p(D, θ) =
KbK
Let m = max(D).
The evidence (the probability that all N samples came from the same uniform
distribution) is
p(D)
=
 ∞
m
KbK
=

K
KbK
if m > b
Derive the posterior p(θ|D), and show that if can be expressed as a Pareto distribution.
b. Compute the posterior mean, mode and median number of taxis in the city, if such quantities exist.
c. Rather than trying to compute a point estimate of the number of taxis, we can compute the predictive
density over the next taxicab number using
p(D′|D, α) =

p(x|D, α)
d. Use the predictive density formula to compute the probability that the next taxi you will see (say,N
N
What is the MLE given this data?
p(θ) = Expon(θ|λ)
has the form
Ga(θ|a, b)
∝
θa−1e−θb
d. What is the posterior, p(θ|D, ˆλ)?
e. Is the exponential prior conjugate to the exponential likelihood?
f. What is the posterior mean, E

θ|D, ˆλ

?
g. Explain why the MLE and posterior mean differ. Which is more reasonable in this example?
(Source: Jaakkola.) In the book, we discussed Bayesian inference of a Bernoulli rate parameter with the
prior p(θ) = Beta(θ|α, β). We know that, with this prior, the MAP estimate is given by
ˆθ =
where N1 is the number of heads, N0 is the number of tails, and N = N0 + N1 is the total number of
trials.
a. Now consider the following prior, that believes the coin is fair, or is slightly biased towards tails:
p(θ)
=
⎧
⎨
⎩
otherwise
Derive the MAP estimate under this prior as a function of N1 and N.
Which prior leads to a better estimate when N is large?
dirichlet prior. Now consider predicting a batch of new data, ˜D = (X1, . . . , Xm), consisting of m single
multinomial trials (think of predicting the next m words in a sentence, assuming they are drawn iid).
Derive an expression for
p( ˜D|D, α)Your answer should be a function of α, and the old and new counts (sufficient statistics), deﬁned as
N old
k
=


I(xi = k)
N new
k
=


D
I(xi = k)
Hint: recall that, for a vector of counts, N1:K, the marginal likelihood (evidence) is given by
p(D|α) =
Γ(α)
Γ(N + α)

k
Γ(Nk + αk)
Γ(αk)
where α = 
k αk and N = 
k Nk.
(Source: Koller.).
a. Suppose we compute the empirical distribution over letters of the Roman alphabet plus the space
What is
your work.
Write a program that can solve for α1 and α2 in terms of m, ℓand u. Hint: write α2 as a function of α1
and m, so the pdf only has one unknown; then write down the probability mass contained in the intervalx belongs to class c is
log p(xi|c, θ)
=
log
W

θxiw
=
W


=
W


xiw log
θcw


w
where W is the number of words in the vocabulary. We can write this more succintly as
log p(xi|c, θ)
=
φ(xi)T βc
βc = (log
θc1
θcW


w
product) of the parameters βc.
log2
b. Intuitively, words that occur in both classes are not very “discriminative”, and therefore should not
θ2,w (or equivalently the conditions on β1,w, β2,w) under which the presence or absence of w in a
test document will have no effect on the class posterior (such a word will be ignored by the classiﬁer).
where the sum is over the nc documents in class c. Consider a particular word w, and suppose it
ignored by our classiﬁer? Explain why or why not.
d. What other ways can you think of which encourage “irrelevant” words to be ignored?
Consider a generative classiﬁer for C classes with class conditional density p(x|y) and uniform class prior
independent (the naive Bayes assumption), we can write
p(x|y = c) =
D

Ber(xj|θjc)
This requires DC parameters.a. Now consider a different model, which we will call the “full” model, in which all the features are fully
dependent (i.e., we make no factorization assumptions). How might we represent p(x|y = c) in this
case? How many parameters are needed to represent p(x|y = c)?
b. Assume the number of features D is ﬁxed. Let there be N training cases. If the sample size N is very
small, which model (naive Bayes or full) is likely to give lower test set error, and why?
c. If the sample size N is very large, which model (naive Bayes or full) is likely to give lower test set error,
and why?
d. What is the computational complexity of ﬁtting the full and naive Bayes models as a function of N
e. What is the computational complexity of applying the full and naive Bayes models at test time to a
single test case?
f. Suppose the test case has missing data. Let xv be the visible features of size v, and xh be the hidden
(missing) features of size h, where v + h = D. What is the computational complexity of computing
p(y|xv, ˆθ) for the full and naive Bayes models, as a function of v and h?
"sports", "is", "for", "play", "healthy", "pizza". We have the following example spam messages "million dollar
offer", "secret offer today", "secret is secret" and normal messages, "low price for valued customer", "play
secret sports today", "sports is healthy", "low price pizza". Give the MLEs for the following parameters:Gaussian models
Introduction
In this chapter, we discuss the multivariate Gaussian or multivariate normal (MVN), which
is the most widely used joint probability density function for continuous variables. It will form
the basis for many of the models we will encounter in later chapters.
Unfortunately, the level of mathematics in this chapter is higher than in many other chapters.
In particular, we rely heavily on linear algebra and matrix calculus. This is the price one must
with a *. In addition, since there are so many equations in this chapter, we have put a box
around those that are particularly important.
Notation
Let us brieﬂy say a few words about notation. We denote vectors by boldface lower case letters,
such as x. We denote matrices by boldface upper case letters, such as X. We denote entries in
All vectors are assumed to be column vectors unless noted otherwise. We use [x1, . . . , xD] to
denote a column vector created by stacking D scalars. Similarly, if we write x = [x1, . . . , xD],
where the left hand side is a tall column vector, we mean to stack the xi along the rows; this is
usually written as x = (xT
D)T , but that is rather ugly. If we write X = [x1, . . . , xD],
where the left hand side is a matrix, we mean to stack the xi along the columns, creating a
matrix.
Basics
N(x|μ, Σ) ≜

μ
u1
u2
λ1
λ2
x1
x2
of (Bishop 2006a).
The expression inside the exponent is the Mahalanobis distance between a data vector x
eigendecomposition of Σ. That is, we write Σ = UΛUT , where U is an orthonormal matrix
of eigenvectors satsifying UT U = I, and Λ is a diagonal matrix of eigenvalues.
Using the eigendecomposition, we have that
D

λi
uiuT
i
where ui is the i’th column of U, containing the i’th eigenvector. Hence we can rewrite the
Mahalanobis distance as follows:
=
(x −μ)T

 D

λi
uiuT
i

(x −μ)
=
D

λi
(x −μ)T uiuT
i (x −μ) =
D

y2
i
λi
where yi ≜uT
i (x −μ). Recall that the equation for an ellipse in 2d is
y2
λ1
+ y2
λ2
Hence we see that the contours of equal probability density of a Gaussian lie along ellipses.
the eigenvalues determine how elogonated it is.
In general, we see that the Mahalanobis distance corresponds to Euclidean distance in aMLE for an MVN
We now describe one way to estimate the parameters of an MVN, using MLE. In later sections,
we will discuss Bayesian inference for the parameters, which can mitigate overﬁtting, and can
provide a measure of conﬁdence in our estimates.
the parameters is given by
ˆμmle
=
N
N

xi ≜x
ˆΣmle
=
N
N

N (
N

xixT
i ) −x xT
That is, the MLE is just the empirical mean and empirical covariance. In the univariate case, we
get the following familiar results:
ˆμ
=
N

i
xi = x
ˆσ2
=
N

i

N

i
x2
i

Proof *
To prove this result, we will need several results from matrix algebra, which we summarize
below. In the equations, a and b are vectors, and A and B are matrices. Also, the notation
tr(A) refers to the trace of a matrix, which is the sum of its diagonals: tr(A) = 
i Aii.
∂(bT a)
∂a
= b
∂(aT Aa)
∂a
= (A + AT )a
∂
∂Atr(BA) = BT
∂
tr(ABC) = tr(CAB) = tr(BCA)
The last equation is called the cyclic permutation property of the trace operator. Using this,
we can derive the widely used trace trick, which reorders the scalar inner product xT Ax as
follows
xT Ax = tr(xT Ax) = tr(xxT A) = tr(AxxT )ℓ(μ, Σ) = log p(D|μ, Σ)
=
N
N

(xi −μ)T Λ(xi −μ)
Using the substitution yi = xi −μ and the chain rule of calculus, we have
∂
=
∂
∂yi
yT
i Σ−1yi
∂yi
∂μ
=
Hence
∂
∂μℓ(μ, Σ)
=
N

N

ˆμ
=
N
N

xi = x
ℓ(Λ)
=
N

i
tr[(xi −μ)(xi −μ)T Λ]
=
N
where
N

(xi −μ)(xi −μ)T
yields
∂ℓ(Λ)
∂Λ
=
N
2ST
Λ−T
=
so
N
N

(xi −μ)(xi −μ)T
(since both parameters must be simultaneously optimized), we get the standard equation for the
MLE of a covariance matrix.Maximum entropy derivation of the Gaussian *
In this section, we show that the multivariate Gaussian is the distribution with maximum entropy
Gaussian is so widely used: the ﬁrst two moments are usually all that we can reliably estimate
from data, so we want a distribution that captures these properties, but otherwise makes as few
addtional assumptions as possible.
To simplify notation, we will assume the mean is zero. The pdf has the form
2xT Σ−1x)
is given by

(2πe)D|Σ|

variance Σ.

h(q) ≤h(p).
≤
KL (q||p) =


q(x) log q(x)
p(x)dx
=
−h(q) −


q(x) log p(x)dx
=∗
−h(q) −


p(x) log p(x)dx
=
−h(q) + h(p)
moments for the quadratic form encoded by log p(x).
Gaussian discriminant analysis
One important application of MVNs is to deﬁne the the class conditional densities in a generative
classiﬁer, i.e.,
p(x|y = c, θ) = N(x|μc, Σc)
The resulting technique is called (Gaussian) discriminant analysis or GDA (even though it is a
diagonal, this is equivalent to naive Bayes.height
weight
(a)
height
weight
(b)
mass is inside the ellipse. Figure generated by gaussHeightWeight.
ˆy(x) = argmax
c
[log p(y = c|π) + log p(x|θc)]
When we compute the probability of x under each class conditional density, we are measuring
the distance from x to the center of each class, μc, using Mahalanobis distance. This can be
thought of as a nearest centroids classiﬁer.
the height and weight of men and women. We can see that the features are correlated, as is
probability mass. If we have a uniform prior over classes, we can classify a new test vector as
follows:
ˆy(x) = argmin
c
c (x −μc)
Quadratic discriminant analysis (QDA)
model by plugging in the deﬁnition of the Gaussian density, as follows:
p(y = c|x, θ)
=

c (x −μc)



c′ (x −μc′)

Thresholding this results in a quadratic function of x.
The result is known as quadratic
look like in 2D.Parabolic Boundary
(a)
Some Linear, Some Quadratic
(b)
Figure generated by
discrimAnalysisDboundariesDemo.
temperature is high (left), the distribution is uniform, whereas when the temperature is low (right), the
distribution is “spiky”, with all its mass on the largest element. Figure generated by softmaxDemo2.
Linear discriminant analysis (LDA)
We now consider a special case in which the covariance matrices are tied or shared across
p(y = c|x, θ)
∝
πc exp

μT

=
exp

μT

2xT Σ−1x]
Since the quadratic term xT Σ−1x is independent of c, it will cancel out in the numerator and
denominator. If we deﬁne
γc
=
βc
=then we can write
p(y = c|x, θ) =
eβT

c′ eβT
where η = [βT
Cx + γC], and S is the softmax function, deﬁned as follows:
S(η)c =
eηc
C
S(η/T)c =

if c = argmaxc′ ηc′
otherwise
In other words, at low temperatures, the distribution spends essentially all of its time in the
for an illustration. Note that this terminology comes from the area of statistical physics, where
it is common to use the Boltzmann distribution, which has the same form as the softmax
function.
function of x. (The reason it is linear is because the xT Σ−1x cancels from the numerator
and denominator.) Thus the decision boundary between any two classes, say c and c′, will be
derive the form of this line as follows:
p(y = c|x, θ)
=
p(y = c′|x, θ)
βT
c x + γc
=
βT
c′x + γc′
xT (βc′ −β)
=
γc′ −γc
An alternative to ﬁtting an LDA model and then deriving the class posterior is to directly
To gain further insight into the meaning of these equations, let us consider the binary case. In
this case, the posterior is given by
=
eβT
eβT
=

(β1 −β0)T x + (γ1 −γ0)
Linear Boundary
(a)
All Linear Boundaries 
(b)
Figure generated by
discrimAnalysisDboundariesDemo.
Now
γ1 −γ0
=
=
So if we deﬁne
w
=
x0
=then we have wT x0 = −(γ1 −γ0), and hence
=
sigm(wT (x −x0))
decision rule is as follows: shift x by x0, project onto the line w, and see if the result is positive
or negative.
shifts right. Thus we see that the class prior, πc, just changes the decision threshold, and not
The magnitude of w determines the steepness of the logistic function, and depends on
theory, it is common to deﬁne the discriminability of a signal from the background noise using
d′ ≜μ1 −μ0
σ
deviation of the noise. If d′ is large, the signal will be easier to discriminate from the noise.
MLE for discriminant analysis
We now discuss how to ﬁt a discriminant analysis model. The simplest way is to use maximum
log p(D|θ)
=
 N

C

I(yi = c) log πc

+
C

⎡
⎣
log N(x|μc, Σc)
⎤
⎦
can estimate these parameters separately. For the class prior, we have ˆπc = Nc
N , as with naive
compute the MLE for each Gaussian:
Nc

xi,
Nc

(xi −ˆμc)(xi −ˆμc)T
See discrimAnalysisFit for a Matlab implementation. Once the model has been ﬁt, you can
Strategies for preventing overﬁtting
The speed and simplicity of the MLE method is one of its greatest appeals. However, the MLE
can badly overﬁt in high dimensions. In particular, the MLE for a full covariance matrix is
close to singular. There are several possible solutions to this problem:•
Use a diagonal covariance matrix for each class, which assumes the features are conditionally
•
Use a full covariance matrix, but force it to be the same for all classes, Σc = Σ. This is an
•
Use a diagonal covariance matrix and forced it to be shared. This is called diagonal covariance
•
Use a full covariance matrix, but impose a prior and then integrate it out.
If we use a
details.
•
Fit a full or diagonal covariance matrix by MAP estimation. We discuss two different kinds
of prior below.
•
Project the data into a low dimensional subspace and ﬁt the Gaussians there.
We discuss some of these options below.
Regularized LDA *
Suppose we tie the covariance matrices, so Σc = Σ, as in LDA, and furthermore we perform
MAP estimation of Σ using an inverse Wishart prior of the form IW(diag( ˆΣmle), ν0) (see
When we evaluate the class conditional densities, we need to compute ˆΣ
mle,
to get around this, as we show below. (Note that this trick cannot be applied to QDA, which is
a nonlinear function of x.)
Let X = UDVT be the SVD of the design matrix, where V is D × N, U is an N × N
orthogonal matrix, and D is a diagonal matrix of size N. Furthermore, deﬁne the N × N
matrix Z = UD; this is like a design matrix in a lower dimensional space (since we assumerewrite the MLE as follows:
ˆΣmle
=
N XT X −μμT
=
=
z VT
=
z )VT
=
V ˆΣzVT
where ˆΣz is the empirical covariance of Z. Hence we can rewrite the MAP estimate as
ˆΣmap
=
V ˜ΣzVT
˜Σz
=
Note, however, that we never need to actually compute the D×D matrix ˆΣmap. This is because
exp(δc), where
δc = −xT βc + γc,
βc = ˆΣ
c βc + log πc
We can compute the crucial βc term for RDA without inverting the D × D matrix as follows:
βc = ˆΣ
the code.
Diagonal LDA
A simple alternative to RDA is to tie the covariance matrices, so Σc = Σ as in LDA, and then to
use a diagonal covariance matrix for each class. This is called the diagonal LDA model, and is
δc(x) = log p(x, y = c|θ) = −
D

2σ2
j
+ log πc
Typically we set ˆμcj = xcj and ˆσ2
j = s2
j, which is the pooled empirical variance of feature j
(pooled across classes) deﬁned by
s2
j =
C

N −C
In high dimensional settings, this model can work much better than LDA and RDA (Bickel andNumber of Genes
λ
Misclassification Error
 
 
Test
Train
CV
Error versus amount of shrinkage for nearest shrunken centroid classiﬁer applied to the
SRBCT gene expression data.
Figure generated by
shrunkenCentroidsSRBCTdemo.
Nearest shrunken centroids classiﬁer *
One drawback of diagonal LDA is that it depends on all of the features. In high dimensional
problems, we might prefer a method that only depends on a subset of the features, for reasons
of accuracy and interpretability. One approach is to use a screening method, perhaps based
μcj = mj + Δcj
We will then put a prior on the Δcj terms to encourage them to be strictly zero and compute(a)
(b)
(c)
(d)
=
Figure generated by
shrunkenCentroidsSRBCTdemo.
these numbers.
Inference in jointly Gaussian distributions
Given a joint distribution, p(x1, x2), it is useful to be able to compute marginals p(x1) and
conditionals p(x1|x2). We discuss how to do this below, and then give some applications. TheseStatement of the result
with parameters
μ =

μ1
μ2

, Σ =

Σ11
Σ12
Σ21
Σ22


Λ11
Λ12
Λ21
Λ22

Then the marginals are given by
p(x1)
=
N(x1|μ1, Σ11)
p(x2)
=
N(x2|μ2, Σ22)
and the posterior conditional is given by
We see that both the marginal and conditional distributions are themselves Gaussian. For the
marginals, we just extract the rows and columns corresponding to x1 or x2. For the conditional,
we have to do a bit more work. However, it is not that complicated: the conditional mean is
just a linear function of x2, and the conditional covariance is just a constant matrix that is
independent of x2. We give three different (but equivalent) expressions for the posterior mean,
and two different (but equivalent) expressions for the posterior covariance; each one is useful in
different circumstances.
Examples
Below we give some examples of these equations in action, which will make them seem more
intuitive.
Marginals and conditionals of a 2d Gaussian
Let us consider a 2d example. The covariance matrix is
Σ =

σ2
ρσ1σ2
ρσ1σ2
σ2

The marginal p(x1) is a 1D Gaussian, obtained by projecting the joint distribution onto the x1
line:
p(x1) = N(x1|μ1, σ2x1
x2
p(x1,x2)
(a)
x1
x2
p(x1)
(b)
x1
x2
(c)
Suppose we observe X2 = x2; the conditional p(x1|x2) is obtained by “slicing” the joint
p(x1|x2)
=
N

x1|μ1 + ρσ1σ2
σ2
(x2 −μ2), σ2
σ2

If σ1 = σ2 = σ, we get
p(x1|x2)
=
N


N

x1|μ1, σ2

, since x2 conveys no information about x1 if they are uncorrelated (and hence
independent).
for N observed points ti.
the noisy data case.) The question is: how does the function behave in between the observed
we shall see how to encode priors over functions, and how to update such a prior with observed
values to get a posterior over functions. But in this section, we take a simpler approach, which
is adequate for MAP estimation of functions deﬁned on 1d inputs. We follow the presentation
We start by discretizing the problem. First we divide the support of the function into D equal
subintervals. We then deﬁne
xj = f(sj),
sj = jh,
h = T(a)
λ=0p1
(b)
(b)
gaussInterpDemo.
to a belief that the function is quite “wiggly”. In vector form, the above equation can be written
as follows:
Lx = ϵ
⎛
⎜
⎜
⎜
⎝
...
⎞
⎟
⎟
⎟
⎠
The corresponding prior has the form

−λ2

Λ = LT L for the precision matrix.proper.
function values. Without loss of generality, assume that the unknown variables are ordered ﬁrst,
then the known variables. Then we can partition the L matrix as follows:
We can also partition the precision matrix of the joint distribution:
Λ = LT L =

Λ11
Λ12
Λ21
Λ22

=

LT
LT
LT
LT

p(x1|x2)
=
=
=
Note that we can compute the mean by solving the following system of linear equations:
interpolates in between, as desired.
We see that the variance goes up as we move away from the
terestingly, λ has no effect on the posterior mean, since it cancels out when multiplying Λ11
precision affects the smoothness of posterior mean estimate.
The marginal credibility intervals do not capture the fact that neighboring locations are
correlated.
We can represent that by drawing complete functions (i.e., vectors x) from the
Data imputation
Suppose we are missing some entries in a design matrix. If the columns are correlated, we can
in each row. We then inferred the missing entries given the observed entries, using the true
(generating) model. More precisely, for each row i, we compute p(xhi|xvi, θ), where hi and vi
are the indices of the hidden and visible entries in case i. From this, we compute the marginal
distribution of each missing variable, p(xhij|xvi, θ). We then plot the mean of this distribution,
ˆxij = E [xj|xvi, θ]; this represents our “best guess” about the true value of that entry, in theobserved
imputed
truth
Illustration of data imputation. Left column: visualization of three rows of the data matrix
with missing entries.
Middle column: mean of the posterior predictive, based on partially observed
data in that row, but the true model parameters.
Right column: true values.
Figure generated by
gaussImputationDemo.
to the observed value, ˆxij = xij.)
We can use var

xhij|xvi, θ

as a measure of conﬁdence in this guess, although this is not
shown. Alternatively, we could draw multiple samples from p(xhi|xvi, θ); this is called multiple
imputation.
lihood of each partially observed row in the table, p(xvi|θ), which can be computed using
Information form
Suppose x ∼N(μ, Σ). One can show that E [x] = μ is the mean vector, and cov [x] = Σ is
the covariance matrix. These are called the moment parameters of the distribution. However,
it is sometimes useful to use the canonical parameters or natural parameters, deﬁned as
We can convert back to the moment parameters using
Using the canonical parameters, we can write the MVN in information form (i.e., in exponential
Nc(x|ξ, Λ)
=


where we use the notation Nc() to distinguish from the moment parameterization N().
It is also possible to derive the marginalization and conditioning formulas in information
form. We ﬁnd
p(x2)
=
p(x1|x2)
=
Nc(x1|ξ1 −Λ12x2, Λ11)Thus we see that marginalization is easier in moment form, and conditioning is easier in
information form.
Another operation that is signiﬁcantly easier in information form is multiplying two Gaussians.
One can show that
Nc(ξf, λf)Nc(ξg, λg)
=
Nc(ξf + ξg, λf + λg)
However, in moment form, things are much messier:
N(μf, σ2
f)N(μg, σ2
g) = N

μfσ2
g + μgσ2
f
σ2g + σ2g
,
σ2
fσ2
g
σ2g + σ2g

Proof of the result *
skip this section. We ﬁrst derive some results that we will need here and elsewhere in the book.
We will return to the proof at the end.
Inverse of a partitioned matrix using Schur complements
The key tool we need is a way to invert a partitioned matrix. This can be done using the
following result.
M =

E
F
G
H

where we assume E and H are invertible. We have
=


=


where
≜
E −FH−1G
≜
H −GE−1F
inverse formula.
Proof. If we could block diagonalize M, it would be easier to invert. To zero out the top right

I
I
 E
F
G
H

=
E −FH−1G
G
H

E −FH−1G
G
H
 
I
−H−1G
I

=

E −FH−1G
H

Putting it all together we get

I
I

!
"#
$
X

E
F
G
H

!
"#
$
M

I
−H−1G
I

!
"#
$
Z
=

E −FH−1G
H

!
"#
$
W
Taking the inverse of both sides yields
=
and hence
=
ZW−1X
Substituting in the deﬁnitions we get

E
F
G
H
=

I
−H−1G
I
 
 
I
I

=

 
I
I

=


GE−1F), yielding

E
F
G
H
=


The matrix inversion lemma
We now derive some useful corollaries of the above result.

E
F
G
H

,
where we assume E and H are invertible. We have
=
=
|E −FH−1G|
=The third equation is known as the matrix determinant
lemma.
Let E = Σ
be a N × N diagonal matrix, let F = GT = X of size N × D, where N ≫D, and let
Another application concerns computing a rank one update of an inverse matrix.
Let
=
=
This is useful when we incrementally add a data vector to a design matrix, and want to update
our sufficient statistics. (One can derive an analogous formula for removing a data vector.)
Proof of Gaussian conditioning formulas
p(x1, x2) as p(x2)p(x1|x2) as follows:
E
=
exp
%

x1 −μ1
x2 −μ2
T 
Σ11
Σ12
Σ21
Σ22
x1 −μ1
x2 −μ2
&
E
=
exp
%

x1 −μ1
x2 −μ2
T 
I
I

×

I
I
 
x1 −μ1
x2 −μ2
'
=
exp

(
× exp

'
This is of the form
exp(quadratic form in x1, x2) × exp(quadratic form in x2)Hence we have successfully factorized the joint as
p(x1, x2)
=
p(x1|x2)p(x2)
=
where the parameters of the conditional distribution can be read off from the above equations
using
=
=
correct:
=
=
where d1 = dim(x1) and d2 = dim(x2).
Linear Gaussian systems
a noisy observation of x. Let us assume we have the following prior and likelihood:
p(x) = N(x|μx, Σx)
p(y|x) = N(y|Ax + b, Σy)
where A is a matrix of size Dy × Dx. This is an example of a linear Gaussian system. We
how to “invert the arrow”, that is, how to infer x from y. We state the result below, then give
several examples, and ﬁnally we derive the result. We will see many more applications of these
results in later chapters.
Statement of the result
p(x|y) = N(x|μx|y, Σx|y)
x
y A
yIn addition, the normalization constant p(y) is given by
Examples
In this section, we give some example applications of the above result.
Inferring an unknown scalar from noisy measurements
Suppose we make N noisy measurements yi of some underlying quantity x; let us assume the
p(yi|x)
=
y )
Now let us use a Gaussian prior for the value of the unknown source:
p(x)
=
We want to compute p(x|y1, . . . , yN, σ2). We can convert this to a form that lets us apply
Bayes rule for Gaussians by deﬁning y = (y1, . . . , yN), A = 1T
y
= diag(λyI). Then we get
p(x|y)
=
N )
λN
=
μN
=
λN
=
y +
λ0
μ0
between the MLE and the prior.
If the prior is weak relative to the signal strength (λ0 is
We can rewrite the results in terms of the posterior variance, rather than posterior precision, 
 
prior
lik
post
 
 
prior
lik
post
similar to the MLE. Figure generated by gaussInferParamsMean1d.
as follows:
p(x|D, σ2)
=
N)
N
=
N
σ2 +
=
μN
=
N
μ0
+ Ny
σ2

=
σ2
We can also compute the posterior sequentially, by updating after each observation.
If
posterior):
p(x|y)
=
N(x|μ1, Σ1)
Σ1
=
Σ0
Σy
=
ΣyΣ0
Σ0 + Σy
μ1
=
Σ1
 μ0
Σ0
+ y
Σy

μ1
=
Σy
Σy + Σ0
μ0 +
Σ0
Σy + Σ0
y
=
μ0 + (y −μ0)
Σ0
Σy + Σ0
=
y −(y −μ0)
Σy
Σy + Σ0The ﬁrst equation is a convex combination of the prior and the data. The second equation is the
prior mean adjusted towards the data. The third equation is the data adjusted towards the prior
mean; this is called shrinkage. These are all equivalent ways of expressing the tradeoff between
likelihood and prior. If Σ0 is small relative to ΣY , corresponding to a strong prior, the amount
which is deﬁned as follows:
SNR ≜E

X2
E [ϵ2] = Σ0 + μ2
Σy
is the noise term.
Inferring an unknown vector from noisy measurements
y , we have
p(x|y1, . . . , yN)
=
N(x|μN, ΣN)
N
=
y
μN
=
location of an object in 2d space, such as a missile or airplane, and the yi as being noisy
observations, such as radar “blips”. As we receive more blips, we are better able to localize the
the famous Kalman ﬁlter algorithm.
Now suppose we have multiple measuring devices, and we want to combine them together;
responding to sensors with different reliabilities), the posterior will be an appropriate weighted




reliable in the y1 component (horizontal direction). In this case, the posterior mean uses y1’s
vertical component and y2’s horizontal component.data
prior
Illustration of Bayesian inference for the mean of a 2d Gaussian. (a) The data is generated
covariance Σy is known but x is unknown.
The black cross represents x.
(b) The prior is p(x) =
gaussInferParamsMean2d.
(a)
(b)
(c)
(black cross). (a) Equally reliable sensors, so the posterior mean estimate is in between the two circles.
appropriate combination of the two measurements. Figure generated by sensorFusion2d.
ing an unweighted average would give the wrong result. However, we have assumed the sensor
precisions are known. When they are not, we should model out uncertainty about Σ1 and Σ2
Interpolating noisy data
observations.
Instead, let us assume that we obtain N noisy observations yi; without loss
of generality, assume these correspond to x1, . . . , xN. We can model this setup as a linearGaussian system:
y = Ax + ϵ
A =


(away from the data) is high.
The posterior mean can also be computed by solving the following optimization problem:
min
x
2σ2
N

D

)
as a discrete approximation to the following problem:
min
f
2σ2


(f(t) −y(t))2dt + λ


[f ′(t)]2dt
where f ′(t) is the ﬁrst derivative of f. The ﬁrst term measures ﬁt to the data, and the second
term penalizes functions that are “too wiggly”. This is an example of Tikhonov regularization,
approaches, which enforce higher order smoothness (so the resulting samples look less “jagged”).
Proof of the result *
In more detail, we proceed as follows. The log of the joint distribution is as follows (dropping
irrelevant constants):
y (y −Ax −b)
This is clearly a joint Gaussian distribution, since it is the exponential of a quadratic form.
Expanding out the quadratic terms involving x and y, and ignoring linear and constant terms,
we have
Q
=
y Ax
=

x
y
x
y A
y
y A
y
 
x
y

=

x
y
T

x
y
(a)
λ=0p1
(b)
generated by gaussInterpNoisyDemo. See also splineBasisDemo.
where the precision matrix of the joint is deﬁned as
x
y A
y
y A
y

≜Λ =

Λxx
Λxy
Λyx
Λyy

p(x|y)
=
N(μx|y, Σx|y)
Σx|y
=
x
μx|y
=
Σx|y


=
Σx|y

y (y −b)

Digression: The Wishart distribution *
The Wishart distribution is the generalization of the Gamma distribution to positive deﬁnite
variate) normal distribution in order of importance and usefuleness in multivariate statistics”.
We will mostly use it to model our uncertainty in covariance matrices, Σ, or their inverses,
The pdf of the Wishart is deﬁned as follows:
Wi(Λ|S, ν)
=
ZWi


Here ν is called the “degrees of freedom” and S is the “scale matrix”.
(We shall get more
intuition for these parameters shortly.) The normalization constant for this distribution (whichrequires integrating over all symmetric pd matrices) is the following formidable expression
ZWi
=
where ΓD(a) is the multivariate gamma function:
D


Hence Γ1(a) = Γ(a) and
D


)
There is a connection between the Wishart distribution and the Gaussian.
In particular,
Then the scatter matrix S = N
i
has a Wishart distribution:
Wi(S, ν) are given by
Inverse Wishart distribution
λ ∼IG(a, b). Similarly,
IW(Σ|S, ν)
=
ZIW


ZIW
=
One can show that the distribution has these properties
mean =
mode =(a)
σ2
σ2
(b)
Visualization of the Wishart distribution. Left: Some samples from the Wishart distribution,
there is a lot of uncertainty about the value of the correlation coefficient ρ (see the almost uniform
increases, the sampled matrices are more concentrated on the prior S. Figure generated by wiPlotDemo.
Visualizing the Wishart distribution *
Since the Wishart is a distribution over matrices, it is hard to plot as a density function. However,
we can easily sample from it, and in the 2d case, we can use the eigenvectors of the resulting
For higher dimensional matrices, we can plot marginals of the distribution. The diagonals of
a Wishart distributed matrix have Gamma distributions, so are easy to plot. It is hard in general
the distribution, and then compute the distribution empirically. In particular, we can convert
each sampled matrix to a correlation matrix, and thus compute a Monte Carlo approximation
S
S

R(Σ(s))ij
where Σ(s) ∼Wi(Σ, ν) and R(Σ) converts matrix Σ into a correlation matrix:
Rij =
Σij
	
ΣiiΣjj
Inferring the parameters of an MVN
known. We now discuss how to infer the parameters themselves. We will assume the data hassimplify the presentation, we derive the posterior in three parts: ﬁrst we compute p(μ|D, Σ);
then we compute p(Σ|D, μ); ﬁnally we compute the joint p(μ, Σ|D).
which is useful for modeling our uncertainty about its value.
The likelihood has the form
N Σ)
For simplicity, we will use a conjugate prior, which in this case is a Gaussian. In particular, if
p(μ|D, Σ)
=
N(μ|mN, VN)
N
=
mN
=
This is exactly the same process as inferring the location of an object based on noisy radar
“blips”, except now we are inferring the mean of a distribution based on noisy samples. (To a
Bayesian, there is no difference between uncertainty about parameters and uncertainty about
anything else.)
We can model an uninformative prior by setting V0 = ∞I. In this case we have p(μ|D, Σ) =
N Σ), so the posterior mean is equal to the MLE. We also see that the posterior variance
Posterior distribution of Σ *
We now discuss how to compute p(Σ|D, μ). The likelihood has the form
p(D|μ, Σ)
∝
|Σ|−N


Recall that this has the following pdf:
∝


of the prior, and hence plays a role analogous to the sample size N.eigenvalue
 
 
eigenvalue
 
 
eigenvalue
 
 
We plot the eigenvalues in descending order for the true covariance matrix (solid black), the MLE (dotted
by shrinkcovDemo.
Multiplying the likelihood and prior we ﬁnd that the posterior is also inverse Wishart:
p(Σ|D, μ)
∝
|Σ|−N


exp

2tr(Σ−1S0)

=
exp

2tr


=
IW(Σ|SN, νN)
νN
=
ν0 + N
N
=
In words, this says that the posterior strength νN is the prior strength ν0 plus the number of
observations N, and the posterior scatter matrix SN is the prior scatter matrix S0 plus the data
MAP estimation
If N < D, this is not
full rank, and hence will be uninvertible. And even if N > D, it may be the case that ˆΣ is
To solve these problems, we can use the posterior mode (or mean). One can show (using
techniques analogous to the derivation of the MLE) that the MAP estimate is given by
ˆΣmap =
SN
N0 + N
MLE.as a convex combination of the prior mode and the MLE. To see this, let Σ0 ≜S0
N0 be the prior
mode. Then the posterior mode can be rewritten as
ˆΣmap = S0 + Sx
N0 + N =
N0
N0 + N
S0
N0
+
N
N0 + N
S
N0
This begs the question: where do the parameters of the prior come from? It is common to
use squared loss. This is arguably not the most natural loss function for covariance matrices
(because it ignores the postive deﬁnite constraint), but it results in a simple estimator, which
later.
As for the prior covariance matrix, S0, it is common to use the following (data dependent)
prior: S0 = diag( ˆΣmle). In this case, the MAP estimate is given by
ˆΣmap(i, j) =
 ˆΣmle(i, j)
if i = j
otherwise
estimation, or regularized estimation.
of the MAP estimate is much closer to that of the true matrix than the MLE’s. The eigenvectors,
however, are unaffected.
The importance of regularizing the estimate of Σ will become apparent in later chapters,
when we consider ﬁtting covariance matrices to high dimensional data.
Univariate posterior
In the 1d case, the likelihood has the form
p(D|σ2)
∝

2σ2
N


The standard conjugate prior is the inverse Gamma distribution, which is just the scalar version
of the inverse Wishart:
σ2 )σ2
 
 
Sequential updating of the posterior for σ2 starting from an uninformative prior. The data
by gaussSeqUpdateSigma1D.
Multiplying the likelihood and the prior, we see that the posterior is also IG:
p(σ2|D)
=
IG(σ2|aN, bN)
aN
=
bN
=
N

The form of the posterior is not quite as pretty as the multivariate case, because of the
the IG(a0, b0) distribution is that the strength of the prior is encoded in both a0 and b0.
To avoid both of these problems, it is common (in the statistics literature) to use an alternative
This is deﬁned as follows:
2σ2 )
Here ν0 controls the strength of the prior, and σ2
prior, the posterior becomes
p(σ2|D, μ)
=
N)
νN
=
ν0 + N
σ2
N
=
ν0σ2
νN
We see that the posterior dof νN is the prior dof ν0 plus N, and the posterior sum of squares
νNσ2
N is the prior sum of squares ν0σ2
intuitive sense (since it corresponds to a zero virtual sample size).We now discuss how to compute p(μ, Σ|D). These results are a bit complex, but will prove
useful later on in this book. Feel free to skip this section on a ﬁrst reading.
Likelihood
The likelihood is given by
p(D|μ, Σ)
=

N


Now one can show that
N

=
Hence we can rewrite the likelihood as follows:
p(D|μ, Σ)
=

−N

exp

−N

We will use this form below.
Prior
The obvious prior to use is the following
p(μ, Σ) = N(μ|m0, V0)IW(Σ|S0, ν0)
the posterior.
conditionals, p(μ|Σ) and p(Σ|μ), are individually conjugate. To create a full conjugate prior,
distribution of the form
p(μ, Σ) = p(Σ)p(μ|Σ)NIW(μ, Σ|m0, κ0, ν0, S0) ≜
κ0
Σ) × IW(Σ|S0, ν0)
=
ZNIW
+
−κ0
,
exp

2tr(Σ−1S0)

=
ZNIW
× exp

−κ0
2tr(Σ−1S0)

ZNIW
=
where ΓD(a) is the multivariate Gamma function.
κ0 is how strongly we believe this prior; and S0 is (proportional to) our prior mean for Σ, and
One can show (Minka 2000f) that the (improper) uninformative prior has the form
lim
∝
∝
|Σ|−( D
must be large too. This makes sense intuitively, since if the data has large spread, it may be hard to pin down its mean.Posterior
p(μ, Σ|D)
=
NIW(μ, Σ|mN, κN, νN, SN)
mN
=
κ0m0 + Nx
κN
=
κ0
κ0 + N m0 +
N
κ0 + N x
κN
=
κ0 + N
νN
=
ν0 + N
SN
=
S0 + Sx +
κ0N
κ0 + N (x −m0)(x −m0)T
=
S0 + S + κ0m0mT
N
where we have deﬁned S ≜N
to update incrementally than the centered version).
This result is actually quite intuitive: the posterior mean is a convex combination of the prior
mean and the MLE, with “strength” κ0 + N; and the posterior scatter matrix SN is the prior
scatter matrix S0 plus the empirical scatter matrix Sx plus an extra term due to the uncertainty
in the mean (which creates its own virtual scatter matrix).
Posterior mode
The mode of the joint distribution has the following form:
argmax p(μ, Σ|D)
=
(mN,
SN
argmax p(μ, Σ|D)
=
(x,
S0 + Sx
denominator, because this is the mode of the joint, not the mode of the marginal.
Posterior marginals
The posterior marginal for Σ is simply
p(Σ|D)
=


The mode and mean of this marginal are given by
ˆΣmap =
SN
E [Σ] =
SN
p(μ|D)
=


p(μ, Σ|D)dΣ = T (μ|mN,
This follows from the fact that the Student distribution can be represented as a scaled mixtureThe NIχ2(m0, κ0, ν0, σ2
believe this; σ2
Posterior predictive
The posterior predictive is given by
p(x|D) = p(x, D)
p(D)
so it can be easily evaluated in terms of a ratio of marginal likelihoods.
p(x|D)
=

 

=
T (x|mN,
Posterior for scalar data
We now specialise the above results to the case where xi is 1d. These results are widely usedNIχ2(μ, σ2|m0, κ0, ν0, σ2
≜
∝

−ν0σ2
2σ2

for small values of σ2, which makes sense, since if the data is low variance, we will be able to
estimate its mean more reliably.
One can show that the posterior is given by
p(μ, σ2|D)
=
NIχ2(μ, σ2|mN, κN, νN, σ2
N)
mN
=
κ0m0 + Nx
κN
κN
=
κ0 + N
νN
=
ν0 + N
νNσ2
N
=
ν0σ2
N

Nκ0
The posterior marginal for σ2 is just
p(σ2|D)
=


N)
with the posterior mean given by E

σ2|D

=
νN
νN−2σ2
N.
mixture representation of the student:
p(μ|D)
=


p(μ, σ2|D)dσ2 = T (μ|mN, σ2
with the posterior mean given by E [μ|D] = mN.
Let us see how these results look if we use the following uninformative prior:
With this prior, the posterior has the form
p(μ, σ2|D)
=
N = s2)
where
s2
≜
N

N
mle
estimate of the variance.) Hence the marginal posterior for the mean is given by
p(μ|D) = T (μ|x, s2var [μ|D] =
νN
νN −2σ2
s2
N
The square root of this is called the standard error of the mean:
	
var [μ|D] ≈
s
√
N
√
N
A more common scenario is when we want to test if two paired samples have
the same mean. More precisely, suppose yi ∼N(μ1, σ2) and zi ∼N(μ2, σ2). We want to
as follows:
p(μ > μ0|D) =

 ∞
μ0
To calculate the posterior, we must specify a prior. Suppose we use an uninformative prior.
p(μ|D) = T (μ|x, s2
Now let us deﬁne the following t statistic:
t ≜x −μ0
√
N
where the denominator is the standard error of the mean. We see thatConnection with frequentist statistics *
If we use an uninformative prior, it turns out that the above Bayesian analysis gives the same
Speciﬁcally, from the above results, we see that
μ −x
	
This has the same form as the sampling distribution of the MLE:
μ −X
	
The reason is that the Student distribution is symmetric in its ﬁrst two arguments, so T (x|μ, σ2, ν) =
method. See bayesTtestDemo for an example.
Despite the superﬁcial similarity, these two results have a different interpretation: in the
Bayesian approach, μ is unknown and x is ﬁxed, whereas in the frequentist approach, X
More equivalences between frequentist and Bayesian inference
Sensor fusion with unknown precisions *
case where the precision of each measurement device is unknown. This generalizes the results
precision. The unknown precision case turns out to give qualitatively different results, yielding
reliability of the sources is unknown. Speciﬁcally, suppose we have two different measurement
y ). We
make two independent measurements with each device, which turn out to be
the posterior would be Gaussian:
p(μ|D, λx, λy)
=
N )
λN
=
mN
=
λxNxx + λyNyyx =
Nx
Nx
Ny
Ny
precision is the sum of the measurement precisions, and the posterior mean is a weighted sum
However, the measurement precisions are not known.
Initially we will estimate them by
ℓ(μ, λx, λy)
=

i

i
The MLE is obtained by solving the following simultaneous equations:
∂ℓ
∂μ
=
∂ℓ
∂λx
=
λx
Nx
Nx

∂ℓ
∂λy
=
λy
Ny
Ny

This gives
ˆμ
=
Nxˆλxx + Nyˆλyy
Nxˆλx + Nyˆλy
=
Nx

i
=
Ny

i
x
y, where s2
x =
Nx
Nx
y =
Ny
Ny
sensor according to its estimated precision.
Since sensor y was estimated to be much less
reliable than sensor x, we have E
)
μ|D, ˆλx, ˆλy
*
≈x, so we effectively ignore the y sensor.
Now we will adopt a Bayesian approach and integrate out the unknown precisions, rather
than trying to estimate them. That is, we compute
p(μ|D) ∝p(μ)


 

Since the x and y terms are symmetric, we will just focus on one of them. The key integral is
I =


∝


exp

−Nx

I =


We recognize this as proportional to the integral of an unnormalized Gamma density
Ga(λ|a, b) ∝λa−1e−λb
x. Hence the integral is proportional to the normalizing
constant of the Gamma distribution, Γ(a)b−a, so we get
I ∝


∝

x
and the posterior becomes
p(μ|D) ∝
reliable than the y one, and vice versa. The weight of the ﬁrst mode is larger, since the data
from the x sensor agree more with each other, so it seems slightly more likely that the x sensor
is the reliable one. (They obviously cannot both be reliable, since they disagree on the values
that they are reporting.) However, the Bayesian solution keeps open the possibility that the y
sensor is the more reliable one; from two measurements, we cannot tell, and choosing just the
narrow).
Exercises
by X).
not independent, since Y is a function of X.(a)
(b)
(b) Exact posterior.
Figure generated by
sensorFusionUnknownPrec.
Hint: use the deﬁnition of covariance
cov [X, Y ] = E [XY ] −E [X] E [Y ]
and the rule of iterated expectation
E [XY ] = E [E [XY |W]]

Hint: diagonalize Σ and use the fact that |Σ| = 
dimensional Gaussians in a transformed coordinate system. (You will need the change of variables formula.)
Finally, use the normalization constant for univariate Gaussians.
Σ =
 σ2
ρσ1σ2
ρσ1σ2
σ2

where ρ is the correlation coefficient. Show that the pdf is given by
p(x1, x2)
=
2πσ1σ2

exp

−
σ2
σ2
−2ρ(x1 −μ1)
σ1
(x2 −μ2)
σ2
raw
standarized
whitened
Consider a bivariate Gaussian distribution p(x1, x2) = N(x|μ, Σ) where
Σ =
 σ2
σ12
σ21
σ2

= σ1σ2
 σ1
σ2
ρ
ρ
σ2
σ1

where the correlation coefficient is given by
ρ ≜σ12
σ1σ2
a. What is P(X2|x1)? Simplify your answer by expressing it in terms of ρ, σ2, σ1, μ1,μ2 and x1.
mean and covariance. Plot your Gaussian as an ellipse (use gaussPlot2d), superimposing on your
your ﬁgure and code.
done by computing
xij−xj
σj
, where σj is the empirical std of dimension j. Standardize the data and
c. Whitening or sphereing the data means ensuring its empirical covariance matrix is proportional to
I, so the data is uncorrelated and of equal variance along each dimension.
This can be done by
moves from the right hand side to the left).
Suppose we have two sensors with known (and different) variances v1 and v2, but unknown (and the same)
i
∼N(μ, v1) from the ﬁrst sensor and n2 observationsi
∼N(μ, v2) from the second sensor.
variance) mercury thermometer.) Let D represent all the data from both sensors. What is the posterior
N(x −μ)(x −μ)T + κ0(μ −m0)(μ −m0)T
= κN(μ −mN)(μ −mN)T + κ0N
κN (x −m0)(x −m0)T
(Source: Jaakkola.)
BIC = log p(D|ˆθML) −d
where d is the number of free parameters in the model and N is the number of samples. In this question,
we will see how to use this to choose between a full covariance Gaussian and a Gaussian with a diagonal
covariance. Obviously a full covariance Gaussian has higher likelihood, but it may not be “worth” the extra
parameters if the improvement over a diagonal covariance matrix is too small. So we use the BIC score to
choose the model.
log p(D| ˆΣ, ˆμ)
=
−N

ˆΣ
−1ˆS

−N
|Σ|)
ˆS
=
N
N


(xi −x)(xi −x)T
where ˆS is the scatter matrix (empirical covariance), the trace of a matrix is the sum of its diagonals, and
we have used the trace trick.
a. Derive the BIC score for a Gaussian in D dimensions with full covariance matrix. Simplify your answer
as much as possible, exploiting the form of the MLE. Be sure to specify the number of free parameters
d.
b. Derive the BIC score for a Gaussian in D dimensions with a diagonal covariance matrix. Be sure to
specify the number of free parameters d. Hint: for the digaonal case, the ML estimate of Σ is the same
b =
c1
2c2 and w =
c2
4c2 + c0.(Source: DeGroot.)
n). (This is called a credible interval, and is the Bayesian analog of a
conﬁdence interval.) How big does n have to be to ensure
(Source: Jaakkola.)
Consider samples x1, . . . , xn from a Gaussian random variable with known variance σ2 and unknown
a. Calculate the MAP estimate ˆμMAP . You can state the result without proof. Alternatively, with a lot
more work, you can compute derivatives of the log posterior, set to zero and solve.
b. Show that as the number of samples n increase, the MAP estimate converges to the maximum likelihood
estimate.
c. Suppose n is small and ﬁxed. What does the MAP estimator converge to if we increase the prior
variance s2?
d. Suppose n is small and ﬁxed. What does the MAP estimator converge to if we decrease the prior
variance s2?
ˆΣ = Cn
=
n


(xi −mn)(xi −mn)T
It is clear that it takes O(nd2) time to compute Cn. If the data points arrive one at a time, it is more
efficient to incrementally update these estimates than to recompute from scratch.
a. Show that the covariance can be sequentially udpated as follows
n
Cn +
c. Show that we can sequentially update the precision matrix using
n

n
−
n
n

repeat here for convenience:
=d. What is the time complexity per update?
are MVN p(x|y = j) = N(x|μj, Σj). By Bayes rule, we have
as possible.
Form of Σj
Cov
Num parameters
Arbitrary
Σj
Shared
Σj = Σ
d
Shared, spherical
Σj = σ2I
data. Compute the misclassiﬁcation rate of both of these models on the training set. Turn in your numbers
and code.
c)
Let the parameter vectors be as follows:
Consider a generative classiﬁer with class conditional densities of the form N(x|μc, Σc). In LDA, we
assume Σc = Σ, and in QDA, each Σc is arbitrary.
interpretation of your result, if possible.
(Source: Jaakkola.) Suppose we train the following binary classiﬁers via maximum likelihood.
a. GaussI: A generative classiﬁer, where the class conditional densities are Gaussian, with both covariance
matrices set to I (identity matrix), i.e., p(x|y = c) = N(x|μc, I). We assume p(y) is uniform.
b. GaussX: as for GaussI, but the covariance matrices are unconstrained, i.e., p(x|y = c) = N(x|μc, Σc).c. LinLog: A logistic regression model with linear features.
d. QuadLog: A logistic regression model, using linear and quadratic features (i.e., polynomial basis function
After training we compute the performance of each model M on the training set as follows:
n
n


log p(yi|xi, ˆθ, M)
now want to compare the performance of each model. We will write L(M) ≤L(M ′) if model M must
have lower (or equal) log likelihood (on the training set) than M ′, for any training set (in other words, M is
worse than M ′, at least as far as training set logprob is concerned). For each of the following model pairs,
state whether L(M) ≤L(M ′), L(M) ≥L(M ′), or whether no such statement can be made (i.e., M
explain why.
a. GaussI, LinLog.
b. GaussX, QuadLog.
c. LinLog, QuadLog.
d. GaussI, QuadLog.
e. Now suppose we measure performance in terms of the average misclassiﬁcation rate on the training
set:
n
n


I(yi ̸= ˆy(xi))
Is it true in general that L(M) > L(M ′) implies that R(M) < R(M ′)? Explain why or why not.
a. Find the decision region
R1 = {x : p(x|μ1, σ1) ≥p(x|μ2, σ2)}
Sketch the result. Hint: draw the curves and ﬁnd where they intersect. Find both solutions of the
equation
p(x|μ1, σ1) = p(x|μ2, σ2)
x = −b ±
√
b2 −4ac
2aConsider a three category classiﬁcation problem. Let the prior probabilites:
Σ1 =

, Σ2 =

, Σ3 =

Classify the following points:
[Note: you can solve this exercise by hand or using a computer (matlab, R, whatever). In either case, show
a. Fit a Bayes classiﬁer to this data, using maximum likelihood estimation, i.e., estimate the parameters of
the class conditional likelihoods
p(x|y = c) = N(x; μc, σc)
and the class prior
p(y = c) = πc
make an arithmetic error).
prediction.)
c. What would be a simple way to extend this technique if you had multiple attributes per person, such
as height and weight? Write down your proposed model as an equation.Bayesian statistics
Introduction
We have now seen a variety of different probability models, and we have discussed how to
ﬁt them to data, i.e., we have discussed how to compute MAP parameter estimates ˆθ =
argmax p(θ|D), using a variety of different priors. We have also discussed how to compute
the full posterior p(θ|D), as well as the posterior predictive density, p(x|D), for certain special
cases (and in later chapters, we will discuss algorithms for the general case).
Using the posterior distribution to summarize everything we know about a set of unknown
variables is at the core of Bayesian statistics. In this chapter, we discuss this approach to
frequentist or classical statistics.
Summarizing posterior distributions
section, we discuss some simple quantities that can be derived from a probability distribution,
such as a posterior. These summary statistics are often easier to understand and visualize than
the full joint.
MAP estimation
We can easily compute a point estimate of an unknown quantity by computing the posterior
valued quantity, and the vector of posterior marginals is the best choice for a discrete quantity.
However, the posterior mode, aka the MAP estimate, is the most popular choice because it
reduces to an optimization problem, for which efficient algorithms often exist. Futhermore, MAP
Although this approach is computationally appealing, it is important to point out that there
are various drawbacks to MAP estimation, which we brieﬂy discuss below. This will provide
motivation for the more thoroughly Bayesian approach which we will study later in this chapter
(and elsewhere in this book).(a)
(b)
(a) A bimodal distribution in which the mode is very untypical of the distribution. The thin
blue vertical line is the mean, which is arguably a better summary of the distribution, since it is near the
majority of the probability mass. Figure generated by bimodalDemo. (b) A skewed distribution in which
the mode is quite different from the mean. Figure generated by gammaPlotDemo.
No measure of uncertainty
The most obvious drawback of MAP estimation, and indeed of any other point estimate such
as the posterior mean or median, is that it does not provide any measure of uncertainty. In
many applications, it is important to know how much one can trust a given estimate. We can
Plugging in the MAP estimate can result in overﬁtting
In machine learning, we often care more about predictive accuracy than in interpreting the
parameters of our models. However, if we don’t model the uncertainty in our parameters, then
and we will see more examples later. Overconﬁdence in predictions is particularly problematic
The mode is an untypical point
Choosing the mode as a summary of a posterior distribution is often a very poor choice, since
the mode is usually quite untypical of the distribution, unlike the mean or median. This is
point of measure zero, whereas the mean and median take the volume of the space into account.
skewed distributions often arise when inferring variance parameters, especially in hierarchical
models. In such cases the MAP estimate (and hence the MLE) is obviously a very bad estimate.
How should we summarize a posterior if the mode is not a good choice? The answer is to
get “points” if you make no errors, otherwise you get nothing: there is no “partial credit” underpX
pY
g
Example of the transformation of a density under a nonlinear transform. Note how the mode
2006b). Figure generated by bayesChangeOfVar.
the posterior median.
MAP estimation is not invariant to reparameterization *
rameterize the probability distribution. Changing from one representation to another equivalent
representation changes the result, which is not very desirable, since the units of measurement
are arbitrary (e.g., when measuring distance, we can use centimetres or inches).
To understand the problem, suppose we compute the posterior for x. If we deﬁne y = f(x),
py(y) = px(x)
--dx
dy
--
The | dx
dy | term is called the Jacobian, and it measures the change in size of a unit volume passed
through f. Let ˆx = argmaxx px(x) be the MAP estimate for x. In general it is not the case
f(x) =
nonlinearity. In particular, we see that the mode of the transformed distribution is not equal to
the transform of the original mode.To see how this problem arises in the context of MAP estimation, consider the following
example, due to Michael Jordan. The Bernoulli distribution is typically parameterized by its
parameterizations can pick different points in this interval arbitrarily.
ˆθMAP = arg max
dφ
ˆφMAP = arg max
Thus the MAP estimate depends on the parameterization. The MLE does not suffer from this
since the likelihood is a function, not a probability density. Bayesian inference does not suffer
from this problem either, since the change of measure is taken into account when integrating
over the parameter space.
One solution to the problem is to optimize the following objective function:
ˆθ = argmax
θ
This
appeal of the whole approach.
Credible intervals
In addition to point estimates, we often want a measure of conﬁdence. A standard measure of
tail; this is called a central interval.(a)
(b)
If the posterior has a known functional form, we can compute the posterior central interval
Of course, the posterior is not always Gaussian. For example, in our coin example, if we
this).
If we don’t know the functional form, but we can draw samples from the posterior, then we
can use a Monte Carlo approximation to the posterior quantiles: we simply sort the S samples,
to the true quantile. See mcQuantileDemo for a demo.
usually what people want to compute, but conﬁdence intervals are usually what they actually
nately, the mechanics of computing a credible interval is just as easy as computing a conﬁdence
interval (see e.g., betaCredibleInt for how to do it in Matlab).
Highest posterior density regions *
A problem with central intervals is that there might be points outside the CI which have higher
This motivates an alternative quantity known as the highest posterior density or HPD region.(a)
pMIN
(b)
(a) Central interval and (b) HPD region for a hypothetical multimodal posterior. Based on
probability mass. More formally, we ﬁnd the threshold p∗on the pdf such that


and then deﬁne the HPD as
Cα(D) = {θ : p(θ|D) ≥p∗}
In 1d, the HPD region is sometimes called a highest density interval or HDI. For example,
point inside of it has higher density than every point outside of it.
of the mass and has minimal width. This can be done by 1d numerical optimization if we know
the inverse CDF of the distribution, or by search over the sorted data points if we have a bag of
samples (see betaHPD for a demo).
for an example. However, summarizing multimodal posteriors is always difficult.
Inference for a difference in proportions
Sometimes we have multiple parameters, and we are interested in computing the posterior
distribution of some function of these parameters. For example, suppose you are about to buy 
 
p(θ1|data)
p(θ2|data)
(a)
δ
pdf
(b)
(a) Exact posteriors p(θi|Di). (b) Monte Carlo approximation to p(δ|D). We use kernel density
amazonSellerDemo,
better since it has had so few reviews. In this section, we sketch a Bayesian analysis of this
problem. Similar methodology can be used to compare rates or proportions across groups for a
variety of other settings.
can compute the desired quantity using numerical integration:
=


See
amazonSellerDemo for the code. (It is also possible to solve the integral analytically (Cook
A simpler way to solve the problem is to approximate the posterior p(δ|D) by Monte Carlo
distributions, which can be sampled from using standard methods. The distributions p(θi|Di)
amazonSellerDemo for the code.)
Bayesian model selectiona regularization parameter results in overﬁtting, and too large a value results in underﬁtting. In
general, when faced with a set of models (i.e., families of parametric distributions) of different
complexity, how should we choose the best one? This is called the model selection problem.
models, and then to pick the model that seems the best. However, this requires ﬁtting each
model K times, where K is the number of CV folds. A more efficient approach is to compute
the posterior over models,
p(m|D) =
p(D|m)p(m)

From this, we can easily compute the MAP model, ˆm = argmax p(m|D).
This is called
Bayesian model selection.
maximizes
p(D|m) =


This quantity is called the marginal likelihood, the integrated likelihood, or the evidence for
ﬁrst we give an intuitive interpretation of what this quantity means.
Bayesian Occam’s razor
One might think that using p(D|m) to select models would always favor the model with the
most parameters. This is true if we use p(D|ˆθm) to select models, where ˆθm is the MLE or
MAP estimate of the parameters for model m, because models with more parameters will ﬁt the
data better, and hence achieve higher likelihood. However, if we integrate out the parameters,
rather than maximizing them, we are automatically protected from overﬁtting: models with
more parameters do not necessarily have higher marginal likelihood. This is called the Bayesian
known as Occam’s razor, which says one should pick the simplest model that adequately
explains the data.
One way to understand the Bayesian Occam’s razor is to notice that the marginal likelihood
all the previous ones. (Of course, the order of the data does not matter in the above expression.)
If a model is too complex, it will overﬁt the “early” examples and will then predict the remaining
ones poorly.
Another way to understand the Bayesian Occam’s razor effect is to note that probabilities must
sum to one. Hence 
models, which can predict many things, must spread their probability mass thinly, and hence
will not obtain as large a probability for any given data set as simpler models. This is sometimesA schematic illustration of the Bayesian Occam’s razor. The broad (green) curve corresponds
to a complex model, the narrow (blue) curve to a simple model, and the middle (red) curve is just right.
produced on real data.
horizontal axis we plot all possible data sets in order of increasing complexity (measured in
one, M1; a medium one, M2; and a complex one, M3. We also indicate the actually observed
also assigns D0 relatively low probability, because it can predict many data sets, and hence it
data with a reasonable degree of conﬁdence, but does not predict too many other things. Hence
quadratic).
that the maximum evidence occurs at roughly the same point as the minimum of the test MSE,
which also corresponds to the point chosen by CV.
When using the Bayesian approach, we are not restricted to evaluating the evidence at a
CV estimate, but it can be computed more efficiently.(a)
(b)
(c)
M
P(M|D)
(d)
Bayes. The solid green curve is the true function, the dashed red curve is the prediction (dotted blue lines
represent ±σ around the mean). (d) We plot the posterior over models, p(d|D), assuming a uniform prior
Computing the marginal likelihood (evidence)
When discussing parameter inference for a ﬁxed model, we often wrote
p(θ|D, m) ∝p(θ|m)p(D|θ, m)
However, when comparing models, we need to know how to compute the marginal likelihood,
p(D|m). In general, this can be quite hard, since we have to integrate over all possible parameter
values, but when we have a conjugate prior, it is easy to compute, as we now show.
Let p(θ) = q(θ)/Z0 be our prior, where q(θ) is an unnormalized distribution, and Z0 is
the normalization constant of the prior. Let p(D|θ) = q(D|θ)/Zℓbe the likelihood, where Zℓ(a)
(b)
(c)
M
P(M|D)
(d)
rior, where q(θ|D) = q(D|θ)q(θ) is the unnormalized posterior, and ZN is the normalization
constant of the posterior. We have
p(θ|D)
=
p(D|θ)p(θ)
p(D)
q(θ|D)
ZN
=
q(D|θ)q(θ)
ZℓZ0p(D)
p(D)
=
ZN
Z0Zℓ
So assuming the relevant normalization constants are tractable, we have an easy way to compute
the marginal likelihood. We give some examples below.where a′ = a + N1 and b′ = b + N0, we know the normalization constant of the posterior is
B(a′, b′). Hence
p(θ|D)
=
p(D|θ)p(θ)
p(D)
=
p(D)

 
N
N1


=

N
N1

p(D)
B(a, b)

So
B(a + N1, b + N0)
=

N
N1

p(D)
B(a, b)
p(D)
=

N
N1
 B(a + N1, b + N0)
B(a, b)
the
 N
N1

term.
p(D) = B(N + α)
B(α)
where
B(α) =
K
Γ(
k αk)
Hence we can rewrite the above result in the following form, which is what is usually presented
in the literature:
p(D) =
Γ(
k αk)
Γ(N + 
k αk)


k
Γ(Nk + αk)
Γ(αk)
We will see many applications of this equation later.
Consider the case of an MVN with a conjugate NIW prior. Let Z0 be the normalizer for thelikelihood. Then it is easy to see that
p(D)
=
ZN
Z0Zl
=
+
2π
κN
+
2π
κ0
=
 κ0
κN
This equation will prove useful later.
BIC approximation to log marginal likelihood
approximation is known as the Bayesian information criterion or BIC, which has the following
BIC ≜log p(D|ˆθ) −dof(ˆθ)
where dof(ˆθ) is the number of degrees of freedom in the model, and ˆθ is the MLE for the
log likelihood is given by
log p(D|ˆθ) = −N
Hence the BIC score is as follows (dropping constant terms)
BIC = −N
where D is the number of variables in the model. In the statistics literature, it is common to
use an alternative deﬁnition of BIC, which we call the BIC cost (since we want to minimize it):
In the context of linear regression, this becomes
such as mixtures of Gaussians, the ML estimate can be poorly behaved, so it is better to evaluate the BIC score usingThe BIC method is very closely related to the minimum description length or MDL principle,
which characterizes the score for a model in terms of how well it ﬁts the data, minus how
AIC, deﬁned as
AIC(m, D) ≜log p(D|ˆθMLE) −dof(m)
This is derived from a frequentist framework, and cannot be interpreted as an approximation
to the marginal likelihood. Nevertheless, the form of this expression is very similar to BIC. We
see that the penalty for AIC is less than for BIC. This causes AIC to pick more complex models.
further discussion on such information criteria.
Effect of the prior
Sometimes it is not clear how to set the prior. When we are performing posterior inference, the
details of the prior may not matter too much, since the likelihood often overwhelms the prior
anyway. But when computing the marginal likelihood, the prior plays a much more important
role, since we are averaging the likelihood over all possible parameter settings, as weighted by
the prior.
α is large, the weights are “forced” to be small, so we need to use a complex model with many
small parameters (e.g., a high degree polynomial) to ﬁt the data. Conversely, if α is small, we
will favor simpler models, since each parameter is “allowed” to vary in magnitude by a lot.
If the prior is unknown, the correct Bayesian procedure is to put a prior on the prior. That is,
marginal likelihood, we should integrate out all unknowns, i.e., we should compute
p(D|m) =

 

p(D|w)p(w|α, m)p(α|m)dwdα
Bayesian hierarchy, the less sensitive are the results to the prior settings. So we can usually
A computational shortcut is to optimize α rather than integrating it out. That is, we use
p(D|m) ≈


p(D|w)p(w|ˆα, m)dw
where
ˆα = argmax
α
p(D|α, m) = argmax
α


p(D|w)p(w|α, m)dwInterpretation
BF <
Decisive evidence for M0
BF <
Strong evidence for M0
Moderate evidence for M0
Weak evidence for M0
Weak evidence for M1
Moderate evidence for M1
Strong evidence for M1
Decisive evidence for M1
Jeffreys’ scale of evidence for interpreting Bayes factors.
Bayes factors
picking the model with the highest marginal likelihood. Now suppose we just have two models
we are considering, call them the null hypothesis, M0, and the alternative hypothesis, M1.
Deﬁne the Bayes factor as the ratio of marginal likelihoods:
p(D|M0) = p(M1|D)
p(M0|D)/p(M1)
p(M0)
(This is like a likelihood ratio, except we integrate out the parameters, which allows us to
p(M0|D) =
=
Example: Testing if a coin is fair
Suppose we observe some coin tosses, and want to decide if the data was generated by a fair
the ﬁrst model by M0 and the second model by M1. The marginal likelihood under M0 is
simply
p(D|M0) =
N
H0). Note that has almost nothing to do with what we really want to know, which is p(H0|D).log10 p(D|M1)
(a)
BIC approximation to log10 p(D|M1)
(b)
(a) Log marginal likelihood for the coins example. (b) BIC approximation. Figure generated by
coinsModelSelDemo.
where N is the number of coin tosses. The marginal likelihood under M1, using a Beta prior, is
p(D|M1)
=


B(α1, α0)
is a simpler model (it has no free parameters) — it would be a suspicious coincidence if the
counts become more extreme, we favor the biased coin hypothesis. Note that, if we plot the log
marginal likelihood, which is all that matters for model selection purposes, since the absolute
scale is irrelevant. In particular, it favors the simpler model unless the data is overwhelmingly
in support of the more complex model.
example, consider testing the hypotheses M0 : θ ∈Θ0 vs M1 : θ ∈Θ1. To deﬁne the marginal
p(θ)
=
p(θ|M0)p(M0) + p(θ|M1)p(M1)This is only meaningful if p(θ|M0) and p(θ|M1) are proper (normalized) density functions. In
this case, the posterior is given by
p(M0|D)
=
p(M0)p(D|M0)
p(M0)p(D|M0) + p(M1)p(D|M1)
=
p(M0)

p(M0)


Now suppose we use improper priors, p(θ|M0) ∝c0 and p(θ|M1) ∝c1. Then
p(M0|D)
=
p(M0)c0

p(M0)c0


=
p(M0)c0ℓ0
p(M0)c0ℓ0 + p(M1)c1ℓ1
where ℓi =

p(M0|D)
=
c0ℓ0
c0ℓ0 + c1ℓ1
=
ℓ0
Thus we can change the posterior arbitrarily by choosing c1 and c0 as we please. Note that
using proper, but very vague, priors can cause similar problems. In particular, the Bayes factor
will always favor the simpler model, since the probability of the observed data under a complex
Thus it is important to use proper priors when performing model selection. Note, however,
that, if M0 and M1 share the same prior over a subset of the parameters, this part of the prior
can be improper, since the corresponding normalization constant will cancel out.
Priors
The most controversial aspect of Bayesian statistics is its reliance on priors. Bayesians argue
this is unavoidable, since nobody is a tabula rasa or blank slate: all inference must be done
conditional on certain assumptions about the world. Nevertheless, one might be interested in
minimizing the impact of one’s prior assumptions. We brieﬂy discuss some ways to do this
below.
Uninformative priors
The issue of designing uninformative priors is actually somewhat tricky.
As an example
this case is E [θ|D] =
N1
prior wasn’t completely uninformative after all.Clearly by decreasing the magnitude of the pseudo counts, we can lessen the impact of the
lim
called the Haldane prior. Note that the Haldane prior is an improper prior, meaning it does not
will be proper.
Clearly the difference in practice between these three priors is very likely negligible. In general,
it is advisable to perform some kind of sensitivity analysis, in which one checks how much
one’s conclusions or predictions change in response to change in the modeling assumptions,
processing. If the conclusions are relatively insensitive to the modeling assumptions, one can
have more conﬁdence in the results.
Jeffreys priors *
--dφ
--
so the prior will in general change. However, let us pick
pφ(φ) ∝(I(φ))
where I(φ) is the Fisher information:
I(φ) ≜−E
d log p(X|φ)
dφ


This is a measure of curvature of the expected negative log likelihood and hence a measure of
d log p(x|θ)
=
d log p(x|φ)
dφ
dφ
Squaring and taking expectations over x, we have
I(θ)
=
−E
d log p(X|θ)
= I(φ)
dφ
I(θ)
=
I(φ)
--so we ﬁnd the transformed prior is
=
pφ(φ)
--dφ
-- ∝(I(φ))
-- = I(θ)
Some examples will make this clearer.
Example: Jeffreys prior for the Bernoulli and multinoulli
Suppose X ∼Ber(θ). The log likelihood for a single sample is
s(θ)
≜
d
J(θ)
=
−d2
The Fisher information is the expected information:
I(θ)
=
E[J(θ|X)|X ∼θ] = θ
θ2 +
Hence Jeffreys’ prior is
	
Now consider a multinoulli random variable with K states. One can show that the Jeffreys’
prior is given by
Example: Jeffreys prior for location and scale parameters
One can show that the Jeffreys prior for a location parameter, such as the Gaussian mean, is
that the probability mass assigned to any interval, [A, B] is the same as that assigned to any
other shifted interval of the same width, such as [A −c, B −c]. That is,

 B−c
A−c

 B
Aas we have seen a single data point.
Similarly, one can show that the Jeffreys prior for a scale parameter, such as the Gaussian
property that the probability mass assigned to any interval [A, B] is the same as that assigned
example, if we change units from meters to feet we do not want that to affect our inferences.)
This can be achieved by using
To see this, note that

p(s)ds
=
=
log(B) −log(A) =

 B
A
p(s)ds
Robust priors
In many cases, we are not very conﬁdent in our prior, so we want to make sure it does not have
an undue inﬂuence on the result. This can be done by using robust priors (Insua and Ruggeri
mean.
constraints.
satisfactory.
our example. But this time we ﬁnd (using numerical method integration: see robustPriorDemo
Mixtures of conjugate priors
Robust priors are useful, but can be computationally expensive to use. Conjugate priors simplifysuch priors provide a good compromise between computational convenience and ﬂexibility.
For example, suppose we are modeling coin tosses, and we think the coin is either fair, or
is biased towards heads. This cannot be represented by a beta distribution. However, we can
model it using a mixture of two beta distributions. For example, we might use
biased towards heads.
We can represent a mixture by introducing a latent indicator variable z, where z = k means
p(θ) =

k
p(z = k)p(θ|z = k)
where each p(θ|z = k) is conjugate, and p(z = k) are called the (prior) mixing weights. One can
as follows:
p(θ|D)
=

k
p(z = k|D)p(θ|D, z = k)
where p(Z = k|D) are the posterior mixing weights given by
p(Z = k|D)
=
p(Z = k)p(D|Z = k)

k′ p(Z = k′)p(D|Z = k′)
Example
Suppose we use the mixture prior
becomesmixture of Beta distributions
 
 
prior
posterior
A mixture of two Beta distributions. Figure generated by mixBetaDemo.
Application: Finding conserved regions in DNA and protein sequences
us give a simple example to illustrate some of the machinery that has developed. Speciﬁcally,
which represent coding regions of the genome. Such locations often have the same letter across
all sequences, because of evolutionary pressure. So we need to ﬁnd columns which are “pure”,
or nearly so, in the sense that they are mostly all As, mostly all Ts, mostly all Cs, or mostly all
nearly deterministic (pure).
But suppose we want to associate a conﬁdence measure with our estimates of purity. This
can be useful if we believe adjacent locations are conserved together. In this case, we can let
In any case, we need to deﬁne a likelihood model, p(Nt|Zt), where Nt is the vector of
(A,C,G,T) counts for column t. It is natural to make this be a multinomial distribution with
and thus compute the marginal likelihood
p(Nt|Zt) =


be a (nearly) pure column of As, Cs, Gs, or Ts. A natural approach is to use a mixture of Dirichlet
e.g.,
Since this is conjugate, we can easily compute p(Nt|Zt).Hierarchical Bayes
A key requirement for computing the posterior p(θ|D) is the speciﬁcation of a prior p(θ|η),
use uninformative priors, we we discussed above. A more Bayesian approach is to put a prior on
η →θ →D
there are multiple levels of unknown quantities. We give a simple example below, and we will
see many others later in the book.
Example: modeling related cancer rates
Consider the problem of predicting cancer rates in various cities (this example is from (Johnson
cities, Ni, and the number of people who died of cancer in these cities, xi.
We assume
them all separately, but this will suffer from the sparse data problem (underestimation of the
called parameter tying. The resulting pooled MLE is just ˆθ =

i xi

i Ni . But the assumption that
all the cities have the same rate is a rather strong one. A compromise approach is to assume
distribution can be written as
p(D, θ, η|N)
=
p(η)
N


Bin(xi|Ni, θi)Beta(θi|η)
where η = (a, b).
Note that it is crucial that we infer η = (a, b) from the data; if we just clamp it to a constant,
Suppose we compute the joint posterior p(η, θ|D).
From this we can get the posterior(a)
(b)
axes of the ﬁrst two rows so that the differences between the other cities are visible. Third row: MLE ˆθi.
cancerRatesEb
has the largest impact on the posterior estimate of η, which in turn will impact the estimate of
highest posterior uncertainty, reﬂecting the fact that such a high estimate is in conﬂict with the
prior (which is estimated from all the other cities).
In the above example, we have one parameter per city, modeling the probability the response
is on. By making the Bernoulli rate parameter be a function of covariates, θi = sigm(wT
i x), we
Empirical Bayes
In hierarchical Bayesian models, we need to compute the posterior on multiple levels of latent
p(η, θ|D) ∝p(D|θ)p(θ|η)p(η)
just computing p(η|D).
prior on η. Then the estimate becomes
ˆη = argmax p(D|η) = argmax


where the quantity inside the brackets is the marginal or integrated likelihood, sometimes called
the evidence.
likelihood. In machine learning, it is sometimes called the evidence procedure.
Empirical Bayes violates the principle that the prior should be chosen independently of the
data. However, we can just view it as a computationally cheap approximation to inference in a
hierarchical Bayesian model, just as we viewed MAP estimation as an approximation to inference
one performs, the “more Bayesian” one becomes:
Method
Deﬁnition
Maximum likelihood
MAP estimation
ˆη = argmaxη

ˆη = argmaxη

Full Bayes
p(θ, η|D) ∝p(D|θ)p(θ|η)p(η)
Note that EB can be shown to have good frequentist properties (see e.g., (Carlin and Louis
down the marginal likelihood directly, as follows:
p(D|a, b)
=


i


=


i
B(a + xi, b + Ni −xi)
B(a, b)
Various ways of maximizing this wrt a and b are discussed in (Minka 2000e).
p(θi|ˆa,ˆb, D) in the usual way, using conjugate analysis. The net result is that the posterior
We now study another example that is analogous to the cancer rates example, except the data is
down the solution analytically.
In particular, suppose we have data from multiple related groups. For example, xij could be
the mean score for each school, θj. However, since the sample size, Nj, may be small forsome schools, we can regularize the problem by using a hierarchical Bayesian model, where we
The joint distribution has the following form:
p(θ, D|η, σ2) =
D


Nj


N(xij|θj, σ2)
explain how to estimate η below. Once we have estimated η = (μ, τ), we can compute the
following form, exploiting the fact that Nj Gaussian measurements with values xij and variance
σ2 are equivalent to one measurement of value xj ≜
Nj
Nj
This yields
p(θ, D|ˆη, σ2) =
D


j )
=
j )
ˆBj
≜
σ2
j
σ2
the data is reliable for group j (e.g., because the sample size Nj is large), then σ2
j will be small
However, groups with small sample sizes will get regularized (shrunk towards the overall mean
μ) more heavily. We will see an example of this below.
If σj = σ for all groups j, the posterior mean becomes
ˆθj
=
Example: predicting baseball scores
We now give an example of shrinkage applied to baseball batting averages, from (Efron and
the empirical batting average. However, we can use an EB approach to do better.
To apply the Gaussian shrinkage approach described above, we require that the likelihood beMLE (top) and shrinkage estimates (bottom)
(a)
player number
MSE
true
shrunk
MLE
(b)
(a) MLE parameters (top) and corresponding shrunken estimates (bottom). (b) We plot the
generated by shrinkageDemoBaseball.
since xj already represents the average for player j.)
However, in this example we have a
binomial likelihood. While this has the right mean, E [xj] = θj, the variance is not constant:
tion:
yj = f(yj) =
√
√
from a large number of independent games.) We see that, on average, the shrunken estimate
is much closer to the true parameters than the MLE is. Speciﬁcally, the mean squared error,
N
D
θj than using the MLEs ˆθj.
In this section, we give an algorithm for estimating η. Suppose initially that σ2
j = σ2 is the
same for all groups. In this case, we can derive the EB estimate in closed form, as we now show.
=


Let Y = f(X).
f(μ) + (X −μ)f′(μ). Hence var [Y ] ≈f′(μ)2var [X −μ] = f′(μ)2σ2(μ). A variance stabilizing transformationHence the marginal likelihood is
D


have
D
D

xj = x
which is the overall mean.
For the variance, we can use moment matching (which is equivalent to the MLE for a
Gaussian): we simply equate the model variance to the empirical variance:
D
D

estimate:
Hence the shrinkage factor is
ˆB =
σ2
σ2
σ2 + (s2 −σ2)+
In the case where the σ2
j ’s are different, we can no longer derive a solution in closed form.
discusses how to perform full Bayesian inference in this hierarchical model.
Bayesian decision theory
We have seen how probability theory can be used to represent and updates our beliefs about
the state of the world. However, ultimately our goal is to convert our beliefs into actions. In this
section, we discuss the optimal way to do this.
We can formalize any given statistical decision problem as a game against nature (as opposed
to a game against other strategic players, which is the topic of game theory, see e.g., (Shoham
then have to make a decision, that is, we have to choose an action a from some action space
A. Finally we incur some loss, L(y, a), which measures how compatible our action a is with
nature’s hidden state y. For example, we might use misclassiﬁcation loss, L(y, a) = I(y ̸= a),optimal action for each possible input. By optimal, we mean the action that minimizes the
expected loss:
δ(x) = argmin
E [L(y, a)]
In economics, it is more common to talk of a utility function; this is just negative loss,
U(y, a) = −L(y, a). Thus the above rule becomes
δ(x) = argmax
E [U(y, a)]
This is called the maximum expected utility principle, and is the essence of what we mean
by rational behavior.
Note that there are two different interpretations of what we mean by “expected”.
In the
Bayesian version, which we discuss below, we mean the expected value of y given the data we
expected value of y and x that we expect to see in the future.
In the Bayesian approach to decision theory, the optimal action, having observed x, is deﬁned
as the action a that minimizes the posterior expected loss:
ρ(a|x) ≜Ep(y|x) [L(y, a)] =

y
L(y, a)p(y|x)
(If y is continuous (e.g., when we want to estimate a parameter vector), we should replace the
sum with an integral.) Hence the Bayes estimator, also called the Bayes decision rule, is given
by
δ(x) = arg min
Bayes estimators for common loss functions
In this section we show how to construct Bayes estimators for the loss functions most commonly
arising in machine learning.
L(y, a) = I(y ̸= a) =
if a = y
if a ̸= y
This is commonly used in classiﬁcation problems where y is the true class label and a = ˆy is
the estimate.
For example, in the two class case, we can write the loss matrix as follows:

	

	


		









For some regions of input space, where the class posteriors are uncertain, we may prefer not
The posterior expected loss is
Hence the action that minimizes the expected loss is the posterior mode or MAP estimate
y∗(x) = arg max
Reject option
In classiﬁcation problems where p(y|x) is very uncertain, we may prefer to choose a reject
action, in which we refuse to classify the example as any of the speciﬁed classes, and instead
for an illustration. This is useful in risk averse domains such as medicine and ﬁnance.
We can formalize the reject option as follows.
Suppose we deﬁne the loss function as
L(y = j, a = i) =
⎧
⎨
⎩
λr
λs
otherwise
you will show that the optimal action is to pick the reject action if the most probable class has
λs ; otherwise you should just pick the most probable class.(a)
(b)
(c)
generated by lossFunctionFig.
Posterior mean minimizes ℓ2 (quadratic) loss
For continuous parameters, a more appropriate loss function is squared error, ℓ2 loss, or
quadratic loss, deﬁned as
The posterior expected loss is given by
ρ(a|x)
=
E


= E

y2|x

−2aE [y|x] + a2
Hence the optimal estimate is the posterior mean:
∂


yp(y|x)dy
This is often called the minimum mean squared error estimate or MMSE estimate.
In a linear regression problem, we have
p(y|x, θ) = N(y|xT w, σ2)
In this case, the optimal estimate given some training data D is given by
E [y|x, D] = xT E [w|D]
thing to do no matter what prior we use for w.
Posterior median minimizes ℓ1 (absolute) loss
The ℓ2 loss penalizes deviations from the truth quadratically, and thus is sensitive to outliers. A
Supervised learning
which gives the cost of predicting y′ when the truth is y. We can deﬁne the loss incurred byof the data generating mechanism) as follows:
L(θ, δ) ≜E(x,y)∼p(x,y|θ) [ℓ(y, δ(x)] =

x

y
L(y, δ(x))p(x, y|θ)
This is known as the generalization error. Our goal is to minimize the posterior expected loss,
given by
ρ(δ|D) =


The false positive vs false negative tradeoff
following more general loss matrix:
LF N
LF P
where LF N is the cost of a false negative, and LF P is the cost of a false positive. The
posterior expected loss for the two possible actions is given by
=
=
>
>
LF P
LF N
to choose a speciﬁc threshold.
ROC curves and all that
Suppose we are solving a binary decision problem, such as classiﬁcation, hypothesis testing,Truth
Σ
Estimate
TP
FP
FN
TN
ˆN−= FN + TN
Σ
N−= FP + TN
N = TP + FP + FN + TN
“called” number of positives, N−is the true number of negatives, ˆ
N−is the “called” number of negatives.
Estimating p(ˆy|y) from a confusion matrix. Abbreviations: FNR = false negative rate, FPR =
false positive rate, TNR = true negative rate, TPR = true positive rate.
some threshold parameter. For each given value of τ, we can apply our decision rule and count
the number of true positives, false positives, true negatives, and false negatives that occur, as
From this table, we can compute the true positive rate (TPR), also known as the sensitivity,
false positive rate (FPR), also called the false alarm rate, or the type I error rate, by using
However, rather than than computing the TPR and FPR for a ﬁxed threshold τ, we can run
our detector for a set of thresholds, and then plot the TPR vs FPR as an implicit function of
as positive. If a system is performing at chance level, then we can achieve any point on the
diagonal line TPR = FPR by choosing an appropriate threshold. A system that perfectly
separates the positives from negatives has a threshold that can achieve the top left corner,
The quality of a ROC curve is often summarized as a single number using the area under the
statistic that is used is the equal error rate or EER, also called the cross over rate, deﬁned
EER by drawing a line from the top left to the bottom right and seeing where it intersects thefpr
tpr
A
B
(a)
recall
precision
A
B
(b)
(a) ROC curves for two hypothetical classiﬁcation systems. A is better than B. We plot the
true positive rate (TPR) vs the false positive rate (FPR) as we vary the threshold τ. We also indicate the
equal error rate (EER) with the red and blue dots, and the area under the curve (AUC) for classiﬁer B. (b)
PRhand.
Estimating p(y|ˆy) from a confusion matrix. Abbreviations: FDP = false discovery probability,
NPV = negative predictive value, PPV = positive predictive value,
Precision recall curves
When trying to detect a rare event (such as retrieving a relevant document or ﬁnding a face
Hence all the
“action” in the ROC curve will occur on the extreme left. In such cases, it is common to plot
the TPR versus the number of false positives, rather than vs the false positive rate.
the number of patches examined — and hence the number of true negatives — is a parameter
of the algorithm, not part of the problem deﬁnition. So we would like to use a measure that
only talks about positives.
using
P =

i yiˆyi

i ˆyi
, R =

i yiˆyi

i yi
A precision recall curve is a plot of precision vs recall as we vary the threshold τ.
See
This curve can be summarized as a single number using the mean precision (averaging overPooled
recall values), which approximates the area under the curve. Alternatively, one can quote the
This is called the average precision at K score. This measure is widely used when evaluating
information retrieval systems.
For a ﬁxed threshold, one can compute a single precision and recall value. These are often
combined into a single statistic called the F score, or F1 score, which is the harmonic mean of
precision and recall:
F1 ≜
R + P
F1 =
N
This is a widely used measure in information retrieval systems.
where F1(c) is the F1 score obtained on the task of distinguishing class c from all the others.
counts from each class’s contingency table.False discovery rates *
Suppose we are trying to discover a rare phenomenon using some kind of high throughput
measurement device, such as a gene expression micro array, or a radio telescope. We will need
be large. This is called multiple hypothesis testing. Note that the difference from standard
binary classiﬁcation is that we are classifying yi based on all the data, not just based on xi. So
this is a simultaneous classiﬁcation problem, where we might hope to do better than a series of
individual classiﬁcation problems.
How should we set the threshold τ? A natural approach is to try to minimize the expected
number of false positives. In the Bayesian approach, this can be computed as follows:
FD(τ, D) ≜

i
! "# $
pr. error
I(pi > τ)
!
"#
$
discovery
We then deﬁne the posterior expected false discovery rate as follows:
FDR(τ, D) ≜FD(τ, D)/N(τ, D)
where N(τ, D) = 
i I(pi > τ) is the number of discovered items.
Given a desired FDR
Other topics *
In this section, we brieﬂy mention a few other topics related to Bayesian decision theory. We do
not have space to go into detail, but we include pointers to the relevant literature.
Contextual bandits
The game is this: you insert some money, pull an arm, and wait for the machine to stop; if
you’re lucky, you win some money. Now imagine there is a bank of K such machines to choose
using Bayesian decision theory: there are K possible actions, and each action has an unknown
reward (payoff function) rk. By maintaining a belief state, p(r1:K|D) = 
k p(rk|D), one can
one should try each action before deciding to go with the winner.
Now consider an extension where each arm, and the player, has an associated feature vector;
to show to the user, and the features could represent properties of these ads or articles, suchas a bag of words, as well as properties of the user, such as demographics. If we assume a
linear model for reward, rk = θT
k x, we can maintain a distribution over the parameters of each
arm, p(θk|D), where D is a series of tuples of the form (a, x, r), which speciﬁes which arm
logistic regression models in later chapters.
Given the posterior, we must decide what action to take. One common heuristic, known as
UCB (which stands for “upper conﬁdence bound”) is to take the action which maximizes
k∗=
K
argmax
μk + λσk
and exploitation. The intuition is that we should pick actions about which we believe are good
An even simpler method, known as Thompson sampling, is as follows. At each step, we pick
action k with a probability that is equal to its probability of being the optimal action:
pk =


I(E [r|a, x, θ] = max
We can approximate this by drawing a single sample from the posterior, θt ∼p(θ|D), and then
choosing k∗= argmaxk E

r|x, k, θt
. Despite its simplicity, this has been shown to work quite
Utility theory
Suppose we are a doctor trying to decide whether to operate on a patient or not. We imagine
patient has breast cancer. Since the action and state space is discrete, we can represent the loss
function L(θ, a) as a loss matrix, such as the following:
Surgery
No surgery
No cancer
Lung cancer
Breast cancer
These numbers reﬂects the fact that not performing surgery when the patient has cancer is
It is natural to ask where these numbers come from. Ultimately they represent the personal
preferences or values of a ﬁctitious doctor, and are somewhat arbitrary: just as some peopleSequential decision theory
sequential decision problems. Such problems frequently arise in many business and engineering
settings. This is closely related to the problem of reinforcement learning. However, further
discussion of this point is beyond the scope of this book.
Exercises
Consider a case where we have learned a conditional probability distribution P(y|x). Suppose there are
predicted
true label y
label ˆy
λ01
λ10
a. Show that the decision ˆy that minimizes the expected loss is equivalent to setting a probability threshold
your work.)
In many classiﬁcation problems one has the option either of assigning x to class j or, if you are too
uncertain, of choosing the reject option. If the cost for rejects is less than the cost of falsely classifying
nature. Deﬁne the loss function as follows
λ(αi|Y = j) =
⎧
⎨
⎩
λr
λs
otherwise
tradeoffs, and one needs to use some kind of “currency” to compare different courses of action. Insurance companies
do this all the time. Ross Schachter, a decision theorist at Stanford University, likes to tell a story of a school board who
“inhumane” because they put a dollar value on children’s health; the result of rejecting the report was that the absestos
was not removed, which is surely more “inhumane”. In medical domains, one often measures utility in terms of QALY, or
inverse reinforcement learning.Decision
true label y
ˆy
reject
a. Show that the minimum risk is obtained if we decide Y = j if p(Y = j|x) ≥p(Y = k|x) for all k
λs ; otherwise we decide to reject.
increases).
In many applications, the classiﬁer is allowed to “reject” a test example rather than classifying it into one
c. Show that in general, for this loss matrix, but for any posterior distribution, there will be two thresholds
how much quantity Q of some product (e.g., newspapers) to buy to maximize your proﬁts. The optimal
amount will depend on how much demand D you think there is for your product, as well as its cost
to you C and its selling price P. Suppose D is unknown but has pdf f(D) and cdf F(D). We can
evaluate the expected proﬁt by considering two cases: if D > Q, then we sell all Q items, and make proﬁt
π = (P −C)Q; but if D < Q, we only sell D items, at proﬁt (P −C)D, but have wasted C(Q −D)
on the unsold items. So the expected proﬁt if we buy quantity Q is
Eπ(Q) =
 ∞
Q
(P −C)Qf(D)dD +
 Q
(P −C)Df(D) −
 Q
C(Q −D)f(D)dD
Simplify this expression, and then take derivatives wrt Q to show that the optimal quantity Q∗(which
maximizes the expected proﬁt) satisﬁes
F(Q∗) = P −C
P
one computed by thresholding B, and the other computed by thresholding p(H1|D). Will they be the
same or different? Explain why.
Let Δ be a quantity that we want to predict, let D be the observed data and M be a ﬁnite set of models.
Suppose our action is to provide a probabilistic prediction p(), and the loss function is L(Δ, p()) =−log p(Δ). We can either perform Bayes model averaging and predict using
pBMA(Δ) =


p(Δ|m, D)p(m|D)
or we could predict using any single model (a plugin approximation)
pm(Δ) = p(Δ|m, D)
E

L(Δ, pBMA)

≤E [L(Δ, pm)]
where the expectation over Δ is with respect to
p(Δ|D) =


p(Δ|m, D)p(m|D)
(Source: Jaakkola.)
reports to you the outcome, y. But this person is unreliable and only reports the result correctly with
θ2
θ2
p(x, y|θ) = p(y|x, θ2)p(x|θ1)
(You may leave your answer in
fractional form if you wish.)
L(m) =
n


log p(xi, yi|m, ˆθ(D−i))
and ˆθ(D−i)) denotes the MLE computed on D excluding row i. Which model will CV pick and why?
Hint: notice how the table of counts changes when you omit each training case one at a time.e. Recall that an alternative to CV is to use the BIC score, deﬁned as
BIC(M, D) ≜log p(D|ˆθMLE) −dof(M)
log N
where dof(M) is the number of free parameters in the model, Compute the BIC scores for both models
(use log base e). Which model does BIC prefer?
Prove that the posterior median is optimal estimate under L1 loss.Frequentist statistics
Introduction
Attempts have been made to devise approaches to statistical inference that avoid treating
parameters like random variables, and which thus avoid the use of priors and Bayes rule. Such
approaches are known as frequentist statistics, classical statistics or orthodox statistics.
Instead of being based on the posterior distribution, they are based on the concept of a sampling
distribution. This is the distribution that an estimator has when applied to multiple data sets
of variation across repeated trials that forms the basis for modeling uncertainty used by the
frequentist approach.
By contrast, in the Bayesian approach, we only ever condition on the actually observed data;
there is no notion of repeated trials. This allows the Bayesian to compute the probability of
used in machine learning.
Sampling distribution of an estimator
some data D, so ˆθ = δ(D). The parameter is viewed as ﬁxed and the data as random, which
is the exact opposite of the Bayesian approach. The uncertainty in the parameter estimate can
be measured by computing the sampling distribution of the estimator. To understand this
random.
However, we have seen that it is perfectly reasonable to use a probability distribution to represent one’s
uncertainty about an unknown constant.(a)
(b)
A bootstrap approximation to the sampling distribution of ˆθ for a Bernoulli distribution. We
concept, imagine sampling many different data sets D(s) from some true model, p(·|θ∗), i.e., let
D(s) = {x(s)
i }N
the sampled data set, and N is the size of each such dataset. Now apply the estimator ˆθ(·)
on ˆθ(·) is the sampling distribution of the estimator. We will discuss various ways to use the
sampling distribution in later sections. But ﬁrst we sketch two approaches for computing the
sampling distribution itself.
Bootstrap
The bootstrap is a simple Monte Carlo technique to approximate the sampling distribution. This
is particularly useful in cases where the estimator is a complex function of the true parameters.
datasets, each of size N, from the true distribution, xs
We could then compute our estimator from each sample, ˆθs = f(xs
unknown, the idea of the parametric bootstrap is to generate the samples using ˆθ(D) instead.
i (with replacement)
from the original data D, and then compute the induced distribution as before. Some methods
for speeding up the bootstrap when applied to massive data sets are discussed in (Kleiner et al.
essentially the same.) We see that the sampling distribution is asymmetric, and therefore quite
theory suggests (see below).
A natural question is: what is the connection between the parameter estimates ˆθs = ˆθ(xs
computed by the bootstrap and parameter values sampled from the posterior, θs ∼p(·|D)?Conceptually they are quite different. But in the common case that that the prior is not very
the posterior and the sampling distribution are quite similar. So one can think of the bootstrap
However, perhaps surprisingly, bootstrap can be slower than posterior sampling. The reason
is that the bootstrap has to ﬁt the model S times, whereas in posterior sampling, we usually
only ﬁt the model once (to ﬁnd a local mode), and then perform local exploration around the
mode. Such local exploration is usually much faster than ﬁtting a model from scratch.
Large sample theory for the MLE *
In some cases, the sampling distribution for some estimators can be computed analytically. In
particular, it can be shown that, under certain conditions, as the sample size tends to inﬁnity,
the sampling distribution of the MLE becomes Gaussian. Informally, the requirement for this
result to hold is that each parameter in the model gets to “see” an inﬁnite amount of data, and
that the model be identiﬁable. Unfortunately this excludes many of the models of interest to
machine learning. Nevertheless, let us assume we are in a simple setting where the theorem
holds.
The center of the Gaussian will be the MLE ˆθ. But what about the variance of this Gaussian?
Intuitively the variance of the estimator will be (inversely) related to the amount of curvature of
the likelihood surface at its peak. If the curvature is large, the peak will be “sharp”, and the
variance low; in this case, the estimate is “well determined”. By contrast, if the curvature is
small, the peak will be nearly “ﬂat”, so the variance is high.
Let us now formalize this intuition. Deﬁne the score function as the gradient of the log
likelihood evaluated at some point ˆθ:
s(ˆθ) ≜∇log p(D|θ)|ˆθ
Deﬁne the observed information matrix as the gradient of the negative score function, or
equivalently, the Hessian of the NLL:
θ log p(D|θ)|ˆθ
In 1D, this becomes
J(ˆθ(D)) = −d
Since we are studying the sampling distribution, D = (x1, . . . , xN) is a set of random
variables. The Fisher information matrix is deﬁned to be the expected value of the observed
)
J(ˆθ|D)
*

d

, that
is, the variance of the score function.

d
N
N
the data, is assumed known, so we just write IN(ˆθ) ≜IN(ˆθ|θ∗) for short. Furthermore, it is
and just write I(ˆθ) ≜I1(ˆθ). This is the notation that is usually used.
Now let ˆθ ≜ˆθmle(D) be the MLE, where D ∼θ∗. It can be shown that
MLE is asymptotically normal.
What about the variance of the MLE, which can be used as some measure of conﬁdence
in the MLE? Unfortunately, θ∗is unknown, so we can’t evaluate the variance of the sampling
Consequently, the approximate standard errors of ˆθk are given by
ˆsek ≜IN(ˆθ)
kk
model is
I(θ)
=
So the approximate standard error of the MLE is
ˆse
=
IN(ˆθ)
=
NI(ˆθ)
=

N
where ˆθ =
N

under a uniform prior.
Frequentist decision theory
In frequentist or classical decision theory, there is a loss function and a likelihood, but there is
no prior and hence no posterior or posterior expected loss. Thus there is no automatic way of
deriving an optimal estimator, unlike the Bayesian case. Instead, in the frequentist approach, we
the gradient must be zero at a maximum), so the variance reduces to the expected square of the score function:

( d
.

( d
=

d2

, so now the Fisher information reduces to the expected second derivative of the NLL, which
is a much more intuitive quantity than the variance of the score.
sequential decision problems, which adapt to data online.Having chosen an estimator, we deﬁne its expected loss or risk as follows:
R(θ∗, δ) ≜Ep( ˜
D|θ∗)
)
L(θ∗, δ( ˜D))
*
=


L(θ∗, δ( ˜D))p( ˜D|θ∗)d ˜D
other words, the expectation is wrt the sampling distribution of the estimator. Compare this to
the Bayesian posterior expected loss:
ρ(a|D, π) ≜Ep(θ|D,π) [L(θ, a)] =


Θ
(which is known), whereas the frequentist approach averages over ˜D (thus ignoring the observed
unknown. Consequently, we cannot compare different estimators in terms of their frequentist
risk. We discuss various solutions to this below.
Bayes risk
How do we choose amongst estimators? We need some way to convert R(θ∗, δ) into a single
RB(δ) ≜Ep(θ∗) [R(θ∗, δ)] =


A Bayes estimator or Bayes decision rule is one which minimizes the expected risk:
δB ≜argmin
δ
RB(δ)
Note that the integrated risk is also called the preposterior risk, since it is before we have seen
the data. Minimizing this can be useful for experiment design.
We will now prove a very important theorem, that connects the Bayesian and frequentist
approaches to decision theory.
each x.
Proof. By switching the order of integration, we have
RB(δ)
=

 
x

y
L(y, δ(x))p(x, y|θ∗)

=

x

y


Θ
=

x

y
L(y, δ(x))p(y|x)dy

p(x)
=

x
ρ(δ(x)|x) p(x)R(θ, δ2)
R(θ, δ1)
θ
R
are overly conservative.
To minimize the overall expectation, we just minimize the term inside for each x, so our decision
rule is to pick
δB(x) = argmin
ρ(a|x)
approach) is optimal on average (as in the frequentist approach). In other words, the Bayesian
approach provides a good way of achieving frequentist goals. In fact, one can go further and
prove the following.
to some, possibly improper, prior distribution.
This theorem shows that the best way to minimize frequentist risk is to be Bayesian!
See
Minimax risk
though this is only in the evaluation of the estimator, not necessarily as part of its construction).
An alternative approach is as follows. Deﬁne the maximum risk of an estimator as
Rmax(δ) ≜max
θ∗R(θ∗, δ)
A minimax rule is one which minimizes the maximum risk:
δMM ≜argmin
δ
Rmax(δ)how to compute a risk function for an actual model).
Minimax estimators have a certain appeal.
However, computing them can be hard.
And
furthermore, they are very pessimistic.
In fact, one can show that all minimax estimators
are equivalent to Bayes estimators under a least favorable prior. In most statistical situations
(excluding game theoretic ones), assuming nature is an adversary is not a reasonable assumption.
Admissible estimators
bution p(·|θ∗) in order to evaluate the risk. However, It might be the case that some estimators
other estimator.
Example
decision rules or estimators ˆθ(x) = δ(x) are as follows:
•
δ1(x) = x, the sample mean
•
δ2(x) = ˜x, the sample median
•
δ3(x) = θ0, a ﬁxed value
•
δκ(x) =
N
N + κx +
κ
strong prior, κ = ∞.)
Let us now derive the risk functions analytically. (We can do this since in this toy example,
into squared bias plus variance:
MSE(ˆθ(·)|θ∗)
=
var
)
ˆθ
*
+ bias2(ˆθ)
The sample mean is unbiased, so its risk is
MSE(δ1|θ∗) = var [x] = σ2
Nθ*
R(θ*,δ)
 
 
mle
median
fixed
postmean1
postmean5
(a)
θ*
R(θ*,δ)
 
 
mle
median
fixed
postmean1
postmean5
(b)
The solid dark blue horizontal line is the MLE, the solid light blue curved line is the posterior mean when
Figure generated by riskFnGauss.
The sample median is also unbiased. One can show that the variance is approximately π/(2N),
so
MSE(δ2|θ∗) =
π
2N
Finally, for the posterior mean, we have
MSE(δκ|θ∗)
=
E
)
=
E
)
=
w2 σ2
=

the posterior mean with a weak prior) is usually desirable, assuming our prior mean is sensible.inadmissible estimator for this particular problem (where the data is assumed to come from a
Gaussian).
In practice, the sample median is often better than the sample mean, because it is more
robust to outliers. One can show (Minka 2000d) that the median is the Bayes estimator (under
squared loss) if we assume the data comes from a Laplace distribution, which has heavier tails
than a Gaussian. More generally, we can construct robust estimators by using ﬂexible models
then computing the posterior mean or median.
Stein’s paradox *
obvious estimator is the MLE, which in this case sets ˆθi = xi. It turns out that this is an
one such estimator, and is deﬁned as follows:
ˆθi
=
where x =
N
N
θi towards the overall mean. (We derive this estimator using an empirical Bayes approach in
It can be shown that this shrinkage estimator has lower frequentist risk (MSE) than the MLE
other student’s scores? One can create even more paradoxical examples by making the different
dimensions be qualitatively different, e.g., θ1 is my IQ, θ2 is the average rainfall in Vancouver,
etc.
since
E


= E

i
x2
i

=
N


i

Consequently we can reduce our risk by pooling information, even from unrelated sources, and
Admissibility is not enough
It seems clear that we can restrict our search for good estimators to the class of admissible
estimators. But in fact it is easy to construct admissible estimators, as we show in the following
example.a constant independent of the data. This is an admissible estimator.
R(θ∗, δ2) =


(δ2(x) −θ0)2p(x|θ0)dx
Desirable properties of estimators
Since frequentist decision theory does not provide an automatic way to choose the best estimator,
we need to come up with other heuristics for choosing amongst them. In this section, we discuss
some properties we would like estimators to have. Unfortunately, we will see that we cannot
achieve all of these properties at the same time.
Consistent estimators
An estimator is said to be consistent if it eventually recovers the true parameters that generated
the data as the sample size goes to inﬁnity, i.e., ˆθ(D) →θ∗as |D| →∞(where the arrow
denotes convergence in probability). Of course, this concept only makes sense if the data actually
data. Nevertheless, it can be a useful theoretical property.
mizing likelihood is equivalent to minimizing KL
+
p(·|θ∗)||p(·|ˆθ)
,
, where p(·|θ∗) is the true
Unbiased estimators
The bias of an estimator is deﬁned as
bias(ˆθ(·)) = Ep(D|θ∗)
)
ˆθ(D) −θ∗
*
means the sampling distribution is centered on the true parameter. For example, the MLE for a
Gaussian mean is unbiased:
bias(ˆμ) = E [x] −μ = E

N
N

xi

but for which the induced distribution, p(·|ˆθ), is the same as the exact distribution. Such parameters are said to be
likelihood equivalent.However, the MLE for a Gaussian variance, ˆσ2, is not an unbiased estimator of σ2. In fact, one
E

ˆσ2
N
σ2
However, the following estimator
ˆσ2
N
N

is an unbiased estimator, which we can easily prove as follows:
E

ˆσ2

= E

N

=
N
N
σ2 = σ2
In Matlab, var(X) returns ˆσ2
N, the difference will be negligible.
Although the MLE may sometimes be a biased estimator, one can show that asymptotically, it
is always unbiased. (This is necessary for the MLE to be a consistent estimator.)
Minimum variance estimators
It seems intuitively reasonable that we want our estimator to be unbiased (although we shall
give some arguments against this claim below). However, being unbiased is not enough. For
example, suppose we want to estimate the mean of a Gaussian from D = {x1, . . . , xN}. The
estimator that just looks at the ﬁrst data point, ˆθ(D) = x1, is an unbiased estimator, but will
of an estimator is also important.
Rao lower bound, provides a lower bound on the variance of any unbiased estimator. More
precisely,
var
)
ˆθ
*
≥
nI(θ0)
It can be shown that the MLE achieves the Cramer Rao lower bound, and hence has the
smallest asymptotic variance of any unbiased estimator. Thus MLE is said to be asymptotically
optimal.Although using an unbiased estimator seems like a good idea, this is not always the case. To see
why, suppose we use quadratic loss. As we showed above, the corresponding risk is the MSE.
We now derive a very useful decomposition of the MSE. (All expectations and variances are wrt
the true distribution p(D|θ∗), but we drop the explicit conditioning for notational brevity.) Let

ˆθ

denote the expected value of the estimate (as we
vary D). Then we have
E

=
E

(ˆθ −θ) + (θ −θ∗)
=
E

ˆθ −θ

ˆθ −θ

=
E

ˆθ −θ
=
var

ˆθ

+ bias2(ˆθ)
In words,
MSE = variance + bias2
it might be wise to use a biased estimator, so long as it reduces our variance, assuming our goal
is to minimize squared error.
Example: estimating a Gaussian mean
var [x|θ∗] = σ2
N
˜x ≜
N
N + κ0
x +
κ0
N + κ0
posterior mean, since the mean and mode of a Gaussian are the same.) The bias and variance
are given by
E [˜x] −θ∗
=
var [˜x]
=
w2 σ2
N 
 
postMean0
postMean1
postMean2
postMean3
(a)
sample size
relative MSE
MSE of postmean / MSE of MLE
 
 
postMean0
postMean1
postMean2
postMean3
(b)
Left: Sampling distribution of the MAP estimate with different prior strengths κ0. (The MLE
is biased away from the truth, but has lower variance (is narrower) than that of the MLE.
the prior mean is wrong. It is clearly important to “tune” the strength of the prior, a topic we
discuss later.
Example: ridge regression
Another important example of the bias variance tradeoff arises in ridge regression, which we
small, which reduces overﬁtting; the precision term, λ, controls the strength of this prior. Setting
and on the right plots the average ﬁtted curve. We see that as we increase the strength of the
regularizer, the variance decreases, but the bias increases.that the individual ﬁts are similar to each other (low variance), but the average is far from the truth (high
bias). Bottom row: lightly regularized: we see that the individual ﬁts are quite different from each other
generated by biasVarModelComplexity3.
the correct side of the decision boundary, then the bias is negative, and decreasing the variance
will decrease the misclassiﬁcation rate. But if the estimate is on the wrong side of the decision
boundary, then the bias is positive, so it pays to increase the variance (Friedman 1997a). This
It is better to focus on expected loss (see below), not directly on bias and variance. We can
Empirical risk minimization
Frequentist decision theory suffers from the fundamental problem that one cannot actually
compute the risk function, since it relies on knowing the true data distribution. (By contrast,
the Bayesian posterior expected loss can always be computed, since it conditions on the the
and that is where the task is to predict observable quantities, as opposed to estimating hidden
variables or parameters. That is, instead of looking at loss functions of the form L(θ, δ(D)),prediction given the input x. In this case, the frequentist risk becomes
R(p∗, δ) ≜E(x,y)∼p∗[L(y, δ(x)] =

x

y
L(y, δ(x))p∗(x, y)
where p∗represents “nature’s distribution”. Of course, this distribution is unknown, but a simple
approach is to use the empirical distribution, derived from some training data, to approximate
p∗, i.e.,
N
N

δxi(x)δyi(y)
We then deﬁne the empirical risk as follows:
N
N

L(yi, δ(xi))
We deﬁne the task of empirical risk minimization or ERM as ﬁnding a decision procedure
(typically a classiﬁcation rule) to minimize the empirical risk:
δERM(D) = argmin
δ
Remp(D, δ)
In the unsupervised case, we eliminate all references to y, and replace L(y, δ(x)) with
N
N

L(xi, δ(xi))
Regularized risk minimization
Note that the empirical risk is equal to the Bayes risk if our prior about “nature’s distribution” is
that it is exactly equal to the empirical distribution (Minka 2001b):
E [R(p∗, δ)|p∗= pemp] = Remp(D, δ)
Therefore minimizing the empirical risk will typically result in overﬁtting. It is therefore often
necessary to add a complexity penalty to the objective function:
R′(D, δ) = Remp(D, δ) + λC(δ)of the complexity penalty. This approach is known as regularized risk minimization (RRM).
Note that if the loss function is negative log likelihood, and the regularizer is a negative log
prior, this is equivalent to MAP estimation.
a linear model, we can deﬁne the complexity of in terms of its degrees of freedom, discussed in
Structural risk minimization
The regularized risk minimization principle says that we should ﬁt the model, for a given
complexity penalty, by using
ˆδλ = argmin
δ
[Remp(D, δ) + λC(δ)]
true risk, a problem known as optimism of the training error. As an alternative, we can use
ˆλ = argmin
λ
ˆR(ˆδλ)
where ˆR(δ) is an estimate of the risk. There are two widely used estimates: cross validation
and theoretical upper bounds on the risk. We discuss both of these below.
Estimating the risk using cross validation
We can estimate the risk of some estimator using a validation set. If we don’t have a separate
precisely, CV is deﬁned as follows. Let there be N = |D| data cases in the training set. Denote
the data in the k’th test fold by Dk and all the other data by D−k. (In stratiﬁed CV, these folds
are chosen so the class proportions (if discrete labels are present) are roughly equal in each
fold.) Let F be a learning algorithm or ﬁtting function that takes a dataset and a model index
m (this could a discrete index, such as the degree of a polynomial, or a continuous index, such
as the strength of a regularizer) and returns a parameter vector:
ˆθm = F(D, m)
Finally, let P be a prediction function that takes an input and a parameter vector and returns a
prediction:
ˆy = P(x, ˆθ) = f(x, ˆθ)
fm(x, D) = P(x, F(D, m))N
K


L (yi, P(xi, F(D−k, m)))
Note that we can call the ﬁtting algorithm once per fold. Let f k
m(x) = P(x, F(D−k, m)) be
the function that was trained on all the data except for the test data in fold k. Then we can
rewrite the CV estimate as
N
K


L

yi, f k
m(xi)

N
N

L

yi, f k(i)
m (xi)

where k(i) is the fold in which i is used as test data. In other words, we predict yi using a
model that was trained on data that does not contain xi.
Of K = N, the method is known as leave one out cross validation or LOOCV. In this case,
n the estimated risk becomes
N
N

L

yi, f −i
m (xi)

where f i
m(x) = P(x, F(D−i, m)). This requires ﬁtting the model N times, where for f −i
m we
omit the i’th training case. Fortunately, for some model classes and loss functions (namely linear
models and quadratic loss), we can ﬁt the model once, and analytically “remove” the effect of
the i’th training case. This is known as generalized cross validation or GCV.
As a concrete example, consider picking the strength of the ℓ2 regularizer in penalized linear
regression. We use the following rule:
ˆλ = arg
min
λ∈[λmin,λmax] R(λ, Dtrain, K)
R(λ, Dtrain, K) =
|Dtrain|
K


L(yi, f k
λ(xi))
where f k
of a CV estimate of the risk vs log(λ), where the loss function is squared error.
When we have more than one or two tuning parameters, this approach becomes infeasible.
In such cases, one can use empirical Bayes, which allows one to optimize large numbers of
for details.log lambda
mean squared error
 
 
train mse
test mse
(a)
log lambda
mse
(b)
clarity. The blue line corresponds to the value chosen by the one standard error rule. Figure generated by
linregPolyVsRegDemo.
The one standard error rule
The above procedure estimates the risk, but does not give any measure of uncertainty.
A
standard frequentist measure of uncertainty of an estimate is the standard error of the mean,
deﬁned by
se =
ˆσ
√
N
=

ˆσ2
N
where ˆσ2 is an estimate of the variance of the loss:
N
N

Li = L(yi, f k(i)
m (xi))
N
N

Li
Note that σ measures the intrinsic variability of Li across samples, whereas se measures our
uncertainty about the mean L.
Suppose we apply CV to a set of models and compute the mean and se of their estimated
risks. A common heuristic for picking a model from these noisy estimates is to pick the value
which corresponds to the simplest model whose risk is no more than one standard error above
curve, but one that is slightly to its right, since that corresponds to a more heavily regularized
model with essentially the same empirical performance.If we are performing unsupervised learning, we must use a loss function such as L(x, δ(x)) =
since we will always get lower loss with a more complex model, even if evaluated on the test set.
This is because more complex models will compress the data less, and induce less distortion.
Consequently, we must either use probabilistic models, or invent other heuristics.
Upper bounding the risk using statistical learning theory *
The principle problem with cross validation is that it is slow, since we have to ﬁt the model
multiple times. This motivates the desire to compute analytic approximations or bounds to
the generalization error. This is the studied in the ﬁeld of statistical learning theory (SLT).
More precisely, SLT tries to bound the risk R(p∗, h) for any data distribution p∗and hypothesis
hypothesis space H.
Let us initially consider the case where the hypothesis space is ﬁnite, with size dim(H) = |H|.
bounded as follows:
P

max
	
Proof. To prove this, we need two useful results. First, Hoeffding’s inequality, which states that
where x =
N

N
events, then P(∪d
d
Finally, for notational brevity, let R(h) = R(h, p∗) be the true risk, and ˆRN(h) = Remp(D, h)
be the empirical risk.
Using these results we have
P

max
	
=
P

 

| ˆRN(h) −R(h)| > ϵ

≤

P

| ˆRN(h) −R(h)| > ϵ

≤
creases with N = |D|, as is to be expected.
Stepping back from all the theory, the key intuition behind statistical learning theory is quite
simple. Suppose we ﬁnd a model with low empirical risk. If the hypothesis space H is very
big, relative to the data size, then it is quite likely that we just got “lucky” and were given a
that such a function will have low generalization error. But if the hypothesis class is sufficiently
in this way, so a low empirical risk is evidence of a low true risk.
Note that optimism of the training error does not necessarily increase with model complexity,
but it does increase with the number of different models that are being searched over.
The advantage of statistical learning theory compared to CV is that the bounds on the risk
are quicker to compute than using CV. The disadvantage is that it is hard to compute the VC
dimension for many interesting models, and the upper bounds are usually very loose (although
One can extend statistical learning theory by taking computational complexity of the learner
into account. This ﬁeld is called computational learning theory or COLT. Most of this work
a low empirical risk, and the hypothesis space is suitably “small”, then we can say that our
estimated function is probably approximately correct or PAC. A hypothesis space is said to be
Surrogate loss functions
and hence is hard to optimize. One alternative is to use maximum likelihood estimation instead,
Then the corresponding probability distribution on the output label is
p(yi|xi, w) = sigm(yiηi)η
loss
 
 
hinge
logloss
Illustration of various loss functions for binary classiﬁcation. The horizontal axis is the margin
This loss function forms the basis of a popular classiﬁcation method known as support vector
The surrogate is usually chosen to be a convex upper bound, since convex functions are easy
Pathologies of frequentist statistics *
I believe that it would be very difficult to persuade an intelligent person that current
[frequentist] statistical practice was sensible, but that there would be much less difficulty
Frequentist statistics exhibits various forms of weird and undesirable behaviors, known as
pathologies. We give a few examples below, in order to caution the reader; these and otherA conﬁdence interval is an interval derived from the sampling distribution of an estimator
C′
Let us step back for a moment and think about what is going on. In Bayesian statistics,
we condition on what is known — namely the observed data, D — and average over what
hypothetical future data sets ˜D.
p(x|θ) =
⎧
⎨
⎩
if x = θ
otherwise
Let m = min(x1, x2) and deﬁne the following conﬁdence interval:
[ℓ(D), u(D)] = [m, m]
For the above samples this yields
“conﬁdence” in this fact.
of a Bernoulli distribution. Let x =
N

N

called a Wald interval and is based on a Gaussian approximation to the Binomial distribution;
which seems even worse. It can be argued that the above ﬂaw is because we approximated
the true sampling distribution with a Gaussian, or because the sample size was to small, or the
parameter “too extreme”. However, the Wald interval can behave badly even for large N, andSuppose we want to decide whether to accept or reject some baseline model, which we will
call the null hypothesis. We need to deﬁne some decision rule. In frequentist statistics, it
pvalue(D) ≜P(f( ˜D) ≥f(D)| ˜D ∼H0)
This quantity relies on computing a tail area probability of the sampling distribution; we give
an example of how to do this below.
between the observed test statistic and the expected test statistic is statistically signiﬁcant at
level α. This approach is known as null hypothesis signiﬁcance testing, or NHST.
This procedure guarantees that our expected type I (false positive) error rate is at most α.
This is sometimes interpreted as saying that frequentist hypothesis testing is very conservative,
since it is unlikely to accidently reject the null hypothesis. But in fact the opposite is the case:
because this method only worries about trying to reject the null, it can never gather evidence
overstate the evidence against the null, and are thus very “trigger happy”.
care about, which is the posterior probability of the null hypothesis given the data, p(H0|D).
“signiﬁcant” evidence of an effect that cannot be explained by the null hypothesis, whereas
about when to stop collecting data, even if these decisions don’t change the data you actually
The relevant sampling model is the binomial
Bin(s|n, θ) =
n
s
	


s
	p2 =


reject the null hypothesis.
this case, f is ﬁxed and n (and hence s = n −f) is random. The probability model becomes
the negative binomial distribution, given by
NegBinom(s|f, θ) =
	
where f = n −s.
∞

	
Obviously this is ridiculous: the data is the same, so our inferences about the coin should be
the same. After all, I could have chosen the experimental protocol at random. It is the outcome
of the experiment that matters, not the details of how I decided which one to run.
Although this might seem like just a mathematical curiosity, this also has signiﬁcant practical
value means that frequentists often do not terminate experiments early, even when it is obvious
what the conclusions are, lest it adversely affect their statistical analysis. If the experiments are
costly or harmful to people, this is obviously a bad idea. Perhaps it is not surprising, then, that
the US Food and Drug Administration (FDA), which regulates clinical trials of new drugs, has
recently become supportive of Bayesian methods6, since Bayesian methods are not affected by
the stopping rule.
The likelihood principle
The fundamental reason for many of these pathologies is that frequentist inference violates
the likelihood principle, which says that inference should be based on the likelihood of the
observed data, not based on hypothetical future data that you have not observed. Bayes obviously
satisﬁes the likelihood principle, and consequently does not suffer from these pathologies.
who showed that it followed automatically from two simpler principles. The ﬁrst of these is the
sufficiency principle, which says that a sufficient statistic contains all the relevant informationabout an unknown parameter (arguably this is true by deﬁnition).
The second principle is
known as weak conditionality, which says that inferences should be based on the events that
happened, not which might have happened. To motivate this, consider an example from (Berger
York or in California. The two labs seem equally good, so a fair coin is used to decide between
them. The coin comes up heads, so the California lab is chosen. When the results come back,
should it be taken into account that the coin could have come up tails and thus the New York
lab could have been used? Most people would argue that the New York lab is irrelevant, since
the tails event didn’t happen. This is an example of weak conditionality. Given this principle,
one can show that all inferences should only be based on what was observed, which is in
the likelihood principle.
Why isn’t everyone a Bayesian?
Given these fundamental ﬂaws of frequentist statistics, and the fact that Bayesian methods
do not have such ﬂaws, an obvious question to ask is: “Why isn’t everyone a Bayesian?” The
paper is well worth reading for anyone interested in this topic. Below we quote his opening
section:
The title is a reasonable question to ask on at least two counts. First of all, everone used
to be a Bayesian. Laplace wholeheatedly endorsed Bayes’s formulation of the inference
This included Gauss, whose
statistical work is usually presented in frequentist terms.
A second and more important point is the cogency of the Bayesian argument. Modern
ical arguments for preferring Bayesian inference. A byproduct of this work is a disturbing
catalogue of inconsistencies in the frequentist point of view.
century.
Time will tell whether Lindley was right....
Exercises
(Source: Witten05, p152.). Suppose we have a completely random labeled dataset (i.e., the features x tell us
What is the best misclassiﬁcation rate any method can achieve? What is the estimated misclassiﬁcation
rate of the same method using LOOCV?(The other terms,
Gaussian assumption is reasonable)? i.e. is it likely to be too large or too small, or just right?
not need to compute the numerical answer; just brieﬂy explain what would happen qualitatively, and
why.
MLE is biased
Show that ˆσ2
MLE =
N
N
EX1,...,Xn∼N (μ,σ)[ˆσ2(X1, . . . , Xn) ̸= σ2
Hint: note that X1, . . . , XN are independent, and use the fact that the expectation of a product of
independent random variables is the product of the expectations.
MLE for σ2 in this case. Is it unbiased?Linear regression
Introduction
Linear regression is the “work horse” of statistics and (supervised) machine learning. When
linear relationships. And when the Gaussian output is replaced with a Bernoulli or multinoulli
distribution, it can be used for classiﬁcation, as we will see below. So it pays to study this model
in detail.
Model speciﬁcation
p(y|x, θ) = N(y|wT x, σ2)
p(y|x, θ) = N(y|wT φ(x), σ2)
This is known as basis function expansion. (Note that the model is still linear in the parameters
w, so it is still called linear regression; the importance of this will become clear below.) A simple
example are polynomial basis functions, where the model has the form
increasingly complex functions.
Maximum likelihood estimation (least squares)
A common way to esitmate the parameters of a statistical model is to compute the MLE, which
is deﬁned as
ˆθ ≜arg max
θ
log p(D|θ)(a)
(b)
Linear regression applied to 2d data. Vertical axis is temperature, horizontal axes are location
within a room. Data was collected by some remote sensing motes at Intel’s lab in Berkeley, CA (data
courtesy of Romain Thibaux).
(a) The ﬁtted plane has the form ˆf(x) = w0 + w1x1 + w2x2.
(b)
Temperature data is ﬁtted with a quadratic of the form ˆf(x) = w0 + w1x1 + w2x2 + w3x2
Produced by surfaceFitDemo.
It is common to assume the training examples are independent and identically distributed,
ℓ(θ) ≜log p(D|θ) =
N

log p(yi|xi, θ)
hood or NLL:
NLL(θ) ≜−
N

log p(yi|xi, θ)
The NLL formulation is sometimes more convenient, since many optimization software packages
are designed to ﬁnd the minima of functions, rather than maxima.
Now let us apply the method of MLE to the linear regression setting. Inserting the deﬁnition
of the Gaussian into the above, we ﬁnd that the log likelihood is given by
ℓ(θ)
=
N

log

2πσ2
exp

	
=
2σ2 RSS(w) −N
RSS stands for residual sum of squares and is deﬁned by
RSS(w)
≜
N

squared error or MSE. It can also be written as the square of the ℓ2 norm of the vector ofprediction
truth
(a)
Sum of squares error contours for linear regression
w0
w1
(b)
(a) In linear least squares, we try to minimize the sum of squared distances from each training
point (denoted by a red circle) to its approximation (denoted by a blue cross), that is, we minimize the
sum of the lengths of the little vertical blue lines. The red diagonal line represents ˆy(x) = w0 + w1x,
which is the least squares regression line. Note that these residual lines are not perpendicular to the least
contoursSSEdemo.
residual errors:
N

ϵ2
i
We see that the MLE for w is the one that minimizes the RSS, so this method is known
shown as red circles, the estimated values (xi, ˆyi) are shown as blue crosses, and the residuals
ϵi = yi −ˆyi are shown as vertical blue lines. The goal is to ﬁnd the setting of the parameters
(the slope w1 and intercept w0) such that the resulting red line minimizes the sum of squared
residuals (the lengths of the vertical blue lines).
quadratic “bowl” with a unique minimum, which we now derive. (Importantly, this is true even
if we use basis function expansion, such as polynomials, because the NLL is still linear in the
parameters w, even if it is not linear in the inputs x.)
Derivation of the MLE
First, we rewrite the objective in a form that is more amenable to differentiation:
NLL(w)
=
2wT (XT X)w −wT (XT y)where
XT X =
N

xixT
i =
N

⎛
⎜
⎝
x2
· · ·
xi,1xi,D
...
· · ·
x2
i,D
⎞
⎟
⎠
is the sum of squares matrix and
XT y =
N

xiyi.
g(w)
=
[XT Xw −XT y] =
N

xi(wT xi −yi)
Equating to zero we get
XT Xw
=
XT y
This is known as the normal equation. The corresponding solution ˆw to this linear system of
equations is called the ordinary least squares or OLS solution, which is given by
ˆwOLS = (XT X)−1XT y
Geometric interpretation
This equation has an elegant geometrical intrepretation, as we now explain. We assume N > D,
so we have more examples than features.
The columns of X deﬁne a linear subspace of
dimensionality D which is embedded in N dimensions. Let the j’th column be ˜xj, which is
dimensions:
X =
⎛
⎝
⎞
⎠, y =
⎛
⎝
⎞
⎠
i.e., we want to ﬁnd
argmin
ˆy = w1˜x1 + · · · + wD˜xD = Xwy
x1
x2
ˆy
x1
x2
x3
˜x2 are vectors in R3; together they deﬁne a 2D plane. y is also a vector in R3 but does not lie on this
2D plane. The orthogonal projection of y onto this plane is denoted ˆy. The red line from y to ˆy is
the residual, whose norm we want to minimize. For visual clarity, all vectors have been converted to unit
norm. Figure generated by leastSquaresProjection.
To minimize the norm of the residual, y −ˆy, we want the residual vector to be orthogonal to
every column of X, so ˜xT
˜xT
Hence our projected value of y is given by
ˆy = X ˆw = X(XT X)−1XT y
This corresponds to an orthogonal projection of y onto the column space of X. The projection
matrix P ≜X(XT X)−1XT is called the hat matrix, since it “puts the hat on y”.
Convexity
When discussing least squares, we noted that the NLL had a bowl shape with a unique minimum.
The technical term for functions like this is convex. Convex functions play a very important
role in machine learning.
have(a)
(b)
(a) Illustration of a convex set. (b) Illustration of a nonconvex set.
x
y
λ
(a)
A
B
(b)
(a) Illustration of a convex function. We see that the chord joining (x, f(x)) to (y, f(y)) lies
above the function. (b) A function that is neither convex nor concave. A is a local minimum, B is a global
minimum. Figure generated by convexFnHand.
A function f(θ) is called convex if its epigraph (the set of points above the function) deﬁnes
a convex set. Equivalently, a function f(θ) is called convex if it is deﬁned on a convex set and
√
θ.
Intuitively, a (strictly) convex function has a “bowl shape”, and hence has a unique global
positive everywhere,
d
f often corresponds to the NLL.
∂f2(θ)
∂θj∂θk . Also, recall that aLinear data with noise and outliers
 
 
least squares
laplace
(a)
 
 
L2
L1
huber
(b)
(a) Illustration of robust linear regression. Figure generated by linregRobustDemoCombined.
(b) Illustration of ℓ2, ℓ1, and Huber loss functions. Figure generated by huberLossDemo.
Models where the NLL is convex are desirable, since this means we can always ﬁnd the
globally optimal MLE. We will see many examples of this later in the book. However, many
models of interest will not have concave likelihoods. In such cases, we will discuss ways to
derive locally optimal parameter estimates.
Robust linear regression *
It is very common to model the noise in regression models using a Gaussian distribution
maximizing likelihood is equivalent to minimizing the sum of squared residuals, as we have
seen. However, if we have outliers in our data, this can result in a poor ﬁt, as illustrated in
error penalizes deviations quadratically, so points far from the line have more affect on the ﬁt
than points near to the line.
One way to achieve robustness to outliers is to replace the Gaussian distribution for the
response variable with a distribution that has heavy tails. Such a distribution will assign higher
likelihood to outliers, without having to perturb the straight line to “explain” them.
as our observation model for regression, we get the following likelihood:
p(y|x, w, b)
=
b |y −wT x|)
assume b is ﬁxed. Let ri ≜yi −wT xi be the i’th residual. The NLL has the form
ℓ(w) =

i
|ri(w)|Likelihood
Prior
Name
Section
Gaussian
Uniform
Least squares
Gaussian
Gaussian
Ridge
Gaussian
Laplace
Lasso
Laplace
Uniform
Robust regression
Student
Uniform
Robust regression
Summary of various likelihoods and priors used for linear regression. The likelihood refers to
the distributional form of p(y|x, w, σ2), and the prior refers to the distributional form of p(w). MAP
estimation with a uniform distribution corresponds to MLE.
can convert the NLL to a linear objective, subject to linear constraints, using the following split
variable trick. First we deﬁne
i −r−
i
i
i
Now the
constrained objective becomes
min

i
i −r−
i )
s.t.
i + r−
i = yi
This is an example of a linear program with D + 2N unknowns and 3N constraints.
Since this is a convex optimization problem, it has a unique solution. To solve an LP, we must
ﬁrst write it in standard form, which as follows:
min
An alternative to using NLL under a Laplace likelihood is to minimize the Huber loss function
LH(r, δ) =

if |r| ≤δ
if |r| > δ
using the fact that
d
We can also check that the function is C1
continuous, since the gradients of the two parts of the function match at r = ±δ, namely
d
Newton) instead of linear programming.
similiar to the probabilistic methods. (In fact, it turns out that the Huber method also has a(a)
(b)
get wider as the ﬁt gets smoother, since we are ascribing more of the data variation to the noise. Figure
generated by linregPolyVsRegDemo.
Ridge regression
One problem with ML estimation is that it can result in overﬁtting. In this section, we discuss a
way to ameliorate this problem by using MAP estimation with a Gaussian prior. For simplicity,
we assume a Gaussian likelihood, rather than a robust likelihood.
Basic idea
The reason that the MLE can overﬁt is that it is picking the parameter values that are the
best for modeling the training data; but if the data is noisy, such parameters often result in
corresponding least squares coefficients (excluding w0) are as follows:
We see that there are many large positive and negative numbers. These balance out exactly
to make the curve “wiggle” in just the right way so that it almost perfectly interpolates the data.
But this situation is unstable: if we changed the data a little, the coefficients would change a lot.
We can encourage the parameters to be small, thus resulting in a smoother curve, by using a
p(w) =

j
becomes
argmax
w
N

log N(yi|w0 + wT xi, σ2) +
D
log lambda
mean squared error
 
 
train mse
test mse
(a)
log lambda
 
 
negative log marg. likelihood
CV estimate of MSE
(b)
Solid black: negative log marginal likelihood, −log p(D|λ). Both curves have been vertically rescaled to
It is a simple exercise to show that this is equivalent to minimizing the following:
N
N

j w2
solution is given by
ˆwridge = (λID + XT X)−1XT y
This technique is known as ridge regression, or penalized least squares. In general, adding
a Gaussian prior to the parameters of a model to encourage them to be small is called ℓ2
regularization or weight decay. Note that the offset term w0 is not regularized, since this just
affects the height of the function, not its complexity. By penalizing the sum of the magnitudes
which is the simplest possible function, corresponding to a constant.)
haveunderﬁts.
In
We will consider a variety of different priors in this book. Each of these corresponds to a
different form of regularization. This technique is very widely used to prevent overﬁtting.
Numerically stable computation *
Interestingly, ridge regression, which works better statistically, is also easier to ﬁt numerically,
since (λID + XT X) is much better conditioned (and hence more likely to be invertible) than
Nevertheless, inverting matrices is still best avoided, for reasons of numerical stability. (Indeed,
a useful trick for ﬁtting ridge regression models (and hence by extension, computing vanilla
OLS estimates) that is more numerically robust. We assume the prior has the form p(w) =
First let us augment the original data with some “virtual data” coming from the prior:
˜X =
√
Λ
	
,
˜y =

	
where Λ =
√
Λ
√
Λ
T is a Cholesky decomposition of Λ. We see that ˜X is (N + D) × D,
We now show that the NLL on this expanded data is equivalent to penalized NLL on the
original data:
f(w)
=
(˜y −˜Xw)T (˜y −˜Xw)
=

	
−
√
Λ
	
w
	T 
	
−
√
Λ
	
w
	
=
σ(y −Xw)
−
√
Λw
σ(y −Xw)
−
√
Λw
	
=
σ2 (y −Xw)T (y −Xw) + (
√
Λw)T (
√
Λw)
=
σ2 (y −Xw)T (y −Xw) + wT Λw
Hence the MAP estimate is given by
as we claimed.Now let
˜X = QR
be the QR decomposition of X, where Q is orthonormal (meaning QT Q = QQT = I), and
R is upper triangular. Then
Hence
ˆwridge = R−1R−T RT QT ˜y = R−1Q˜y
Note that R is easy to invert since it is upper triangular. This gives us a way to compute the
ridge estimate while avoiding having to invert (Λ + XT X).
We can use this technique to ﬁnd the MLE, by simply computing the QR decomposition of
the unaugmented matrix X, and using the original y. This is the method of choice for solving
least squares problems. (In fact, it is so sommon that it can be implemented in one line of
an N × D matrix takes O(ND2) time, and is numerically very stable.
If D ≫N, we should ﬁrst perform an SVD decomposition. In particular, let X = USVT be
the SVD of X, where VT V = IN, UUT = UT U = IN, and S is a diagonal N × N matrix.
Now let Z = UD be an N × N matrix. Then we can rewrite the ridge estimate thus:
ˆwridge
=
V(ZT Z + λIN)−1ZT y
coordinate system in which all but the ﬁrst N coordinates are zero. This does not affect the
solution since the spherical Gaussian prior is rotationally invariant. The overall time is now
Connection with PCA *
ˆwridge = V(S2 + λI)−1SUT y
Hence the ridge predictions on the training set are given by
ˆy
=
X ˆwridge = USVT V(S2 + λI)−1SUT y
=
U˜SUT y =
D

uj ˜SjjuT
j yprior mean
MAP Estimate
ML Estimate
u1
u2
Geometry of ridge regression. The likelihood is shown as an ellipse, and the prior is shown
where
˜Sjj
≜
[S(S2 + λI)−1S]jj =
σ2
j
σ2
j + λ
and σj are the singular values of X. Hence
ˆy
=
X ˆwridge =
D

uj
σ2
j
σ2
j + λuT
j y
In contrast, the least squares prediction is
ˆy
=
X ˆwls = (USVT )(VS−1UT y) = UUT y =
D

ujuT
j y
If σ2
view of this, we deﬁne the effective number of degrees of freedom of the model as follows:
dof(λ) =
D

σ2
j
σ2
j + λ
Let us try to understand why this behavior is desirable.
we are most uncertain about w are determined by the eigenvectors of this matrix with the
squared singular values σ2
j are equal to the eigenvalues of XT X. Hence small singular values σj
correspond to directions with high posterior variance. It is these directions which ridge shrinks
the most.Hence wmap
is close to ˆwmle
, but wmap
There is a related, but different, technique called principal components regression. The idea
is this: ﬁrst use PCA to reduce the dimensionality to K dimensions, and then use these low
dimensional features as input to regression. However, this technique does not work as well as
only the ﬁrst K (derived) dimensions are retained, and the remaining D −K dimensions are
entirely ignored. By contrast, ridge regression uses a “soft” weighting of all the dimensions.
Regularization effects of big data
Regularization is the most common way to avoid overﬁtting. However, another effective approach
— which is not always available — is to use lots of data. It should be intuitively obvious that
error to decrease to some plateau as N increases.
achieved by polynomial regression models of different degrees vs N (a plot of error vs training
set size is known as a learning curve). The level of the plateau for the test error consists of
two terms: an irreducible component that all models incur, due to the intrinsic variability of
the generating process (this is called the noise ﬂoor); and a component that depends on the
discrepancy between the generating process (the “truth”) and the model: this is called structural
error.
for models M2 and M25 is zero, since both are able to capture the true generating process.
However, the structural error for M1 is substantial, which is evident from the fact that the
plateau occurs high above the noise ﬂoor.
For any model that is expressive enough to capture the truth (i.e., one with small structural
zero faster for simpler models, since there are fewer parameters to estimate. In particular, for
ﬁnite training sets, there will be some discrepancy between the parameters that we estimate
and the best parameters that we could estimate given the particular model class. This is called
However, there are still reasons to study more sophisticated learning methods, because there
domain as web search, as soon as we want to start personalizing the results, the amount of data
available for any given user starts to look small again (relative to the complexity of the problem).
informatively sampled data can help even more; this is the motivation for an approach known as active learning, where
you get to choose your training data.size of training set
mse
 
 
train
test
(a)
size of training set
mse
 
 
train
test
(b)
size of training set
mse
 
 
train
test
(c)
size of training set
mse
 
 
train
test
(d)
and has high test error even given large amounts of training data. Figure generated by linregPolyVsN.
In such cases, we may want to learn multiple related models at the same time, which is known
data and to share it with tasks with little data. We will discuss ways to do later in the book.
Bayesian linear regression
Although ridge regression is a useful way to compute a point estimate, sometimes we want to
compute the full posterior over w and σ2. For simplicity, we will initially assume the noisethe general case, where we compute p(w, σ2|D). We assume throughout a Gaussian likelihood
model. Performing Bayesian inference with a robust likelihood is also possible, but requires more
Computing the posterior
In linear regression, the likelihood is given by
p(y|X, w, μ, σ2)
=
N(y|μ + Xw, σ2IN)
∝
exp

2σ2 (y −μ1N −Xw)T (y −μ1N −Xw)
	
p(y|X, w, σ2) ∝exp

	
where y =
N

N
assume the output has been centered, and write y for y −y1N.
The conjugate prior to the above Gaussian likelihood is also a Gaussian, which we will denote
by
p(w|X, y, σ2)
∝
N(w|w0, V0)N(y|Xw, σ2IN) = N(w|wN, VN)
wN
=
σ2 VNXT y
N
=
σ2 XT X
VN
=
λ = σ2
To gain insight into the posterior distribution (and not just its mode), let us consider a 1D
example:
y(x, w) = w0 + w1x + ϵ
the likelihood, the posterior, and some samples from the posterior predictive. In particular,
N(w|wN, VN) is a sample from the parameter posterior. Initially, when we sample from the
prior (ﬁrst row), our predictions are “all over the place”, since our prior is uniform. After we see
one data point (second row), our posterior becomes constrained by the corresponding likelihood,
and our predictions pass close to the observed data. However, we see that the posterior has W0 
 W1 
 x 
 y 
data space
 W0 
 W1 
 W0 
 W1 
 x 
 y 
 W0 
 W1 
 W0 
 W1 
 x 
 y 
 W0 
 W1 
 W0 
 W1 
 x 
 y 
likelihood
Sequential Bayesian updating of a linear regression model p(y|x) = N(y|w0x0 +w1x1, σ2).
current data point. Middle column: posterior given data so far, p(w|x1:n, y1:n) (so the ﬁrst line is the
observation. After we see two data points (third row), the posterior becomes much narrower,
row), the posterior is essentially a delta function centered on the true value, indicated by a white
cross. (The estimate converges to the truth since the data was generated from this model, and
Computing the posterior predictive
It’s tough to make predictions, especially about the future. — Yogi Berrapoint x is also Gaussian:
p(y|x, D, σ2)
=


N(y|xT w, σ2)N(w|wN, VN)dw
=
N(y|wT
Nx, σ2
N(x))
σ2
N(x)
=
σ2 + xT VNx
The variance in this prediction, σ2
N(x), depends on two terms: the variance of the observation
noise, σ2, and the variance in the parameters, VN. The latter translates into variance about
observations in a way which depends on how close x is to the training data D. This is illustrated
points, representing increased uncertainty. This is important for applications such as active
learning, where we want to model what we don’t know as well as what we do. By contrast, the
plugin approximation has constant sized error bars, since
p(y|x, D, σ2)
≈


N(y|xT w, σ2)δ ˆw(w)dw = p(y|x, ˆw, σ2)
Bayesian inference when σ2 is unknown *
σ2 was known. In the case where we use an uninformative prior, we will see some interesting
connections to frequentist statistics.
Conjugate prior
As usual, the likelihood has the form
p(y|X, w, σ2) = N(y|Xw, σ2IN)
form:
p(w, σ2)
=
NIG(w, σ2|w0, V0, a0, b0)
≜
N(w|w0, σ2V0)IG(σ2|a0, b0)
=
ba0
× exp


2σ2

plugin approximation (MLE)
 
 
prediction
training data
(a)
Posterior predictive (known variance)
 
 
prediction
training data
(b)
functions sampled from plugin approximation to posterior
(c)
functions sampled from posterior
(d)
Posterior predictive density, obtained by integrating out the parameters. Black curve is posterior mean,
linregPostPredDemo.
With this prior and likelihood, one can show that the posterior has the following form:
p(w, σ2|D)
=
NIG(w, σ2|wN, VN, aN, bN)
wN
=
VN
=
aN
=
bN
=

wT
N wN

The expressions for wN and VN are similar to the case where σ2 is known. The expression for
aN is also intuitive, since it just updates the counts. The expression for bN can be interpretedas follows: it is the prior sum of squares, b0, plus the empirical sum of squares, yT y, plus a
term due to the error in the prior on w.
The posterior marginals are as follows:
p(σ2|D)
=
IG(aN, bN)
p(w|D)
=
T (wN, bN
aN
VN, 2aN)
In particular, given m new test inputs ˜X, we have
p(˜y| ˜X, D)
=
T (˜y| ˜XwN, bN
aN
(Im + ˜XVN ˜XT ), 2aN)
close the test inputs are to the training data.
We will see below that if we use an uninformative prior, the posterior precision given N
N
= XT X. The unit information prior is deﬁned to contain as much
=
g = N.
Uninformative prior
An uninformative prior can be obtained by considering the uninformative limit of the conjugate
posterior is given by
p(w, σ2|D)
=
NIG(w, σ2|wN, VN, aN, bN)
wN
=
ˆwmle = (XT X)−1XT y
VN
=
aN
=
N −D
bN
=
s2
s2
≜
(y −X ˆwmle)T (y −X ˆwmlewj
E [wj|D]

var [wj|D]
sig
w0
*
w1
*
w2
*
w3
w4
*
w5
*
w6
w7
w8
w9
w10
Posterior mean, standard deviation and credible intervals for a linear regression model with an
uninformative prior ﬁt to the caterpillar data. Produced by linregBayesCaterpillar.
The marginal distribution of the weights is given by
p(w|D) = T (w| ˆw,
s2
N −DC, N −D)
An example where Bayesian and frequentist inference coincide *
p(wj|D) = T(wj| ˆwj, Cjjs2
N −D, N −D)
This is equivalent to the sampling distribution of the MLE which is given by the following (see
wj −ˆwj
sj
∼tN−D
where
sj =

s2Cjj
N −D
distributions.)
Consequently, the frequentist conﬁdence interval and the Bayesian marginal
credible interval for the parameters are the same in this case.
(The details of what the data mean don’t matter for our present purposes.) We can computefrequentist methods (see linregBayesCaterpillar for the code).
We can also use these marginal posteriors to compute if the coefficients are “signiﬁcantly”
by this measure, so we put a little star by them. It is easy to check that these results are the
gies. Also, note that the MLE does not even exist when N < D, so standard frequentist inference
theory breaks down in this setting. Bayesian inference theory still works, although it requires
the case where D > N.)
EB for linear regression (evidence procedure)
So far, we have assumed the prior is known. In this section, we describe an empirical Bayes
The evidence procedure provides an alternative to using cross validation. For example, in
maximum value found by the optimizer. We see that, in this example, we get the same result
comparable.)
The principle practical advantage of the evidence procedure over CV will become apparent
can be used to perform feature selection, using a technique known as automatic relevancy
determination or ARD. By contrast, it would not be possible to use CV to tune D different
The evidence procedure is also useful when comparing different kinds of models, since it
provides a good approximation to the evidence:
p(D|m)
=

 

p(D|w, m)p(w|m, η)p(η|m)dwdη
≈
max
η


p(D|w, m)p(w|m, η)p(η|m)dw
It is important to (at least approximately) integrate over η rather than setting it arbitrarily, forlog lambda
mse
(a)
log alpha
log evidence
(b)
indicated by the vertical line. Note the vertical scale is in log units. (c) Log marginal likelihood vs log(α).
The largest value is indicated by the vertical line. Figure generated by linregPolyVsRegDemo.
approach, in which we model our uncertainty about η rather than computing point estimates,
Exercises
The error on the test will always decrease as we get more training data, since the model will be better
can increase we we get more training data, until we reach some plateau. Explain why.
(Source: Jaakkola.)
When we have multiple independent outputs in linear regression, the model becomes
p(y|x, W) =
M

N(yj|wT
j xi, σ2
j )
Since the likelihood factorizes across dimensions, so does the MLE. Thus
ˆ
W = [ ˆw1, . . . , ˆwM]
where ˆwj = (XT X)−1Y:,j.x
y
Let us embed each xi into 2d using the following basis function:
The model becomes
ˆy = WT φ(x)
J(w, w0)
=
(y −Xw −w01)T (y −Xw −w01) + λwT w
is
ˆw0
=
y
w
=
(XT X + λI)−1XT y
Show that the MLE for the error variance in linear regression is given by
N
N


(yi −xT
This is just the empirical variance of the residual errors when we plug in our estimate of ˆw.
design matrix, so we can solve for the offset term w0 term and the other parameters w at the same time
using the normal equations. However, it is also possible to solve for w and w0 separately. Show that
ˆw0
=
N


i
N


i
xT
i w = y −xT w
So ˆw0 models the difference in the average output from the average predicted output. Also, show that
ˆw = (XT
c Xc)−1XT
c yc =
 N


(xi −x)(xi −x)T


(yi −y)(xi −x)

where Xc is the centered input matrix containing xc
i = xi −x along its rows, and yc = y −y is
the centered output vector. Thus we can ﬁrst compute ˆw on centered data, and then estimate w0 using
y −xT ˆw.this case is given by the following equations, which may be familiar from basic statistics classes:
w1
=

i(xi −x)(yi −¯y)

=

i xiyi −Nx y

i x2
i −Nx2
≈cov [X, Y ]
var [X]
w0
=
See linregDemo1 for a demo.
(Source: Jaakkola.) Consider ﬁtting the model ˆy = w0 + w1x using least squares. Unfortunately we did
not keep the original data, xi, yi, but we do have the following functions (statistics) of the data:
x(n)
=
n
n


n
n


yi
C(n)
xx
=
n
n


n
n


(xi −x)(yi −y), C(n)
n
n


looking at the old data, which we have not stored. (This is useful for online learning.) Show that we
can this for x as follows.
≜


xi =


=
x(n) +
This has the form: new estimate is old estimate plus correction. We see that the size of the correction
diminishes over time (i.e., as we get more samples). Derive a similar expression to update y
xy
recursively using
xy
=

Derive a similar expression to update Cxx.
e. Implement the online learning algorithm, i.e., write a function of the form [w,ss] = linregUpdateSS(ss,
x, y), where x and y are scalars and ss is a structure containing the sufficient statistics.
f. Plot the coefficients over “time”, using the dataset in linregDemo1.
(Speciﬁcally, use [x,y] =
polyDataMake(’sampling’,’thibaux’).) Check that they converge to the solution given by the
Turn in your derivation, code and plot.
(Source: Bolstad.) Consider ﬁtting a model of the form
p(y|x, θ) = N(y|w0 + w1x, σ2)
to the data shown below:time
weights
online linear regression
 
 
w0
w1
w0 batch
w1 batch
a. Compute an unbiased estimate of σ2 using
ˆσ2 =
N


and ˆw = ( ˆw0, ˆw1) is the MLE.
b. Now assume the following prior on w:
p(w) = p(w0)p(w1)
a Gaussian prior of the form p(w) = N(w|w0, V0). What are w0 and V0?
c. Compute the marginal posterior of the slope, p(w1|D, σ2), where D is the data above, and σ2 is the
unbiased estimate computed above. What is E

w1|D, σ2
and var

w1|D, σ2
Show your work. (You
can use Matlab if you like.) Hint: the posterior variance is a very small number!
Linear regression is the problem of estimating E[Y |x] using a linear function of the form w0 + wT x.
Typically we assume that the conditional distribution of Y given X is Gaussian. We can either estimate this
conditional Gaussian directly (a discriminative approach), or we can ﬁt a Gaussian to the joint distribution
of X, Y and then derive E[Y |X = x].
E[Y |x]
=
w0 + wT x
w0
=
y −xT w
w
=
(XT
c Xc)−1XT
c ycwhere Xc = X −¯X is the centered input matrix, and ¯X = 1nxT replicates x across the rows. Similarly,
yc = y −y is the centered output vector, and y = 1ny replicates y across the rows.
Show your work.
b. What are the advantages and disadvantages of this approach compared to the standard discriminative
approach?
following form:
p(w, σ2|D)
=
NIG(w, σ2|wN, VN, aN, bN)
VN
=
g
wN
=
g
aN
=
bN
=
s2
mleXT X ˆwmleLogistic regression
Introduction
One way to build a probabilistic classiﬁer is to create a joint model of the form p(y, x) and
then to condition on x, thereby deriving p(y|x). This is called the generative approach. An
native approach, and is the approach we adopt in this chapter. In particular, we will assume
models.
Model speciﬁcation
cation model:
p(y|x, w) = Ber(y|sigm(wT x))
linear decision boundary, whose normal (perpendicular) is given by w.
Model ﬁtting
In this section, we discuss algorithms for estimating the parameters of a logistic regression
model.x2
x1
x2
x1
x2
x1
x2
x1
x2
x1
x2
x1
x2
x1
x2
x1
x2
x1
x2
x1
w1
w2
Plots of sigm(w1x1 + w2x2).
Here w = (w1, w2) deﬁnes the normal to the decision
MLE
NLL(w)
=
−
N

i
=
−
N

NLL(w) =
N

Unlike linear regression, we can no longer write down the MLE in closed form. Instead, we
need to use an optimization algorithm to compute it. For this, we need to derive the gradient
and Hessian.(a)
(b)
steepestDescentDemo.
of this are given by the following
g
=
d
dwf(w) =

i
(μi −yi)xi = XT (μ −y)
H
=
d
dwg(w)T =

i
i =

i
i
=
XT SX
Hence the NLL is convex and has a unique global minimum. Below we discuss some methods
for ﬁnding this minimum.
Steepest descent
Perhaps the simplest algorithm for unconstrained optimization is gradient descent, also known
as steepest descent. This can be written as follows:
where ηk is the step size or learning rate. The main issue in gradient descent is: how should
we set the step size? This turns out to be quite tricky. If we use a constant learning rate, but
make it too small, convergence will be very slow, but if we make it too large, the method can fail
see that the algorithm starts oscillating up and down the sides of the valley and never converges
to the optimum.(a)
(b)
Figure generated by steepestDescentDemo. (b) Illustration of the fact that at the end of a line search
(top of picture), the local gradient of the function will be perpendicular to the search direction. Based on
teed to converge to a local optimum no matter where we start. (This property is called global
convergence, which should not be confused with convergence to the global optimum!)
By
Taylor’s theorem, we have
f(θ + ηd) ≈f(θ) + ηgT d
where d is our descent direction. So if η is chosen small enough, then f(θ +ηd) < f(θ), since
the gradient will be negative. But we don’t want to choose the step size η too small, or we will
move very slowly and may not reach the minimum. So let us pick η to minimize
φ(η) = f(θk + ηdk)
This is called line minimization or line search. There are various methods for solving this 1d
behavior.
A
we have found a stationary point, or g ⊥d, which means that exact search stops at a point
where the local gradient is perpendicular to the search direction. Hence consecutive directions
=In the optimization
Newton’s method
Evaluate gk = ∇f(θk);
Evaluate Hk = ∇2f(θk);
Solve Hkdk = −gk for dk;
Use line search to ﬁnd stepsize ηk along dk;
One can derive faster optimization methods by taking the curvature of the space (i.e., the
Hessian) into account.
These are called second order optimization metods.
The primary
example is Newton’s algorithm. This is an iterative algorithm which consists of updates of the
form
k gk
This algorithm can be derived as follows.
fquad(θ) = fk + gT
Let us rewrite this as
where
The minimum of fquad is at
k gkf(x)
fquad(x)
xk
(a)
f(x)
fquad(x)
xk
(b)
Illustration of Newton’s method for minimizing a 1d function.
(a) The solid curve is the
function f(x). The dotted line fquad(x) is its second order approximation at xk. The Newton step dk
nonconvex function. We ﬁt a quadratic around the current point xk and move to its stationary point,
generated by newtonsMethodNonConvex.
In its simplest form (as listed), Newton’s method requires that Hk be positive deﬁnite, which
will hold if the function is strictly convex. If not, the objective function is not convex, then
k gk may not be a descent direction (see
dk = −gk. The Levenberg Marquardt algorithm is an adaptive way to blend between Newton
steps and steepest descent steps. This method is widely used when solving nonlinear least
squares problems.
k gk
directly, we can solve the linear system of equations Hkdk = −gk for dk using conjugate
gradient (CG). If Hk is not positive deﬁnite, we can simply truncate the CG iterations as soon
as negative curvature is detected; this is called truncated Newton.
Iteratively reweighted least squares (IRLS)
Let us now apply Newton’s algorithm to ﬁnd the MLE for binary logistic regression. The Newton
=
wk −H−1gk
=
wk + (XT SkX)−1XT (y −μk)
=
(XT SkX)wk + XT (y −μk)
 
=
(XT SkX)−1XT [SkXwk + y −μk]
=
(XT SkX)−1XT Skzk
where we have deﬁned the working response as
k (y −μk)N

Since Sk is a diagonal matrix, we can rewrite the targets in component form (for each case
zki = wT
k xi +
yi −μki
This algorithm is known as iteratively reweighted least squares or IRLS for short, since at
each iteration, we solve a weighted least squares problem, where the weight matrix Sk changes
ηi = w0 + wT xi;
μi = sigm(ηi);
zi = ηi + yi−μi
si
;
S = diag(s1:N) ;
w = (XT SX)−1XT Sz;
Newton methods iteratively build up an approximation to the Hessian using information gleaned
from the gradient vector at each step. The most common method is called BFGS (named after
its inventors, Broyden, Fletcher, Goldfarb and Shanno), which updates the approximation to the
=
Bk + ykyT
k
yT
k sk
−(Bksk)(Bksk)T
sT
k Bksk
sk
=
yk
=
(under certain restrictions on the step size). We typically start with a diagonal approximation,
Hessian.k ,
as follows:
=

I −skyT
k
yT
k sk
	
Ck

I −yksT
k
yT
k sk
	
+ sksT
k
yT
k sk
Since storing the Hessian takes O(D2) space, for very large problems, one can use limited
k
is approximated by a diagonal plus low rank
k gk can be obtained by performing a sequence of inner
products with sk and yk, using only the m most recent (sk, yk) pairs, and ignoring older
good performance.
often the method of choice for most unconstrained smooth optimization problems that arise in
ℓ2 regularization
Just as we prefer ridge regression to linear regression, so we should prefer MAP estimation for
logistic regression to computing the MLE. In fact, regularization is important in the classiﬁcation
setting even if we have lots of data. To see why, suppose the data is linearly separable. In
this case, the MLE is obtained when ||w|| →∞, corresponding to an inﬁnitely steep sigmoid
function, I(wT x > w0), also known as a linear threshold unit. This assigns the maximal
amount of probability mass to the training data. However, such a solution is very brittle and
will not generalize well.
To prevent this, we can use ℓ2 regularization, just as we did with ridge regression. We note
that the new objective, gradient and Hessian have the following forms:
f ′(w)
=
NLL(w) + λwT w
g′(w)
=
g(w) + λw
H′(w)
=
H(w) + λI
Now we consider multinomial logistic regression, sometimes called a maximum entropy
classiﬁer. This is a model of the form
p(y = c|x, W) =
exp(wT
c x)

C
c′x)
A slight variant, known as a conditional logit model, normalizes over a different set of classes
for each data case; this can be useful for modeling choices that users make between different
sets of items that are offered to them.column vector.
ℓ(W)
=
log
N

C

μyic
ic =
N

C

=
N


 C

yicwT
c xi

−log

 C

exp(wT
c′xi)


Deﬁne the NLL as
f(w) = −ℓ(w)
structured, the notation gets a bit heavy, but the ideas are simple. It helps to deﬁne A ⊗B
be the kronecker product of matrices A and B. If A is an m × n matrix and B is a p × q
matrix, then A × B is the mp × nq block matrix
A ⊗B =
⎡
⎢⎣
a11B
· · ·
a1nB
...
...
...
am1B
· · ·
amnB
⎤
⎥⎦
g(W) = ∇f(w) =
N

(μi −yi) ⊗xi
g(W) =

i
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
(μi1 −yi1)xi1
(μi1 −yi1)xi2
(μi1 −yi1)xi3
(μi2 −yi2)xi1
(μi2 −yi2)xi2
(μi2 −yi2)xi3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
In other words, for each class c, the derivative for the weights in the c’th column is
∇wcf(W)
=

i
(μic −yic)xi
This has the same form as in the binary logistic regression case, namely an error term times xi.
(This turns out to be a general property of distributions in the exponential family, as we will seeH(W)
=
∇2f(w) =
N

i ) ⊗(xixT
i )
H(W)
=

i

μi1 −μ2
i1
μi2 −μ2
i2
	
⊗
⎛
⎝
xi1xi1
xi1xi2
xi1xi3
xi2xi1
xi2xi2
xi2xi3
xi3xi1
xi3xi2
xi3xi3
⎞
⎠
=

i

(μi1 −μ2
i1)Xi
(μi2 −μ2
i2)Xi
	
where Xi = xixT
i . In other words, the block c, c′ submatrix is given by
Hc,c′(W)
=

i
μic(δc,c′ −μi,c′)xixT
i
This is also a positive deﬁnite matrix, so there is a unique MLE.
Now consider minimizing
f ′(W) ≜−log p(D|w) −log p(W)
where p(W) = '
f ′(W)
=

c
g′(W)
=

c
wc)
H′(W)
=
that the Hessian has size O ((CD) × (CD)), which is C times more row and columns than
in the binary case, so limited memory BFGS is more appropriate than Newton’s method. See
logregFit for some Matlab code.
Bayesian logistic regression
It is natural to want to compute the full posterior over the parameters, p(w|D), for logistic
regression models. This can be useful for any situation where we want to associate conﬁdence
intervals with our predictions (e.g., this is necessary when solving contextual bandit problems,
Unfortunately, unlike the linear regression case, this cannot be done exactly, since there is no
convenient conjugate prior for logistic regression. We discuss one simple approximation below;
binary logistic regression.Laplace approximation
In this section, we discuss how to make a Gaussian approximation to a posterior distribution.
Z e−E(θ)
ized log posterior, E(θ) = −log p(θ, D), with Z = p(D) being the normalization constant.
where g is the gradient and H is the Hessian of the energy function evaluated at the mode:
g ≜∇E(θ)
((
θ∗, H ≜∂2E(θ)
∂θ∂θT |θ∗
ˆp(θ|D)
≈
Z e−E(θ∗) exp




=
Z = p(D)
≈


The last line follows from normalization constant of the multivariate Gaussian.
in the statistics community, the term “Laplace approximation” refers to a more sophisticated
method (see e.g.
It may therefore be better to use the term
A Gaussian approximation is often a
size increases, for reasons analogous to the central limit theorem.
(In physics, there is an
analogous technique known as a saddle point approximation.)
Derivation of the BIC
We can use the Gaussian approximation to write the log marginal likelihood as follows, dropping
irrelevant constants:
The penalization terms which are added to the log p(D|θ∗) are sometimes called the OccamWe now focus on approximating the third term.
We have H = 
N
∇∇log p(Di|θ). Let us approximate each Hi by a ﬁxed matrix ˆH. Then we have
log |H| = log |N ˆH| = log(N d| ˆH|) = D log N + log | ˆH|
where D = dim(θ) and we have assumed H is full rank. We can drop the log | ˆH| term, since
it is independent of N, and thus will get overwhelmed by the likelihood. Putting all the pieces
log p(D) ≈log p(D|ˆθ) −D
Gaussian approximation for logistic regression
Now let us apply the Gaussian approximation to logistic regression. We will use a a Gaussian
posterior is given by
p(w|D)
≈
where ˆw = arg minw E(w), E(w) = −(log p(D|w) + log p(w)), and H = ∇2E(w)| ˆw.
There are many
likelihood by driving ||w|| to inﬁnity (subject to being on this line), since large regression weights
make the sigmoid function very steep, turning it into a step function. Consequently the MLE is
not well deﬁned when the data is linearly separable.
Multiplying this spherical prior by the likelihood surface results in a highly skewed posterior,
regions of parameter space (in a “soft” fashion) which disagree with the data.) The MAP estimate
is shown by the blue dot. Unlike the MLE, this is not at inﬁnity.
a symmetric distribution, and therefore not a great approximation. Of course, it gets the mode
correct (by construction), and it at least represents the fact that there is more uncertainty along
separating lines) than perpendicular to this.
Although a crude approximation, this is surely
better than approximating the posterior by a delta function, which is what MAP estimation does.
Approximating the posterior predictive
Given the posterior, we can compute credible intervals, perform hypothesis tests, etc., just as we
focusses on prediction. The posterior predictive distribution has the form
p(y|x, D)
=


p(y|x, w)p(w|D)dwdata
(a)
Log−Likelihood
(b)
Log−Unnormalised Posterior
(c)
Laplace Approximation to Posterior
(d)
in parameter space, corresponding to the lines in (a). (c) Unnormalized log posterior (assuming vague
spherical prior).
(d) Laplace approximation to posterior.
Based on a ﬁgure by Mark Girolami.
Figure
generated by logregLaplaceGirolamiDemo.
Unfortunately this integral is intractable.
form
≈
where E [w] is the posterior mean. In this context, E [w] is called the Bayes point. Of course,
below.(a)
decision boundary for sampled w
(b)
(c)
(d)
Posterior predictive distribution for a logistic regression model in 2d. Top left: contours of
over these samples. Bottom right: moderated output (probit approximation). Based on a ﬁgure by Mark
Girolami. Figure generated by logregLaplaceGirolamiDemo.
Monte Carlo approximation
A better approach is to use a Monte Carlo approximation, as follows:
≈
S
S

sigm((ws)T x)
where ws ∼p(w|D) are samples from the posterior. (This technique can be trivially extended
these samples for prediction. If we made a Gaussian approximation to the posterior, we can
draw independent samples from the Gaussian using standard methods.(a)
 
 
sigmoid
probit
(b)
(a) Posterior predictive density for SAT data. The red circle denotes the posterior mean, the
blue cross the posterior median, and the blue lines denote the 5th and 95th percentiles of the predictive
distribution. Figure generated by logregSATdemoBayes. (b) The logistic (sigmoid) function sigm(x) in

2006b). Figure generated by probitPlot. Figure generated by probitRegDemo.
shows the average of these samples. By averaging over multiple predictions, we see that the
uncertainty in the decision boundary “splays out” as we move further from the training data.
So although the decision boundary is linear, the posterior predictive density is not linear. Note
also that the posterior mean decision boundary is roughly equally far from both classes; this is
posterior predictive; the small blue star is the median. We see that, with the Bayesian approach,
we are able to model our uncertainty about the probability a student will pass the exam based
on his SAT score, rather than just getting a point estimate.
Probit approximation (moderated output) *
If we have a Gaussian approximation to the posterior p(w|D) ≈N(w|mN, VN), we can also
compute a deterministic approximation to the posterior predictive distribution, at least in the
binary case. We proceed as follows:
≈


sigm(wT x)p(w|D)dw =


sigm(a)N(a|μa, σ2
a)da
a
≜
wT x
μa
≜
E [a] = mT
Nx
σ2
a
≜
var [a] =


p(a|D)[a2 −E

a2 
]da
=

Thus we see that we need to evaluate the expectation of a sigmoid with respect to a Gaussian.
This can be approximated by exploiting the fact that the sigmoid function is similar to the
probit function, which is given by the cdf of the standard normal:
Φ(a) ≜

 a
−∞
The advantage of using the probit is that one can convolve it with a Gaussian analytically:


Φ(λa)N(a|μ, σ2)da = Φ

a
	
We now plug in the approximation sigm(a) ≈Φ(λa) to both sides of this equation to get


sigm(a)N(a|μ, σ2)da
≈
sigm(κ(σ2)μ)
κ(σ2)
≜
Applying this to the logistic regression model we get the following expression (ﬁrst suggested in
≈
sigm(κ(σ2
a)μa)
However, the decision
approximation.
So the number of misclassiﬁcations will be the same for the two methods,
Residual analysis (outlier detection) *
It is sometimes useful to detect data cases which are “outliers”. This is called residual analysis
or case analysis. In a regression setting, this can be performed by computing ri = yi−ˆyi, where
are correct.
quantiles of a Gaussian distribution against the N empirical quantiles of the ri. Points that
deviate from the straightline are potential outliers.Classical methods, based on residuals, do not work well for binary data, because they rely
on asymptotic normality of the test statistics.
However, adopting a Bayesian approach, we
can just deﬁne outliers to be points which which p(yi|ˆyi) is small, where we typically use
ˆyi = sigm( ˆwT xi). Note that ˆw was estimated from all the data. A better method is to exclude
(xi, yi) from the estimate of w when predicting yi. That is, we deﬁne outliers to be points
by
p(yi|xi, x−i, y−i) =


p(yi|xi, w)

i′̸=i
p(yi′|xi′, w)p(w)dw
Online learning and stochastic optimization
Traditionally machine learning is performed offline, which means we have a batch of data, and
we optimize an equation of the following form
N
N

f(θ, zi)
where zi = (xi, yi) in the supervised case, or just xi in the unsupervised case, and f(θ, zi) is
some kind of loss function. For example, we might use
f(θ, zi) = −log p(yi|xi, θ)
in which case we are trying to maximize the likelihood. Alternatively, we might use
f(θ, zi) = L(yi, h(xi, θ))
where h(xi, θ) is a prediction function, and L(y, ˆy) is some other loss function such as squared
error or the Huber loss. In frequentist decision theory, the average loss is called the risk (see
for details).
However, if we have streaming data, we need to perform online learning, so we can update
our estimates as each new data point arrives rather than waiting until “the end” (which may
never occur). And even if we have a batch of data, we might want to treat it like a stream if it is
are too large to hold in memory. First note that the naive implementation makes a pass over the data ﬁle, from the
beginning to end, accumulating the sufficient statistics and gradients as it goes; then an update is performed and the
process repeats. Unfortunately, at the end of each pass, the data from the beginning of the ﬁle will have been evicted
from the cache (since are are assuming it cannot all ﬁt into memory). Rather than going back to the beginning of the
ﬁle and reloading it, we can simply work backwards from the end of the ﬁle, which is already in memory. We thenOnline learning and regret minimization
Suppose that at each step, “nature” presents a sample zk and the “learner” must respond with
online learning is the regret, which is the averaged loss incurred relative to the best we could
have gotten in hindsight using a single ﬁxed parameter value:
k
k

f(θt, zt) −min
θ∗∈Θ
k
k

f(θ∗, zt)
stock j, and let zj be the return on this stock. Our loss function is f(θ, z) = −θT z. The regret
is how much better (or worse) we did by trading at each step, rather than adopting a “buy and
hold” strategy using an oracle to choose which stocks to buy.
is as follows: at each step k, update the parameters using
∇f(θk, zk) is the gradient, and ηk is the step size. (The projection step is only needed if
details.) Below we will see how this approach to regret minimization relates to more traditional
objectives, such as MLE.
There are a variety of other approaches to regret minimization which are beyond the scope
Stochastic optimization and risk minimization
Now suppose that instead of minimizing regret with respect to the past, we want to minimize
expected loss in the future, as is more common in (frequentist) statistical learning theory. That
is, we want to minimize
f(θ) = E [f(θ, z)]
where the expectation is taken over future data.
Optimizing functions where some of the
Suppose we receive an inﬁnite stream of samples from the distribution. One way to optimize
typically want a single parameter estimate, we can use a running average:
k
k

θt
it is also possible to apply stochastic optimization algorithms to deterministic objectives. Examples include simulatedSetting the step size
We now discuss some sufficient conditions on the learning rate to guarantee convergence of
∞

ηk = ∞,
∞

η2
k < ∞.
The set of values of ηk over time is called the learning rate schedule. Various formulas are
ηk = (τ0 + k)−κ
which old values of are forgotten.
The need to adjust these tuning parameters is one of the main drawback of stochastic
data, and try a range of η values on this subset; then choose the one that results in the fastest
decrease in the objective and apply it to all the rest of the data. Note that this may not result
in convergence, but the algorithm can be terminated when the performance improvement on a
One drawback of SGD is that it uses the same step size for all parameters. We now brieﬂy
make an update as follows:
gi(k)
τ0 +

si(k)
where the diagonal step size vector is the gradient vector squared, summed over all time steps.
This can be recursively updated as follows:
method was original derived for the regret minimization case, but it can be applied more
generally.SGD compared to batch learning
If we don’t have an inﬁnite data stream, we can “simulate” one by sampling data points at
expectation with respect to the empirical distribution.
Randomly permute data;
g = ∇f(θ, zi);
θ ←projΘ(θ −ηg);
Update η;
In theory, we should sample with replacement, although in practice it is usually better to
randomly permute the data and sample without replacement, and then to repeat. A single such
get a fairly good estimate of the gradient by looking at just a few examples. Carefully evaluating
precise gradients using large datasets is often a waste of time, since the algorithm will have
to recompute the gradient again anyway at the next step. It is often a better use of computer
time to have a noisy estimate and to move rapidly through parameter space. As an extreme
example, suppose we double the training set by duplicating every example. Batch methods will
take twice as long, but online methods will be unaffected, since the direction of the gradient
has not changed (doubling the size of the data changes the magnitude of the gradient, but that
is irrelevant, since the gradient is being scaled by the step size anyway).
In addition to enhanced speed, SGD is often less prone to getting stuck in shallow local
minima, because it adds a certain amount of “noise”. Consequently it is quite popular in the
The LMS algorithm
As an example of SGD, let us consider how to compute the MLE for linear regression in an
k is given by
gk = xi(θT
k xi −yi)black line = LMS trajectory towards LS soln (red cross)
w0
w1
(a)
RSS vs iteration
(b)
Note that it does not decrease monotonically. Figure generated by LMSdemo.
where i = i(k) is the training example to use at iteration k. If the data set is streaming, we use
to interpret: it is the feature vector xk weighted by the difference between what we predicted,
ˆyk = θT
k xk, and the true response, yk; hence the gradient acts like an error signal.
After computing the gradient, we take a step along it as follows:
(There is no need for a projection step, since this is an unconstrained optimization problem.)
This algorithm is called the least mean squares or LMS algorithm, and is also known as the
Note that LMS may require multiple passes through the data to ﬁnd the optimum.
By
contrast, the recursive least squares algorithm, which is based on the Kalman ﬁlter and which
The perceptron algorithm
Now let us consider how to ﬁt a binary logistic regression model in an online manner. The
formWe now consider an approximation to this algorithm. Speciﬁcally, let
ˆyi = arg max
gradient expression with ˆyi. Thus the approximate gradient becomes
case, our prediction becomes
ˆyi = sign(θT xi)
At each step, we update the weight vector by adding on the gradient. The key observation is
that, if we predicted correctly, then ˆyi = yi, so the (approximate) gradient is zero and we do
not change the weight vector. But if xi is misclassiﬁed, we update the weights as follows: If
the learning rate η and just write the update, in the case of a misclassiﬁcation, as
linearly separable, the algorithm will not converge, and even if it does converge, it may take
a long time. There are much better ways to train logistic regression models (such as using
the perceptron algorithm is historically important: it was one of the ﬁrst machine learning
hardware. In addition, the algorithm can be used to ﬁt models where computing marginals
p(yi|x, θ) is more expensive than computing the MAP output, arg maxy p(y|x, θ); this arises
A Bayesian view
Another approach to online learning is to adopt a Bayesian view. This is conceptually quite
simple: we just apply Bayes rule recursively:
This has the obvious advantage of returning a posterior instead of just a point estimate. It also
cannot be used in an online setting. Finally, it has the (less obvious) advantage that it can bei ←k mod N;
if ˆyi ̸= yi then
else
quicker than SGD. To see why, note that by modeling the posterior variance of each parameter
in addition to its mean, we effectively associate a different learning rate for each parameter
variances can then be adapted using the usual rules of probability theory. By contrast, getting
regression model online. Unlike the LMS algorithm, this converges to the optimal (offline) answer
Generative vs discriminative classiﬁers
sigm(wT x). The decision boundary is therefore a linear function of x in both cases. Note,
however, that many generative models can give rise to a logistic regression posterior, e.g., if each
are much stronger than the assumptions made by logistic regression.
inative model, we usually maximize the conditional log likelihood 
N
when ﬁtting a generative model, we usually maximize the joint log likelihood, 
N
When the Gaussian assumptions made by GDA are correct, the model will need less training
data than logistic regression to achieve a certain level of performance, but if the GaussianThis is
because discriminative models do not need to model the distribution of the features. This is
the class posterior, p(y = c|x), is a simple sigmoidal function, centered on the threshold value
“job” is in some sense easier. However, accuracy is not the only important factor when choosing
a method. Below we discuss some other advantages and disadvantages of each approach.
Pros and cons of each approach
•
Easy to ﬁt? As we have seen, it is usually very easy to ﬁt generative classiﬁers. For example,
by simple counting and averaging. By contrast, logistic regression requires solving a convex
•
Fit classes separately? In a generative classiﬁer, we estimate the parameters of each class
conditional density independently, so we do not have to retrain the model when we add
more classes. In contrast, in discriminative models, all the parameters interact, so the whole
model must be retrained if we add a new class. (This is also the case if we train a generative
•
Handle missing features easily? Sometimes some of the inputs (components of x) are not
observed. In a generative classiﬁer, there is a simple method for dealing with this, as we
to this problem, since the model assumes that x is always available to be conditioned on
•
which uses unlabeled data to help solve a supervised task. This is fairly easy to do using
with discriminative models.
•
Symmetric in inputs and outputs?
We can run a generative model “backwards”, and
infer probable inputs given the output by computing p(x|y). This is not possible with a
discriminative model. The reason is that a generative model deﬁnes a joint distribution on x
and y, and hence treats both inputs and outputs symmetrically.
•
Can handle feature preprocessing? A big advantage of discriminative methods is that they
allow us to preprocess the input in arbitrary ways, e.g., we can replace x with φ(x), which
It is often hard to
in complex ways.
•
are usually better calibrated in terms of their probability estimates.
We see that there are arguments for and against both kinds of models. It is therefore usefulLinear Multinomial Logistic Regression
(a)
Kernel−RBF Multinomial Logistic Regression
(b)
Figure generated by logregMultinomKernelDemo.
x
class conditional densities
(a)
x
(b)
= c) (left) may be more complex than the
class posteriors p(y = c|x) (right).
Figure generated by
generativeVsDiscrim.
regression techniques we cover in this book.
Dealing with missing data
Sometimes some of the inputs (components of x) are not observed; this could be due to a
sensor failure, or a failure to complete an entry in a survey, etc. This is called the missing data
of the biggest advantages of generative models.
that speciﬁes whether each value xi is observed or not.
The joint model has the form
p(xi, ri|θ, φ) = p(ri|xi, φ)p(xi|θ), where φ are the parameters controlling whether the itemModel
Section
Discriminant analysis
Classif
Gen
Param
Naive Bayes classiﬁer
Classif
Gen
Param
Classif
Gen
Param
Linear regression
Regr
Discrim
Param
Logistic regression
Classif
Discrim
Param
Both
Discrim
Param
Mixture of experts
Both
Discrim
Param
Multilayer perceptron (MLP)/ Neural network
Both
Discrim
Param
Conditional random ﬁeld (CRF)
Classif
Discrim
Param
K nearest neighbor classiﬁer
Classif
Gen
Non
(Inﬁnite) Mixture Discriminant analysis
Classif
Gen
Non
Classiﬁcation and regression trees (CART)
Both
Discrim
Non
Boosted model
Both
Discrim
Non
Both
Discrim
Non
Relevance vector machine (RVM)
Both
Discrim
Non
Support vector machine (SVM)
Both
Discrim
Non
Gaussian processes (GP)
Both
Discrim
Non
Smoothing splines
Regr
Discrim
Non
List of various models for classiﬁcation and regression which we discuss in this book. Columns
are as follows: Model name; is the model suitable for classiﬁcation, regression, or both; is the model
tSupervised.html for the PMTK equivalents of these models. Any generative probabilistic model (e.g.,
HMMs, Boltzmann machines, Bayesian networks, etc.) can be turned into a classiﬁer by using it as a class
conditional density.
is observed or not. If we assume p(ri|xi, φ) = p(ri|φ), we say the data is missing completely
at random or MCAR. If we assume p(ri|xi, φ) = p(ri|xo
i , φ), where xo
i is the observed part of
xi, we say the data is missing at random or MAR. If neither of these assumptions hold, we say
the data is not missing at random or NMAR. In this case, we have to model the missing data
mechanism, since the pattern of missingness is informative about the values of the missing data
and the corresponding parameters. This is the case in most collaborative ﬁltering problems, for
MAR.
ness only at test time (so the training data is complete data), from the harder case when there
is missingness also at training time. We will discuss these two cases below. Note that the class
label is always missing at test time, by deﬁnition; if the class label is also sometimes missing atMissing data at test time
In a generative classiﬁer, we can handle features that are MAR by marginalizing them out. For
example, if we are missing the value of x1, we can compute
p(y = c|x2:D, θ)
∝
p(y = c|θ)p(x2:D|y = c, θ)
=
p(y = c|θ)

x1
p(x1, x2:D|y = c, θ)
If we make the naive Bayes assumption, the marginalization can be performed as follows:

x1
p(x1, x2:D|y = c, θ) =

x1
p(x1|θ1c)
 D

p(xj|θjc) =
D

p(xj|θjc)
where we exploited the fact that 
can simply ignore missing features at test time. Similarly, in discriminant analysis, no matter
what regularization method was used to estimate the parameters, we can always analytically
Missing data at training time
Missing data at training time is harder to deal with. In particular, computing the MLE or MAP
Fisher’s linear discriminant analysis (FLDA) *
Discriminant analysis is a generative approach to classiﬁcation, which requires ﬁtting an MVN to
the features. As we have discussed, this can be problematic in high dimensions. An alternative
matrix, z = Wx, where W is a L × D matrix. One approach to ﬁnding W would be to use
are essentially equivalent. However, PCA is an unsupervised technique that does not take class
labels into account. Thus the resulting low dimensional features are not necessarily optimal
An alternative approach is to ﬁnd the matrix
The assumption of Gaussianity is reasonable since we are
Fisher’s linear discriminant analysis, or FLDA.
FLDA is an interesting hybrid of discriminative and generative techniques. The drawback ofmeans
fisher
pca
(a)
fisher
(b)
pca
(c)
Example of Fisher’s linear discriminant. (a) Two class data in 2D. Dashed green line = ﬁrst
principal basis vector. Dotted red line = Fisher’s linear discriminant vector. Solid black line joins the
Projection of points onto PCA vector shows poor class separation. Figure generated by fisherLDAdemo.technique.
Derivation of the optimal 1d projection
N1

N2

xi
projection of the data onto the line. The variance of the projected points is proportional to
s2
k =

The goal is to ﬁnd w such that we maximize the distance between the means, m2 −m1, while
also ensuring the projected clusters are “tight”:
s2
We can rewrite the right hand side of the above in terms of w as follows
J(w) = wT SBw
wT SW w
SB = (μ2 −μ1)(μ2 −μ1)T
SW =

(xi −μ1)(xi −μ1)T +

(xi −μ2)(xi −μ2)T
To see this, note that
wT SBw = wT (μ2 −μ1)(μ2 −μ1)T w = (m2 −m1)(m2 −m1)
and
wT SW w
=

wT (xi −μ1)(xi −μ1)T w +

=


SBw = λSW w(a)
(b)
(a) PCA projection of vowel data to 2d. (b) FLDA projection of vowel data to 2d. We see there
fisherDiscrimVowelDemo, by Hannes Bretschneider.
where
λ = wT SBw
wT SW w
it to a regular eigenvalue problem:
W SBw = λw
However, in the two class case, there is a simpler solution. In particular, since
SBw = (μ2 −μ1)(μ2 −μ1)T w = (μ2 −μ1)(m2 −m1)
λ w
=
W (μ2 −μ1)(m2 −m1)
w
∝
W (μ2 −μ1)
Since we only care about the directionality, and not the scale factor, we can just set
W (μ2 −μ1)
matrix is isotropic, then w is proportional to the vector that joins the class means. This is an
Extension to higher dimensions and multiple classes
We can extend the above idea to multiple classes, and to higher dimensional subspaces, by
ﬁnding a projection matrix W which maps from D to L so as to maximize
J(W) = |WΣBWT |
|WΣW WT |where
ΣB
≜

c
Nc
N (μc −μ)(μc −μ)T
ΣW
≜

c
Nc
N Σc
Σc
≜
Nc

(xi −μc)(xi −μc)T
The solution can be shown to be
W = Σ
W U
where U are the L leading eigenvectors of Σ
W ΣBΣ
is singular, we can ﬁrst perform PCA on all the data.)
than PCA.
severe restriction which limits the usefulness of FLDA.
Probabilistic interpretation of FLDA *
To ﬁnd a valid probabilistic interpretation of FLDA, we follow the approach of (Kumar and Andreo
works as follows. Let W be a D × D invertible matrix, and let zi = Wxi be a transformed
version of the data. We now ﬁt full covariance Gaussians to the transformed data, one per class,
H = D −L components will be shared across classes, and will thus not be discriminative. That
is, we use
p(zi|θ, yi = c)
=
N(zi|μc, Σc)
μc
≜
(mc; m0)
Σc
≜

Sc
S0
	
where m0 is the shared H dimensional mean and S0 is the shared H × H covariace. The pdf
of the original (untransformed) data is given by
p(xi|yi = c, W, θ)
=
|W| N(Wxi|μc, Σc)
=
|W| N(WLxi|mc, Sc) N(WHxi|m0, S0)
where W =
WL
WH
	
W using gradient methods.In view of this this result, it should be clear that HLDA will outperform LDA if the class
covariances are not equal within the discriminative subspace (i.e., if the assumption that Σc is
independent of c is a poor assumption). This is easy to demonstrate on synthetic data, and is
Furthermore, we can extend the model by allowing each class to use its own projection matrix;
Exercises
•
on the list. The list contains words such as “business”, “free”, “george”, etc. (The data was collected by
George Forman, so his name occurs quite a lot.)
•
the list. The characters are
;
(
[
!
$
#
•
•
•
One can imagine performing several kinds of preprocessing to this data.
Try each of the following
separately:
For each version of the data, ﬁt a logistic regression model. Use cross validation to choose the strength
of the ℓ2 regularizer. Report the mean error rate on the training and test sets. You should get numbers
similar to this:
method
train
test
stnd
log
binary
(The precise values will depend on what regularization value you choose.) Turn in your code and numerical
results.a. Use naiveBayesFit and naiveBayesPredict on the binarized spam data. What is the training and
test error? (You can try different settings of the pseudocount α if you like (this corresponds to the
with maximum likelihood. What are the training and test error rates on the standardized data and the
a. Let σ(a) =
dσ(a)
da
b. Using the previous result and the chain rule of calculus, derive an expression for the gradient of the
positive, and that X is full rank.)
∂μik
∂ηij = μik(δkj −μij)
b. Hence show that
∇wcℓ
=


i
(yic −μic)xi
Hint: use the chain rule and the fact that 
c. Show that the block submatrix of the Hessian for classes c and c′ is given by
Hc,c′
=
−


i
μic(δc,c′ −μi,c′)xixT
i
Multiclass logistic regression has the form
p(y = c|x, W) =
exp(wc0 + wT
c x)
C
k x)
p(y = c|x, W) =
exp(wc0 + wT
c x)
k x)If we don’t “clamp” one of the vectors to some constant value, the parameters will be unidentiﬁable.
by optimizing
N


log p(yi|xi, W) −λ
C


Show that at the optimum we have C
(Source: Jaaakkola.). Consider minimizing
where
ℓ(w, D) =
|D|


log σ(yixT
i w)
(Source: Jaaakkola.)
Suppose we ﬁt the model by maximum likelihood, i.e., we minimize
J(w) = −ℓ(w, Dtrain)
where ℓ(w, Dtrain) is the log likelihood on the training set. Sketch a possible decision boundary
corresponding to ˆw. (Copy the ﬁgure ﬁrst (a rough sketch is enough), and then superimpose your
answer on your copy, since you will need multiple versions of this ﬁgure). Is your answer (decision
boundary) unique? How many classiﬁcation errors does your method make on the training set?
b. Now suppose we regularize only the w0 parameter, i.e., we minimize
J0(w) = −ℓ(w, Dtrain) + λw2
unregularized. Sketch a possible decision boundary. How many classiﬁcation errors does your method
make on the training set? Hint: consider the behavior of simple linear regression, w0 + w1x1 + w2x2
c. Now suppose we heavily regularize only the w1 parameter, i.e., we minimize
J1(w) = −ℓ(w, Dtrain) + λw2
Sketch a possible decision boundary. How many classiﬁcation errors does your method make on the
training set?Data for logistic regression question.
d. Now suppose we heavily regularize only the w2 parameter. Sketch a possible decision boundary. How
many classiﬁcation errors does your method make on the training set?Generalized linear models and the
exponential family
Introduction
We have now encountered a wide variety of probability distributions: the Gaussian, the Bernoulli,
the Student t, the uniform, the gamma, etc. It turns out that most of these are members of a
various properties of this family. This allows us to derive theorems and algorithms with very
broad applicability.
density in order to make a generative classiﬁer.
In addition, we will discuss how to build
discriminative models, where the response variable has an exponential family distribution, whose
mean is a linear function of the inputs; this is known as a generalized linear model, and
generalizes the idea of logistic regression to other kinds of response variables.
The exponential family
Before deﬁning the exponential family, we mention several reasons why it is important:
•
It can be shown that, under certain regularity conditions, the exponential family is the only
for online learning, as we will see later.
•
The exponential family is the only family of distributions for which conjugate priors exist,
•
The exponential family can be shown to be the family of distributions that makes the least
•
•
have ﬁxed support independent of the parameter values.Deﬁnition
exponential family if it is of the form
p(x|θ)
=
Z(θ)h(x) exp[θT φ(x)]
=
h(x) exp[θT φ(x) −A(θ)]
where
Z(θ)
=


X m h(x) exp[θT φ(x)]dx
A(θ)
=
log Z(θ)
vector of sufficient statistics, Z(θ) is called the partition function, A(θ) is called the log
If
φ(x) = x, we say it is a natural exponential family.
p(x|θ) = h(x) exp[η(θ)T φ(x) −A(η(θ))]
dim(θ) < dim(η(θ)), it is called a curved exponential family, which means we have more
sufficient statistics than parameters. If η(θ) = θ, the model is said to be in canonical form.
We will assume models are in canonical form unless we state otherwise.
Examples
Let us consider some examples to make things clearer.
Bernoulli
can just deﬁne


x log

μ
	
Now we have φ(x) = x, θ = log

μ

μ = sigm(θ) =
Multinoulli
We can represent the multinoulli as a minimal exponential family as follows (where xk = I(x =
k)):
Cat(x|μ)
=
K

μxk
k = exp
 K


=
exp




xk


μk)

=
exp

xk log


μk


μk)

=
exp

xk log
 μk
μK
	

Cat(x|θ)
=
exp(θT φ(x) −A(θ))
θ
=
μK
μK
]
φ(x)
=
We can recover the mean parameters from the canonical parameters using
μk
=
From this, we ﬁnd
μK
=


and hence
A(θ)
=
log




KUnivariate Gaussian
The univariate Gaussian can be written in exponential family form as follows:
N(x|μ, σ2)
=
(2πσ2)
=
(2πσ2)
2σ2 x2 + μ
σ2 x −
=
Z(θ) exp(θT φ(x))
where
θ
=
μ/σ2
2σ2
	
φ(x)
=

x
x2
	
Z(μ, σ2)
=
√
2πσ exp[ μ2
2σ2 ]
A(θ)
=
−θ2
Not all distributions of interest belong to the exponential family. For example, the uniform
distribution, X ∼Unif(a, b), does not, since the support of the distribution depends on the
have the required form.
Log partition function
An important property of the exponential family is that derivatives of the log partition function
For this reason, A(θ) is
sometimes called a cumulant function.
second moments are its mean E [X] and E

X2
.derivative we have
dA
=
d

log


exp(θφ(x))h(x)dx
	
=
d
)
exp(θφ(x))h(x)dx
)
exp(θφ(x))h(x)dx
=
)
φ(x) exp(θφ(x))h(x)dx
exp(A(θ))
=


φ(x) exp(θφ(x) −A(θ))h(x)dx
=


φ(x)p(x)dx = E [φ(x)]
For the second derivative we have
d2A
=


φ(x) exp (θφ(x) −A(θ)) h(x)(φ(x) −A′(θ))dx
=


φ(x)p(x)(φ(x) −A′(θ))dx
=


φ2(x)p(x)dx −A′(θ)


φ(x)p(x)dx
=
E

φ2(X)
 
where we used the fact that A′(θ) = dA
In the multivariate case, we have that
∂2A
∂θi∂θj
= E [φi(x)φj(x)] −E [φi(x)] E [φj(x)]
and hence
∇2A(θ) = cov [φ(x)]
Example: the Bernoulli distribution
given by
dA
The variance is given by
d2A
=
d
=
e−θMLE for the exponential family
The likelihood of an exponential family model has the form
p(D|θ) =
 N

h(xi)

g(θ)N exp


η(θ)T [
N

φ(xi)]

We see that the sufficient statistics are N and
φ(D) = [
N

φ1(xi), . . . ,
N

φK(xi)]
For example, for the Bernoulli model we have φ = [
Gaussian, we have φ = [
i xi, 
i x2
i ]. (We also need to know the sample size, N.)
exponential family is the only family of distributions with ﬁnite sufficient statistics. (Here, ﬁnite
means of a size independent of the size of the data set.)
One of the conditions required in this theorem is that the support of the distribution not be
dependent on the parameter. For a simple example of such a distribution, consider the uniform
distribution
The likelihood is given by
So the sufficient statistics are N and s(D) = maxi xi.
form distribution is not in the exponential family because its support set, X, depends on the
parameters.
We now descibe how to compute the MLE for a canonical exponential family model. Given
log p(D|θ) = θT φ(D) −NA(θ)
concave, and hence has a unique global maximum. To derive this maximum, we use the fact
that the derivative of the log partition function yields the expected value of the sufficient statistic
∇θ log p(D|θ) = φ(D) −NE [φ(X)]
Setting this gradient to zero, we see that at the MLE, the empirical average of the sufficient
statistics must equal the model’s theoretical expected sufficient statistics, i.e., ˆθ must satisfy
N
N

φ(xi)This is called moment matching. For example, in the Bernoulli distribution, we have φ(X) =
N
N

Bayes for the exponential family *
We have seen that exact Bayesian analysis is considerably simpliﬁed if the prior is conjugate to
the likelihood. Informally this means that the prior p(θ|τ) has the same form as the likelihood
p(D|θ). For this to make sense, we require that the likelihood have ﬁnite sufficient statistics, so
that we can write p(D|θ) = p(s(D)|θ). This suggests that the only family of distributions for
which conjugate priors exist is the exponential family. We will derive the form of the prior and
posterior below.
Likelihood
The likelihood of the exponential family is given by
p(D|θ) ∝g(θ)N exp

η(θ)T sN

where sN = 
N
p(D|η) ∝exp(NηT s −NA(η))
N sN.
Prior
The natural conjugate prior has the form


Posterior
The posterior is given by
p(η|D)
∝
exp


=
ν0 + N
)Posterior predictive density
Let us derive a generic expression for the predictive density for future observables D′ =
(˜x1, . . . , ˜xN ′) given past data D = (x1, . . . , xN) as follows.
For notational brevity, we
will combine the sufficient statistics with the size of the data, as follows:
˜s(D) = (N, s(D)), and ˜s(D′) = (N ′, s(D′)). So the prior becomes
The likelihood and posterior have a similar form. Hence
p(D′|D)
=


=
⎡
⎣
N ′

h(˜xi)
⎤


× exp
⎛
⎝
k
ηk(θ)(τk +
N

sk(xi) +
N ′

sk(˜xi)
⎞
=
⎡
⎣
N ′

h(˜xi)
⎤
normalizer of the posterior divided by the normalizer of the prior, multiplied by a constant.
Example: Bernoulli distribution
The likelihood is given by


log(
θ

i
xi

Hence the conjugate prior is given by
p(θ|ν0, τ0)
∝

log(
θ
	
=
We can derive the posterior as follows, where s = 
p(θ|D)
∝
=
We can derive the posterior predictive distribution as follows. Assume p(θ) = Beta(θ|α, β),
and let s = s(D) be the number of heads in the past data. We can predict the probability of agiven sequence of future heads, D′ = (˜x1, . . . , ˜xm), with sufficient statistic s′ = 
m
p(D′|D)
=

=
Γ(αn + βn)
Γ(αn)Γ(βn)

=
Γ(αn + βn)
Γ(αn)Γ(βn)
where
=
αn + s′ = α + s + s′
=
βn + (m −s′) = β + (n −s) + (m −s′)
Maximum entropy derivation of the exponential family *
Although the exponential family is convenient, is there any deeper justiﬁcation for its use? It
turns out that there is: it is the distribution that makes the least number of assumptions about
suppose all we know is the expected values of certain features or functions:

x
fk(x)p(x) = Fk
where Fk are known constants, and fk(x) is an arbitrary function. The principle of maximum
entropy or maxent says we should pick the distribution with maximum entropy (closest to
uniform), subject to the constraints that the moments of the distribution match the empirical
moments of the speciﬁed functions.
J(p, λ) = −

x

x
p(x)) +

k
λk(Fk −

x
p(x)fk(x))
We can use the calculus of variations to take derivatives wrt the function p, but we will adopt
a simpler approach and treat p as a ﬁxed length vector (since we are assuming x is discrete).
Then we have
∂J

k
λkfk(x)
Setting
∂J
Z exp(−

k
λkfk(x))xi
w
ηi
μi
θi
g
Ψ
=

x
Z

x
exp(−

k
λkfk(x))
Hence the normalization constant is given by
Z
=

x
exp(−

k
λkfk(x))
known as the Gibbs distribution.
Generalized linear models (GLMs)
Linear and logistic regression are examples of generalized linear models, or GLMs (McCullagh
through a possibly nonlinear function, such as the logistic function. We describe GLMs in more
detail below. We focus on scalar outputs for notational simplicity. (This excludes multinomial
logistic regression, but this is just to simplify the presentation.)
Basics
To understand GLMs, let us ﬁrst consider the case of an unconditional dstribution for a scalar
response variable:
p(yi|θ, σ2)
=
exp

σ2
+ c(yi, σ2)


partition function, and c is a normalization constant.
For example, in the case of logistic
μDistrib.
Link g(μ)
θ = ψ(μ)
N(μ, σ2)
identity
θ = μ
μ = θ
Bin(N, μ)
logit
θ = log(
μ
μ = sigm(θ)
Poi(μ)
log
θ = log(μ)
Canonical link functions ψ and their inverses for some common GLMs.
ηi = wT xi
We now make the mean of the distribution be some invertible monotonic function of this linear
The inverse of the mean function, namely g(), is called the link function. We are free to
One particularly simple form of link function is to use g = ψ; this is called the canonical
link function. In this case, θi = ηi = wT xi, so the model becomes
p(yi|xi, w, σ2)
=
exp

yiwT xi −A(wT xi)
σ2
+ c(yi, σ2)


whose inverse is the logistic function, μ = sigm(η).
variable are as follows:
E

y|xi, w, σ2 
=
μi = A′(θi)
var

y|xi, w, σ2 
=
σ2
i = A′′(θi)σ2
To make the notation clearer, let us consider some simple examples.
•
For linear regression, we have
log p(yi|xi, w, σ2)
=
i
σ2
 y2
i
σ2 + log(2πσ2)
	•
For binomial regression, we have
log p(yi|xi, w)
=
yi log(
πi

Ni
yi
	
•
For poisson regression, we have
log p(yi|xi, w)
=
applications, where yi might represent the number of diseases of a given person or place,
ML and MAP estimation
One of the appealing properties of GLMs is that they can be ﬁt using exactly the same methods
ℓ(w) = log p(D|w)
=
σ2
N

ℓi
ℓi
≜
θiyi −A(θi)
We can compute the gradient vector using the chain rule as follows:
dℓi
dwj
=
dℓi
dηi
dηi
dwj
=
dηi
xij
=
dηi
xij
If we use a canonical link, θi = ηi, this simpliﬁes to
σ2
 N

(yi −μi)xi

which is a sum of the input vectors, weighted by the errors. This can be used inside a (stochastic)
σ2
N

xixT
σ2 XT SXName
Formula
Logistic
eη
Probit
Summary of some possible mean functions for binary regression.
=
(XT StX)−1XT Stzt
zt
=
t (y −μt)
the expected Hessian (known as the Fisher information matrix) instead of the actual Hessian is
known as the Fisher scoring method.
sian prior: we just modify the objective, gradient and Hessian, just as we added ℓ2 regularization
Bayesian inference
Probit regression
standard normal. This is known as probit regression. The probit function is very similar to
logistic regression, as we will see.gi ≜
d
dw
d
log p(˜yi|wT xi) = xi
˜yiφ(μi)
where φ is the standard normal pdf, and Φ is its cdf. Similarly, the Hessian for a single case is
given by
Hi =
d
dw2 log p(˜yi|wT xi) = −xi
	
xT
i
We can modify these expressions to compute the MAP estimate in a straightforward manner. In
log likelihood have the form 
Latent variable interpretation
We can interpret the probit (and logistic) model as follows. First, let us associate each item
precisely, the model is as follows:
u0i
≜
wT
u1i
≜
wT
yi
=
I(u1i > u10)
making that we have chosen not to (or are unable to) model. This is called a random utility
Since it is only the difference in utilities that matters, let us deﬁne zi = u1i −u0i + ϵi, where
zi
≜
wT xi + ϵi
ϵi
∼
=
model.
When we marginalize out zi, we recover the probit model:
=


=
=Ordinal probit regression *
One advantage of the latent variable interpretation of probit regression is that it is easy to extend
to the case where the response variable is ordinal, that is, it can take on C discrete values which
can be ordered in some way, such as low, medium and high. This is called ordinal regression.
yi = j
if
of probability mass falls in each interval, so as to match the empirical frequencies of each class
label.
Finding the MLEs for this model is a bit trickier than for binary probit regression, since
we need to optimize for w and γ, and the latter must obey an ordering constraint. See e.g.,
Multinomial probit models *
Now consider the case where the response variable can take on C unordered categorical values,
zic
=
wT xic + ϵic
ϵ
∼
yi
=
arg max
c
zic
more details on the model and its connection to multinomial logistic regression. (By deﬁning
formulation zic = xT
i wc.) Since only relative utilities matter, we constrain R to be a correlation
as multivariate probit, which is one way to model C correlated binary outcomes (see e.g.,Sometimes we want to ﬁt many related classiﬁcation or regression models. It is often reasonable
better performance by ﬁtting all the parameters at the same time. In machine learning, this
j might index schools, i might index students within a school, and yij might be the test score,
the identity of the item that was purchased (this is known as discrete choice modeling (Train
for all j.
Although some groups may have lots of data, there is often a long tail, where the majority
of groups have little data. Thus we can’t reliably ﬁt each model separately, but we don’t want
to use the same model for all groups.
As a compromise, we can ﬁt a separate model for
each group, but encourage the model parameters to be similar across groups. More precisely,
suppose E [yij|xij] = g(xT
ijβj), where g is the link function for the GLM. Furthermore, suppose
βj ∼N(β∗, σ2
j I), and that β∗∼N(μ, σ2
∗I).
In this model, groups with small sample
size borrow statistical strength from the groups with larger sample size, because the βj’s are
The term σ2
j controls how much group j depends on the common parents and the σ2
∗term
controls the strength of the overall prior.
j and σ2
∗are all known (e.g., they could be set
by cross validation). The overall log probability has the form
log p(D|β) + log p(β) =

j

2σ2
j

2σ2∗
natively, we can perform an iterative optimization scheme, alternating between optimizing the
βj and the β∗; since the likelihood and prior are convex, this is guaranteed to converge to the
global optimum. Note that once the models are trained, we can discard β∗, and use each model
separately.
Application to personalized email spam ﬁltering
want to ﬁt one classiﬁer per user, βj. Since most users do not label their email as spam or not,
it will be hard to estimate these models independently. So we will let the βj have a common
prior β∗, representing the parameters of a generic user.In this case, we can emulate the behavior of the above model with a simple trick (Daume
one concatenated with the user id, and one not. The effect will be to learn a predictor of the
form
where u is the user id. In other words,
E [yi|xi, u = j] = (βT
∗+ wj)T xi
Thus β∗will be estimated from everyone’s email, whereas wj will just be estimated from user
j’s email.
To see the correspondence with the above hierarchical Bayesian model, deﬁne wj = βj −β∗.
Then the log probability of the original model can be rewritten as

j

2σ2
j

2σ2∗
If we assume σ2
j = σ2
∗, the effect is the same as using the augmented feature trick, with the
same regularizer strength for both wj and β∗. However, one typically gets better performance
by not requiring that σ2
j be equal to σ2
Application to domain adaptation
Domain adaptation is the problem of training a set of classiﬁers on data drawn from different
distributions, such as email and newswire text. This problem is obviously a special case of
ably large improvements over ﬁtting separate models to each dataset, and small improvements
over the approach of pooling all the data and ﬁtting a single model.
Other kinds of prior
other priors are more suitable. For example, consider the task of conjoint analysis, which
requires ﬁguring out which features of a product customers like best. This can be modelled
It is not always reasonable to assume that all tasks are all equally similar. If we pool the
parameters across tasks that are qualitatively different, the performance will be worse than not
using pooling, because the inductive bias of our prior is wrong. Indeed, it has been found
(this is called negative transfer).One way around this problem is to use a more ﬂexible prior, such as a mixture of Gaussians.
Generalized linear mixed models *
mation at the group level, xj, as well as at the item level, xij. Similarly, we can allow the
parameters to vary across groups, βj, or to be tied across groups, α. This gives rise to the
following model:
E [yij|xij, xj] = g

φ1(xij)T βj + φ2(xj)T β′
j + φ3(xij)T α + φ4(xj)T α′
where the φk are basis functions.
This model can be represented pictorially as shown in
Note that the number of βj
parameters grows with the number of groups, whereas the size of α is ﬁxed.
Frequentists call the terms βj random effects, since they vary randomly across groups, but
they call α a ﬁxed effect, since it is viewed as a ﬁxed but unknown constant. A model with
both ﬁxed and random effects is called a mixed model. If p(y|x) is a GLM, the overall model
is called a generalized linear mixed effects model or GLMM. Such models are widely used in
statistics.
mineral density (SBMD) for person j at measurement i. Let xij be the age of person, and let
xj be their ethnicity, which can be one of: White, Asian, Black, or Hispanic. The primary goal
is to determine if there are signiﬁcant differences in the mean SBMD among the four ethnic
see that there is variation across individuals within each group, so we will use a mixed effects
and φ4(xj) = [I(xj = w), I(xj = a), I(xj = b), I(xj = h)] to account for the effect of the
different ethnicities. Furthermore, we use a linear link function. The overall model is therefore
E [yij|xij, xj]
=
βj + αT b(xij) + ϵij
+α′
wI(xj = w) + α′
aI(xj = a) + α′
bI(xj = b) + α′
hI(xj = h)
contains the parametric part of the model related to ethnicity, and βj is a random offset
for person j.
We endow all of these regression coefficients with separate Gaussian priors.xj
xij
yij
Nj
βj
J
μβ
σ2
β
α
μα
σ2
α
σ2
y
(a)
age in years
Asian
Black
Hispanic
White
(b)
(a) Directed graphical model for generalized linear mixed effects model with J groups. (b)
Spinal bone mineral density vs age for four different ethnic groups. Raw data is shown in the light gray
lines. Fitted model shown in black (solid is the posterior predicted mean, dotted is the posterior predictivecomputational details). After ﬁtting the model, we can compute the prediction for each group.
Computational issues
The principle problem with GLMMs is that they can be difficult to ﬁt, for two reasons. First,
of the priors η = (μ, σ).
One approach is to adopt fully Bayesian inference methods, such as variational Bayes (Hall
An alternative approach is to use empirical Bayes, which we discuss in general terms in
E step we compute p(θ|η, D), and in the M step we optimize η. If the linear regression setting,
the E step can be performed exactly, but in general we need to use approximations. Traditional
In frequentist statistics, there is a popular method for ﬁtting GLMMs called generalized
In addition, it can only provide estimates of the population parameters α, but not the random
effects βj, which are sometimes of interest in themselves.
Learning to rank *
In this section, we discuss the learning to rank or LETOR problem. That is, we want to learn a
function that can rank order a set of items (we will be more precise below). The most common
application is to information retrieval. Speciﬁcally, suppose we have a query q and a set of
documents d1, . . . , dm that might be relevant to q (e.g., all documents that contain the string q).
We would like to sort these documents in decreasing order of relevance and show the top k to
the user. Similar problems arise in other areas, such as collaborative ﬁltering. (Ranking players
Below we summarize some methods for solving this problem, following the presentation of
lack of a better place.
A standard way to measure the relevance of a document d to a query q is to use a probabilistic
language model based on a bag of words model. That is, we deﬁne sim(q, d) ≜p(q|d) =
'n
estimated from document d. In practice, we need to smooth the estimated distribution, for
example by using a Dirichlet prior, representing the overall frequency of each word. This can beestimated from all documents in the system. More precisely, we can use
LEN(d) + λp(t|background)
where TF(t, d) is the frequency of term t in document d, LEN(d) is the number of words in d,
However, there might be many other signals that we can use to measure relevance.
For
example, the PageRank of a web document is a measure of its authoritativeness, derived from
where the query occurs in the document. Below we discuss how to learn how to combine all
The pointwise approach
Suppose we collect some training data representing the relevance of a set of documents for each
query. Speciﬁcally, for each query q, suppose that we retrieve m possibly relevant documents
document. Furthermore, suppose we have a set of labels yj representing the degree of relevance
of document dj to query q. Such labels might be binary (e.g., relevant or irrelevant), or they may
represent a degree of relevance (e.g., very relevant, somewhat relevant, irrelevant). Such labels
can be obtained from query logs, by thresholding the number of times a document was clicked
on for a given query.
use ordinal regression to predict the rating, p(y = r|x(q, d)). In either case, we can then sort
the documents by this scoring metric. This is called the pointwise approach to LETOR, and
is widely used because of its simplicity. However, this method does not take into account the
location of each document in the list. Thus it penalizes errors at the end of the list just as much
as errors at the beginning, which is often not the desired behavior. In addition, each decision
about relevance is made very myopically.
The pairwise approach
relevance of two items rather than absolute relevance. Consequently, the data might tell us
that dj is more relevant than dk for a given query, or vice versa. We can model this kind of
One way to model such a function is as follows:where f(x) is a scoring function, often taken to be linear, f(x) = wT x. This is a special
discussion of neural networks). We can ﬁnd the MLE of w by maximizing the log likelihood, or
equivalently, by minimizing the cross entropy loss, given by
L
=
N

mi

mi

Lijk
−Lijk
=
This can be optimized using gradient descent. A variant of RankNet is used by Microsoft’s Bing
The listwise approach
The pairwise approach suffers from the problem that decisions about relevance are made just
based on a pair of items (documents), rather than considering the full context. We now consider
methods that look at the entire list of items at the same time.
We can deﬁne a total order on a list by specifying a permutation of its indices, π. To model
p(π|s) =
m

sj

m
Then we have that p(π) is the probability of A being ranked ﬁrst, times the probability of B
being ranked second given that A is ranked ﬁrst, times the probabilty of C being ranked third
given that A and B are ranked ﬁrst and second. In other words,
p(π|s) =
sA
sA + sB + sC
×
sB
sB + sC
× sC
sC
To incorporate features, we can deﬁne s(d) = f(x(q, d)), where we often take f to be a
this model, let yi be the relevance scores of the documents for query i. We then minimize the
cross entropy term
−

i

π
p(π|yi) log p(π|si)
Of course, as stated, this is intractable, since the i’th term needs to sum over mi! permutations.
To make this tractable, we can consider permutations over the top k positions only:
p(π1:k|s1:m) =
k

sj

mentropy term (and its derivative) in O(m) time.
In the special case where only one document from the presented list is deemed relevant, say
yi = c, we can instead use multinomial logistic regression:
p(yi = c|x) =
exp(sc)

m
This often performs at least as well as ranking methods, at least in the context of collaborative
Loss functions for ranking
rize below.
•
Mean reciprocal rank (MRR). For a query q, let the rank position of its ﬁrst relevant
This is a very simple performance measure.
•
Mean average precision (MAP). In the case of binary relevance labels, we can deﬁne the
precision at k of some ordering as follows:
P@k(π) ≜num. relevant documents in the top k positions of π
k
We then deﬁne the average precision as follows:
AP(π) ≜

k P@k(π) · Ik
num. relevant documents
For example, if we have the relevancy labels
precision as the AP averaged over all queries.
•
ple levels. We can deﬁne the discounted cumulative gain of the ﬁrst k items in an ordering
as follows:
DCG@k(r) = r1 +
k

ri
log2 i
where ri is the relevance of item i and the log2 term is used to discount items later in
stronger emphasis on retrieving relevant documents, uses
DCG@k(r) =
k

The trouble with DCG is that it varies in magnitude just because the length of a returned
list may vary. It is therefore common to normalize this measure by the ideal DCG, which isi
ri
log2 i
ri
log2 i
_cumulative_gain. The value ri is the relevance score of the item in position i. From this, we see
the DCG obtained by using the optimal ordering: IDCG@k(r) = argmaxπ DCG@k(r). This
can be easily computed by sorting r1:m and then computing DCG@k. Finally, we deﬁne
simple numerical example. The NDCG can be averaged over queries to give a measure of
performance.
•
Rank correlation. We can measure the correlation between the ranked list, π, and the
relevance judegment, π∗, using a variety of methods. One approach, known as the (weighted)
Kendall’s τ statistics, is deﬁned in terms of the weighted pairwise inconsistency between the
two lists:
τ(π, π∗) =

u −π∗
v)]
A variety of other measures are commonly used.
These loss functions can be used in different ways. In the Bayesian approach, we ﬁrst ﬁt the
model using posterior inference; this depends on the likelihood and prior, but not the loss. We
then choose our actions at test time to minimize the expected future loss. One way to do this is
to sample parameters from the posterior, θs ∼p(θ|D), and then evaluate, say, the precision@k
approach.
In the frequentist approach, we try to minimize the empirical loss on the training set. The
problem is that these loss functions are not differentiable functions of the model parameters.
function instead. Cross entropy loss (i.e., negative log likelihood) is an example of a widely used
surrogate loss function.
the precision@k loss. WARP is deﬁned as follows:
WARP(f(x, :), y)
≜
L(rank(f(x, :), y))
rank(f(x, :), y)
=

y′̸=y
I(f(x, y′) ≥f(x, y))
L(k)
≜
k

αj,or, in IR terms, for each possible document corresponding to input query x. The expression
rank(f(x, :), y) measures the rank of the true label y assigned by this scoring function. Finally,
optimize the top k in the ranked list, which will induce good performance as measured by
MAP or precision@k. As it stands, WARP loss is still hard to optimize, but it can be further
approximated by Monte Carlo sampling, and then optimized by gradient descent, as described
Exercises
Show that we can write the MVN in exponential family form. Hint: use the information form deﬁned inDirected graphical models (Bayes nets)
Introduction
I basically know of two principles for treating complicated systems in simple ways: the
ﬁrst is the principle of modularity and the second is the principle of abstraction.
I
am an apologist for computational probability in machine learning because I believe that
probability theory implements these two principles in deep and intriguing ways — namely
through factorization and through averaging. Exploiting these two mechanisms as fully
as possible seems to me to be the way forward in machine learning. — Michael Jordan,
Suppose we observe multiple correlated variables, such as words in a document, pixels in an
image, or genes in a microarray. How can we compactly represent the joint distribution p(x|θ)?
How can we use this distribution to infer one set of variables given another in a reasonable
amount of computation time? And how can we learn the parameters of this distribution with a
reasonable amount of data? These questions are at the core of probabilistic modeling, inference
and learning, and form the topic of this chapter.
Chain rule
By the chain rule of probability, we can always represent a joint distribution as follows, using
any ordering of the variables:
with this expression is that it becomes more and more complicated to represent the conditional
For example, suppose all the variables have K states. We can represent p(x1) as a table
can represent p(x2|x1) as a table of O(K2) numbers by writing p(x2 = j|x1 = i) = Tij; we
say that T is a stochastic matrix, since it satisﬁes the constraint O(K3) numbers. These are called conditional probability tables or CPTs. We see that there
are O(KV ) parameters in the model. We would need an awful lot of data to learn so many
parameters.
is not useful for other kinds of prediction tasks, since each variable depends on all the previous
variables. So we need another approach.
Conditional independence
The key to efficiently representing large joint distributions is to make some assumptions about
pendent given Z, denoted X ⊥Y |Z, if and only if (iff) the conditional joint can be written as
a product of conditional marginals, i.e.,
X ⊥Y |Z ⇐⇒p(X, Y |Z) = p(X|Z)p(Y |Z)
“the future is independent of the past given the present”. This is called the (ﬁrst order) Markov
assumption. Using this assumption, plus the chain rule, we can write the joint distribution as
follows:
p(x1:V ) = p(x1)
V
more information.
Graphical models
how can we deﬁne distributions on 2d images, or 3d videos, or, in general, arbitrary collections
of variables (such as genes belonging to some biological pathway)?
This is where graphical
models come in.
A graphical model (GM) is a way to represent a joint distribution by making CI assumptions.
In particular, the nodes in the graph represent random variables, and the (lack of) edges represent
CI assumptions. (A better name for these models would in fact be “independence diagrams”,
but the term “graphical models” is now entrenched.) There are several kinds of graphical model,
depending on whether the graph is directed, undirected, or some combination of directed and
undirected. In this chapter, we just study directed graphs. We consider undirected graphs in(a)
(b)
Graph terminology
Before we continue, we must deﬁne a few basic terms, most of which are very intuitive.
Here are some other terms we will commonly use:
•
Parent For a directed graph, the parents of a node is the set of all nodes that feed into it:
•
Child For a directed graph, the children of a node is the set of all nodes that feed out of it:
•
Family For a directed graph, the family of a node is the node and its parents, fam(s) =
{s} ∪pa(s).
•
Root For a directed graph, a root is a node with no parents.
•
Leaf For a directed graph, a leaf is a node with no children.
•
That is, the ancestors of t is the set of nodes that connect to t via a trail: anc(t) ≜{s : s ;
t}.
•
a node. That is, the descendants of s is the set of nodes that can be reached via trails from
s: desc(s) ≜{t : s ; t}.
•
Neighbors For any graph, we deﬁne the neighbors of a node as the set of all immediatelywrite s ∼t to indicate that s and t are neighbors (so (s, t) ∈E is an edge in the graph).
•
Degree The degree of a node is the number of neighbors. For directed graphs, we speak of
•
Cycle or loop For any graph, we deﬁne a cycle or loop to be a series of nodes such that
•
DAG A directed acyclic graph or DAG is a directed graph with no directed cycles. See
•
Topological ordering For a DAG, a topological ordering or total ordering is a numbering
of the nodes such that parents have lower numbers than their children. For example, in
•
Path or trail A path or trail s ; t is a series of directed edges leading from s to t.
•
Tree An undirected tree is an undirectecd graph with no cycles. A directed tree is a DAG in
which there are no directed cycles. If we allow a node to have multiple parents, we call it a
polytree, otherwise we call it a moral directed tree.
•
Forest A forest is a set of trees.
•
their corresponding edges, GA = (VA, EA).
•
Clique For an undirected graph, a clique is a set of nodes that are all neighbors of each
other. A maximal clique is a clique which cannot be made any larger without losing the
Directed graphical models
A directed graphical model or DGM is a GM whose graph is a DAG. These are more commonly
known as Bayesian networks. However, there is nothing inherently “Bayesian” about Bayesian
networks: they are just a way of deﬁning probability distributions. These models are also called
belief networks. The term “belief” here refers to subjective probability. Once again, there is
nothing inherently subjective about the kinds of probability distributions represented by DGMs.
Finally, these models are sometimes called causal networks, because the directed arrows are
sometimes interpreted as representing causal relations. However, there is nothing inherently
use the more neutral (but less glamorous) term DGM.
The key property of DAGs is that the nodes can be ordered such that parents come before
children. This is called a topological ordering, and it can be constructed from any DAG. Given
such an order, we deﬁne the ordered Markov property to be the assumption that a node only
depends on its immediate parents, not on all predecessors in the ordering, i.e.,
xs ⊥xpred(s)\pa(s)|xpa(s)
where pa(s) are the parents of node s, and pred(s) are the predecessors of node s in the
general DAGs.Y
X1
X2
X3
X4
(a)
Y
X1
X2
X3
X4
(b)
=
p(x1)p(x2|x1)p(x3|x1,
x2)p(x4|
x1, x2, x3)p(x5|
x1,
x2, x3,
x4)
=
p(x1)p(x2|x1)p(x3|x1)p(x4|x2, x3)p(x5|x3)
In general, we have
p(x1:V |G) =
V
p(xt|xpa(t))
where each term p(xt|xpa(t)) is a CPD. We have written the distribution as p(x|G) to emphasize
that this equation only holds if the CI assumptions encoded in DAG G are correct. However,
we will usual drop this explicit conditioning for brevity. If each node has O(F) parents and
K states, the number of parameters in the model is O(V KF ), which is much less than the
O(KV ) needed by a model which makes no CI assumptions.
Examples
niently represented as DGMs.
Naive Bayes classiﬁers
allows us to write the joint distirbution as follows:
p(y, x) = p(y)
D

p(xj|y)
The naive Bayes assumption is rather naive, since it assumes the features are conditionally
independent. One way to capture correlation between the features is to use a graphical model.x1
x2
x3
· · ·
(a)
x1
x2
x3
x4
· · ·
(b)
A ﬁrst and second order Markov chain.
x1
x2
xT
z1
z2
zT
Markov and hidden Markov models
joint has the following form:
p(x1:T ) = p(x1, x2)p(x3|x1, x2)p(x4|x2, x3) . . . = p(x1, x2)
T

discussion of Markov models.
range correlations amongst the observations. We can’t keep building ever higher order models,
since the number of parameters will blow up. An alternative approach is to assume that there
the data is a noisy observation of this process. The result is known as a hidden Markov model
and xt is the observed variable. (We put “time” in quotation marks, since these models can be
applied to any kind of sequence data, such as genomics or language, where t represents location
observation model.h0
h1
h2
θ0
brevity.
The hidden variables often represent quantities of interest, such as the identity of the word
that someone is currently speaking. The observed variables are what we measure, such as the
acoustic waveform. What we would like to do is estimate the hidden state given the data, i.e., to
compute p(zt|x1:t, θ). This is called state estimation, and is just another form of probabilistic
Medical diagnosis
Consider modeling the relationship between various variables that are measured in an intensive
care unit (ICU), such as the breathing rate of a patient, their blood pressure, etc. The alarm
Since this model was created by hand, by a process called knowledge engineering, it is
to learn the graph structure itself.
A different kind of medical diagnosis network, known as the quick medical reference or
diseases. The QMR model is a bipartite graph structure, with diseases (causes) at the top and
symptoms or ﬁndings at the bottom. All nodes are binary. We can write the distribution as
follows:
p(v, h) =

s
p(hs)

t
p(vt|hpa(t))
where hs represent the hidden nodes (diseases), and vt represent the visible nodes (symptoms).
The CPD for the root nodes are just Bernoulli distributions, representing the prior probability
of that disease. Representing the CPDs for the leaves (symptoms) using CPTs would require
sigm(wT
t hpa(t)). (A DGM in which the CPDs are logistic regression distributions is known as a
at random. In this case, even if the parent is on, the child may be off. To model this moreHRBP
ErrCauter
HRSAT
TPR
MinVol
PVSAT
PAP
 Pulm  
Embolus
Shunt
Intubation
Press
Disconnect
VentMach
VentTube
VentLung
VentAlv
Artco2
BP
Anaphy
Laxis 
 Hypo  
Volemia
PCWP
CO
LvFailure
 Lved 
Volume
Stroke
Volume
History
CVP
Errlow
Output
HrEKG
HR
Insuff
Anesth
Catechol
SAO2
ExpCo2
MinVolset
Kinked
 Tube 
FIO2
(a)
IOX
KHDUW
GLVHDVH
ERWXOLVP
VH[ )
DEGRPHQ
SDLQ
GLVHDVHV
V\PSWRPV
:%&
FRXQW
(b)
(a) The alarm network.
Figure generated by visualizeAlarmNetwork.
(b) The QMR
network.Gp
Gm
p(X = a)
p(X = b)
p(X = o)
p(X = ab)
a
a
a
b
a
o
b
a
b
b
b
o
o
a
o
b
o
o
only way for the child to be off is if all the links from all parents that are on fail independently
at random. Thus

st
a data case would get probability zero under the model, which is problematic, because it is
possible that someone exhibits a symptom but does not have any of the speciﬁed diseases. To
handle this, we add a dummy leak node h0, which is always on; this represents “all other
causes”. The parameter q0t represents the probability that the background leak can cause the
'
for a numerical example.
If we deﬁne wst ≜log(θst), we can rewrite the CPD as


w0t +

s
hswst

We see that this is similar to a logistic regression model.
θst parameters by hand, based on domain expertise. However, it is also possible to learn them
Genetic linkage analysis *
Another important (and historically very early) application of DGMs is to the problem of genetic
linkage analysis.
We start with a pedigree graph, which is a DAG that representing the
DGM, as we explain below. Finally we perform probabilistic inference in the resulting model.





(a)












/RFXV
/RFXV
(b)
Left: family tree, circles are females, squares are males. Individuals with the disease of
interest are highlighted. Right: DGM for two loci. Blue nodes Xij is the observed phenotype for individual
ij
ijl
zmIn more detail, for each person (or animal) i and location or locus j along the genome, we
create three nodes: the observed marker Xij (which can be a property such as blood type,
or just a fragment of DNA that can be measured), and two hidden alleles, Gm
ij and Gp
ij, one
inherited from i’s mother (maternal allele) and the other from i’s father (paternal allele). Together,
the ordered pair Gij = (Gm
ij, Gp
ij) constitutes i’s hidden genotype at locus j.
Obviously we must add Gm
cause phenotypes (observed manifestations of genotypes). The CPD p(Xij|Gm
ij, Gp
ij) is called
person i’s observed bloodtype, and Gm
ij, Gp
dominates O, so if a person has genotype AO or OA, their phenotype will be A.
In addition, we add arcs from i’s mother and father into Gij, reﬂecting the Mendelian
inheritance of genetic material from one’s parents. More precisely, let mi = k be i’s mother.
Then Gm
ij could either be equal to Gm
kj or Gp
kj, that is, i’s maternal allele is a copy of one of its
mother’s two alleles. Let Zm
ij be a hidden variable than speciﬁes the choice. We can model this
using the following CPD, known as the inheritance model:
p(Gm
ij|Gm
kj, Gp
kj, Zm
ij ) =
 I(Gm
ij = Gm
kj)
if Zm
ij = m
I(Gm
ij = Gp
kj)
if Zm
ij = p
We can deﬁne p(Gp
ij|Gm
kj, Gp
kj, Zp
ij) similarly, where k = pi is i’s father. The values of the Zij
are said to specify the phase of the genotype. The values of Gp
i,j, Gm
i,j, Zp
i,j and Zm
i,j constitute
Next, we need to specify the prior for the root nodes, p(Gm
ij) and p(Gp
ij). This is called
the founder model, and represents the overall prevalence of difference kinds of alleles in the
population. We usually assume independence between the loci for these founder alleles.
Finally, we need to specify priors for the switch variables that control the inheritance process.
These variables are spatially correlated, since adjacent sites on the genome are typically inherited
model.
augmented with switching Z variables, which are linked using Markov chains.
(There is a
evolution amongst phylogenies.)
As a simpliﬁed example of how this model can be used, suppose we only have one locus,
corresponding to blood type. For brevity, we will drop the j index. Suppose we observe xi = A.
There is ambiguity
This is known as an inverse problem. Fortunately, we can use the blood types of relatives to
help disambiguate the evidence. Information will “ﬂow” from the other xi′’s up to their Gi′’s,
then across to i’s Gi via the pedigree DAG. Thus we can combine our local evidence p(xi|Gi)
ij, Gm
ij }; however,
the phased or hidden genotype is not directly measurable.with an informative prior, p(Gi|x−i), conditioned on the other data, to get a less entropic local
posterior, p(Gi|x) ∝p(xi|Gi)p(Gi|x−i).
causing gene is assumed to lie — this is the genetic linkage analysis task. The method works as
follows. First, suppose all the parameters of the model, including the distance between all the
and hence the distance dℓbetween the disease gene and its nearest known locus. We measure
the quality of that model using its likelihood, p(D|ˆθℓ). We then can then pick the model with
highest likelihood (which is equivalent to the MAP model under a uniform prior).
Note, however, that computing the likelihood requires marginalizing out all the hidden Z
methods for this task; these are based on the variable elimination algorithm, which we discuss
Directed Gaussian graphical models *
form:
p(xt|xpa(t)) = N(xt|μt + wT
t xpa(t), σ2
t )
This is called a linear Gaussian CPD. As we show below, multiplying all these CPDs together
results in a large joint Gaussian distribution of the form p(x) = N(x|μ, Σ). This is called a
directed GGM, or a Gaussian Bayes net.
xt = μt +

wts(xs −μs) + σtzt
It is easy to see that the global mean is just the concatenation of the local means, μ =
(μ1, . . . , μD). We now derive the global covariance, Σ. Let S ≜diag(σ) be a diagonal matrix
containing the standard deviations.
follows:
(x −μ) = W(x −μ) + Sz
is much messier, as can be seen by looking at (Bishop 2006b, p370).Now let e be a vector of noise terms:
e ≜Sz
We can rearrange this to get
e = (I −W)(x −μ)
I −W is lower triangular with 1s on the diagonal. Hence
⎛
⎜
⎜
⎜
⎝
e1
e2
...
ed
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−w21
−w32
−w31
...
...
−wd1
−wd2
. . .
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
x1 −μ1
x2 −μ2
...
xd −μd
⎞
⎟
⎟
⎟
⎠
Since I −W is always invertible, we can write
x −μ = (I −W)−1e ≜Ue = USz
decomposition of Σ, as we now show:
Σ
=
cov [x] = cov [x −μ]
=
cov [USz] = US cov [z] SUT = US2UT
Inference
tions. Given such a joint distribution, what can we do with it? The main use for such a joint
distribution is to perform probabilistic inference. This refers to the task of estimating unknown
said that one of the goals is to estimate the hidden states (e.g., words) from the observations
one of the goals is to estimate the likelihood of the data under various DAGs, corresponding to
In general, we can pose the inference problem as follows. Suppose we have a set of correlated
random variables with joint distribution p(x1:V |θ).
(In this section, we are assuming the
Let us partition this vector into the visible variables xv, which are observed, and the hidden
variables, xh, which are unobserved. Inference refers to computing the posterior distribution
of the unknowns given the knowns:
p(xh|xv, θ)
=
p(xh, xv|θ)
p(xv|θ)
=
p(xh, xv|θ)

x′
h p(x′
h, xv|θ)
Essentially we are conditioning on the data by clamping the visible variables to their observed
values, xv, and then normalizing, to go from p(xh, xv) to p(xh|xv). The normalization constant
p(xv|θ) is the likelihood of the data, also called the probability of the evidence.Sometimes only some of the hidden variables are of interest to us. So let us partition the
hidden variables into query variables, xq, whose value we wish to know, and the remaining
nuisance variables, xn, which we are not interested in. We can compute what we are interested
in by marginalizing out the nuisance variables:
p(xq|xv, θ) =

xn
p(xq, xn|xv, θ)
we explain how to exploit the factorization encoded by the GM to perform these operations in
models, inference takes time linear in the number of nodes. Unfortunately, for more general
graphs, exact inference can take time exponential in the number of nodes, as we explain in
book.
Learning
In the graphical models literature, it is common to distinguish between inference and learning.
Inference means computing (functions of) p(xh|xv, θ), where v are the visible nodes, h are the
means computing a MAP estimate of the parameters given data:
ˆθ = argmax
θ
N

log p(xi,v|θ) + log p(θ)
to the MLE, as usual.
If we adopt a Bayesian view, the parameters are unknown variables and should also be
inferred. Thus to a Bayesian, there is no distinction between inference and learning. In fact, we
can just add the parameters as nodes to the graph, condition on D, and then infer the values
of all the nodes. (We discuss this in more detail below.)
In this view, the main difference between hidden variables and parameters is that the number
of hidden variables grows with the amount of training data (since there is usually a set of hidden
variables for each observed data case), whereas the number of parameters in usually ﬁxed (at
least in a parametric model). This means that we must integrate out the hidden variables to avoid
overﬁtting, but we may be able to get away with point estimation techniques for parameters,
which are fewer in number.
Plate notation
When inferring parameters from data, we often assume the data is iid. We can represent thisθ
X1
XN
N
θ
Xi
Right: Plate notation.
This
represents the same model as the one on the left, except the repeated xi nodes are inside a box, known as
a plate; the number in the lower right hand corner, N, speciﬁes the number of repetitions of the Xi node.
assumption that each data case was generated independently but from the same distribution.
the data cases are dependent. Nevertheless, we can see that, in this example, the order in which
the same sufficient statistics. Hence we say the data is exchangeable.
To avoid visual clutter, it is common to use a form of syntactic sugar called plates: we
simply draw a little box around the repeated variables, with the convention that nodes within
the box will get repeated when the model is unrolled. We often write the number of copies or
corresponding joint distribution has the form
p(θ, D) = p(θ)
 N

p(xi|θ)

classiﬁer that has been “unrolled” for D features, but uses a plate to represent repetition over
be nested or crossing. Notational devices for modeling more complex parameter tying patterns
example of context speciﬁc independence, since the CI relationship xij ⊥θjc only holds if
yi ̸= c.θc1
. . .
θcD
C
Xi1
. . .
XiD
Yi
N
π
(a)
D
C
N
θjc
Xij
Yi
π
(b)
Naive Bayes classiﬁer as a DGM. (a) With single plates. (b) WIth nested plates.
Learning from complete data
If all the variables are fully observed in each case, so there is no missing data and there are no
hidden variables, we say the data is complete. For a DGM with complete data, the likelihood is
given by
p(D|θ)
=
N

p(xi|θ) =
N

V
p(xit|xi,pa(t), θt) =
V
p(Dt|θt)
where Dt is the data associated with node t and its parents, i.e., the t’th family. This is a
product of terms, one per CPD. We say that the likelihood decomposes according to the graph
structure.
Now suppose that the prior factorizes as well:
p(θ) =
V
p(θt)
Then clearly the posterior also factorizes:
p(θ|D) ∝p(D|θ)p(θ) =
V
p(Dt|θt)p(θt)
This means we can compute the posterior of each CPD independently. In other words,
factored prior plus factored likelihood implies factored posterior
Let us consider an example, where all CPDs are tabular, thus extending the earlier results of
multinoulli distribution) for each conditioning case, i.e., for each combination of parent values,of states for node t, Ct ≜'
number of nodes. Obviously 
Let us put a separate Dirichlet prior on each row of each CPT, i.e., θtc ∼Dir(αtc). Then we
can compute the posterior by simply adding the pseudo counts to the empirical counts to get
θtc|D ∼Dir(Ntc + αtc), where Ntck is the number of times that node t is in state k while its
parents are in state c:
Ntck ≜
N

I(xi,t = k, xi,pa(t) = c)
θtck =
Ntck + αtck

k′(Ntck′ + αtck′)
x1
x2
x3
x4
x5
x2
x3
terms, i.e.,
ˆθtck =
Ntck

k′ Ntck′
important to use a prior to regularize the estimation problem.
only compute a locally optimal ML or MAP estimate. Bayesian inference of the parameters is
even harder. We discuss suitable approximate inference techniques in later chapters.Conditional independence properties of DGMs
At the heart of any graphical model is a set of conditional indepence (CI) assumptions. We write
xA ⊥G xB|xC if A is independent of B given C in the graph G, using the semantics to be
deﬁned below. Let I(G) be the set of all such CI statements encoded by the graph.
I(G) ⊆I(p), where I(p) is the set of all CI statements that hold for distribution p. In other
distribution. This allows us to use the graph as a safe proxy for p when reasoning about p’s CI
properties. This is helpful for designing algorithms that work for large classes of distributions,
It remains to specify how to determine if xA ⊥G xB|xC. Deriving these independencies
because of the need to respect the orientation of the directed edges. We give the details below.
nodes E (containing the evidence) iff at least one of the following conditions hold:
descendant of m.
given E, based on the above deﬁnition. The idea is this. We “shade” all nodes in E, indicating
that they are observed. We then place “balls” at each node in A, let them “bounce around”
according to some rules, and then ask if any of the balls reach any of the nodes in B. The three
We see that a ball can pass through a chain, but not if it is shaded in the middle. Similarly, a
ball can pass through a fork, but not if it is shaded in the middle. However, a ball cannot pass
Z, which encodes
p(x, y, z) = p(x)p(y|x)p(z|y)X
Y
Z
(a)
X
Y
Z
(b)
X
Y
Z
(c)
X
Y
Z
(d)
X
Y
Z
(e)
X
Y
Z
(f)
Bayes ball rules. A shaded node is one we condition on. If there is an arrow hitting a bar, it
When we condition on y, are x and z independent? We have
p(x, z|y)
=
p(x)p(y|x)p(z|y)
p(y)
= p(x, y)p(z|y)
p(y)
= p(x|y)p(z|y)
and therefore x ⊥z|y. So observing the middle node of chain breaks it in two (as in a Markov
chain).
p(x, y, z) = p(y)p(x|y)p(z|y)x
y
(a)
x
y
(b)
y′
y
x
z
(c)
is an observed child of y, rendering y “effectively observed”, so the ball bounces back up on its way from
x to z.
When we condition on y, are x and z independent? We have
p(x, z|y)
=
p(x, y, z)
p(y)
= p(y)p(x|y)p(z|y)
p(y)
= p(x|y)p(z|y)
and therefore x ⊥z|y. So observing a root node separates its children (as in a naive Bayes
p(x, y, z) = p(x)p(z)p(y|x, z)
When we condition on y, are x and z independent? We have
p(x, z|y) = p(x)p(z)p(y|x, z)
p(y)
so x ̸⊥z|y. However, in the unconditional distribution, we have
p(x, z) = p(x)p(z)
so we see that x and z are marginally independent. So we see that conditioning on a common
observe the “sum” of their values. A priori, the coins are independent, but once we observe
To
copy of Y . Then if we observe Y ′, we effectively observe Y as well, so the parents X and Z
bounce back.A DGM.
Other Markov properties of DGMs
t ⊥nd(t) \ pa(t)|pa(t)
A special case of this property is when we only look at predecessors of a node according to
some topological ordering. We have
t ⊥pred(t) \ pa(t)|pa(t)
which follows since pred(t) ⊆nd(t). This is called the ordered Markov property, which
We have now described three Markov properties for DAGs: the directed global Markov property
=⇒
L
=⇒
O. What is less
for the proof). Hence all these properties are equivalent.
this is called the factorization property F. It is obvious that O =⇒F, but one can show that
Markov blanket and full conditionals
The set of nodes that renders a node t conditionally independent of all the other nodes in
the graph is called t’s Markov blanket; we will denote this by mb(t). One can show that thei.e., other nodes who are also parents of its children:
mb(t) ≜ch(t) ∪pa(t) ∪copa(t)
p(xt, x−t)/p(x−t), all the terms that do not involve xt will cancel out between numerator and
denominator, so we are left with a product of CPDs which contain xt in their scope. Hence
p(xt|x−t)
∝
p(xt|xpa(t))

p(xs|xpa(s))
The resulting expression is called t’s full conditional, and will prove to be important when we
Inﬂuence (decision) diagrams *
tion nodes), represented by rectangles, and utility nodes (also called value nodes), represented
by diamonds. The original random variables are called chance nodes, and are represented by
ovals, as usual.
this problem, you have to decide whether to drill an oil well or not. You have two possible
specify the utility function U(d, o). Since the states and actions are discrete, we can represent
it as a table (analogous to a CPT in a DGM). Suppose we use the following numbers, in dollars:
We see that if you don’t drill, you incur no costs, but also make no money. If you drill a dry
Oil
Utility
Drill
(a)
Oil
Utility
Drill
Sound
(b)
Oil
Utility
Drill
Sound
Test
Cost
(c)
(a) Inﬂuence diagram for basic oil wild catter problem. (b) An extension in which we have
an information arc from the Sound chance node to the Drill decision node. (c) An extension in which we
get to decide whether to perform the test or not.
and therefore the optimal action is to drill:
Now let us consider a slight extension to the model. Suppose you perform a sounding to
test will be available before we decide whether to drill or not; hence we add an information
Let us model the reliability of our sensor using the following conditional distribution for
p(s|o):Now your posterior expected utility of performing action d is

You can compute your expected proﬁt or maximum expected utility as follows:
MEU =

s
p(s)EU(d∗(s)|s)
This is the expected utility given possible outcomes of the sounding test, assuming you act
optimally given the outcome. The prior marginal on the outcome of the test is
p(s) =

o
Hence your maximum expected utility is
Now suppose you can choose whether to do the test or not. This can be modelled as shown
special unknown state. There is also some cost associated with performing the test.
Is it worth doing the test? This depends on how much our MEU changes if we know theat
zt
xt
Rt
(a)
at
xt
Rt
(b)
(a) A POMDP, shown as an inﬂuence diagram. zt are hidden world states. We implicitly
make the no forgetting assumption, which effectively means that at has arrows coming into it from all
previous observations, x1:t. (b) An MDP, shown as an inﬂuence diagram.
called the value of perfect information (VPI). So we should do the test as long as it costs less
In terms of graphical models, the VPI of a variable T can be determined by computing the
MEU for the base inﬂuence diagram, I, and then computing the MEU for the same inﬂuence
diagram where we add information arcs from T to the action nodes, and then computing the
difference. In other words,
where D is the decision node and T is the variable we are measuring.
the optimal policy given an inﬂuence diagram. These methods essentially work backwards from
details.
We could continue to extend the model in various ways. For example, we could imagine a
dynamical system in which we test, observe outcomes, perform actions, move on to the next
oil well, and continue drilling (and polluting) in this way. In fact, many problems in robotics,
business, medicine, public policy, etc. can be usefully formulated as inﬂuence diagrams unrolled
This is known as a partially
A special case of a POMDP, in which the states are fully observed, is called a Markov decision
to compute a mapping from observed states to actions.
This can be solved using dynamic
In the POMDP case, the information arc from xt to at is not sufficient to uniquely determine(a)
A
B
C
E
F
I
D
G
H
(b)
J
G
A
D
H
I
E
F
B
C
(c)
Some DGMs.
the best action, since the state is not fully observed. Instead, we need to choose actions based
on our belief state, p(zt|x1:t, a1:t).
Since the belief updating process is deterministic (see
Exercises
(Source: Koller.)
Specify (and justify) which extra edges need to be added.
(Source: Jordan.)
Here we compute some global independence statements from some directed graphical models. You can
graph (all should give the same results).
Prove that the full conditional for node i in a DGM is given by
p(Xi|X−i)
∝
p(Xi|Pa(Xi))

p(Yj|Pa(Yj))
where ch(Xi) are the children of Xi and Pa(Yj) are the parents of Yj.
top to bottom. The graph on the left deﬁnes the joint as


h(a)
(b)
(a) Weather BN. (b) Fishing BN.
where we have marginalized over the hidden variable H. The graph on the right deﬁnes the joint as
and we want to estimate the parameters of the CPDs using maximum likelihood. For which model is
this easier? Explain your answer.
Explain why.
c. Find maximum likelihood estimates of α, β, γ using the following data set, where each row is a training
case. (You may state your answers without proof.)
V
G
R
S
the following variablesX1
X2
X3
X4
X5
Z1
Z2
Z3
(a)
X1
X2
X4
Z1
Z2
Z3
(b)
The corresponding conditional probability tables are
p(x1)
=


, p(x2|x1) =
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠
p(x3|x2)
=

, p(x4|x2) =


Note that in p(x4|x2), the rows represent x2 and the columns x4 (so each row sums to one and represents
Answer the following queries. You may use matlab or do it by hand. In either case, show your work.
do not know the exact value of X1, but we have a distribution over it.) Suppose the lightness has not
been measured but it is known that the ﬁsh is thin. Classify the ﬁsh as salmon or sea bass.
b. Suppose all we know is that the ﬁsh is thin and medium lightness. What season is it now, most likely?
Use p(x1) =


b. Now suppose we partition the leaves into three groups: on, off and unknown. Clearly we can remove the
unknown leaves, since they are hidden and do not affect their parents. Show that we can analytically
remove the leaves that are in the “off state”, by absorbing their effect into the prior of the parents.
Consider the QMR network. Let d be the hidden diseases, f −be the negative ﬁndings (leaf nodes that are
off), and f −be the positive ﬁndings (leaf nodes that are on). We can compute the posterior p(d|f ,f +) in
two steps: ﬁrst absorb the negative ﬁndings, p(d|f −) ∝p(d)p(f −|d), then absorb the positive ﬁndings,
p(d|f −, f +) ∝p(d|f −)p(f +|d). Show that the ﬁrst step can be done in O(|d||f −|) time, where |d| is
the number of dieases and |f −| is the number of negative ﬁndings. For simplicity, you can ignore leak
nodes. (Intuitively, the reason for this is that there is no correlation induced amongst the parents when
the ﬁnding is off, since there is no explaining away.)Recall that the process of moralizing a DAG means connecting together all “unmarried” parents that share
a common child, and then dropping all the arrows. Let M be the moralization of DAG G. Show that
CI(M) ⊆CI(G), where CI are the set of conditional independence statements implied by the model.Mixture models and the EM algorithm
Latent variable models
probability distributions.
The basic idea is to model dependence between two variables by
adding an edge between them in the graph.
(Technically the graph represents conditional
independence, but you get the point.)
An alternative approach is to assume that the observed variables are correlated because they
arise from a hidden common “cause”. Model with hidden variables are also known as latent
variable models or LVMs. As we will see in this chapter, such models are harder to ﬁt than
models with no latent variables. However, they can have signiﬁcant advantages, for two main
reasons. First, LVMs often have fewer parameters than models that directly represent correlation
Second, the hidden variables in an LVM can serve as a bottleneck, which computes a
compressed representation of the data. This forms the basis of unsupervised learning, as we
In general there are L latent variables, zi1, . . . , zIL, and D visible variables, xi1, . . . , xiD,
observed variable; such models form the basis of probabilistic matrix factorization, discussed
others. Depending on the form of the likelihood p(xi|zi) and the prior p(zi), we can generate
Mixture models
use a discrete prior for this, p(zi) = Cat(π). For the likelihood, we use p(xi|zi = k) = pk(xi),+
SDUDPHWHUV

SDUDPHWHUV
A DGM with and without hidden variables. The leaves represent medical symptoms. The
roots represent primary causes, such as smoking, diet and exercise. The hidden variable can represent
mediating factors, such as heart disease, which might not be directly visible.
xi1
. . .
xiD
zi1
. . . ziL
(a)
xi1
. . .
xiD
zi
(b)
xi
zi1
. . . ziL
(c)
xi
zi
(d)
(c)
where pk is the k’th base distribution for the observations; this can be of any type. The overall
model is known as a mixture model, since we are mixing together the K base distributions as
follows:
p(xi|θ) =
K

πkpk(xi|θ)
This is a convex combination of the pk’s, since we are taking a weighted sum, where the
Kp(xi|zi)
p(zi)
Name
Section
MVN
Discrete
Mixture of Gaussians
Prod. Discrete
Discrete
Mixture of multinomials
Prod. Gaussian
Prod. Gaussian
Prod. Gaussian
Prod. Laplace
Prod. Discrete
Prod. Gaussian
Multinomial PCA
Prod. Discrete
Dirichlet
Latent Dirichlet allocation
Prod. Bernoulli
Prod. Bernoulli
Prod. Bernoulli
Sigmoid belief net
Summary of some popular directed latent variable models. Here “Prod” means product, so
“Prod. Discrete” in the likelihood means a factored distribution of the form 
j Cat(xij|zi), and “Prod.
Gaussian” means a factored distribution of the form 
j N(xij|zi). “PCA” stands for “principal components
analysis”. “ICA” stands for “indepedendent components analysis”.
(a)
(b)
Figure generated by mixGaussPlotDemo.
Mixtures of Gaussians
The most widely used mixture model is the mixture of Gaussians (MOG), also called a Gaussian
mixture model or GMM. In this model, each base distribution in the mixture is a multivariate
p(xi|θ) =
K

πkN(xi|μk, Σk)
different set of eliptical contours. Given a sufficiently large number of mixture components, a
GMM can be used to approximate any density deﬁned on RD.Mixture of multinoullis
We can use mixture models to deﬁne density models on many kinds of data. For example,
conditional density is a product of Bernoullis:
p(xi|zi = k, θ) =
D

Ber(xij|μjk) =
D

μxij
The latent variables do not have to any meaning, we might simply introduce latent variables
mean and covariance of the mixture distribution are given by
E [x]
=

k
cov [x]
=

k
k ] −E [x] E [x]T
So although the component distributions are factorized,
the joint distribution is not. Thus the mixture distribution can capture correlations between
Using mixture models for clustering
density model, p(xi). This can be useful for a variety of tasks, such as data compression, outlier
The second, and more common, application of mixture models is to use them for clustering.
model, and then compute p(zi = k|xi, θ), which represents the posterior probability that point
i belongs to cluster k. This is known as the responsibility of cluster k for point i, and can be
computed using Bayes rule as follows:
rik ≜p(zi = k|xi, θ)
=
p(zi = k|θ)p(xi|zi = k, θ)

K
This procedure is called soft clustering, and is identical to the computations performed when
using a generative classiﬁer. The difference between the two models only arises at training time:
in the mixture case, we never observe zi, whereas with a generative classiﬁer, we do observe yi
(which plays the role of zi).
Assuming this is small, it may be reasonable to compute a hard clustering using the MAP
estimate, given by
z∗
i = arg max
k
rik = arg max
k
log p(xi|zi = k, θ) + log p(zi = k|θ)time
genes
yeast microarray data
(a)
K−Means centroids
(b)
corresponding cluster means, μk. The numbers on top of each image represent the mixing weights ˆπk.
No labels were used when training the model. Figure generated by mixBerMnistEM.
resenting the height and weight of people. The colors represent the hard assignments. Note
that the identity of the labels (colors) used is immaterial; we are free to rename all the clusters,
without affecting the partitioning of the data; this is called label switching.
We see that there are several kinds of genes, such as those whose expression level goes up
monotonically over time (in response to a given stimulus), those whose expression level goes
down monotonically, and those with more complex response patterns. We have clustered theresults aren’t great: it has created multiple clusters for some digits, and no clusters for others.
There are several possible reasons for these “errors”:
•
The model is very simple and does not capture the relevant visual characteristics of a digit.
For example, each pixel is treated independently, and there is no notion of shape or a stroke.
•
clusters to adequately model this data. However, if we set K to be large, there is nothing
in the model or algorithm preventing the extra clusters from being used to create multiple
versions of the same digit, and indeed this is what happens. We can use model selection
to prevent too many clusters from being chosen but what looks visually appealing and what
makes a good density estimator may be quite different.
•
The likelihood function is not convex, so we may be stuck in a local optimum, as we explain
This example is typical of mixture modeling, and goes to show one must be very cautious
trying to “interpret” any clusters that are discovered by the method.
(Adding a little bit of
supervision, or using informative priors, can help a lot.)
Mixtures of experts
can also use them to create discriminative models for classiﬁcation and regression. For example,
regression functions, each applying to a different part of the input space. We can model this by
p(yi|xi, zi = k, θ)
=
N(yi|wT
k xi, σ2
k)
p(zi|xi, θ)
=
Cat(zi|S(VT xi))
each submodel is considered to be an “expert” in a certain region of input space. The function
p(zi = k|xi, θ) is called a gating function, and decides which expert to use, depending on
model, obtained using
p(yi|xi, θ) =

k
p(zi = k|xi, θ)p(yi|xi, zi = k, θ)(a)
(b)
(c)
(a) Some data ﬁt with three separate regression lines. (b) Gating functions for three different
“experts”. (c) The conditionally weighted average of the three expert predictions. Figure generated by
mixexpDemo.
yi
zi
xi
(a)
yi
z1
i
z2
i
xi
(b)
(a) A mixture of experts. (b) A hierarchical mixture of experts.forwards problem
(a)
expert predictions
(b)
prediction
 
 
mean
mode
(c)
(a) Some data from a simple forwards model. (b) Some data from the inverse model, ﬁt
Figure generated by mixexpDemoOneToMany.
It should be clear that we can “plug in” any model for the expert. For example, we can use
is known as a mixture density network. Such models are slower to train, but can be more
It is also possible to make each expert be itself a mixture of experts. This gives rise to a
Application to inverse problems
Mixtures of experts are useful in solving inverse problems. These are problems where we have
end effector (hand) y is uniquely determined by the joint angles of the motors, x. However,
for any given location y, there are many settings of the joints x that can produce it. Thus the
due to self occlusion, etc.θz
z1
zN
· · ·
x1
xN
θx
(a)
θz
N
zi
xi
θx
(b)
A LVM represented as a DGM. Left: Model is unrolled for N examples. Right: same model
using plate notation.
We see that this
deﬁnes a function, y = f(x), since for every value x along the horizontal axis, there is a
unique response y. This is sometimes called the forwards model. Now consider the problem
obtained by simply interchanging the x and y axes. Now we see that for some values along
the horizontal axis, there are multiple possible outputs, so the inverse is not uniquely deﬁned.
p(x|y, θ) is multimodal.
mean. Note that the posterior mean does not yield good predictions. In fact, any model which
is trained to minimize mean squared error — even if the model is a ﬂexible nonlinear model,
such as neural network — will work poorly on inverse problems such as this. However, the
posterior mode, where the mode is input dependent, provides a reasonable approximation.
Parameter estimation for mixture models
We have seen how to compute the posterior over the hidden variables given the observed
variables, assuming the parameters are known. In this section, we discuss how to learn the
parameters.
posterior over the parameters also factorizes, making computation very simple. Unfortunately
and hence the posterior will factorize. But since, in an LVM, the zi are hidden, the parameters
are no longer independent, and the posterior does not factorize, making it much harder to(a)
μ1
μ2
(b)
to their true values. We see the two symmetric modes, reﬂecting the unidentiﬁability of the parameters.
Figure generated by mixGaussLikSurfaceDemo.
compute. This also complicates the computation of MAP and ML estimates, as we discus below.
Unidentiﬁability
The main problem with computing p(θ|D) for an LVM is that the posterior may have multiple
modes. To see why, consider a GMM. If the zi were all observed, we would have a unimodal
posterior for the parameters:
p(θ|D) = Dir(π|D)
K

NIW(μk, Σk|D)
Consequently we can easily ﬁnd the globally optimal MAP estimate (and hence globally optimal
MLE).
But now suppose the zi’s are hidden. In this case, for each of the possible ways of “ﬁlling in”
the zi’s, we get a different unimodal likelihood. Thus when we marginalize out over the zi’s, we
We say the parameters are not identiﬁable, since there is not a unique MLE.
Therefore there cannot be a unique MAP estimate (assuming the prior does not rule out certain
labelings), and hence the posterior must be multimodal. The question of how many modes there
p(x|θ). In the latter case, if we have K clusters, we would expect to only get K peaks, although it is theoreticallyare in the parameter posterior is hard to answer. There are K! possible labelings, but some of
the peaks might get merged. Nevertheless, there can be an exponential number, since ﬁnding
Unidentiﬁability can cause a problem for Bayesian inference.
For example, suppose we
draw some samples from the posterior, θ(s) ∼p(θ|D), and then average them, to try to
approximate the posterior mean, θ =
S

S
will be meaningless. Note, however, that it is reasonable to average the posterior predictive
distributions, p(x) ≈
S

S
mode the parameters came from.
A variety of solutions have been proposed to the unidentiﬁability problem. These solutions
depend on the details of the model and the inference algorithm that is used. For example, see
The approach we will adopt in this chapter is much simpler: we just compute a single
local mode, i.e., we perform approximate MAP estimation. (We say “approximate” since ﬁnding
is also a reasonable approximation, at least if the sample size is sufficiently large. To see why,
one data point each. However, there are only two latent parameters, each of which gets to
see N data points. So the posterior uncertainty about the parameters is typically much less
than the posterior uncertainty about the latent variables. This justiﬁes the common strategy
hierarchical Bayesian models, which essentially put structure on top of the parameters. In such
models, it is important to model p(θ|D), so that the parameters can send information between
themselves. If we used a point estimate, this would not be possible.
In the previous sections, we have argued, rather heuristically, that the likelihood function has
multiple modes, and hence that ﬁnding an MAP or ML estimate will be hard. In this section, we
show this result by more algebraic means, which sheds some additional insight into the problem.
log p(D|θ) =

i
log

zi
p(xi, zi|θ)

Unfortunately, this objective is hard to maximize. since we cannot push the log inside the sum.
This precludes certain algebraic simplications, but does not prove the problem is hard.
Now suppose the joint probability distribution p(zi, xi|θ) is in the exponential family, which
means it can be written as follows:
p(x, z|θ) =
Z(θ) exp[θT φ(x, z)]family, as are nearly all of the distributions we have encountered so far, including Dirichlet,
more, mixtures of exponential families are also in the exponential family, providing the mixing
With this assumption, the complete data log likelihood can be written as follows:
ℓc(θ) =

i
log p(xi, zi|θ) = θT (

i
φ(xi, zi)) −NZ(θ)
a unique maximum.
Now consider what happens when we have missing data. The observed data log likelihood
is given by
ℓ(θ) =

i
log

zi
p(xi, zi|θ) =

i
log

zi

−N log Z(θ)
know that Z(θ) is convex. However, the difference of two convex functions is not, in general,
convex. So the objective is neither convex nor concave, and has local optima.
timum.
Most optimization algorithms will only ﬁnd a local optimum; which one they ﬁnd
lowed to run “inﬁnitely long”). In practice, we will run a local optimizer, perhaps using multiple
random restarts to increase out chance of ﬁnding a “good” local optimum. Of course, careful
Note that a convex method for ﬁtting mixtures of Gaussians has been proposed. The idea
penalty, rather than trying to optimize the locations of the cluster centers. See (Lashkari and
ℓ1 penalty, although convex, is not necessarily a good way to promote sparsity, as discussed in
The EM algorithm
For many models in machine learning and statistics, computing the ML or MAP parameter
estimate is easy provided we observe all the values of all the relevant random variables, i.e., ifModel
Section
Mix. Gaussians
Mix. experts
Factor analysis
Student T
Probit regression
DGM with hidden variables
MVN with missing data
HMMs
Shrinkage estimates of Gaussian means
parameter estimate.
negative log likelihood or NLL, given by
N log p(D|θ)
However, we often have to enforce constraints, such as the fact that covariance matrices must be
such cases, it is often much simpler (but not always faster) to use an algorithm called expectation
Furthermore, the algorithm automatically enforce the required constraints.
easy to compute. In particular, EM is an iterative algorithm which alternates between inferring
the missing values given the parameters (E step), and then optimizing the parameters given the
“ﬁlled in” data (M step). We give the details below, followed by several examples. We end with
a summary of the applications of EM in this book.
Basic idea
Let xi be the visible or observed variables in case i, and let zi be the hidden or missing
variables. The goal is to maximize the log likelihood of the observed data:
ℓ(θ) =
N

log p(xi|θ) =
N

log

zi
p(xi, zi|θ)

Unfortunately this is hard to optimize, since the log cannot be pushed inside the sum.EM gets around this problem as follows. Deﬁne the complete data log likelihood to be
ℓc(θ) ≜
N

log p(xi, zi|θ)
This cannot be computed, since zi is unknown. So let us deﬁne the expected complete data
log likelihood as follows:

ℓc(θ)
where t is the current iteration number. Q is called the auxiliary function. The expectation
known as the expected sufficient statistics or ESS. In the M step, we optimize the Q function
θt = arg max
θ
To perform MAP estimation, we modify the M step as follows:
θt = argmax
θ
The E step remains unchanged.
the observed data (plus the log prior, if doing MAP estimation), or it stays the same. So if the
objective ever goes down, there must be a bug in our math or our code. (This is a surprisingly
useful debugging tool!)
Below we explain how to perform the E and M steps for several simple models, that should
make things clearer.
EM for GMMs
In this section, we discuss how to ﬁt a mixture of Gaussians using EM. Fitting other kinds ofAuxiliary function
The expected complete data log likelihood is given by
≜
E

i
log p(xi, zi|θ)

=

i
E

log
 K


=

i

k
E [I(zi = k)] log[πkp(xi|θk)]
=

i

k
=

i

k
rik log πk +

i

k
rik log p(xi|θk)
This is computed in the E step, described below.
E step
The E step has the following simple form, which is the same for any mixture model:
rik
=
k
)

k′
)
M step
πk
=
N

i
rik = rk
N
where rk ≜
i rik is the weighted number of points assigned to cluster k.
and Σk. We see that the result is
ℓ(μk, Σk)
=

k

i
rik log p(xi|θk)
=

i
rik

k (xi −μk)
 
This is just a weighted version of the standard problem of computing the MLEs of an MVN (see
μk
=

i rikxi
rk
Σk
=

i rik(xi −μk)(xi −μk)T
rk
=

i rikxixT
i
rk
kThese equations make intuitive sense: the mean of cluster k is just the weighted average of all
points assigned to cluster k, and the covariance is proportional to the weighted empirical scatter
matrix.
next E step.
Example
color(i) = ri1blue + ri2red
clustering. (The data was standardized, by removing the mean and dividing by the standard
deviation, before processing. This often helps convergence.)
which we now discuss. Consider a GMM in which we make the following assumptions: Σk =
during the E step:
p(zi = k|xi, θ) ≈I(k = z∗
i )
where zi∗= argmaxk p(zi = k|xi, θ). This is sometimes called hard EM, since we are making
a hard assignment of points to clusters. Since we assumed an equal spherical covariance matrix
for each cluster, the most probable cluster for xi can be computed by ﬁnding the nearest
prototype:
z∗
i = arg min
Hence in each E step, we must ﬁnd the Euclidean distance between N data points and K cluster
centers, which takes O(NKD) time. However, this can be sped up using various techniques,
Given the hard cluster assignments, the M step updates each cluster center by computing the
mean of all points assigned to it:
Nk

xi(a)
(b)
(c)
(d)
(e)
(f)
Illustration of the EM for a GMM applied to the Old Faithful data. (a) Initial (random) values
of the parameters. (b) Posterior responsibility of each point computed in the ﬁrst E step. The degree of
redness indicates the degree to which the point belongs to the red cluster, and similarly for blue; this
purple points have a roughly uniform posterior over clusters. (c) We show the updated parameters afterUpdate each cluster center by computing the mean of all points assigned to it:
μk =
Nk

(a)
(b)
Vector quantization
interpreted as a greedy algorithm for approximately minimizing a loss function related to data
compression, as we now explain.
simple approach to this is to use vector quantization or VQ. The basic idea is to replace each
most similar prototype, where similarity is measured in terms of Euclidean distance:
encode(xi)
=
arg min
We can deﬁne a cost function that measures the quality of a codebook by computing the
reconstruction error or distortion it induces:
N
N

N
N

for minimizing this objective.
Of course, we can achieve zero distortion if we assign one prototype to every data vector,accuracy). However, in many data sets, we see similar vectors repeatedly, so rather than storing
them many times, we can store them once and then create pointers to them. Hence we can
reduce the space requirement to O(N log2 K + KDC): the O(N log2 K) term arises because
each of the N data vectors needs to specify which of the K codewords it is using (the pointers);
and the O(KDC) term arises because we have to store each codebook entry, each of which is
the rate of the encoding scheme (number of bits needed per object) as O(log2 K), which is
typically much less than O(DC).
modelled spatial correlation between the pixels, e.g., if we encoded 5x5 blocks (as used by JPEG).
This is because the residual errors (differences from the model’s predictions) would be smaller,
and would take fewer bits to encode.
Initialization and avoiding local minima
to make these be the initial cluster centers. Or we can pick the centers sequentially so as to try
to “cover” the data. That is, we pick the initial point uniformly at random. Then each subsequent
point is picked from the remaining points with probability proportional to its squared distance
can be shown to guarantee that the distortion is never more than O(log K) worse than optimal
An heuristic that is commonly used in the speech recognition community is to incrementally
“grow” GMMs: we initially give each cluster a score based on its mixture weight; after each
round of training, we consider splitting the cluster with the highest score into two, with the new
centroids being random perturbations of the original centroid, and the new scores being half of
the old scores. If a new cluster has too small a score, or too narrow a variance, it is removed.
We continue in this way until the desired number of clusters is reached. See (Figueiredo and
MAP estimation
As usual, the MLE may overﬁt. The overﬁtting problem is particularly severe in the case of
GMMs. To understand the problem, suppose for simplicity that Σk = σ2
point, say x1, since then the 1st term makes the following contribution to the likelihood:
N(x1|μ2, σ2
2I) =

2πσ2
e0x
p(x)
(a)
dimensionality
fraction of times EM for GMM fails
 
 
MLE
MAP
(b)
(a) Illustration of how singularities can arise in the likelihood function of GMMs. Based on
MAP estimation vs ML estimation when ﬁtting a Gaussian mixture model. We plot the fraction of times
mixGaussMLvsMAP.
call this the “collapsing variance problem”.
An easy solution to this is to perform MAP estimation. The new auxiliary function is the
Q′(θ, θold) =

i

k
rik log πik +

i

k
rik log p(xi|θk)

+ log p(π) +

k
Note that the E step remains unchanged, but the M step needs to be modiﬁed, as we now
explain.
For the prior on the mixture weights, it is natural to use a Dirichlet prior, π ∼Dir(α), since
this is conjugate to the categorical distribution. The MAP estimate is given by
πk
=
N + 
k αk −K
The prior on the parameters of the class conditional densities, p(θk), depends on the form of
the class conditional densities. We discuss the case of GMMs below, and leave MAP estimation
For simplicity, let us consider a conjugate prior of the form
p(μk, Σk) = NIW(μk, Σk|m0, κ0, ν0, S0)ˆμk
=
rkxk + κ0m0
rk + κ0
xk
≜

i rikxi
rk
ˆΣk
=
S0 + Sk +
κ0rk
Sk
≜

i
rik(xi −xk)(xi −xk)T
We now illustrate the beneﬁts of using MAP estimation instead of ML estimation in the
context of GMMs. We apply EM to some synthetic data in D dimensions, using either ML or
MAP estimation. We count the trial as a “failure” if there are numerical issues involving singular
estimation crashes and burns, whereas MAP estimation never encounters numerical problems.
case, the MAP estimates simplify to ˆμk = xk and ˆΣk =
Now we discuss how to set S0. One possibility is to use
S0 =
D)
N
(The reason
for the
K |diag(s2
D)|.)
The parameter ν0 controls how strongly we believe this prior.
The
choice.
EM for mixture of experts
We can ﬁt a mixture of experts model using EM in a straightforward manner. The expected
complete data log likelihood is given by
Q(θ, θold)
=
N

K

rik log[πikN(yi|wT
k xi, σ2
k)]
πi,k
≜
S(VT xi)k
rik
∝
πold
ik N(yi|xT
i wold
k , (σold
So the E step is the same as in a standard mixture model, except we have to replace πk with
πi,k when computing rik.In the M step, we need to maximize Q(θ, θold) wrt wk, σ2
k and V.
For the regression
parameters for model k, the objective has the form
Q(θk, θold) =
N

rik

σ2
k
(yi −wT
k xi)
*
We recognize this as a weighted least squares problem, which makes intuitive sense: if rik is
small, then data point i will be downweighted when estimating model k’s parameters. From
wk = (XT RkX)−1XT Rky
where Rk = diag(r:,k). The MLE for the variance is given by
σ2
k =

N

N
We replace the estimate of the unconditional mixing weights π with the estimate of the gating
parameters, V. The objective has the form
ℓ(V)
=

i

k
rik log πi,k
ri. Thus we can estimate V by ﬁtting a logistic regression model to soft target labels.
EM for DGMs with hidden variables
We can generalize the ideas behind EM for mixtures of experts to compute the MLE or MAP
below.
let us write each CPT as follows:
p(xit|xi,pa(t), θt) =
Kpa(t)

Kt

θ
tck
log p(D|θ) =
V

Kpa(t)

Kt

where Ntck = 
N
E [log p(D|θ)]
=

t

c

kwhere
N tck =
N

E

I(xit = i, xi,pa(t) = c)
 
=

i
p(xit = k, xi,pa(t) = c|Di)
where Di are all the visible variables in case i.
The quantity p(xit, xi,pa(t)|Di, θ) is known as a family marginal, and can be computed
using any GM inference algorithm. The N tjk are the expected sufficient statistics, and constitute
the output of the E step.
Given these ESS, the M step has the simple form
ˆθtck =
N tck

k′ N tjk′
This can be proved by adding Lagrange multipliers (to enforce the constraint 
separately. We can modify this to perform MAP estimation with a Dirichlet prior by simply
adding pseudo counts to the expected counts.
EM for the Student distribution *
probability only decays quadratically with distance from the center. A more robust alternative is
the Student t distribution, as discussed in Section ??.
Unlike the case of a Gaussian, there is no closed form formula for the MLE of a Student, even
if we have no missing data, so we must resort to iterative optimization methods. The easiest
one to use is EM, since it automatically enforces the constraints that ν is positive and that Σ
is symmetric positive deﬁnite. In addition, the resulting algorithm turns out to have a simple
intuitive form, as we see below.
At ﬁrst blush, it might not be apparent why EM can be used, since there is no missing data.
The key idea is to introduce an “artiﬁcial” hidden or auxiliary variable in order to simplify the
algorithm. In particular, we will exploit the fact that a Student distribution can be written as a
Gaussian scale mixture:
T (xi|μ, Σ, ν) =


N(xi|μ, Σ/zi)Ga(zi|ν
mixture of Gaussians, each one with a slightly different covariance matrix.
Treating the zi as missing data, we can write the complete data log likelihood as
ℓc(θ)
=
N

=
N



−D
+ν

where we have deﬁned the Mahalanobis distance to be
have, dropping irrelevant constants,
ℓc(θ)
=
LN(μ, Σ) + LG(ν)
LN(μ, Σ)
≜
N

LG(ν)
≜
2ν
N

(log zi −zi)
EM with ν known
Let us ﬁrst derive the algorithm with ν assumed known, for simplicity. In this case, we can
ignore the LG term, so we only need to ﬁgure out how to compute E [zi] wrt the old parameters.
p(zi|xi, θ)
=
Ga(zi|ν + D
, ν + δi
)
z(t)
i
≜E
+
zi|xi, θ(t),
= ν(t) + D
ν(t) + δ(t)
i
The M step is obtained by maximizing E [LN(μ, Σ)] to yield
=

i z(t)
i xi

i z(t)
i
ˆΣ
=
N

i
z(t)
=
N

i
z(t)
i xixT
i −

 N

z(t)
i


These results are quite intuitive: the quantity zi is the precision of measurement i, so if it is
This is how the Student achieves robustness to outliers.
EM with ν unknown
To compute the MLE for the degrees of freedom, we ﬁrst need to compute the expectation of
LG(ν), which involves zi and log zi. Now if zi ∼Ga(a, b), then one can show that
ℓ
(t)
i
≜E
+
log zi|θ(t),
= Ψ(a) −log bBankrupt
Solvent
(a)
Bankrupt
Solvent
(b)
Mixture modeling on the bankruptcy data set. Left: Gaussian class conditional densities.
to each mixture component, are computed. If these are incorrect, the point is colored red, otherwise it is
colored blue. (Training data is in black.) Figure generated by mixStudentBankruptcyDemo.
where Ψ(x) =
d
ℓ
(t)
i
=
Ψ(ν(t) + D
) −log(ν(t) + δ(t)
i
)
=
log(z(t)
i ) + Ψ(ν(t) + D
) −log(ν(t) + D
)
E [LG(ν)]
=

i
(ℓ
(t)
i
−z(t)
i )
The gradient of this expression is equal to
d
dν E [LG(ν)]
=
−N

i
(ℓ
(t)
i
−z(t)
i )
optimizer.
an example of what is known as the generalized EM algorithm. One can show that EM will still
converge to a local optimum even if we only perform a “partial” improvement to the parameters
in the M step.
Mixtures of Student distributions
for the details.
set regarding the bankrupty patterns of certain companies. The ﬁrst feature speciﬁes the ratioof retained earnings (RE) to total assets, and the second feature speciﬁes the ratio of earnings
before interests and taxes (EBIT) to total assets. We ﬁt two models to this data, ignoring the
model to classify the data. We compute the most probable cluster membership and treat this
as ˆyi. We then compare ˆyi to the true labels yi and compute an error rate. If this is more
versa), and then recompute the error rate. Points which are misclassiﬁed are then shown in red.
causing the Gaussian to be a poor choice.
EM for probit regression *
how to ﬁt this model using EM. (Although it is possible to ﬁt probit regression models using
that it generalized to many other kinds of models, as we will see later on.)
ℓ(z, w|V0)
=
=

i
The posterior in the E step is a truncated Gaussian:
p(zi|yi, xi, w) =

E [zi|w, xi] =
-
μi +
φ(μi)
Φ(μi)
μi −
φ(μi)
Φ(−μi) = μi −
φ(μi)
trying to predict. Speciﬁcally, we have
The EM algorithm is simple, but can be much slower than direct gradient methods, as
we only observe that z is positive or negative, but are given no information from the likelihood
about its magnitude.
Using a stronger regularizer can help speed convergence, because it
constrains the range of plausible z values. In addition, one can use various speedup tricks, suchiter
penalized NLL
 
 
em
minfunc
by probitRegDemo.
Theoretical basis for EM *
In this section, we show that EM monotonically increases the observed data log likelihood until
it reaches a local maximum (or saddle point, although such points are usually unstable). Our
derivation will also serve as the basis for various generalizations of EM that we will discuss later.
Expected complete data log likelihood is a lower bound
Consider an arbitrary distribution q(zi) over the hidden variables.
The observed data log
likelihood can be written as follows:
ℓ(θ) ≜
N

log

zi
p(xi, zi|θ)

=
N

log

zi
q(zi)p(xi, zi|θ)
q(zi)

following lower bound:
ℓ(θ) ≥

i

zi
qi(zi) log p(xi, zi|θ)
qi(zi)
Let us denote this lower bound as follows:
Q(θ, q)
≜

i
Eqi [log p(xi, zi|θ)] + H (qi)
where H (qi) is the entropy of qi.
The above argument holds for any positive distribution q. Which one should we choose?
Intuitively we should pick the q that yields the tightest lower bound. The lower bound is a sumover i of terms of the following form:
L(θ, qi)
=

zi
qi(zi) log p(xi, zi|θ)
qi(zi)
=

zi
qi(zi) log p(zi|xi, θ)p(xi|θ)
qi(zi)
=

zi
qi(zi) log p(zi|xi, θ)
qi(zi)
+

zi
qi(zi) log p(xi|θ)
=
−KL (qi(zi)||p(zi|xi, θ)) + log p(xi|θ)
The p(xi|θ) term is independent of qi, so we can maximize the lower bound by setting qi(zi) =
p(zi|xi, θ). Of course, θ is unknown, so instead we use qt
estimate of the parameters at iteration t. This is the output of the E step.
Plugging this in to the lower bound we get
Q(θ, qt)
=

i
Eqt
i [log p(xi, zi|θ)] + H

qt
i

We recognize the ﬁrst term as the expected complete data log likelihood. The second term is a
θ
Q(θ, θt) = arg max
θ

i
Eqt
i [log p(xi, zi|θ)]
as usual.
Now comes the punchline. Since we used qt
i(zi) = p(zi|xi, θt), the KL divergence becomes
zero, so L(θt, qi) = log p(xi|θt), and hence
Q(θt, θt) =

i
log p(xi|θt) = ℓ(θt)
We see that the lower bound is tight after the E step. Since the lower bound “touches” the
function, maximizing the lower bound will also “push up” on the function itself. That is, the
M step is guaranteed to modify the parameters so as to increase the likelihood of the observed
data (unless it is already at a local maximum).
(blue curve), and ﬁt a new bound at that point (dotted green curve). The maximum of this new
ﬁts and then optimizes a quadratic approximation.)
EM monotonically increases the observed data log likelihood
We now prove that EM monotonically increases the observed data log likelihood until it reaches
a local optimum. We have 
 
Q(θ,θt)
l(θ)
Figure generated by emLogLikelihoodMax.
where the ﬁrst inequality follows since Q(θ, ·) is a lower bound on ℓ(θ); the second inequality
As a consequence of this result, if you do not observe monotonic increase of the observed
MAP estimation, you must add on the log prior term to the objective.) This is a surprisingly
powerful debugging tool.
Online EM
When dealing with large or streaming datasets, it is important to be able to learn online, as
bound Q(θ, q1, . . . , qN) one qi at a time; however, this requires storing the expected sufficient
requires constant memory use. We explain both approaches in more detail below, following the
presentation of (Liang and Klein Liang and Klein).
Batch EM review
Before explaining online EM, we review batch EM in a more abstract setting. Let φ(x, z) be a
vector of sufficient statistics for a single data case. (For example, for a mixture of multinoullis,
this would be the count vector a(j), which is the number of cluster j was used in z, plus the
matrix B(j, v), which is of the number of times the hidden state was j and the observed letter
was v.) Let si = 
z p(z|xi, θ)φ(xi, z) be the expected sufficient statistics for case i, and
μ = 
N
mixtures of multinoullis, we just need to normalize a and each row of B.) With this notationsi := 
z p(z|xi, θ(μ))φ(xi, z) ;
μnew := μnew + si; ;
μ := μnew;
Incremental EM
to a data case, we swap out the old si and replace it with the new snew
i
, as shown in the code
i
i si;
snew
i
:= 
z p(z|xi, θ(μ))φ(xi, z) ;
μ := μ + snew
i
−si;
si := snew
i
;
This can be viewed as maximizing the lower bound Q(θ, q1, . . . , qN) by optimizing q1, then
maximum of the lower bound and to the log likelihood itself.
Stepwise EM
each update. It is possible to optimize m and κ to maximize the training set likelihood, by
μ

ηk

θ(μ) = θ(m)), since scaling the counts by a global constant has no effect.



uated_optimization.
trying different values in parallel for an initial trial period; this can signiﬁcantly speed up the
algorithm.
si := 
z p(z|xi, θ(μ))φ(xi, z) ;
(Liang and Klein Liang and Klein) compare batch EM, incremental EM, and stepwise EM
on four different unsupervised language modeling tasks. They found that stepwise EM (using
EM. In terms of accuracy, stepwise EM was usually as good or sometimes even better than batch
EM; incremental EM was often worse than either of the other methods.
Other EM variants *
ingly, many variations have been proposed. We brieﬂy mention a few below, some of which we
•
Annealed EM In general, EM will only converge to a local maximum. To increase the chance
of ﬁnding the global maximum, we can use a variety of methods. One approach is to use
the posterior “landscape” by raising it to a temperature, and then gradually cooling it, all thetrue log−likelihood
lower bound
training time
(a)
true log−likelihood
lower bound
training time
(b)
Illustration of possible behaviors of variational EM. (a) The lower bound increases at each
iteration, and so does the likelihood.
(b) The lower bound increases but the likelihood decreases.
In
this case, the algorithm is closing the gap between the approximate and true posterior. This can have a
•
make qi be the exact posterior over the latent variables, qt
i(zi) = p(zi|xi, θt). In this case,
the lower bound on the log likelihood will be tight, so the M step will “push up” on the
inference in the E step, but we may be able to perform approximate inference. If we can
ensure that the E step is performing inference based on a a lower bound to the likelihood,
inference methods that can be used in the E step.
•
Monte Carlo EM Another approach to handling an intractable E step is to use a Monte
Carlo approximation to the expected sufficient statistics. That is, we draw samples from the
posterior, zs
i ∼p(zi|xi, θt), and then compute the sufficient statistics for each completed
vector, (xi, zs
i), and then average the results. This is called Monte Carlo EM or MCEM (Wei
have to wait for MCMC to converge inside each E step, the method becomes very slow. An
alternative is to use stochastic approximation, and only perform “brief” sampling in the E
step, followed by a partial parameter update. This is called stochastic approximation EM
MCMC to infer the parameters as well as the latent variables (a fully Bayesian approach), thus
•
Generalized EM Sometimes we can perform the E step exactly, but we cannot perform the
M step exactly. However, we can still monotonically increase the log likelihood by performing
a “partial” M step, in which we merely increase the expected complete data log likelihood,
rather than maximizing it. For example, we might follow a few gradient steps. This is callediterations
loglik
 
 
(a)
iterations
loglik
 
 
(b)
The actual running time is printed in the legend. Figure generated by mixGaussOverRelaxedEmDemo.
the generalized EM or GEM algorithm. (This is an unfortunate term, since there are many
ways to generalize EM....)
•
ECM(E) algorithm The ECM algorithm stands for “expectation conditional maximization”,
and refers to optimizing the parameters in the M step sequentially, if they turn out to be
a variant of ECM in which we maximize the expected complete data log likelihood (the Q
function) as usual, or the observed data log likelihood, during one or more of the conditional
maximization steps. The latter can be much faster, since it ignores the results of the E step,
and directly optimizes the objective of interest. A standard example of this is when ﬁtting
the Student T distribution. For ﬁxed ν, we can update Σ as usual, but then to update ν,
information.
•
the usual update computed during the M step. Obviously this reduces to standard EM if
illustration. Unfortunately, using too large a value of η can cause the algorithm to fail to
converge.
Finally, note that EM is in fact just a special case of a larger class of algorithms known asModel selection for latent variable models
When using LVMs, we must specify the number of latent variables, which controls the model
complexity.
In particuarl, in the case of mixture models, we must specify K, the number
of clusters. Choosing these parameters is an example of model selection. We discuss some
approaches below.
Model selection for probabilistic models
marginal likelihood, K∗= argmaxk p(D|K).
There are two problems with this.
First, evaluating the marginal likelihood for LVMs is
quite difficult. In practice, simple approximations, such as BIC, can be used (see e.g., (Fraley
measure, although this can be slow, since it requires ﬁtting each model F times, where F is the
number of CV folds.
The second issue is the need to search over a potentially large number of models. The usual
approach is to perform exhaustive search over all candidate values of K. However, sometimes
we can set the model to its maximal size, and then rely on the power of the Bayesian Occam’s
when we discuss variational Bayes.
An alternative approach is to perform stochastic sampling in the space of models. Traditional
and use birth moves to propose new centers, and death moves to kill off old centers. However,
this can be slow and difficult to implement. A simpler approach is to use a Dirichlet process
mixture model, which can be ﬁt using Gibbs sampling, but still allows for an unbounded number
of evaluating the quality of each K separately. The reason is that ﬁtting the model for each
K is often slow. By contrast, the sampling methods can often quickly determine that a certain
value of K is poor, and thus they need not waste time in that part of the posterior.
means algorithm? Since this does not correspond to a probability model, there is no likelihood,
so none of the methods described above can be used.
struction error of a data set D, using model complexity K, as follows:
E(D, K) =
|D|

error decreases with increasing model complexity! The reason for this behavior is as follows:MSE on test vs K for K−means
(a)
NLL on test set vs K for GMM
(b)
Figure generated by kmeansModelSel1d.
Xtrain
(a)
(b)
(c)
(c) GMM density model estimated by EM for for the same values of K.
Figure generated by
kmeansModelSel1d.accurately represent it as K increases, thus decreasing reconstruction error. However, if we use
models of different complexity, but this is not the case with unsupervised learning. Although
of the standard references in this ﬁeld), it is perhaps not as widely appreciated as it should be.
In fact, it is one of the more compelling arguments in favor of probabilistic models.
Given that cross validation doesn’t work, and supposing one is unwilling to use probabilistic
models (for some bizarre reason...), how can one choose K? The most common approach is to
plot the reconstruction error on the training set versus K, and to try to identify a knee or kink
in the curve. The idea is that for K < K∗, where K∗is the “true” number of clusters, the rate
of decrease in the error function will be high, since we are splitting apart things that should
not be grouped together. However, for K > K∗, we are splitting apart “natural” clusters, which
does not reduce the error by as much.
loss function usually drops off gradually. A different approach to “kink ﬁnding” is described in
Fitting models with missing data
Suppose we want to ﬁt a joint density model by maximum likelihood, but we have “holes” in our
compute
ˆθ = argmax
θ
p(Xv|θ, O)
p(Xv|θ, O) =
N

p(xiv|θ)
log p(Xv|θ) =

i
log p(xiv|θ)
where
p(xiv|θ) =

xih
p(xiv, xih|θ)and xih is the vector of hidden variables for case i (assumed discrete for notational simplicity).
Substituting in, we get
log p(Xv|θ) =

i
log

xih
p(xiv, xih|θ)

Unfortunately, this objective is hard to maximize. since we cannot push the log inside the sum.
However, we can use the EM algorithm to compute a local optimum. We give an example of
this below.
EM for the MLE of an MVN with missing data
Suppose we want to ﬁt an MVN by maximum likelihood, but we have missing data. We can use
EM to ﬁnd a local maximum of the objective, as we explain below.
Getting started
procedures, and then compute an initial MLE.
E step
follows:
=
E
 N


=
−N

i
E

 
=
−N
i
E

(xi −μ)(xi −μ)T  
=
−N
2tr(Σ−1E [S(μ)])
where
E [S(μ)] ≜

i

E

xixT
i
 
to compute 
i E [xi] and 
i E

xixT
i
 
; these are the expected sufficient statistics.
i, where components v are observed and components h are unobserved. We have
xih|xiv, θ
∼
N(mi, Vi)
mi
≜
vv (xiv −μv)
Vi
≜
vv ΣvhHence the expected sufficient statistics are
E [xi] = (E [xih] ; xiv) = (mi; xiv)
where we have assumed (without loss of generality) that the unobserved variables come before
the observed variables in the node ordering.
To compute E

xixT
i
 
, we use the result that cov [x] = E

xxT  
−E [x] E

xT  
. Hence
E

xixT
i
 
=
E


xih
xiv
	 
xT
ih
xT
iv


=
 E

xihxT
ih
 
E [xih] xT
iv
xivE [xih]T
xivxT
iv
	
E

xihxT
ih
 
=
E [xih] E [xih]T + Vi
M step
ESS into the usual MLE equations to get
μt
=
N

i
E [xi]
Σt
=
N

i
E

xixT
i
 
−μt(μt)T
Thus we see that EM is not equivalent to simply replacing variables by their expectations and
applying the standard MLE formula; that would ignore the posterior variance and would result
in an incorrect estimate. Instead we must compute the expectation of the sufficient statistics,
and plug that into the usual equation for the MLE. We can easily modify the algorithm to
perform MAP estimation, by plugging in the ESS into the equation for the MAP estimate. For an
implementation, see gaussMissingFitEm.
Example
As an example of this procedure in action, let us reconsider the imputation problem from
us ﬁt the parameters using EM. Call the resulting parameters ˆθ. We can use our model for
predictions by computing E
+
xih|xiv, ˆθ
,
using the learned parameters are almost as good as with the true parameters. Not surprisingly,
performance improves with more data, or as the fraction of missing data is reduced.
Extension to the GMM case
It is straightforward to ﬁt a mixture of Gaussians in the presence of partially observed data
vectors xi. We leave the details as an exercise.
Exercisestruth
imputed
truth
imputed
truth
imputed
imputation with true params
truth
imputed
(a)
truth
imputed
truth
imputed
truth
imputed
imputation with em
truth
imputed
(b)
Illustration of data imputation.
ing true parameters.
(b) Same as (b), but using parameters estimated with EM. Figure generated by
gaussImputationDemo.
Show that the M step for ML estimation of a mixture of Gaussians is given by
μk
=

i rikxi
rk
Σk
=

i rik(xi −μk)(xi −μk)T
rk
=

i rikxixT
k
rk
•
Show that the M step for ML estimation of a mixture of Bernoullis is given by
μkj
=

i rikxij

i rik
•
Show that the M step for MAP estimation of a mixture of Bernoullis with a β(α, β) prior is given by
μkj
=
(
(
Derive the EM algorithm for ML estimation of a mixture of multivariate Student T distributions.
Consider the Gaussian mixture model
p(x|θ) =


k
πkN(x|μk, Σk)
Deﬁne the log likelihood as
ℓ(θ) =
N


log p(xn|θ)N
m
l
μj
σk
xn
Jn
Kn
p
q
A mixture of Gaussians with two discrete latent indicators. Jn speciﬁes which mean to use,
and Kn speciﬁes which variance to use.
Deﬁne the posterior responsibility that cluster k has for datapoint n as follows:
rnk ≜p(zn = k|xn, θ) =
πkN(xn|μk, Σk)
K
d
ℓ(θ) =


n
k (xn −μk)
c. One way to handle the constraint that K
πk ≜
ewk
K
d
dwk ℓ(θ) =


n
rnk −πk
(There may be a constant factor missing in the above expression...) Hint: use the chain rule and the
fact that
dπj
dwk =
if j = k
−πjπk
if j ̸= k
terize using a Cholesky decomposition, Σk = RT
p(xn|θ) =
m


pj

l


qkN(xn|μj, σ2
k)
l } are all the parameters. Here pj ≜P(Jn =
j) and qk ≜P(Kn = k) are the equivalent of mixture weights. We can think of this as a mixture
l
k), combining Gaussians with different variances (scales).
We will now derive a generalized EM algorithm for this model. (Recall that in generalized EM, we do a
partial update in the M step, rather than ﬁnding the exact maximum.)
a. Derive an expression for the responsibilities, P(Jn = j, Kn = k|xn, θ), needed for the E step.
N


log P(Jn, Kn, xn|θnew)
σ2
j ’s, and subsequently
solve for σ2
∂Q
step is the following matrix:
R =
⎡
⎣
⎤
⎦
where entry ri,c is the probability of obervation xi belonging to cluster c (the responsibility of cluster c for
data point i). You just have to compute the M step. You may state the equations for maximum likelihood
estimates of these quantities (which you should know) without proof; you just have to apply the equations
to this data set. You may leave your answer in fractional form. Show your work.
a. Write down the likelihood function you are trying to optimize.
b. After performing the M step for the mixing weights π1, π2, what are the new values?
Consider a mixture of K Gaussians
p(x) =
K


πkN(x|μk, Σk)
a. Show that
E [x] =


kSome data points in 2d. Circles represent the initial guesses for m1 and m2.
b. Show that
cov [x]
=


k
k ] −E [x] E [x]T
Hint: use the fact that cov [x] = E

xxT 
−E [x] E [x]T .
(Source: Jaakkola.)
and group together all the points assigned to each center). Hint: think about shortest Euclidean distance.
Show that
K






i′:zi′ =k
K


nk




i
=


i
=


i


i


i
(xi −x)(μ −x)
=
n
n


i
(xi −x)(μ −x)
=
(μ −x)
%
(


i
xi) −nx
&
Show that the joint distribution p(x, z|θ) for a 1d GMM can be represented in exponential family form.inverse temperature
survival time
regression with censored data; red x = censored, green * = predicted
 
 
EM
OLS
Example of censored linear regression. Black circles are observed training points, red crosses
are observed but censored training points. Green stars are predicted values of the censored training points.
Consider a model of the form
p(yi|xi, w, σ2, ν) = T (yi|wT xi, σ2, ν)
Derive an EM algorithm to compute the MLE for w. You may assume ν and σ2 are ﬁxed, for simplicity.
j are not equal (but are known). Hint: treat
M step.
Censored regression refers to the case where one knows the outcome is at least (or at most) a certain
value, but the precise value is unknown. This arises in many different settings. For example, suppose one
is trying to learn a model that can predict how long a program will take to run, for different settings of
its parameters. One may abort certain runs if they seem to be taking too long; the resulting run times are
that is, yi = min(zi, ci), where zi is the true running time and yi is the observed running time. We
more, since the model takes into account the fact that the truncated values are actually higher than the
observed values.
get to observe positive outcomes. An example of this is when zi represents “desired investment”, and yi is actualwe do not observe zi, but we observe the fact that it is above some threshold, namely we observe the event
σ
for probit regression.) Show that
=
μi + σH
ci −μi
σ

and
E

z2

=
μ2
i + σ2 + σ(ci + μi)H
ci −μi
σ

where we have deﬁned
H(u) ≜
φ(u)
and where φ(u) is the pdf of a standard Gaussian, and Φ(u) is its cdf.
p(E) , where E is some event of interest.
d
and hence
 c
bLatent linear models
Factor analysis
One problem with mixture models is that they only use a single latent variable to generate the
observations. In particular, each observation can only come from one of K prototypes. One can
of the cluster identity. But because these variables are mutually exclusive, the model is still
limited in its representational power.
to use is a Gaussian (we will consider other choices later):
p(zi) = N(zi|μ0, Σ0)
Just as in linear regression, we will assume the mean is a linear function of the (hidden) inputs,
thus yielding
p(xi|zi, θ) = N(Wzi + μ, Ψ)
where W is a D×L matrix, known as the factor loading matrix, and Ψ is a D×D covariance
matrix. We take Ψ to be diagonal, since the whole point of the model is to “force” zi to explain
the correlation, rather than “baking it in” to the observation’s covariance. This overall model
is called factor analysis or FA. The special case in which Ψ = σ2I is called probabilistic
principal components analysis or PPCA. The reason for this name will become apparent later.
We take an isotropic Gaussian “spray can” and slide it along the 1d line deﬁned by wzi + μ.
This induces an ellongated (and hence correlated) Gaussian in 2d.
FA is a low rank parameterization of an MVN
FA can be thought of as a way of specifying a joint density model on x using a small number
p(xi|θ) is a Gaussian:
p(xi|θ)
=


N(xi|Wzi + μ, Ψ)N(zi|μ0, Σ0)dzi
=Z
[
[
[
[
]
S
]

S
[

ȝ
S
]

]_Z_
S
[_]

ȝ

“emulate” a correlated prior by using deﬁning a new weight matrix, ˜
W = WΣ
. Then we
ﬁnd
cov [x|θ] = ˜
WT + E

ϵϵT  
= (WΣ
)Σ0(WΣ
)T + Ψ = WWT + Ψ
decomposition:
C ≜cov [x] = WWT + Ψ
This only uses O(LD) parameters, which allows a ﬂexible compromise between a full covariance
Gaussian, with O(D2) parameters, and a diagonal covariance, with O(D) parameters. Note that
if we did not restrict Ψ to be diagonal, we could trivially set Ψ to a full covariance matrix; then
Inference of the latent factors
Although FA can be thought of as just a way to deﬁne a density on x, it is often used because
we hope that the latent factors z will reveal something interesting about the data. To do this,
we need to compute the posterior over the latent factors. We can use Bayes rule for Gaussians
to give
p(zi|xi, θ)
=
N(zi|mi, Σi)
Σi
≜
mi
≜
Note that in the FA model, Σi is actually independent of i, so we can denote it by Σ. Computing
this matrix takes O(L3 +L2D) time, and computing each mi = E [zi|xi, θ] takes O(L2 +LD)
time. The mi are sometimes called the latent scores, or latent factors.Retail
Dealer
Engine
Cylinders
Horsepower
CityMPG
HighwayMPG
Weight
Wheelbase
Length
Width
Honda Insight
Mercedes−Benz CL600
Kia Sorento LX
Mercedes−Benz G500
Saturn Ion1
Nissan Pathfinder Armada SE
corresponding to certain chosen points. Figure generated by faBiplotDemo.
To get a better understanding of the “meaning” of the latent factors, we can project unit vectors
a biplot. We see that the horizontal axis represents price, corresponding to the features labeled
“dealer” and “retail”, with expensive cars on the right. The vertical axis represents fuel efficiency
(measured in terms of MPG) versus size: heavy vehicles are less efficient and are higher up,
whereas light vehicles are more efficient and are lower down. We can “verify” this interpretation
by clicking on some points, and ﬁnding the closest exemplars in the training set, and printing
Unidentiﬁability
Just like with mixture models, FA is also unidentiﬁable. To see this, suppose R is an arbitrary
orthogonal rotation matrix, satisfying RRT = I. Let us deﬁne ˜
W = WR; then the likelihoodfunction of this modiﬁed matrix is the same as for the unmodiﬁed matrix, since
cov [x]
=
˜
WE

zzT  ˜
WT + E

ϵϵT  
=
WRRT WT + Ψ = WWT + Ψ
Geometrically, multiplying W by an orthogonal matrix is like rotating z before generating x;
but since z is drawn from an isotropic Gaussian, this makes no difference to the likelihood.
Consequently, we cannot unique identify W, and therefore cannot uniquely identify the latent
factors, either.
an unconstrained (but symmetric) covariance matrix. This gives us an upper bound on L, as
follows:
√
Unfortunately, even if we set L < Lmax, we still cannot uniquely identify the parameters,
formance of the model. However, it does affect the loading matrix, and hence the interpretation
of the latent factors. Since factor analysis is often used to uncover structure in the data, this
problem needs to be addressed. Here are some commonly used solutions:
•
Forcing W to be orthonormal Perhaps the cleanest solution to the identiﬁability problem
is to force W to be orthonormal, and to order the columns by decreasing variance of the
corresponding latent factors. This is the approach adopted by PCA, which we will discuss in
•
Forcing W to be lower triangular One way to achieve identiﬁability, which is popular
feature is only generated by the ﬁrst latent factor, the second visible feature is only generated
factor loading matrix is given by
W =
⎛
⎜
⎜
⎝
w11
w21
w22
w31
w32
w33
w41
w42
w43
⎞
⎟
⎟
⎠
The total number of parameters in this
identiﬁable parameters. The disadvantage of this method is that the ﬁrst L visible variables,
ﬁrst), and so on.Ψ
π
W k
μk
xi
qi
zi
N
K
Mixture of factor analysers as a DGM.
known as the founder variables, affect the interpretation of the latent factors, and so must
be chosen carefully.
•
This is called sparse factor analysis. This does not necessarily ensure a unique MAP estimate,
•
Choosing an informative rotation matrix There are a variety of heuristic methods that try
to ﬁnd rotation matrices R which can be used to modify W (and hence the latent factors) so
as to try to increase the interpretability, typically by encouraging them to be (approximately)
•
W as well as the latent factors. This technique is known as ICA.
Mixtures of factor analysers
The FA model assumes that the data lives on a low dimensional linear manifold. In reality, most
data is better modeled by some form of low dimensional curved manifold. We can approximate
a curved manifold by a piecewise linear manifold. This suggests the following model: let the
the data. We then sample zi from a Gaussian prior and pass it through the Wk matrix (where
k = qi), and add noise. More precisely, the model is as follows:
p(xi|zi, qi = k, θ)
=
N(xi|μk + Wkzi, Ψ)
p(zi|θ)
=
p(qi|θ)
=
Cat(qi|π)(a)
(b)
Mixture of 1d PPCAs ﬁt to a dataset,
for K
=
Figure generated by
mixPpcaDemoNetlab.
particular, this model needs O(KLD) parameters instead of the O(KD2) parameters needed
for a mixture of full covariance Gaussians. This can reduce overﬁtting. In fact, MFA is a good
EM for factor analysis models
model. With just a little more work, we can ﬁt a mixture of FAs. Below we state the results
without proof. The derivation can be found in (Ghahramani and Hinton 1996a); however, deriving
these equations yourself is a useful exercise if you want to become proﬁcient at the math.
ﬁtting a PPCA model, where the results will turn out to have a particularly simple and elegant
intepretation.
In the E step, we compute the posterior responsibility of cluster c for data point i using
ric ≜p(qi = c|xi, θ) ∝πcN(xi|μc, WcWT
c + Ψ)
The conditional posterior for zi is given by
p(zi|xi, qi = c, θ)
=
N(zi|mic, Σic)
Σic
≜
(IL + WT
mic
≜
Σic(WT
c (xi −μc))
Wc =bic
≜
Cic
≜
E

˜z˜zT |xi, qi = c
 
=
E

zzT |xi, qi = c
 
E [z|xi, qi = c]
E [z|xi, qi = c]T
	
Then the M step is as follows:
ˆ˜
Wc
=

i
ricxibT
ic
 
i
ricCic
ˆΨ
=
N diag
-
ic
ric

xi −ˆ˜
Wcbic

xT
i
.
ˆπc
=
N
N

ric
Note that these updates are for “vanilla” EM. A much faster version of this algorithm, based
Fitting FA models with missing data
In many applications, such as collaborative ﬁltering, we have missing data. One virtue of the
overﬁtting can be a problem if there is a lot of missing data. Consequently it is important to
Principal components analysis (PCA)
Consider the FA model where we constrain Ψ = σ2I, and W to be orthonormal.
It can
probabilistic) principal components analysis ( PCA), also known as the Karhunen Loeve
To make sense of this result, we ﬁrst have to learn about classical PCA. We then connect PCA
to the SVD. And ﬁnally we return to discuss PPCA.
Classical PCA: statement of the theorem
The synthesis view of classical PCA is summarized in the forllowing theorem.
N
N
(a)
(b)
points, crosses are the reconstructions. The red star is the data mean. (a) PCA. The points are orthogonally
projected onto the line. Figure generated by pcaDemo2d. (b) PPCA. The projection is no longer orthogonal:
Figure generated by ppcaDemo2d.
where ˆxi = Wzi, subject to the constraint that W is orthonormal. Equivalently, we can write this
objective as follows:
F
where Z is an N × L matrix with the zi in its rows, and ||A||F is the Frobenius norm of matrix
A, deﬁned by
||A||F =
/
m

n

a2
ij =
The optimal solution is obtained by setting ˆ
W = VL, where VL contains the L eigenvectors
with largest eigenvalues of the empirical covariance matrix, ˆΣ =
N

N
i . (We assume the
of the data is given by ˆzi = WT xi, which is an orthogonal projection of the data onto the column
space spanned by the eigenvectors.
vector w1; this is called the ﬁrst principal component or principal direction. The data points
In general, it is hard to visualize higher dimensional data, but if the data happens to be a
as images, as well as the reconstruction of a speciﬁc image using a varying number of basis
Below we will show that the principal directions are the ones along which the data shows
maximal variance. This means that PCA can be “misled” by directions in which the variance
vertical axis (weight) uses a large range than the horizontal axis (height), resulting in a line that
looks somewhat “unnatural”. It is therefore standard practice to standardize the data ﬁrst, ormean
(a)
(b)
Figure generated by pcaImageDemo.
height
weight
(a)
height
weight
(b)
Right: PCA of standardized data. Figure generated by pcaDemoHeightWeight.
equivalently, to work with correlation matrices instead of covariance matrices. The beneﬁts of
Proof *
vectors.is given by
J(w1, z1)
=
N
N

N
N

(xi −zi1w1)T (xi −zi1w1)
=
N
N

[xT
i xi −2zi1wT
i1wT
=
N
N

[xT
i xi −2zi1wT
i1]
since wT
to zero gives
∂
∂zi1
N [−2wT
So the optimal reconstruction weights are obtained by orthogonally projecting the data onto the
J(w1)
=
N
N

[xT
i xi −z2
N
N

z2
i1
Now the variance of the projected coordinates is given by
var [˜z1] = E

˜z2
 
N
N

z2
since
E [zi1] = E

xT
i w1
 
because the data has been centered. From this, we see that minimizing the reconstruction error
is equivalent to maximizing the variance of the projected data, i.e.,
arg min
w1 J(w1) = arg max
w1 var [˜z1]
This is why it is often said that PCA ﬁnds the directions of maximal variance. This is called the
analysis view of PCA.
The variance of the projected data can be written as
N
N

z2
N
N

wT
i w1 = wT
where ˆΣ =
N

N

i xixT
i is the empirical covariance matrix (or correlation matrix if the
data is standardized).maximize
˜J(w1) = wT
∂
∂w1
˜J(w1)
=
ˆΣw1
=
λ1w1
Hence the direction that maximizes the variance is an eigenvector of the covariance matrix. Left
multiplying by w1 (and using wT
wT
Since we want to maximize the variance, we pick the eigenvector which corresponds to the
largest eigenvalue.
Now let us ﬁnd another direction w2 to further minimize the reconstruction error, subject to
wT
N
N

that
∂J
projecting onto the second principal direction. Substituting in yields
J(w2)
=
n
N

[xT
i xi −wT
i w1 −wT
i w2] = const −wT
Dropping the constant term and adding the constraints yields
˜J(w2) = −wT
largest eigenvalue:
ˆΣw2 = λ2w2
The proof continues in this way. (Formally one can use induction.)Singular value decomposition (SVD)
We have deﬁned the solution to PCA in terms of eigenvectors of the covariance matrix. However,
there is another way to obtain the solution, based on the singular value decomposition, or
SVD. This basically generalizes the notion of eigenvectors from square matrices to any kind of
matrix.
In particular, any (real) N × D matrix X can be decomposed as follows
X
N×D
=
U
N×N
S
N×D
VT
D×D
where U is an N × N matrix whose columns are orthornormal (so UT U = IN), V is D × D
matrix whose rows and columns are orthonormal (so VT V = VVT = ID), and S is a N × D
ﬁlling the rest of the matrix. The columns of U are the left singular vectors, and the columns
Since there are at most D singular values (assuming N > D), the last N −D columns of U
computing these unnecessary elements. Let us denote this decomposition by ˆUˆS ˆV. If N > D,
we have
X
N×D
=
ˆU
N×D
ˆS
D×D
ˆVT
D×D
X
N×D
=
ˆU
N×N
ˆS
N×N
ˆVT
N×D
p254).
The connection between eigenvectors and singular vectors is the following. For an arbitrary
real matrix X, if X = USVT , we have
XT X
=
VST UT USVT = V(ST S)VT = VDVT
where D = S2 is a diagonal matrix containing the squares singular values. Hence
(XT X)V
=
VD
so the eigenvectors of XT X are equal to V, the right singular vectors of X, and the eigenvalues
of XT X are equal to D, the squared singular values. Similarly
XXT
=
USVT VST UT = U(SST )UT
(XXT )U
=
U(SST ) = UD
so the eigenvectors of XXT are equal to U, the left singular vectors of X. Also, the eigenvalues
of XXT are equal to the squared singular values. We can summarize all this as follows:
U = evec(XXT ), V = evec(XT X), S2 = eval(XXT ) = eval(XT X)=
σ1
σD
...
D
D
D
N −D
D
D
N
X
=
U
S
V T
(a)
≃
L
σ1...σL
L
D
L
D
N
X
≃
U L
SL
V T
L
(b)
version, since they are not needed. (b) Truncated SVD approximation of rank L.
Since the eigenvectors are unaffected by linear scaling of a matrix, we see that the right
singular vectors of X are equal to the eigenvectors of the empirical covariance ˆΣ. Furthermore,
the eigenvalues of ˆΣ are a scaled version of the squared singular values. This means we can
perform PCA using just a few lines of code (see pcaPmtk).
represent a rank r matrix as follows:
X = σ1
⎛
⎝
|
u1
|
⎞
⎠
−
vT
−

+ · · · + σr
⎛
⎝
|
ur
|
⎞
⎠
−
vT
r
−

to the matrix as follows:
represent an N × D matrix using a rank L approximation is(a)
(b)
(c)
(d)
log(σi)
i
 
 
original
randomized
obtained by randomly shuffling the pixels (dotted green line). Figure generated by svdImageDemo.numbers is a very good approximation.
One can show that the error in this approximation is given by
Furthermore, one can show that the SVD offers the best rank L approximation to a matrix (best
in the sense of minimizing the above Frobenius norm).
Let us connect this back to PCA. Let X = USVT be a truncated SVD of X. We know that
ˆ
W = V, and that ˆZ = X ˆ
W, so
ˆZ = USVT V = US
Furthermore, the optimal reconstruction is given by ˆX = Z ˆ
WT , so we ﬁnd
ˆX = USVT
This is precisely the same as a truncated SVD approximation! This is another illustration of the
fact that PCA is the best low rank approximation to the data.
Probabilistic PCA
We are now ready to revisit PPCA. One can show the following remarkable result.
and W is orthogonal. The observed data log likelihood is given by
log p(X|W, σ2)
=
−N
N

xT
i C−1xi = −N
where C = WWT + σ2I and S =
N

N
ˆ
W = V(Λ −σ2I)
where R is an arbitrary L × L orthogonal matrix, V is the D × L matrix whose columns are the
ﬁrst L eigenvectors of S, and Λ is the corresponding diagonal matrix of eigenvalues. Without loss
of generality, we can set R = I. Furthermore, the MLE of the noise variance is given by
ˆσ2 =
D −L
D

λj
which is the average variance associated with the discarded dimensions.
the posterior over the latent factors is given by
p(zi|xi, ˆθ)
=
ˆF
≜
ˆ
WT ˆ
W + ˆσ2Iˆ
projection of the data onto the column space of V, as in classical PCA.
Note, however, that if σ2 >, the posterior mean is not an orthogonal projection, since it is
undesirable property, but it means that the reconstructions will be closer to the overall data
mean, ˆμ = x.
EM algorithm for PCA
Although the usual way to ﬁt a PCA model uses eigenvector methods, or the SVD, we can also
use EM, which will turn out to have some advantages that we discuss below. EM for PCA relies
on the probabilistic formulation of PCA. However the algorithm continues to work in the zero
along its columns. Similarly, let ˜X = XT store the original data along its columns. From
˜Z = (WT W)−1WT ˜X
This constitutes the E step. Notice that this is just an orthogonal projection of the data.
ˆ
W
=

i
xiE [zi]T
 
i
E [zi] E [zi]T
W = (
i yixT
i )(
i xixT
replace the observed inputs by the expected values of the latent variables.
In summary, here is the entire algorithm:
•
E step ˜Z = (WT W)−1WT ˜X
•
globally optimal solution. That is, the EM algorithm converges to a solution where W spans
the same linear subspace as that deﬁned by the ﬁrst L eigenvectors. However, if we want W
to be orthogonal, and to contain the eigenvectors in descending order of eigenvalue, we have
to orthogonalize the resulting matrix (which can be done quite cheaply). Alternatively, we can
Consider some points in R2 attached by springs to a rigid rod, whose orientation is deﬁned by a
vector w. Let zi be the location where the i’th spring attaches to the rod. In the E step, we hold
the rod ﬁxed, and let the attachment points slide around so as to minimize the spring energy
(which is proportional to the sum of squared residuals). In the M step, we hold the attachment(a)
(b)
(c)
(d)
black circles are their reconstructions. The weight vector w is represented by blue line. (a) We start with
a random initial guess of w. The E step is represented by the orthogonal projections. (b) We update the
rod w in the M step, keeping the projections onto the rod (black circles) ﬁxed. (c) Another E step. The
(Bishop 2006b). Figure generated by pcaEmStepByStep.
illustration.
Apart from this pleasing intuitive interpretation, EM for PCA has the following advantages
over eigenvector methods:
•
jection operation in the E step, so the overall time is O(TLND), where T is the number of(a)
(b)
(c)
(d)
(e)
(f)
Illustration of estimating the effective dimensionalities in a mixture of factor analysers using
Used with kind permission of Matt Beal.
tor methods, such as the Lanczos algorithm, have running times comparable to EM.
•
EM can be implemented in an online fashion, i.e., we can update our estimate of W as the
data streams in.
•
•
•
EM can be modiﬁed to variational EM or to variational Bayes EM to ﬁt more complex models.
Choosing the number of latent dimensions
If we use a probabilistic model, we can in principle compute L∗= argmaxL p(L|D). However,
there are two problems with this. First, evaluating the marginal likelihood for LVMs is quite
difficult.
In practice, simple approximations, such as BIC or variational lower bounds (see
likelihood as a performance measure, although this can be slow, since it requires ﬁtting each
model F times, where F is the number of CV folds.
The second issue is the need to search over a potentially large number of models. The usual
approach is to perform exhaustive search over all candidate values of L. However, sometimes
we can set the model to its maximal size, and then use a technique called automatic relevancynumber of points 
  per cluster
 
intrinsic dimensionalities
We show the estimated number of clusters, and their estimated dimensionalities, as a
clusters, with larger effective dimensionalities, are discovered as the sample sizes increases.
Source: Table
Used with kind permission of Matt Beal.
The ﬁgures visualize the weight matrices for each cluster, using Hinton diagrams, where where
of training data, in accord with the Bayesian Occam’s razor. In particular, when the sample
size is small, the method automatically prefers simpler models, but as the sample size gets
perform stochastic sampling in the space of models. Traditional approaches, such as (Lopes and
Model selection for PCA
Since PCA is not a probabilistic model, we cannot use any of the methods described above. An
obvious proxy for the likelihood is the reconstruction error:
E(D, L) =
|D|

In the case of PCA, the reconstruction is given by by ˆxi = Wzi + μ, where zi = WT (xi −μ)rmse
num PCs
train set reconstruction error
(a)
rmse
num PCs
test set reconstruction error
(b)
Reconstruction error on MNIST vs number of latent dimensions used by PCA. (a) Training
set. (b) Test set. Figure generated by pcaOverfitDemo.
it drops off quite quickly, indicating that we can capture most of the empirical correlation of the
sum of the discarded eigenvalues:
E(Dtrain, L)
=
D

λj
Therefore an alternative to plotting the error is to plot the retained eigenvalues, in decreasing
order. This is called a scree plot, because “the plot looks like the side of a mountain, and ’scree’
as the residual error plot.
A related quantity is the fraction of variance explained, deﬁned as
F(Dtrain, L) =

L

Lmax
This captures the same information as the scree plot.
Of course, if we use L = rank(X), we get zero reconstruction error on the training set.
To avoid overﬁtting, it is natural to plot reconstruction error on the test set. This is shown in
What is going on? The problem is that PCA is not a proper generative model of the data.
It is merely a compression technique. If you give it more latent dimensions, it will be able to
approximate the test data more accurately. By contrast, a probabilistic model enjoys a Bayesiannegloglik
num PCs
train set negative loglik
(a)
negloglik
num PCs
test set negative loglik
(b)
Negative log likelihood on MNIST vs number of latent dimensions used by PPCA. (a) Training
set. (b) Test set. Figure generated by pcaOverfitDemo.
negative log likelihood, computed using PPCA, vs L. Here, on the test set, we see the usual
Proﬁle likelihood
large errors to relatively small. One way to automate the detection of this is described in (Zhu
on whether k < L or k > L, where L is some threshold which we will determine. To measure
overﬁtting in the case where one regime has less data than the other.) Within each of the two
the MLEs, using a pooled estimate of the variance:
μ1(L)
=

L
, μ2(L) =

N −L
σ2(L)
=

N
We can then evaluate the proﬁle log likelihood
ℓ(L) =
L

log N(λk|μ1(L), σ2(L)) +
K

log N(λk|μ2(L), σ2(L))num PCs
eigenvalue
scree plot
(a)
num PCs
profile log likelihood
(b)
generated by pcaOverfitDemo.
PCA for categorical data
In this section, we consider extending the factor analysis model to the case where the observed
function as follows:
p(zi)
=
p(yi|zi, θ)
=
R

Cat(yir|S(WT
r zi + w0r))
It is interesting to study what kinds of distributions we can induce on the observed variables
by varying the parameters. For simplicity, we assume there is a single ternary response variable,
parameters of the prior, m0 and V0, which is equivalent to varying the parameters of the
likelihood, W1 and w01.
We see that this can deﬁne fairly complex distributions over the
simplex.
This induced distribution is known as the logistic normal distribution (Aitchison
We can ﬁt this model to data using a modiﬁed version of EM. The basic idea is to infer
in the M step.Some examples of the logistic normal distribution deﬁned on the 3d simplex. (a) Diagonal
Used with kind permission of David
Blei.
(a)
(b)
PCA, using variational EM. We have color coded points by the identity of the true “prototype” that generated
them. Figure generated by binaryFaDemoTipping.
One application of such a model is to visualize high dimensional categorical data.
each sample is just a noisy copy of one of three binary prototypes. We ﬁt a 2d catFA to this
+
zi|xi, ˆθ
,
. We see that there
are three distinct clusters, as is to be expected.
of imputing missing entries in design matrices consisting of real and categorical data. This is
useful for analysing social science survey data, which often has missing data and variables of
mixed type.yi
xi
zi
wy
W x
N
(a)
xi
yi
zs
i
zx
i
Bx
W x
W y
N
(b)
xi
yi
zs
i
zx
i
zy
i
Bx
By
W x
W y
N
(c)
Gaussian latent factor models for paired data. (a) Supervised PCA. (b) Partial least squares.
(c) Canonical correlation analysis.
It is common to have a pair of related datasets, e.g., gene expression and gene copy number, or
movie ratings by users and movie reviews. It is natural to want to combine these together into a
“bottleneck”.
Below we discuss various latent Gaussian models for these tasks, following the presentation
Gaussian, so we can easily ﬁt the models using EM, or Gibbs sampling.
We can generalize the models to handle discrete and count data by using the exponential
However, this will require the use of approximate inference in the E step (or an analogous
modiﬁcation to MCMC).Supervised PCA (latent factor regression)
p(zi)
=
p(yi|zi)
=
N(wT
y zi + μy, σ2
y)
p(xi|zi)
=
N(Wxzi + μx, σ2
xID)
regression. This model is like PCA, except that the target variable yi is taken into account when
learning the low dimensional embedding. Since the model is jointly Gaussian, we have
yi|xi
∼
N(xT
i w, σ2
y + wT
y Cwy)
where w = Ψ−1WxCwy, Ψ = σ2
x Ψ−1Wx. So although this is a
joint density model of (yi, xi), we can infer the implied conditional distribution.
gΣ2),
and let X = RVT be the SVD of X, where VT V = I and RT R = Σ2 = diag(σ2
j ) contains
So the dependence of the prior for w on X arises from the fact that w is derived indirectly by
a joint model of X and y.
Although we can no longer
compute the conditional p(yi|xi, θ) in closed form, the model has a similar interpretation to
the regression case, namely that we are predicting the response via a latent “bottleneck”.
The basic idea of compressing xi to predict yi can be formulated using information theory.
In particular, we might want to ﬁnd an encoding distribution p(z|x) such that we minimize
I (X; Z) −βI (X; Y )
be discrete, as in clustering. However, in the Gaussian case, IB is closely related to CCA (Chechik
We can easily generalize CCA to the case where yi is a vector of responses to be predicted, as
i gives to movie j, where the “side information” xi takes the form of a list of i’s friends. The
intuition behind this approach is that knowledge of who your friends are, as well as the ratings
of all other users, should help predict which movies you will like. In general, any setting where
the tasks are correlated could beneﬁt from CCA. Once we adopt a probabilistic view, variousDiscriminative supervised PCA
One problem with this model is that it puts as much weight on predicting the inputs xi as the
outputs yi. This can be partially alleviated by using a weighted objective of the following form
ℓ(θ) =

i
p(yi|ηiy)αyp(xi|ηix)αx
where the αm control the relative importance of the data sources, and ηim = Wmzi. For
Gaussian data, we can see that αm just controls the noise variance:
ℓ(θ) ∝

i
2αx||xT
2αy||yT
This interpretation holds more generally for the exponential family. Note, however, that it is hard
to estimate the αm parameters, because changing them changes the normalization constant of
the likelihood. We give an alternative approach to weighting y more heavily below.
Partial least squares
metric or more “discriminative” form of supervised PCA. The key idea is to allow some of the
(co)variance in the input features to be explained by its own subspace, zx
i , and to let the rest of
the subspace, zs
i, be shared between input and output. The model has the form
p(zi)
=
N(zs
p(yi|zi)
=
N(Wyzs
i + μy, σ2IDy)
p(xi|zi)
=
N(Wxzs
i + Bxzx
i + μx, σ2IDx)
p(vi|θ) =


where vi = (xi; yi), μ = (μy; μx) and
W
=

Wy
Wx
Bx
	
WWT
=
WyWT
y
WxWT
x
WxWT
x
WxWT
x + BxBT
x
	
speciﬁc variation.
This model can be easily generalized to discrete data using the exponential family (VirtanenCanonical correlation analysis
Canonical correlation analysis or CCA is like a symmetric unsupervised version of PLS: it
allows each view to have its own “private” subspace, but there is also a shared subspace. If we
have two observed variables, xi and yi, then we have three latent variables, zs
shared, zx
p(zi)
=
N(zs
p(xi|zi)
=
N(xi|Bxzx
i + Wxzs
i + μx, σ2IDx)
p(yi|zi)
=
N(yi|Byzy
i + Wyzs
i + μy, σ2IDy)
p(vi|θ) =


where
W
=

Wx
Bx
Wy
By
	
WWT
=
WxWT
x + BxBT
x
WxWT
y
WyWT
y
WyWT
y + ByBT
y
	
Independent Component Analysis (ICA)
Consider the following situation. You are in a crowded room and many people are speaking.
Your ears essentially act as two microphones, which are listening to a linear combination of the
different speech signals in the room. Your goal is to deconvolve the mixed signals into their
constituent parts. This is known as the cocktail party problem, and is an example of blind
signal separation (BSS), or blind source separation, where “blind” means we know “nothing”
about the source of the signals. Besides the obvious applications to acoustic signal processing,
this problem also arises when analysing EEG and MEG signals, ﬁnancial data, and any other
dataset (not necessarily temporal) where latent sources or factors get mixed together in a linear
way.
xt = Wzt + ϵttruth
(a)
observed signals
(b)
PCA estimate
(c)
ICA estimate
(d)
Observations. (c) PCA estimate. (d) ICA estimate. Figure generated by icaDemo, written by Aapo Hyvarinen.
as an independent observation, i.e., we do not model temporal correlation (so we could replace
the t index with i, but we stick with t to be consistent with much of the ICA literature). The
is called the mixing matrix. If L = D (number of sources = number of sensors), it will be a
square matrix. Often we will assume the noise level, |Ψ|, is zero, for simplicity.
So far, the model is identical to factor analysis (or PCA if there is no noise, except we don’t in
general require orthogonality of W). However, we will use a different prior for p(zt). In PCA,
we assume each source is independent, and has a Gaussian distribution
p(zt) =
L
uniform data
(a)
uniform data after linear mixing
(b)
PCA applied to mixed data from uniform source
(c)
ICA applied to mixed data from uniform source
(d)
distribution. (a) Latent signals. (b) Observations. (c) PCA estimate. (d) ICA estimate. Figure generated by
icaDemoUniform, written by Aapo Hyvarinen.
distribution
p(zt) =
L

pj(ztj)
because any other variance can be modelled by scaling the rows of W appropriately.
The
resulting model is known as independent component analysis or ICA.
The reason the Gaussian distribution is disallowed as a source prior in ICA is that it does not
likelihood is invariant to any orthogonal transformation of the sources zt and mixing matrix W.
PCA can recover the best linear subspace in which the signals lie, but cannot uniquely recover
the signals themselves.To illustrate this, suppose we have two independent sources with uniform distributions, as
W =

	
data. To uniquely recover the sources, we need to perform an additional rotation. The trouble
is, there is no information in the symmetric Gaussian posterior to tell us which angle to rotate
by. In a sense, PCA solves “half” of the problem, since it identiﬁes the linear subspace; all
that ICA has to do is then to identify the appropriate rotation. (Hence we see that ICA is not
that different from methods such as varimax, which seek good rotations of the latent factors to
enhance interpretability.)
case (e.g., where we have more sources than sensors), we cannot uniquely recover the true signal,
but we can compute the posterior p(zt|xt, ˆ
W), which represents our beliefs about the source.
In both cases, we need to estimate W as well as the source distributions pj. We discuss how
to do this below.
Maximum likelihood estimation
model. As usual, we will assume that the observations have been centered; hence we can also
can be done with PCA.
If the data is centered and whitened, we have E

xxT  
= I. But in the noise free case, we
also have
cov [x] = E

xxT  
= WE

zzT  
WT = WWT
Hence we see that W must be orthogonal. This reduces the number of parameters we have to
T
L

T

log pj(vT
j xt)
trying to be consistent with the notation used earlier in this chapter.where vj is the j’th row of V. Since we are constraining V to be orthogonal, the ﬁrst term is a
constant, so we can drop it. We can also replace the average over the data with an expectation
operator to get the following objective
NLL(V) =
L

E [Gj(zj)]
where zj = vT
j x and Gj(z) ≜−log pj(z). We want to minimize this subject to the constraint
that the rows of V are orthogonal. We also want them to be unit norm, since this ensures
that the variance of the factors is unity (since, with whitened data, E

vT
j x
 
necessary to ﬁx the scale of the weights. In otherwords, V should be an orthonormal matrix.
It is straightforward to derive a gradient descent algorithm to ﬁt this model; however, it
is rather slow. One can also derive a faster algorithm that follows the natural gradient; see
The FastICA algorithm
show is an approximate Newton method for ﬁtting ICA models.
For simplicity of presentation, we initially assume there is only one latent factor. In addition,
we initially assume all source distributions are known and are the same, so we can just write
G(z) = −log p(z).
Let g(z) =
d
dzG(z).
The constrained objective, and its gradient and
Hessian, are given by
f(v)
=
E

G(vT x)
 
∇f(v)
=
E

xg(vT x)
 
−βv
H(v)
=
E

xxT g′(vT x)
 
−βI
E

xxT g′(vT x)
 
≈E

xxT  
E

g′(vT x)
 
= E

g′(vT x)
 
This makes the Hessian very easy to invert, giving rise to the following Newton update:
v∗≜v −E

xg(vT x)
 
−βv
E [g′(vT x)] −β
One can rewrite this in the following way
v∗≜E

xg(vT x)
 
−E

g′(vT x)
 
v
(In practice, the expectations can be replaced by Monte Carlo estimates from the training set,
which gives an efficient online learning algorithm.) After performing this update, one should
project back onto the constraint surface using
vnew ≜
v∗
||v∗||(a)
Gaussian
(b)
Laplace
(c)
Uniform
(d)
in 1d and 2d. Figure generated by subSuperGaussPlot, written by Kevin Swersky.
One iterates this algorithm until convergence. (Due to the sign ambiguity of v, the values of v
may not converge, but the direction deﬁned by this vector should converge, so one can assess
Since the objective is not convex, there are multiple local optima. We can use this fact to
learn multiple different weight vectors or features. We can either learn the features sequentially
and then project out the part of vj that lies in the subspace deﬁned by earlier features, or
we can learn them in parallel, and orthogonalize V in parallel. This latter approach is usually
preferred, since, unlike PCA, the features are not ordered in any way. So the ﬁrst feature is not
“more important” than the second, and hence it is better to treat them symmetrically.Modeling the source densities
So far, we have assumed that G(z) = −log p(z) is known. What kinds of models might be
reasonable as signal priors?
We know that using Gaussians (which correspond to quadratic
•
and hence (in order to ensure unit variance) have heavy tails. The Laplace distribution is
kurtosis of the distribution, deﬁned by
kurt(z) ≜μ4
the mean:
μk ≜E

(X −E [X])k 
deﬁnition of kurtosis to make the kurtosis of a Gaussian variable equal to zero.
•
for “broad”) distribution has negative kurtosis. These are distributions which are much ﬂatter
•
of this is skewness, deﬁned by
skew(z) ≜μ3
σ3
When one looks at the empirical distribution of many natural signals, such as images and
result holds both for the kind of linear ﬁlters found in certain parts of the brain, such as the
simple cells found in the primary visual cortex, as well as for the kinds of linear ﬁlters used in
signal processing, such as wavelet transforms. One obvious choice for modeling natural signals
given by
log p(z) = −
√
√
Since the Laplace prior is not differentiable at the origin, it is more common to use other,
s =
√
π ), is given by the following:
√
√
πψ
T
xt1
xtD
zt1
ztD
qt1
qtD
W
σ1k
μ1k
π
σDk
μDk
Modeling the source distributions using a mixture of univariate Gaussians (the independent
Various ways of estimating G(Z) = −log p(z) are discussed in the seminal paper (Pham and
shape of the source distribution be known (although it is important to know whether it is sub
or super Gaussian). Consequently, it is common to just use G(z) = √z or G(z) = log cosh(z)
instead of the more complex expressions above.
Using EM
An alternative to assuming a particular form for G(z), or equivalently for p(z), is to use a
p(qj = k)
=
πk
p(zj|qj = k)
=
N(μj,k, σ2
j,k)
p(x|z)
=
N(Wz, Ψ)
It is possible to derive an exact EM algorithm for this model. The key observation is that
it is possible to compute E [zt|xt, θ] exactly by summing over all KL combinations of the qt
variables, where K is the number of mixture components per source. (If this is too expensive,
source distributions in parallel by ﬁtting a standard GMM to E [zt]. When the source GMMs areknown, we can compute the marginals pj(zj) very easily, using
pj(zj) =
K

πj,kN(zj|μj,k, σ2
j,k)
Given the pj’s, we can then use an ICA algorithm to estimate W. Of course, these steps should
Other estimation principles *
It is quite common to estimate the parameters of ICA models using methods that seem different
to maximum likelihood.
We will review some of these methods below, because they give
additional insight into ICA. However, we will also see that these methods in fact are equivalent
An early approach to ICA was to ﬁnd a matrix V such that the distribution z = Vx is as far
from Gaussian as possible. (There is a related approach in statistics called projection pursuit.)
Another
measure is the negentropy, deﬁned as
negentropy(z) ≜H

N(μ, σ2)

−H (z)
Gaussian.
We can deﬁne our objective as maximizing
J(V) =

j
negentropy(zj) =

j
H

N(μj, σ2
j )

−H (zj)
where z = Vx. If we ﬁx V to be orthogonal, and if we whiten the data, the covariance of z
will be I independently of V, so the ﬁrst term is a constant. Hence
J(V) =

j
−H (zj) + const =

j
E [log p(zj)] + const
Minimizing mutual information
I(z) ≜KL
⎛
⎝p(z)||

j
p(zj)
⎞
⎠=

j
H(zj) −H(z)We would like to minimize this, since we are trying to ﬁnd independent components.
Put
another way, we want the best possible factored approximation to the joint distribution.
Now since z = Vx, we have
I(z) =

j
H(zj) −H(Vx)
If we constrain V to be orthogonal, we can drop the last term, since then H(Vx) = H(x)
(since multiplying by V does not change the shape of the distribution), and H(x) is a constant
which is is solely determined by the empirical distribution. Hence we have I(z) = 
j H(zj).
Minimizing this is equivalent to maximizing the negentropy, which is equivalent to maximum
likelihood.
Maximizing mutual information (infomax)
Instead of trying to minimize the mutual information between the components of z, let us
imagine a neural network where x is the input and yj = φ(vT
j x) + ϵ is the noisy output, where
the information ﬂow through this system, a principle known as infomax. (Bell and Sejnowski
representation) and x (the observed input signal). We have I(x; y) = H(y) −H(y|x), where
the latter term is constant if we assume the noise has constant variance. One can show that we
can approximate the former term as follows
H(y) =
L

E

log φ′(vT
j x)
 
+ log | det(V)|
where, as usual, we can drop the last term if V is orthogonal. If we deﬁne φ(z) to be a cdf,
then φ′(z) is its pdf, and the above expression is equivalent to the log likelihood. In particular,
if we use a logistic nonlinearity, φ(z) = sigm(z), then the corresponding pdf is the logistic
distribution, and log φ′(z) = log cosh(z) (ignoring irrelevant constants).
Thus we see that
infomax is equivalent to maximum likelihood.
Exercises
Derive the M step for the FA model using conjugate priors for the parameters.
d
d
or not PCA would be useful for analysing the data (the higher the value of σ2 the more useful PCA).a. Let
n
n


(xi −zi1v1 −zi2v2)T (xi −zi1v1 −zi2v2)
Show that
∂J
b. Show that the value of v2 that minimizes
˜J(v2) = −vT
is given by the eigenvector of C with the second largest eigenvalue. Hint: recall that Cv1 = λ1v1 and
∂xT Ax
∂x
= (A + AT )x.
a. Prove that
||xi −
K


i xi −
K


vT
j xixT
i vj
recall zij = xT
i vj.
b. Now show that
JK
≜
n
n


%
xT
i xi −
K


vT
j xixT
i vj
&
n
n


xT
i xi −
K


λj
Hint: recall vT
j Cvj = λjvT
j vj = λj.
terms is given by
JK =
d


λj
Hint: partition the sum d
Show that the maximum of J(w) = wT SBw
wT SW w is given by SBw = λSW w
wT SW w. Hint: recall that the derivative of a ratio of two scalars is given by
d
dx
f(x)
g(x) = f′g−fg′
g2
,
where f ′ =
d
dxf(x) and g′ =
d
dxg(x). Also, recall that
d
dxxT Ax = (A + AT )x.
nXT X, i.e., the principal
basis vectors. These satisfy
vT
j vk =
if j ̸= k
if j = k
We will construct a method for ﬁnding the vj sequentially.As we showed in class, v1 is the ﬁrst principal eigenvector of C, and satisﬁes Cv1 = λ1v1. Now deﬁne
˜xi as the orthogonal projection of xi onto the space orthogonal to v1:
˜xi = P⊥v1 xi = (I −v1vT
dimensional data the component that lies in the direction of the ﬁrst principal direction:
˜X = (I −v1vT
the covariance of the deﬂated matrix is given by
n
nXT X −λ1v1vT
b. Let u be the principal eigenvector of ˜C. Explain why u = v2. (You may assume u is unit norm.)
c. Suppose we have a simple method for ﬁnding the leading eigenvector and eigenvalue of a pd matrix,
denoted by [λ, u] = f(C). Write some pseudo code for ﬁnding the ﬁrst K principal basis vectors of
X that only uses the special f function and simple vector arithmetic, i.e., your code should not use
about being syntactically correct.
(Source: de Freitas.). In this exercise, we study a technique called latent semantic indexing, which applies
capture semantic similarity of words.
A document by term matrix is in
lsiMatrix.txt.
a. Let X be the transpose of lsiMatrix, so each column represents a document. Compute the SVD of X
and make an approximation to it ˆ
b. Consider ﬁnding documents that are about alien abductions. If If you look at lsiWords.txt, there
the 2D subspace to make ˆq. Now compute the cosine similarity between ˆq and the low dimensional
Derive an expression for p(xh|xv, θ) for a FA model.
Derive an expression for p(x| ˆ
W, ˆσ2) for the PPCA model based on plugging in the MLEs and using the
matrix inversion lemma.case of FA.Sparse linear models
Introduction
ﬁnding input variables which had high mutual information with the output. The trouble with
this approach is that it is based on a myopic strategy that only looks at one variable at a time.
This can fail if there are interaction effects. For example, if y = xor(x1, x2), then neither x1 nor
x2 on its own can predict the response, but together they perfectly predict the response. For a
If the model is a generalized linear model, of the form p(y|x) = p(y|f(wT x)) for some link
function f, then we can perform feature selection by encouraging the weight vector w to be
sparse, i.e., to have lots of zeros. This approach turns out to offer signiﬁcant computational
advantages, as we will see below.
•
responding design matrix is short and fat, rather than tall and skinny. This is called the
small N, large D problem. This is becoming increasingly prevalent as we develop more
high throughput measurement devices, For example, with gene microarrays, it is common
examples. (It is perhaps a sign of the times that even our data seems to be getting fatter...)
We may want to ﬁnd the smallest set of features that can accurately predict the response
(e.g., growth rate of the cell) in order to prevent overﬁtting, to reduce the cost of building a
diagnostic device, or to help with scientiﬁc insight into the problem.
•
[κ(x, x1), . . . , κ(x, xN)], where κ is a kernel function. The resulting design matrix has size
N × N. Feature selection in this context is equivalent to selecting a subset of the training
examples, which can help reduce overﬁtting and computational cost. This is known as a
sparse kernel machine.
•
In signal processing, it is common to represent signals (images, speech, etc.) in terms of
wavelet basis functions. To save time and space, it is useful to ﬁnd a sparse representationof the signals, in terms of a small number of such basis functions. This allows us to estimate
signals from a small number of measurements, as well as to compress the signal.
See
Note that the topic of feature selection and sparsity is currently one of the most active areas
main results.
Bayesian variable selection
p(γ|D) =
e−f(γ)

γ′ e−f(γ′)
where f(γ) is the cost function:
f(γ) ≜−[log p(D|γ) + log p(γ)]
In
equations for this below). We order the models in Gray code order, which ensures consecutive
are easier to interpret if we compute the posterior distribution over models, p(γ|D). This is
model
prob
members
are very small (relative to σ2). so these variables are harder to detect. Given enough data, the
method will converge on the true model (assuming the data is generated from a linear model),
but for ﬁnite data sets, there will usually be considerable posterior uncertainty.
Interpreting the posterior over a large number of models is quite difficult, so we will seek
various summary statistics. A natural one is the posterior mode, or MAP estimate
ˆγ = argmax p(γ|D) = argmin f(γ)(a)
log p(model, data)
(b)
p(model|data)
(c)
p(gamma(j)|data
(d)
(d) Marginal inclusion probabilities. Figure generated by linregAllsubsetsGraycodeDemo.
computed using
The above example illustrates the “gold standard” for variable selection: the problem was
Of course, variable selection is most useful in the cases where the number of dimensions is
large. Since there are 2D possible models (bit vectors), it will be impossible to compute the
full posterior in general, and even ﬁnding summaries, such as the MAP estimate or marginalinclusion probabilities, will be intractable. We will therefore spend most of this chapter focussing
on algorithmic speedups. But before we do that, we will explain how we computed p(γ|D) in
the above example.
The spike and slab model
The posterior is given by
p(γ|D) ∝p(γ)p(D|γ)
We ﬁrst consider the prior, then the likelihood.
It is common to use the following prior on the bit vector:
p(γ) =
D

D
useful to write the log prior as follows:
log p(γ|π0)
=
=
=
π0
controls the sparsity of the model.
We can write the likelihood as follows:
p(D|γ) = p(y|X, γ) =

 

p(y|X, w, γ)p(w|γ, σ2)p(σ2)dwdσ2
w), where σ2
w controls how big we expect the coefficients associated with the relevant
variables to be (which is scaled by the overall noise level σ2). We can summarize this prior as
follows:
p(wj|σ2, γj) =

δ0(wj)
w)
The ﬁrst term is a “spike” at the origin. As σ2
a uniform distribution, which can be thought of as a “slab” of constant height. Hence this is
likelihood):
p(D|γ) =

 

N(y|Xγwγ, σ2IN)N(wγ|0Dγ, σ2σ2
wIDγ)p(σ2)dwγdσ2this slightly by deﬁning a prior of the form p(w|γ, σ2) = N(wγ|0Dγ, σ2Σγ) for any positive
Given these priors, we can now compute the marginal likelihood. If the noise variance is
p(D|γ, σ2)
=


Cγ
≜
σ2XγΣγXT
γ + σ2IN
If the noise is unknown, we can put a prior on it and integrate it out. It is common to use
p(D|γ)
=

 

p(y|γ, wγ, σ2)p(wγ|γ, σ2)p(σ2)dwγdσ2
∝
|XT
where S(γ) is the RSS:
S(γ)
≜
yT y −yT Xγ(XT
γ )−1XT
γ y
When the marginal likelihood cannot be computed in closed form (e.g., if we are using logistic
regression or a nonlinear model), we can approximate it using BIC, which has the form
log N
We see that there are two complexity penalties: one arising from the BIC approximation to
the marginal likelihood, and the other arising from the prior on p(γ). Obviously these can be
yi|xi, w, γ, σ2
∼
N(

j
γjwjxij, σ2)
γj
∼
Ber(π0)
wj
∼
w)Gaussian model, although we could also call it the binary mask model, since we can think of
the γj variables as “masking out” the weights wj.
Unlike the spike and slab model, we do not integrate out the “irrelevant” coefficients; they
can be identiﬁed from the likelihood.
One interesting aspect of this model is that it can be used to derive an objective function that
has the form
Hence the scaled unnormalized negative log posterior has the form
f(γ, w)
≜
+ σ2
σ2w
where
π0
)
Now consider the case where σ2
there is no complexity penalty coming from the marginal likelihood or its BIC approximation).
In this case, the objective becomes
This is similar to the BIC objective above.
Instead of keeping track of the bit vector γ, we can deﬁne the set of relevant variables to
follows:
This is called ℓ0 regularization. We have converted the discrete optimization problem (over
objective very non smooth, so this is still hard to optimize. We will discuss different solutions
to this in the rest of this chapter.
Algorithms
Since there are 2D models, we cannot explore the full posterior, or ﬁnd the globally optimal
model. Instead we will have to resort to heuristics of one form or another. All of the methods
we will discuss involve searching through the space of models, and evaluating the cost f(γ) at{}
(a)
subset size
training set error
all subsets on prostate cancer
(b)
prostate cancer data set. The lower envelope is the best RSS achievable for any set of a given size. Based
each point. This requires ﬁtting the model (i.e., computing argmax p(D|w)), or evaluating its
marginal likelihood (i.e., computing
)
p(D|w)p(w)dw) at each step. This is sometimes called
the wrapper method, since we “wrap” our search for the best model (or set of good models)
In order to make wrapper methods efficient, it is important that we can quickly evaluate the
score function for some new model, γ′, given the score of a previous model, γ. This can be
done provided we can efficiently update the sufficient statistics needed to compute f(γ). This
is possible provided γ′ only differs from γ in one bit (corresponding to adding or removing
a single variable), and provided f(γ) only depends on the data via Xγ. In this case, we can
γ′Xγ′ from XT
γ Xγ. These
Greedy search
we can exploit properties of least squares to derive various efficient greedy forwards search
•
Single best replacement The simplest method is to use greedy hill climbing, where at each
step, we deﬁne the neighborhood of the current model to be all models than can be reached
by ﬂipping a single bit of γ, i.e., for each variable, if it is currently out of the model, we
consider adding it, and if it is currently in the model, we consider removing it. In (Soussen
improvement is possible.
•
penalty, there will be no reason to perform deletion steps. In this case, the SBR algorithm isto greedy forwards selection. In this algorithm, we start with the empty set and add the
j∗= arg min
j̸∈γt
min
add at step t, we need to solve D −Dt least squares problems at step t, where Dt = |γt| is
the cardinality of the current active set. Having chosen the best feature to add, we need to
•
ﬁcation is to “freeze” the current weights at their current value, and then to pick the next
feature to add by solving
j∗= arg min
j̸∈γt
min
This inner optimization is easy to solve: we simply set β = xT
y −Xwt is the current residual vector. If the columns are unit norm, we have
j∗= arg max xT
:,jrt
so we are just looking for the column that is most correlated with the current residual. We
requires one least squares calculation per iteration and so is faster than orthogonal least
•
Matching pursuits An even more aggressive approximation is to just greedily add the feature
that is most correlated with the current residual. This is called matching pursuits (Mallat
This is also equivalent to a method known as least squares boosting
•
called saturated model), and then deletes the worst one at each step. This is equivalent
to performing a greedy search from the top of the lattice downwards. This can give better
not is made in the context of all the other variables that might depende on it. However,
this method is typically infeasible for large problems, since the saturated model will be too
expensive to ﬁt.
•
•
it uses a Bayesian marginal likelihood scoring criterion (under a spike and slab model) instead
of a least squares objective. In addition, it uses a form of beam search to explore multiple
paths through the lattice at once.Stochastic search
If we want to approximate the posterior, rather than just computing a mode (e.g. because we
want to compute marginal inclusion probabilities), one option is to use MCMC. The standard
approach is to use Metropolis Hastings, where the proposal distribution just ﬂips single bits.
This enables us to efficiently compute p(γ′|D) given p(γ|D). The probability of a state (bit
conﬁguration) is estimated by counting how many times the random walk visits this state. See
for a very recent method based on evolutionary MCMC.
However, in a discrete state space, MCMC is needlessly inefficient, since we can compute the
(unnormalized) probability of a state directly using p(γ, D) = exp(−f(γ)); thus there is no
need to ever revisit a state. A much more efficient alternative is to use some kind of stochastic
search algorithm, to generate a set S of high scoring models, and then to make the following
approximation
p(γ|D) ≈
e−f(γ)

γ′∈S e−f(γ′)
EM and variational inference *
w). We can replace the delta function with a narrow Gaussian, and
then the E step amounts to classifying wj under the two possible Gaussian models. However,
this is likely to suffer from severe local minima.
wj. In this case, the posterior p(γ|D, w) is intractable to compute because all the bits become
correlated due to explaining away. However, it is possible to derive a mean ﬁeld approximation
of the form '
ℓ1 regularization: basics
When we have many variables, it is computationally difficult to ﬁnd the posterior mode of
analysis), they can of course get stuck in local optima.
the optimization community, it is common to relax hard constraints of this form by replacing
regression. There we exploited the fact that the Laplace has heavy tails. Here we exploit the factIllustration of ℓ1 (left) vs ℓ2 (right) regularization of a least squares problem. Based on Figure
p(w|λ) =
D

D

e−λ|wj|
this prior. The penalized negative log likelihood has the form
D
sparse, for reasons we explain below. Indeed, this can be thought of as a convex approximation
argmin
w
In the case of linear regression, the ℓ1 objective becomes
f(w)
=
N

=
Laplace prior on the parameters and performing MAP estimation is called ℓ1 regularization.
Why does ℓ1 regularization yield sparse solutions?
We now explain why ℓ1 regularization results in sparse solutions, whereas ℓ2 regularization does
not. We focus on the case of linear regression, although similar arguments hold for logistic
regression and other GLMs.min
We can rewrite this as a constrained but smooth objective (a quadratic function with linear
constraints):
min
w RSS(w)
s.t.
later.
Similarly, we can write ridge regression
min
or as a bound constrained form:
min
w RSS(w)
s.t.
the ℓ2 and ℓ1 constraint surfaces. From the theory of constrained optimization, we know that
the optimal solution occurs at the point where the lowest level set of the objective function
intersects the constraint surface (assuming the constraint is active). It should be geometrically
clear that as we relax the constraint B, we “grow” the ℓ1 “ball” until it meets the objective; the
corners of the ball are more likely to intersect the ellipse than one of the sides, especially in high
dimensions, because the corners “stick out” more. The corners correspond to sparse solutions,
which lie on the coordinate axes. By contrast, when we grow the ℓ2 ball, it can intersect the
objective at any point; there are no “corners”, so there is no preference for sparsity.
To see this another away, notice that, with ridge regression, the prior cost of a sparse solution,
√
√
as long as they have the same ℓ2 norm:
√
√
√
√
√
√
√
The most rigorous way to see that ℓ1 regularization results in sparse solutions is to examine
Optimality conditions for lasso
The lasso objective has the formX0
X
c(x − x0)
f(x) − f(x0)
cʼ
c
such that
f(θ) −f(θ0) ≥g(θ −θ0) ∀θ ∈I
a = lim
θ→θ−
f(θ) −f(θ0)
θ −θ0
, b = lim
θ→θ+
f(θ) −f(θ0)
θ −θ0
is denoted ∂f(θ)|θ0. For example, in the case of the absolute value function f(θ) = |θ|, the
subderivative is given by
∂f(θ) =
⎧
⎨
⎩
If the function is everywhere differentiable, then ∂f(θ) = { df(θ)
f(θ) −f(θ0) ≥(θ −θ0)T gFN
(a)
FN
(b)
Left: soft thresholding. The ﬂat region is the interval [−λ, +λ]. Right: hard thresholding.
∂
∂wj
RSS(w)
=
ajwj −cj
aj
=
n

x2
ij
cj
=
n

xij(yi −wT
−jxi,−j)
where w−j is w without component j, and similarly for xi,−j. We see that cj is (proportional
to) the correlation between the j’th feature x:,j and the residual due to the other features,
r−j = y −X:,−jw−j. Hence the magnitude of cj is an indication of how relevant feature j is
for predicting y (relative to the other features and the current parameters).
Adding in the penalty term, we ﬁnd that the subderivative is given by
∂wjf(w)
=
=
⎧
⎨
⎩
{ajwj −cj −λ}
[−cj −λ, −cj + λ]
{ajwj −cj + λ}
We can write this in a more compact fashion as follows:
⎧
⎨
⎩
{−λ}
[−λ, λ]
{λ}
of wj, as follows:aj
dient is zero at ˆwj = cj−λ
aj
In summary, we have
ˆwj(cj) =
⎧
⎨
⎩
(cj + λ)/aj
if cj < −λ
(cj −λ)/aj
if cj > λ
We can write this as follows:
ˆwj
=
soft( cj
aj
; λ
aj
)
where
soft (a; δ)
≜
sign(a) (|a| −δ)+
This is called soft thresholding.
This is
corresponding to the least squares ﬁt. The solid line, which represents the regularized estimate
The
slope of the soft thresholding line does not coincide with the diagonal, which means that even
large coefficients are shrunk towards zero; consequently lasso is a biased estimator. This is
undesirable, since if the likelihood indicates (via cj) that the coefficient wj should be large, we
it stands for “least absolute selection and shrinkage operator”, since it selects a subset of the
λmax = ||XT y||∞= max
j
|yT x:,j|
the maximum penalty for an ℓ1 regularized objective is
λmax = max
jComparison of least squares, lasso, ridge and subset selection
We can gain further insight into ℓ1 regularization by comparing it to least squares, and ℓ2 and
ℓ0 regularized least squares. For simplicity, assume all the features of X are orthonormal, so
XT X = I. In this case, the RSS is given by
RSS(w)
=
=
const +

k
w2

k

i
wkxikyi
so we see this factorizes into a sum of terms, one per dimension. Hence we can write down the
MAP and ML estimates analytically, as follows:
•
MLE The OLS solution is given by
ˆwOLS
k
= xT
:ky
where x:k is the k’th column of X.
We see
that ˆwOLS
k
is just the orthogonal projection of feature k onto the response vector (see
•
Ridge One can show that the ridge estimate is given by
ˆwridge
k
=
ˆwOLS
k
•
k
ˆwlasso
k
=
sign( ˆwOLS
k
)

| ˆwOLS
k
| −λ
	
+
•
Subset selection If we pick the best K features using subset selection, the parameter
estimate is as follows
ˆwSS
k
=

ˆwOLS
k
if rank(|wOLS
k
|) ≤K
otherwise
where rank refers to the location in the sorted list of weight magnitudes. This corresponds
the MSE vs polynomial order. We see that lasso gives similar results to the subset selection
method.
prediction accuracy (at least on this particular data set) than least squares, ridge, and best
subset regression. (In each case, the strength of the regularizer was chosen by cross validation.)
Lasso also gives rise to a sparse solution. Of course, for other problems, ridge may give better
predictive accuracy. In practice, a combination of lasso and ridge, known as the elastic net,
often performs best, since it provides a good combination of sparsity and regularization (seelambda
mse
train
test
(a)
degree
mse
performance of MLE
 
 
train
test
(b)
the right. Figure generated by linregPolyLassoDemo. (b) MSE versus polynomial degree. Note that the
regression models. Figure generated by linregPolyVsDegree.
Term
LS
Best Subset
Ridge
Lasso
Intercept
lcavol
lweight
age
lbph
svi
lcp
gleason
pgg45
Test Error
cases. Methods are: LS = least squares, Subset = best subset regression, Ridge, Lasso. Rows represent
the coefficients; we see that subset regression and lasso give sparse solutions. Bottom row is the mean
prostateComparison.
Regularization path
regularization path. 
 
lcavol
lweight
age
lbph
svi
lcp
gleason
pgg45
(a)
 
 
lcavol
lweight
age
lbph
svi
lcp
gleason
pgg45
(b)
(a) Proﬁles of ridge coefficients for the prostate cancer example vs bound on ℓ2 norm of w,
τ
 
 
lcavol
lweight
age
lbph
svi
lcp
gleason
pgg45
(a)
lars step
 
 
lcavol
lweight
age
lbph
svi
lcp
gleason
pgg45
(b)
Illustration of piecewise linearity of regularization path for lasso on the prostate cancer
example. (a) We plot ˆwj(B) vs B for the critical values of B. (b) We plot vs steps of the LARS algorithm.
Figure generated by lassoPathProstate.
Remarkably, it can be shown that the solution path is a piecewise linear function of B (Efron
one can solve for these critical values analytically. This is the basis of the LARS algorithm
for details). Remarkably, LARS can compute the entire regularization path for roughly the same
This merely affects the scale of the horizontal axis, not the shape of the curves.piecewise linearity is more evident. Below we display the actual coefficient values at each step
along the regularization path (the last line is the least squares solution):
Output of lassoPathProstate
using lasso. One can show that, if D > N, the optimal solution can have at most N variables in
it, before reaching the complete set corresponding to the OLS solution of minimal ℓ1 norm. In
known as the elastic net), we can achieve sparse solutions which contain more variables than
training cases. This lets us explore model sizes between N and D.Model selection
It is tempting to use ℓ1 regularization to estimate the set of relevant variables. In some cases,
we can recover the true sparsity pattern of w∗, the parameter vector that generated the data.
consistent. The details on which methods enjoy this property, and when, are beyond the scope
Instead of going into a theoretical discussion, we will just show a small example. We ﬁrst
y and X.
small. The third row is the least squares estimate of the coefficients which are estimated to be
the relevant coefficients as well as the irrelevant ones. The last row is the least squares estimate
for all the coefficients jointly, ignoring sparsity.
We see that the (debiased) sparse estimate
is an excellent estimate of the original signal. By contrast, least squares without the sparsity
assumption performs very poorly.
predictive accuracy. This is not usually the same value as the one that is likely to recover the
“true” model. To see why, recall that ℓ1 regularization performs selection and shrinkage, that is,
course, this will result in a less sparse model which contains irrelevant variables (false positives).
model selection consistency.
A downside of using ℓ1 regularization to select variables is that it can give quite different
results if the data is perturbed slightly.
The Bayesian approach, which estimates posterior
versions of the data. By computing how often each variable is selected across different trials,
we can approximate the posterior inclusion probabilities. This method is known as stability
We can threshold the stability selection (bootstrap) inclusion probabilities at some level, say
λ). This process of intersecting the sets is a way of eliminating the false positives that vanilla
consistent under a wider range of conditions than vanilla lasso.lasso on sign inconsistent data
−log(λ)
variable index
(a)
bolasso on sign inconsistent data
−log(λ)
variable index
(b)
lasso vs bolasso on sign inconsistent data
−log(λ)
P(correct support)
lasso
bolasso
(c)
bilities) vs. regularization parameter for Lasso. As we move from left to right, we decrease the amount of
regularization, and therefore select more variables. (b) Same as (a) but for bolasso. (c) Probability of correct
sign estimation vs. regularization parameter. Bolasso (red, dashed) and Lasso (black, plain): The number
generated by bolassoDemo.
for more detail on the experimental setup. For dataset n, variable j, and sparsity level k, deﬁne
the correct set of variables is recovered, for lasso and for bolasso with an increasing number of
to be a good compromise between speed and accuracy.
Bayesian inference for linear models with Laplace priors
We have been focusing on MAP estimation in sparse linear models. It is also possible to perform
and median, as well as samples from the posterior, are not sparse; only the mode is sparse. This
estimate is often untypical of the bulk of the posterior.
showed that that plugging in the posterior mean, rather than the posterior mode, is the optimal
thing to do if we want to minimize squared prediction error.
with a Laplace prior, albeit at slightly higher computational cost.ℓ1 regularization: algorithms
In this section, we give a brief review of some algorithms that can be used to solve ℓ1 regularized
estimation problems. We focus on the lasso case, where we have a quadratic loss. However,
most of the algorithms can be extended to more general settings, such as logistic regression (see
area of machine learning is advancing very rapidly, so the methods below may not be state of
Coordinate descent
Sometimes it is hard to optimize all the variables simultaneously, but it easy to optimize them
one by one. In particular, we can solve for the j’th coefficient with all the others held ﬁxed:
w∗
j = argmin
z
f(w + zej) −f(w)
where ej is the j’th unit vector. We can either cycle through the coordinates in a deterministic
fashion, or we can sample them at random, or we can choose to update the coordinate for
which the gradient is steepest.
resulting algorithm was the fastest method in their experimental comparison, which concerned
document classiﬁcation with large sparse feature vectors (representing bags of words). Other
n
ij;
n
wj = soft( cj
aj , λ
aj );
LARS and other homotopy methods
The problem with coordinate descent is that it only updates one variable at a time, so can be
slow to converge. Active set methods update many variables at a time. Unfortunately, they aremore complicated, because of the need to identify which variables are constrained to be zero,
and which are free to be updated.
Active set methods typically only add or remove a few variables at a time, so they can take a
long if they are started far from the solution. But they are ideally suited for generating a set of
similar algorithm was independently invented in (Osborne et al. 2000b,a)). This can compute
is found which has the same correlation (in terms of magnitude) with the current residual as
the ﬁrst variable, where the residual at step k is deﬁned as rk = y −X:,Fkwk, where Fk is
λ analytically, by using a geometric argument (hence the term “least angle”). This allows the
algorithm to quickly “jump” to the next point on the regularization path where the active set
changes. This repeats until all the variables are added.
It is necessary to allow variables to be removed from the active set if we want the sequence of
solutions to correspond to the regularization path of lasso. If we disallow variable removal, we
get a slightly different algorithm called LAR, which tends to be faster. In particular, LAR costs
the same as a single ordinary least squares ﬁt, namely O(ND min(N, D)), which is O(ND2)
if N > D, and O(N 2D) if D > N. LAR is very similar to greedy forward selection, and a
tion path for ℓ1 regularized GLMs, such as logistic regression. In general, one cannot analytically
homotopy method. These methods exploit the fact that we can quickly compute ˆw(λk) from
starting strategy, and computes the full regularization path for any ℓ1 regularized GLM. This has
been implemented in the glmnet package, which is bundled with PMTK.
Proximal and gradient projection methods
In this section, we consider some methods that are suitable for very large scale problems, where
homotopy methods made be too slow. These methods will also be easy to extend to other kindsof regularizers, beyond ℓ1, as we will see later. Our presentation in this section is based on
Consider a convex objective of the form
f(θ) = L(θ) + R(θ)
where L(θ) (representing the loss) is convex and differentiable, and R(θ) (representing the
regularizer) is convex but not necessarily differentiable.
For example, L(θ) = RSS(θ) and
and IC(θ) is the indicator function of a convex set C, deﬁned as
IC(θ) ≜

θ ∈C
+∞
otherwise
suppose L(θ) = RSS(θ), and the design matrix is simply X = I. Then the obective becomes
operator for the convex function R, deﬁned by
proxR(y) = argmin
z

	
Intuitively, we are returning a point that minimizes R but which is also close (proximal) to y.
In general, we will use this operator inside an iterative optimizer, in which case we want to stay
close to the previous iterate. In this case, we use
proxR(θk) = argmin
z

	
larizers R, and how do we extend this technique to more general loss functions L? We discuss
these issues below.
Proximal operators
proxR(θ) = soft(θ, λ)
proxR(θ) = hard(θ,
√
where hard(u, a) ≜uI(|u| > a).
If R(θ) = IC(θ), the proximal operator is given by the projection onto the set C:
proxR(θ) = argmintakes us outside the feasible set.
If we project that point onto the closest point in the set we get
with kind permission of Mark Schmidt.
For some convex sets, it is easy to compute the projection operator. For example, to project
projC(θ)j =
⎧
⎨
⎩
ℓj
θj
uj
projC(θ) =

θ
θ
projC(θ) = soft(θ, λ)
D

Proximal gradient method
We now discuss how to use the proximal operator inside of a gradient descent routine. The
basic idea is to minimize a simple quadratic approximation to the loss function, centered on theθk:
=
argmin
z
R(z) + L(θk) + gT
2tk
where gk = ∇L(θk) is the gradient of the loss, tk is a constant discussed below, and the last
term arises from a simple approximation to the Hessian of the loss of the form ∇2L(θk) ≈
tk I.
Dropping terms that are independent of z, and multiplying by tk, we can rewrite the above
expression in terms of a proximal operator as follows:
=
argmin
z




= proxtkR(uk)
uk
=
θk −tkgk
gk
=
∇L(θk)
known as iterative soft thresholding.
mation to the Hessian ∇2L, we require that
in the least squares sense. Hence
αk = argmin
α
This stepsize can be used with any gradient method, whether
proximal or not. It does not lead to monotonic decrease of the objective, but it is much faster
than standard line search techniques. (To ensure convergence, we require that the objective
When we combine the BB stepsize with the iterative soft thresholding technique (for R(θ) =
BPDN problem known as the SpaRSA algorithm, which stands for “sparse reconstruction by
Nesterov’s method
A faster version of proximal gradient descent can be obtained by epxanding the quadratic
approximation around a point other than the most recent parameter value. In particular, consider
performing updates of the form
=
proxtkR(φk −tkgk)
gk
=
∇L(φk)
φk
=λt = max(s||XT r||∞, λ) // Adapt the regularizer ;
repeat
g = ∇L(θ);
αg;
θ = soft(u, λt
α );
until f(θ) increased too much within the past M steps;
σ2
N
yi
xi
D
wj
τj
γ
Representing lasso using a Gaussian scale mixture prior.
of ways of setting tk; typically one uses line search.
When this method is combined with the iterative soft thresholding technique (for R(θ) =
BPDN problem known as the fast iterative shrinkage thesholding algorithm or FISTA (BeckEM for lasso
In this section, we show how to solve the lasso problem using lasso. At ﬁrst sight, this might
seem odd, since there are no hidden variables. The key insight is that we can represent the
as follows:


j
Thus the Laplace is a GSM where the mixing distibution on the variances is the exponential
j | γ2
p(y, w, τ, σ2|X)
=
IG(σ2|aσ, bσ)
⎡
⎣
j
⎤
⎦
Expanding out, we
get
p(y, w, τ, σ2|X)
∝


	
exp

2wT Dτw
	
exp(−bσ/σ2)

j
exp(−γ2
j )
j and σ2, and in the M step we estimate w. The resulting estimate ˆw is
Why EM?
Before going into the details of EM, it is worthwhile asking why we are presenting this approach
at all, given that there are a variety of other (often much faster) algorithms that directly solve the
ℓ1 MAP estimation problem (see linregFitL1Test for an empirical comparison). The reason
is that the latent variable perspective brings several advantages, such as the following:
•
j ). The EM
algorithm is easy to modify.•
various extensions below.
•
It makes it clear how we can compute the full posterior, p(w|D), rather than just a MAP
The objective function
that do not depend on w)
ℓc(w)
=
2wT Λw + const
j ) is the precision matrix for w.
The E step
The key is to compute E
+
j |wj
,
j |w, D)
=
InverseGaussian

γ2
w2
j
, γ2

(Note that the inverse Gaussian distribution is also known as the Wald distribution.) Hence
E

j
|wj

=
γ
|wj|
Let Λ = diag(E

 
, . . . , E

D
 
) denote the result of this E step.
We also need to infer σ2. It is easy to show that that the posterior is
Hence
E

= aN
bN
≜ω
The M step
The M step consists of computing
ˆw = argmax
w
2wT Λw
This is just MAP estimation under a Gaussian prior:
ˆw = (σ2Λ + XT X)−1XT ynumerically unstable. Fortunately, we can use the SVD of X, given by X = UDVT , as follows:
ˆw
=
where
Ψ = Λ
E

j
 ) = diag( |wj|
π′(wj))
Caveat
tunately, this sometimes does not happen, for numerical reasons. In particular, suppose that in
the true solution, w∗
discussion.
ℓ1 regularization: extensions
In this section, we discuss various extensions of “vanilla” ℓ1 regularization.
Group Lasso
in more complex models, there may be many parameters associated with a given variable. In
particular, we may have a vector of weights for each input, wj. Here are some examples:
•
Multinomial logistic regression Each feature is associated with C different weights, one
per class.
•
vector of length C.
•
For example, we might have C separate regression or binary classiﬁcation problems. Thus
each feature is associated with C different weights. We may want to use a feature for all of
the tasks or none of the tasks, and thus select weights at the group level (Obozinski et al.
If we use an ℓ1 regularizer of the form ||w|| = 
j

c |wjc|, we may end up with with some
elements of wj,: being zero and some not. To prevent this kind of situation, we partition the
parameter vector into G groups. We now minimize the following objective
J(w) = NLL(w) +
G
where
w2
j

dg, where dg is the
number of elements in group g.
objective becomes
J(w) = NLL(w) + λ

√
(w2
√
(w2


ridge regression, since
G


g

w2
By using the square root, we are penalizing the radius of a ball containing the group’s weight
vector: the only way for the radius to be small is if all elements are small. Thus the square root
results in group sparsity.
||wg||∞= max
It is clear that this will also result in group sparsity.
ℓ∞norm has a tendency to make all the elements within a block to have similar magnitude.
examples, and was chosen by hand.)
GSM interpretation of group lasso
Group lasso is equivalent to MAP estimation using the following prior
p(w|γ, σ2) ∝exp


−γ
σ
G

(a)
(b)
Illustration of group lasso where the original signal is piecewise Gaussian. Top left: original
signal. Bottom left:: vanilla lasso estimate. Top right: group lasso estimate using a ℓ2 norm on the blocks.
g
∼
g Idg)
g |γ
∼
, γ
where dg is the size of group g. So we see that there is one variance term per group, each
of which comes from a Gamma prior, whose shape parameter depends on the group size, and
large as well.(a)
(b)
σ2
yi
xi
w11
w12
w21
w22
w23
τ1
τ2
γAlgorithms for group lasso
There are a variety of algorithms for group lasso.
Here we brieﬂy mention two.
The ﬁrst
is separable, R(w) = 
g ||wg||p, the proximal operator decomposes into G separate operators
of the form
proxR(b) = argmin
implemented as follows
otherwise we have
proxR(b) = b −λ
b
proxR(b) = b
Another approach is to modify the EM algorithm. The method is almost the same as for
g(j), where g(j) is the group to which dimension j belongs,
we can use the same full conditionals for σ2 and w as before. The only changes are as follows:
•
We must modify the full conditional for the weight precisions, which are estimated based on
a shared set of weights:
τ 2g
|γ, w, σ2, y, X ∼InverseGaussian(
γ2σ2
, γ2)
jg. For the E step, we can use
E

τ 2g


=
γσ
•
We must modify the full conditional for the tuning parameter, which is now only estimated
g :
G

g
g )●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
Index
CGH
(a)
(b)
(c)
(a) Example of the fused lasso. The vertical axis represents array CGH (chromosomal genome
hybridization) intensity, and the horizontal axis represents location along a genome.
(b) Noisy image. (c) Fused lasso estimate using 2d lattice prior.
Used with kind permission of Holger Hoeﬂing.
Fused lasso
In some problem settings (e.g., functional data analysis), we want neighboring coefficients to be
we want to ﬁt a signal that is mostly “off”, but in addition has the property that neighboring
locations are typically similar in value. We can model this by using a prior of the form
p(w|σ2) ∝exp
⎛
⎝−λ1
σ
D

|wj| −λ2
σ

⎞
⎠
This is known as the fused lasso penalty. In the context of functional data analysis, we often
case, the overall objective has the form
J(w, λ1, λ2) =
N

N

|wi| + λ2

It is possible to generalize this idea beyond chains, and to consider other graph structures,
using a penalty of the form
J(w, λ1, λ2) =


|ws| + λ2

(s,t)∈E
|ws −wt|
from some prior knowledge, e.g., from a database of known biological pathways.
AnotherGSM interpretation of fused lasso
hierarchical model
w|σ2, τ, ω
∼
j |γ1
∼
Expon(γ2
ω2
j |γ2
∼
Expon(γ2
main diagonal
=
j
+
ω2
ω2
j
}
off diagonal
=
ω2
j
}
D
Algorithms for fused lasso
It is possible to generalize the EM algorithm to ﬁt the fused lasso model, by exploiting the
Markov structure of the Gaussian prior for efficiency. Direct solvers (which don’t use the latent
more expensive to ﬁt than the other variants we have considered.
Elastic net (ridge and lasso combined)
Although lasso has proved to be effective as a variable selection technique, it has several
•
If there is a group of variables that are highly correlated (e.g., genes that are in the same
pathway), then the lasso tends to select only one of them, chosen rather arbitrarily. (This
is evident from the LARS algorithm: once one member of the group has been chosen, the
remaining members of the group will not be very correlated with the new residual and hence
will not be chosen.) It is usually better to select all the relevant variables in a group. If we
know the grouping structure, we can use group lasso, but often we don’t know the grouping
structure.
•
In the D > N case, lasso can select at most N variables before it saturates.
•
If N > D, but the variables are correlated, it has been empirically observed that the
prediction performance of ridge is better than that of lasso.a hybrid between lasso and ridge regression, which solves all of these problems. It is apparently
called the “elastic net” because it is “like a stretchable ﬁshing net that retains ’all the big ﬁsh”’
Vanilla version
The vanilla version of the model deﬁnes the following objective function:
minimum, even if X is not full rank.
a grouping effect, which means that the regression coefficients of highly correlated variables
tend to be equal (up to a change of sign if they are negatively correlated). For example, if two
features are equal, so X:j = X:k, one can show that their estimates are also equal, ˆwj = ˆwk.
Algorithms for vanilla elastic net
on modiﬁed data. In particular, deﬁne
˜X = c

X
√λ2ID
	
,
˜y =

y
	
˜w = arg min
and set w = c ˜w.
stop the algorithm after m variables have been included, the cost is O(m3 + Dm2). Note that
we can use m = D if we wish, since ˜X has rank D. This is in contrast to lasso, which cannot
select more than N variables (before jumping to the OLS solution) if N < D.
λ2.
Improved version
Unfortunately it turns out that the “vanilla” elastic net does not produce functions that predict
very accurately, unless it is very close to either pure ridge or pure lasso. Intuitively the reason
is that it performs shrinkage twice: once due to the ℓ2 penalty and again due to the ℓ1 penalty.
The solution is simple: undo the ℓ2 shrinkage by scaling up the estimates from the vanilla
ˆw =
We will call this a corrected estimate.
One can show that the corrected estimates are given by
ˆw = arg min
w wT
XT X + λ2I
	
Now
XT X + λ2I
covariance matrices.)
GSM interpretation of elastic net
The implicit prior being used by the elastic net obviously has the form
p(w|σ2) ∝exp
⎛
⎝−γ1
σ
D

|wj| −γ2
2σ2
D

w2
j
⎞
⎠
which is just a product of Gaussian and Laplace distributions.
j
∼
j
j |γ1
∼
Expon(γ2
It is possible to perform MAP estimation in this model using EM, or Bayesian inference using
Although the Laplace prior results in a convex optimization problem, from a statistical point
of view this prior is not ideal. There are two main problems with it. First, it does not put
not put enough probability mass on large values, so it causes shrinkage of relevant coefficients,
coefficients are signiﬁcantly smaller than their ML estimates, a phenomenon known as bias.)
Both problems can be solved by going to more ﬂexible kinds of priors which have a larger
below.Bridge regression
A natural generalization of ℓ1 regularization, known as bridge regression (Frank and Friedman
ˆw = NLL(w) + λ

j
|wj|b
by
ExpPower(w|μ, a, b) ≜
b


−|x −μ|
a
b
√
ℓ0 regression, which is equivalent to best subset selection. Unfortunately, the objective is not
approximation to the ℓ0 norm.
observation, (x, y), which imposes a single linear constraint of the form, y = wT x, with a
the Gaussian, the MAP estimate is not sparse (the mode does not lie on either of the coordinate
axes).
Hierarchical adaptive lasso
Recall that one of the principal problems with lasso is that it results in biased estimates.
different penalty parameter with each parameter. Of course, it is completely infeasible to tune
D parameters by cross validation, but this poses no problem to the Bayesian: we simply make
j have its own private tuning parameter, γj, which are now treated as random variables
coming from the conjugate prior γj ∼IG(a, b). The full model is as follows:
γj
∼
IG(a, b)
j |γj
∼
j
∼
j )
j , which
scaled mixture of Laplacians.
It turns out that we can ﬁt this model (i.e., compute a local
posterior mode) using EM, as we explain below. The resulting estimate, ˆwHAL, often worksTop: plot of log prior for three different distributions with unit variance: Gaussian, Laplace
and exponential power. Bottom: plot of log posterior after observing a single observation, corresponding
to a single linear constraint. The precision of this observation is shown by the diagonal lines in the top
ﬁgure. In the case of the Gaussian prior, the posterior is unimodal and symmetric. In the case of the
Laplace prior, the posterior is unimodal and asymmetric (skewed). In the case of the exponential prior, the
by Florian Steinke.
much better than the estimate returned by lasso, ˆwL1, in the sense that it is more likely to
contain zeros in the right places (model selection consistency) and more likely to result in good
EM for HAL
Since the inverse Gamma is conjugate to the Laplace, we ﬁnd that the E step for γj is given by
The E step for σ2 is the same as for vanilla lasso.
The prior for w has the following form:
p(w|γ) =

j
2γj
exp(−|wj|/γj)
Hence the M step must optimize
w
log N(y|Xw, σ2) −

jaσ
bσ
σ2
yi
xi
wj
j
γj
a
b
N
D
(a)
HAL
 
 
(b)
(a) DGM for hierarchical adaptive lasso. (b) Contours of Hierarchical adpative Laplace. Based
The expectation is given by
b + |w(t)
j |
≜s(t)
j
Thus the M step becomes a weighted lasso problem:
w

j
s(t)
j |wj|
mated to be large in the previous iteration (so w(t)
j
is large), then the scaling factor s(t)
j
will
be small, so large coefficients are not penalized heavily. Conversely, small coefficients do get
penalized heavily. This is the way that the algorithm adapts the penalization strength of each
coefficient. The result is an estimate that is often much sparser than returned by lasso, but also
less biased.
also closely related to some iteratively reweighted ℓ1 methods proposed in the signal processing
Understanding the behavior of HAL
We can get a better understanding of HAL by integrating out γj to get the following marginal
distribution,
p(wj|a, b) = a
2b
|wj|
bLasso
wMLE
wMAP
(a)
wMLE
wMAP
HAL
 
 
(b)
Thresholding behavior of two penalty functions (negative log priors).
(a) Laplace.
(b) Hierarchical adaptive Laplace.
Figure generated by
normalGammaThresholdPlotDemo.
GT(w|μ, a, c, q) ≜
q

acq
where c is the scale parameter (which controls the degree of sparsity), and a is related to the
√
get the Laplace distribution. In the context of the current model, we see that p(wj|a, b) =
The resulting penalty term has the form
b ) + const
puts much more density along the “spines”, thus enforcing sparsity more aggressively. Note that
this penalty is clearly not convex.
We can gain further understanding into the behavior of this penalty function by considering
applying it to the problem of linear regression with an orthogonal design matrix. In this case,j )
p(γj)
p(wj)
Ref
Fixed
IG(a, b)
Ga(a, b)
NEG(a, b)
Ga(δ, γ2
Fixed
NG(δ, γ)
-
NJ(wj)
IG( δ
Fixed
horseshoe(b)
analytic form. The deﬁnitions of the NEG and NG densities are a bit complicated, but can be found in the
references. The other distributions are deﬁned in the text.
one can show that the objective becomes
J(w)
=
D

πλ(|wj|)
=
D

( ˆwmle
j
D

πλ(|wj|)
where ˆwmle = XT y is the MLE and ˆy = X ˆwmle. Thus we can compute the MAP estimate
one dimension at a time by solving the following 1d optimization problem:
ˆwj = argmin
wj
j
whereas we would like them to be equal to their unshrunken ML estimates.
much more closely.
Other hierarchical priors
summary. In some cases, we can analytically derive the form of the marginal prior for wj.
Generally speaking, this prior is not concave.to hard thresholding. However, this prior has no free parameters, which is both a good thing
(nothing to tune) and a bad thing (no ability to adapt the level of sparsity).
Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)
j p(wj). We have seen how these
j ), where
j |wj), and in the M step we estimate w from y, X and τ.
scale mixtures), or a weighted ℓ1 optimization (in the case of Laplacian scale mixtures). We also
discussed how to perform Bayesian inference in such models, rather than just computing MAP
estimates.
In this section, we discuss an alternative approach based on type II ML estimation (empirical
Bayes), whereby we integrate out w and maximize the marginal likelihood wrt τ.
This EB
procedure can be implemented via EM, or via a reweighted ℓ1 scheme, as we will explain below.
Having estimated the variances, we plug them in to compute the posterior mean of the weights,
E [w|ˆτ, D]; rather surprisingly (in view of the Gaussian prior), the result is an (approximately)
sparse estimate, for reasons we explain below.
In the context of neural networks, this this method is called called automatic relevance
linear models we are considering in this chapter, this method is called sparse Bayesian learning
gives rise to a technique called the relevance vector machine (RVM), which we will discuss in
ARD for linear regression
We will explain the procedure in the context of linear regression; ARD for GLMs requires the use
of the Laplace (or some other) approximation. case can be It is conventional, when discussing
j , and the measurement precision
coefficients!). In particular, we will assume the following model:
p(y|x, w, β)
=
p(w)
=where A = diag(α). The marginal likelihood can be computed analytically as follows:
p(y|X, α, β)
=


=
=
α y)
where
Cα ≜β−1IN + XA−1XT
α y
To regularize the problem, we may put a conjugate prior on each precision, αj ∼Ga(a, b)
and β ∼Ga(c, d). The modiﬁed objective becomes
ℓ(α, β)
≜

j
log Ga(αj|a, b) + log Ga(β|c, d)
=
α y +

j
(a log αj −bαj) + c log β −dβ
However, when performing (type II) point estimation, we will use the improper prior a = b =
proxy for ﬁnding the most probable model setting of γ in the spike and slab model, which in
is much easier to optimize.
Once we have estimated α and β, we can compute the posterior over the parameters using
p(w|D, ˆα, ˆβ) = N(μ, Σ)
μ = ˆβΣXT y
The fact that we compute a posterior over w, while simultaneously encouraging sparsity, is why
the method is called “sparse Bayesian learning”. Nevertheless, since there are many ways to be
sparse and Bayesian, we will use the “ARD” term instead, even in the linear model context. (In
addition, SBL is only “being Bayesian” about the values of the coefficients, rather than reﬂecting
uncertainty about the set of relevant variables, which is typically of more interest.)
Gaussian case generalizes more easily to other cases such as logistic regression.x
y
(a)
C
y
(b)
Illustration of why ARD results in sparsity. The vector of inputs x does not point towards
the vector of outputs y, so the feature should be removed. (a) For ﬁnite α, the probability density is spread
in directions away from y. (b) When α = ∞, the probability density at y is maximized. Based on Figure
Whence sparsity?
j
features automatically have their weights “turned off” or “pruned out”.
so X = x = (x1, x2), and y = (y1, y2). We can plot x and y as vectors in the plane, as
in a nearly orthogonal direction to y. Let us see what happens to the marginal likelihood as we
αxxT
constant, the latter assigns higher probability density to the observed response vector y, so this
is the preferred solution. In other words, the marginal likelihood “punishes” solutions where αj
is small but X:,j is irrelevant, since these waste probability mass. It is more parsimonious (from
the point of view of Bayesian Occam’s razor) to eliminate redundant dimensions.
Connection to MAP estimation
ARD seems quite different from the MAP estimation methods we have been considering earlier
in this chapter. In particular, in ARD, we are not integrating out α and optimizing w, but viceversa. Because the parameters wj become correlated in the posterior (due to explaining away),
when we estimate αj we are borrowing information from all the features, not just feature j.
as the following MAP estimation problem:
ˆwARD
=
arg min
gARD(w)
≜
min

j
αjw2
j + log |Cα|
The proof, which is based on convex analysis, is a little complicated and hence is omitted.
tive corresponds to the global optimum of the ℓ0 objective — a property that ℓ1 regularization,
which has no local minima, does not enjoy.
Algorithms for ARD *
In this section, we review several different algorithms for implementing ARD.
EM algorithm
is given by
Q(α, β)
=
E

 
=
2E
⎡

j
log αj −tr(AwwT )
⎤
=
2N log β −β



j
2tr[A(μμT + Σ)] + const
Suppose we put a Ga(a, b) prior on αj and a Ga(c, d) prior on β. The penalized objective
becomes
Q′(α, β) = Q(α, β) +

j
(a log αj −bαj) + c log β −dβ
Setting dQ′
αj
=
E

w2
j
 
+ 2b =
m2
j + Σjj + 2bα =
D
E [wT w] =
D
The update for β is given by
new
=
N + 2c
dℓ
dℓ
point updates:
αj
←
γj + 2a
m2
j + 2b
←
N −
j γj + 2c
γj
≜
γ = 
discussion.
the results can depend on the initial values.
Iteratively reweighted ℓ1 algorithm
Another approach to solving the ARD problem is based on the view that it is a MAP estimation
problem. Although the log prior g(w) is rather complex in form, it can be shown to be a
reweighted ℓ1 problem of the form
w NLL(w) +

j
λ(t)
j |wj|
jj
λj ←


X:,j

j
|)
XT )−1X:,j

In this case, there are

D
N
	
solutions which perfectly reconstruct the data, Xw = y, and which
solutions that satsify Xw = y but which are much sparser than this. Suppose the method has
found a BFS. We do not want to increase the penalty on a weight just because it is small (as
in adaptive lasso), since that will just reinforce our current local optimum. Instead, we want to
j
so the penalty will not increase much, but if w is sparser than N, the matrix will not be full
ARD for logistic regression
Now consider binary logistic regression, p(y|x, w) = Ber(y|sigm(wT x)), using the same
Gaussian prior is not conjugate to the logistic likelihood, so the E step cannot be done exactly.
then use this approximation inside the same EM procedure as before, except we no longer need
to update β. Note, however, that this is not guaranteed to converge.
methods to compute the inner weighted ℓ1 regularized logistic regression problem, and no
approximations are required.
Sparse coding *
So far, we have been concentrating on sparse priors for supervised learning. In this section, we
discuss how to use them for unsupervised learning.
Laplace distribution, we will be approximating each observed vector xi as a sparse combination
of basis vectors (columns of W); note that the sparsity pattern (controlled by zi) changes from
data case to data case. If we relax the constraint that W is orthogonal, we get a method called
expensive ℓ1 problem.Method
p(zi)
p(W)
W orthogonal
PCA
Gauss
-
yes
FA
Gauss
-
no
ICA
-
yes
Sparse coding
Laplace
-
no
Sparse PCA
Gauss
Laplace
maybe
Sparse MF
Laplace
Laplace
no
Summary of various latent factor models. A dash “-” in the p(W) column means we are
performing ML parameter estimation rather than MAP parameter estimation. Summary of abbreviations:
PCA = principal components analysis; FA = factor analysis; ICA = independent components analysis; MF =
matrix factorization.
sparse coding. In this context, we call the factor loading matrix W a dictionary; each column
which case we call the representation overcomplete.
In sparse coding, the dictionary can be ﬁxed or learned. If it is ﬁxed, it is common to use a
wavelet or DCT basis, since many natural signals can be well approximated by a small number
of such basis functions. However, it is also possible to learn the dictionary, by maximizing the
likelihood
log p(D|W) =
N

log


zi
N(xi|Wzi, σ2I)p(zi)dzi
We discuss ways to optimize this below, and then we present several interesting applications.
coding, we put a sparsity promoting prior on the latent factors zi. Of course, the two techniques
Learning a sparse coding dictionary
approximation:
log p(D|W) ≈
N

max
zi

log N(xi|Wzi, σ2I) + log p(zi)
 
If p(zi) is Laplace, we can rewrite the NLL as
NLL(W, Z) =
N

W and zi notation.To prevent W from becoming arbitrarily large, it is common to constrain the ℓ2 norm of its
s.t.
wT
W is a simple least squares problem. And for a ﬁxed dictionary W, the optimization problem
over Z is identical to the lasso problem, for which many fast algorithms exist. This suggests
an obvious iterative optimization scheme, in which we alternate between optimizing W and Z.
basis W is the analysis phase, and estimating the coefficients Z is the synthesis phase. In cases
min
N

s.t.
learned dictionary may be more interpretable if it is a positive sum of positive “parts”, rather
than a sparse sum of atoms that may be positive or negative. Of course, we can combine NMF
Alternatively, we can drop the positivity constraint, but impose a sparsity constraint on both
the factors zi and the dictionary W. We call this sparse matrix factorization. To ensure strict
min
W,Z
N

s.t.
There are several related objectives one can write down. For example, we can replace the lasso
with parameter π, which can be drawn from a beta distribution. Alternatively, we can use a
unbounded size, rather than having to specify L in advance. One can perform Bayesian inference
in this model using e.g., Gibbs sampling or variational Bayes. One ﬁnds that the effective size
of the dictionary goes down as the noise level goes up, due to the Bayesian Occam’s razor. This
Results of dictionary learning from image patches
One reason that sparse coding has generated so much interest recently is because it explains an
interesting phenomenon in neuroscience. In particular, the dictionary that is learned by applying(a)
(b)
(c)
(d)
(e)
(f)
Illustration of the ﬁlters learned by various methods when applied to natural image patches.
(Each patch is ﬁrst centered and normalized to unit norm.) (a) ICA. Figure generated by icaBasisDemo,
sparse PCA with low sparsity on weight matrix. (f) sparse PCA with high sparsity on weight matrix. Figure
generated by sparseDictDemo, written by Julien Mairal.sparse coding to patches of natural images consists of basis vectors that look like the ﬁlters that
are found in simple cells in the primary visual cortex of the mammalian brain (Olshausen and
has therefore been conjectured that parts of the cortex may be performing sparse coding of the
sensory input; the resulting latent representation is then further processed by higher levels of
the brain.
PCA, as we increase the sparsity of the basis vectors.
Compressed sensing
Although it is interesting to look at the dictionaries learned by sparse coding, it is not necessarily
very useful. However, there are some practical applications of sparse coding, which we discuss
below.
(usually Gaussian). We assume R is a known sensing matrix, corresponding to different linear
projections of x. For example, consider an MRI scanner: each beam direction corresponds to a
Our goal is to infer p(x|y, R). How can we hope to recover all of x if we do not measure
all of x? The answer is: we can use Bayesian inference with an appropriate prior, that exploits
the fact that natural signals can be expressed as a weighted combination of a small number of
suitably chosen basis functions. That is, we assume x = Wz, where z has a sparse prior, and
W is suitable dictionary. This is called compressed sensing or compressive sensing (Candes
For CS to work, it is important to represent the signal in the right basis, otherwise it will
not be sparse. In traditional CS applications, the dictionary is ﬁxed to be a standard form,
get better performance by adapting the projection matrix to the dictionary (Seeger and Nickish
Image inpainting and denoising
Suppose we have an image which is corrupted in some way, e.g., by having text or scratches
in the case of image patches, is translation invariant. This means cov [I(x, y), I(x′, y′)] = f

that the eigenvectors of a matrix of this kind are always sinusoids of different phases, i.e., PCA discovers a Fourier basis.y
R
x
W
z
λ
Schematic DGM for compressed sensing. We observe a low dimensional measurement y
generated by passing x through a measurement matrix R, and possibly subject to observation noise with
variance σ2. We assume that x has a sparse decomposition in terms of the dictionary W and the latent
(a)
(b)
struction.
Used with kind permission of Julien Mairal.
“clean” image. This is called image inpainting. One can use similar techniques for image
denoising.
We can model this as a special kind of compressed sensing problem. The basic idea is as
follows. We partition the image into overlapping patches, yi, and concatenate them to form y.
We deﬁne R so that the i’th row selects out patch i. Now deﬁne V to be the visible (uncorrupted)
components of y, and H to be the hidden components. To perform image inpainting, we just
An alternative approach is to use a graphical model (e.g., the ﬁelds of experts model (S.than using a latent variable model. Unfortunately such models tend to be computationally more
expensive.
Exercises
Deﬁne
a. Show that
∂
∂wk RSS(w)
=
akwk −ck
ak
=
n


x2
ck
=
n


xik(yi −wT
−kxi,−k) = 2xT
:,krk
where w−k = w without component k, xi,−k is xi without component k, and rk = y −wT
−kx:,−k
is the residual due to using all the features except feature k. Hint: Partition the weights into those
involving k and those not involving k.
b. Show that if
∂
ˆwk = xT
:,krk
Hence when we sequentially add features, the optimal weight for feature k is computed by computing
orthogonally projecting x:,k onto the current residual.
ΣXT X
=
ΣXT X + β−1ΣA −β−1ΣA
=
=
=
posterior is also Gaussian, so the Laplace “approximation” is exact. In this case, we get
log p(D|α, β)
=
N


j
The rest is straightforward algebra.p(D|γ)
∝
S(γ)
=
yT y −
g
γ Xγ)−1XT
γ y
Deﬁne
and
˜X = c

X
√
λ2Id

,
˜y =
 y

Show
arg min J1(w) = c(arg min J2(w))
i.e.
J1(cw) = J2(w)
and hence that one can solve an elastic net problem using a lasso solver on modiﬁed data.
Derive an expression for p(γ|α) after integrating out the πj’s. Discuss some advantages and disadvantages
of this approach compared to assuming πj = π0 for ﬁxed π0.ck
wk
Plot of ˆwk vs amount of correlation ck for three different estimators.
Show that
E
j
|wj

=
π′(wj)
|wj|
j
j )
∝
j
exp(−w2
j
j
)
=
|wj|
−2wj
j
exp(−w2
j
j
)
=
|wj|
d
j )
d
d|wj|p(wj) =
p(wj)
d
d|wj| log p(wj)
induced on the weights by a Gamma mixing distribution is called the normal Gamma distribution and isgiven by
NG(wj|δ, ρ)
=

j
=
=
where Kα(x) is the modiﬁed Bessel function of the second kind (the besselk function in Matlab).
Now suppose we have the following prior on the variances
p(σ2
G

p(σ2

The corresponding marginal for each group of weights has the form
where
ug ≜
'

w2
2z exp(−z). Show
that the resulting MAP estimate is equivalent to group lasso.
constraint. Then sketch how to use projected gradient descent to solve this problem. (If you get stuck,
Let f be a convex function. Explain how to ﬁnd a global affine lower bound to f at an arbitrary pointKernels
Introduction
So far in this book, we have been assuming that each object that we wish to classify or cluster
sequence, which can be of variable length? or a molecular structure, which has complex 3d
geometry? or an evolutionary tree, which has variable size and shape?
One approach to such problems is to deﬁne a generative model for the data, and use the
which is essentially an unsupervised way to learn good feature representations.
Another approach is to assume that we have some way of measuring the similarity between
objects, that doesn’t require preprocessing them into feature vector format. For example, when
measure of similarity between objects x, x′ ∈X, where X is some abstract space; we will call κ
a kernel function. Note that the word “kernel” has several meanings; we will discuss a different
In this chapter, we will discuss several kinds of kernel functions. We then describe some
algorithms that can be written purely in terms of kernel function computations. Such methods
can be used when we don’t have access to (or choose not to look at) the “inside” of the objects
x that we are processing.
Kernel functions
give several examples below.RBF kernels
The squared exponential kernel (SE kernel) or Gaussian kernel is deﬁned by
κ(x, x′) = exp

	
If Σ is diagonal, this can be written as
κ(x, x′) = exp
⎛
D

σ2
j
(xj −x′
⎞
⎠
We can interpret the σj as deﬁning the characteristic length scale of dimension j. If σj = ∞,
the corresponding dimension is ignored; hence this is known as the ARD kernel.
If Σ is
spherical, we get the isotropic kernel
κ(x, x′) = exp

2σ2
	
or RBF kernel, since it is only a function of ||x −x′||.
Kernels for comparing documents
When performing document classiﬁcation or retrieval, it is useful to have a way of comparing
two documents, xi and xi′. If we use a bag of words representation, where xij is the number
of times words j occurs in document i, we can use the cosine similarity, which is deﬁned by
κ(xi, xi′) =
xT
i xi′
This quantity measures the cosine of the angle between xi and xi′ when interpreted as vectors.
Unfortunately, this simple method does not work very well, for two main reasons. First, if xi
has any word in common with xi′, it is deemed similar, even though some popular words, such
as “the” or “and” occur in many documents, and are therefore not discriminative. (These are
known as stop words.) Second, if a discriminative word occurs many times in a document, the
similarity is artiﬁcially boosted, even though word usage tends to be bursty, meaning that once
Fortunately, we can signiﬁcantly improve performance using some simple preprocessing. The
tion, which stands for “term frequency inverse document frequency”. We deﬁne this as follows.
This reduces the impact of words that occur many times within one document. Second, the
inverse document frequency is deﬁned as
idf(j) ≜log
N
Nwhere N is the total number of documents, and the denominator counts how many documents
contain term j. Finally, we deﬁne
We then use this inside the cosine similarity measure. That is, our new kernel has the form
κ(xi, xi′) =
φ(xi)T φ(xi′)
Mercer (positive deﬁnite) kernels
Some methods that we will study require that the kernel function satisfy the requirement that
the Gram matrix, deﬁned by
K =
⎛
⎜
⎝
κ(x1, x1)
· · ·
κ(x1, xN)
...
κ(xN, x1)
· · ·
κ(xN, xN)
⎞
⎟
⎠
be positive deﬁnite for any set of inputs {xi}N
The importance of Mercer kernels is the following result, known as Mercer’s theorem. If the
Gram matrix is positive deﬁnite, we can compute an eigenvector decomposition of it as follows
K = UT ΛU
kij = (Λ
Let us deﬁne φ(xi) = Λ
kij = φ(xi)T φ(xj)
Thus we see that the entries in the kernel matrix can be computed by performing an inner
product of some feature vectors that are implicitly deﬁned by the eigenvectors U. In general, if
κ(x, x′) = φ(x)T φ(x′)
where φ depends on the eigen functions of κ (so D is a potentially inﬁnite dimensional space).
=
=
1x2x′This can be written as φ(x)T φ(x′), where
√
2x1,
√
2x2, x2
√
2x1x2]T
a Gaussian kernel, the feature map lives in an inﬁnite dimensional space. In such a case, it is
clearly infeasible to explicitly represent the feature vectors.
by
κ(x, x′) = tanh(γxT x′ + r)
(Note that this uses the tanh function even though it is called a sigmoid kernel.) This kernel
In general, establishing that a kernel is a Mercer kernel is difficult, and requires techniques
from functional analysis. However, one can show that it is possible to build up new Mercer
kernels from simpler ones using a set of standard rules. For example, if κ1 and κ2 are both
Linear kernels
Deriving the feature vector implied by a kernel is in general quite difficult, and only possible if
the kernel is Mercer. However, deriving a kernel from a feature vector is easy: we just use
κ(x, x′) = φ(x)T φ(x′) = ⟨φ(x), φ(x′)⟩
If φ(x) = x, we get the linear kernel, deﬁned by
κ(x, x′) = xT x′
This is useful if the original data is already high dimensional, and if the original features are
individually informative, e.g., a bag of words representation where the vocabulary size is large,
or the expression level of many genes. In such a case, the decision boundary is likely to be
representable as a linear combination of the original features, so it is not necessary to work in
some other feature space.
Of course, not all high dimensional problems are linearly separable. For example, images are
high dimensional, but individual pixels are not very informative, so image classiﬁcation typically
Matern kernels
has the following form
Γ(ν)

√
2νr
ℓ

ν
Kν

√
2νr
ℓ
motion (the corresponding function is continuous but not differentiable, and hence is very
“jagged”).
String kernels
The real power of kernels arises when the inputs are structured objects. As an example, we now
describe one way of comparing two variable length strings using a string kernel. We follow the
Consider two strings x, and x′ of lengths D, D’, each deﬁned over the alphabet A.
{A, R, N, D, C, E, Q, G, H, I, L, K, M, F, P, S, T, W, Y, V }. Let x be the following sequence
IPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGGTV
ERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDYLQEFLGVMNTEWI
PHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAERLQENLQAYRTFHVLLA
RLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEADGMLFEKK
LWGLKVLQELSQWTVRSIHDLRFISSHQTGIP
These strings have the substring LQE in common. We can deﬁne the similarity of two strings
to be the number of substrings they have in common.
More formally and more generally, let us say that s is a substring of x if we can write x = usv
for some (possibly empty) strings u, s and v. Now let φs(x) denote the number of times that
substring s appears in string x. We deﬁne the kernel between two strings x and x′ as
κ(x, x′) =

wsφs(x)φs(x′)
as the Kleene star operator). This is a Mercer kernel, and be computed in O(|x| + |x′|) time
kernel. This deﬁnes φ(x) to be the number of times each character in A occurs in x. If we
many times each possible word occurs. Note that this is a very sparse vector, since most wordsoptimal partial 
matching
matching
Illustration of a pyramid match kernel computed from two images. Used with kind permission
of Kristen Grauman.
Various extensions are possible. For example, we can allow character mismatches (Leslie et al.
Pyramid match kernels
commonly chosen by an interest point detector. The feature vectors at the chosen places are
intersection. It turns out that this provides a good approximation to the similarity measure one
would obtain by performing an optimal bipartite match at the ﬁnest spatial resolution, and then
summing up pairwise similarities between matched points. However, the histogram method is
faster and is more robust to missing and unequal numbers of points. This is a Mercer kernel.Kernels derived from probabilistic generative models
Suppose we have a probabilistic generative model of feature vectors, p(x|θ). Then there are
several ways we can use this model to deﬁne kernel functions, and thereby make the model
suitable for discriminative tasks. We sketch two approaches below.
Probability product kernels
One approach is to deﬁne a kernel as follows:
κ(xi, xj) =


p(x|xi)ρp(x|xj)ρdx
estimate computed using a single data vector. This is called a probability product kernel
Although it seems strange to ﬁt a model to a single data point, it is important to bear in
mind that the ﬁtted model is only being used to see how similar two objects are. In particular,
if we ﬁt the model to xi and then the model thinks xj is likely, this means that xi and xj are
κ(xi, xj) =

	
which is (up to a constant factor) the RBF kernel.
ones with latent variables, such as HMMs. This provides one way to deﬁne kernels on variable
Fisher kernels
A more efficient way to use generative models to deﬁne kernels is to use a Fisher kernel
κ(x, x′) = g(x)T F−1g(x′)
where g is the gradient of the log likelihood, or score vector, evaluated at the MLE ˆθ
g(x) ≜∇θ log p(x|θ)
((ˆθ
and F is the Fisher information matrix, which is essentially the Hessian:
F = ∇∇log p(x|θ)
((ˆθ
Note that ˆθ is a function of all the data, so the similarity of x and x′ is computed in the context
of all the data as well. Also, note that we only have to ﬁt one model.
The intuition behind the Fisher kernel is the following: let g(x) be the direction (in parameter
space) in which x would like the parameters to move (from ˆθ) so as to maximize its own(a)
poly10
(b)
rbf prototypes
(c)
generated by logregXorDemo.
likelihood; call this the directional gradient. Then we say that two vectors x and x′ are similar
if their directional gradients are similar wrt the the geometry encoded by the curvature of the
Using kernels inside GLMs
In this section, we discuss one simple way to use kernels for classiﬁcation and regression. We
will see other approaches later.
Kernel machines
We deﬁne a kernel machine to be a GLM where the input feature vector has the form
φ(x) = [κ(x, μ1), . . . , κ(x, μK)]
feature vector. Note that in this approach, the kernel need not be a Mercer kernel.
We can use the kernelized feature vector for logistic regression by deﬁning p(y|x, θ) =
we have show some data labeled by the xor function, but we have jittered the points to make
top of each other are dispersed with uniform additive noise.RBF basis in 1d. Left column: ﬁtted function. Middle column: basis functions evaluated on
We can also use the kernelized feature vector inside a linear regression model by deﬁning
uniformly spaced RBF prototypes, but with the bandwidth ranging from small to large. Small
hence the corresponding function is just a straight line.
L1VMs, RVMs, and other sparse vector machines
/ posterior is highly multimodal. Furthermore, these techniques is hard to extend to structured
input spaces, where kernels are most useful.
Another approach is to ﬁnd clusters in the data and then to assign one prototype per clustercenter (many clustering algorithms just need a similarity metric as input). However, the regions
of space that have high density are not necessarily the ones where the prototypes are most
useful for representing the output, that is, clustering is an unsupervised task that may not yield
a representation that is useful for prediction. Furthermore, there is the need to pick the number
of clusters.
A simpler approach is to make each example xi be a prototype, so we get
φ(x) = [κ(x, x1), . . . , κ(x, xN)]
Now we see D = N, so we have as many parameters as data points. However, we can use any
the training exemplars. We call this a sparse vector machine.
this of course will not be sparse.
algorithms, although in practice the most common method is the greedy algorithm in (Tipping
PMTK).
Another very popular approach to creating a sparse kernel machine is to use a support
from a Bayesian point of view. Nevertheless, the effect is similar, as we will see.
binary classiﬁcation problem in 2d. For simplicity, λ was chosen by hand for L2VM and L1VM;
for RVMs, the parameters are estimated using empirical Bayes; and for the SVM, we use CV to
We see that all the methods give similar performance. However, RVM is the sparsest (and hence
fastest at test time), then L1VM, and then SVM. RVM is also the fastest to train, since CV for an
SVM is slow. (This is despite the fact that the RVM code is in Matlab and the SVM code is in
C.) This result is fairly typical.
regression problem. Again, we see that predictions are quite similar, but RVM is the sparsest,
The kernel trick
Rather than deﬁning our feature vector in terms of kernels, φ(x) = [κ(x, x1), . . . , κ(x, xN)],
we can instead work with the original feature vectors x, but modify the algorithm so that it
replaces all inner products of the form ⟨x, x′⟩with a call to the kernel function, κ(x, x′). This
is called the kernel trick. It turns out that many algorithms can be kernelized in this way. We
give some examples below. Note that we require that the kernel be a Mercer kernel for this trick
to work.(a)
(b)
(c)
(d)
Black circles denote the support vectors. Figure generated by kernelBinaryClassifDemo.
Kernelized nearest neighbor classiﬁcation
a test vector to all the training points, ﬁnd the closest one, and look up its label. This can be
kernelized by observing that
This allows us to apply the nearest neighbor classiﬁer to structured data objects.
not always appropriate for structured objects. We now describe how to develop a kernelizedlinregL2
(a)
linregL1
(b)
RVM
(c)
SVM
(d)
Example of kernel based regression on the noisy sinc function using an RBF kernel with
retained training exemplars. Figure generated by kernelRegrDemo.
version of the algorithm.
vectors assigned to this cluster, we make each centroid be one of the data vectors themselves.
Thus we always deal with integer indexes, rather than data objects. We assign objects to their
closest centroids as before. When we update the centroids, we look at each object that belongs
to the cluster, and measure the sum of its distances to all the others in the same cluster; we
then pick the one which has the smallest such sum:
mk = argmin

i′:zi′=k
d(i, i′)weights for linregL2
(a)
weights for linregL1
(b)
weights for RVM
(c)
weights for SVM
(d)
kernelRegrDemo.
where
This takes O(n2
computing the nearest medoid for each class. This is known as nearest medoid classiﬁcation
d(i, i′).
Kernelized ridge regression
how to apply it to parametric models such as ridge regression. However, it can be done, as we
now explain. This will serve as a good “warm up” for studying SVMs.
The primal problem
want to minimize
The optimal solution is given by
w = (XT X + λID)−1XT y = (

i
xixT
i + λID)−1XT y
The dual problem
w = XT (XXT + λIN)−1y
more, we see that we can partially kernelize this, by replacing XXT with the Gram matrix K.
But what about the leading XT term?
Let us deﬁne the following dual variables:
α
≜
(K + λIN)−1y
Then we can rewrite the primal variables as follows
w
=
XT α =
N

αixi
This tells us that the solution vector is just a linear sum of the N training vectors. When we
plug this in at test time to compute the predictive mean, we get
ˆf(x) = wT x =
N

αixT
i x =
N

αiκ(x, xi)Scholkopf.
So we have succesfully kernelized ridge regression by changing from primal to dual variables.
This technique can be applied to many other linear models, such as logistic regression.
Computational cost
variables w is O(D3). Hence the kernel method can be useful in high dimensional settings,
using the dual variables takes O(ND) time, while prediction using the primal variables only
Kernel PCA
data using PCA. This required ﬁnding the eigenvectors of the sample covariance matrix S =N

N
of the inner product matrix XXT , as we show below. This will allow us to produce a nonlinear
First, let U be an orthogonal matrix containing the eigenvectors of XXT with corresponding
(XT X)(XT U) = (XT U)Λ
uT
j XXT uj = λjuT
This is a useful trick for regular PCA if D > N, since XT X has size D × D, whereas XXT
has size N × N. It will also allow us to use the kernel trick, as we now show.
Now let K = XXT be the Gram matrix. Recall from Mercer’s theorem that the use of a kernel
implies some underlying feature space, so we are implicitly replacing xi with φ(xi) = φi. Let
Φ be the corresponding (notional) design matrix, and Sφ =
N

i φiφT
i be the corresponding
where U and Λ contain the eigenvectors and eigenvalues of K. Of course, we can’t actually
compute Vkpca, since φi is potentially inﬁnite dimensional. However, we can compute the
projection of a test vector x∗onto the feature space as follows:
φT
∗Vkpca = φT
where k∗= [κ(x∗, x1), . . . , κ(x∗, xN)].
There is one ﬁnal detail to worry about. So far, we have assumed the projected data has
zero mean, which is not the case in general.
We cannot simply subtract off the mean in
feature space.
However, there is a trick we can use.
Deﬁne the centered feature vector as
N

N
˜Kij
=
˜φ
T
i ˜φj
=
φT
N
N

φT
N
N

φT
N

M

φT
k φl
=
N
N

N
N

N

M

This can be expressed in matrix notation as follows:
˜K
=
HKH
N 1N1T
N. is the centering matrix. We can convert all this algebra into the
components, since the rank of Φ is N ×D∗, where D∗is the (potentially inﬁnite) dimensionality
of embedded feature vectors.pca
(a)
kpca
(b)
2d visualization of some 2d data. (a) PCA projection. (b) Kernel PCA projection. Figure
generated by kpcaDemo2, based on code by L.J.P. van der Maaten.
ﬁrst two component separate the three clusters, and following components split the clusters.
kPCA. Obviously PCA perfectly represents the data. kPCA represents each cluster by a different
line.
Of course, there is no need to project 2d data back into 2d. So let us consider a different
in an oil pipeline. (This data, which is widely used to compare data visualization methods, is(a)
(b)
how to make kernelized versions of probabilistic PCA.
Euclidean distance in the embedding space approximates the original dissimilarity matrix. See
Support vector machines (SVMs)
approach is to change the objective function from negative log likelihood to some other loss
risk function
J(w, λ) =
N

where ˆyi = wT xi + w0. (So far this is in the original feature space; we introduce kernels in a
In the ridge regression case, we know that the solution to this has the form ˆw = (XT X +
we can rewrite these equations in a way that only involves inner products of the form xT x′,
which we can replace by calls to a kernel function, κ(x, x′). This is kernelized, but not sparse.
below, we can ensure that the solution is sparse, so that predictions only depend on a subset
of the training data, known as support vectors. This combination of the kernel trick plus a
modiﬁed loss function is known as a support vector machine or SVM. This technique was 
 
L2
ε−insensitive
huber
(a)
x
y(x)
y −ϵ
y
y + ϵ
(b)
ξi = ξ∗
classiﬁcation as we explain below.
Note that SVMs are very unnatural from a probabilistic point of view. First, they encode
sparsity in the loss function rather than the prior. Second, they encode kernels by using an
algorithmic trick, rather than being an explicit part of the model. Finally, SVMs do not result in
reasons. First, they are very popular and widely used, so all students of machine learning should
know about them. Second, they have some computational advantages over probabilistic methods
SVMs for regression
The problem with kernelized ridge regression is that the solution vector w depends on all the
training inputs. We now seek a method to produce a sparse estimate.
the epsilon insensitive loss function, deﬁned by

if |y −ˆy| < ϵ
|y −ˆy| −ϵ
otherwise
The corresponding objective function is usually written in the following form
J = C
N
convex and unconstrained, but not differentiable, because of the absolute value function in the
algorithms we could use. One popular approach is to formulate the problem as a constrained
optimization problem. In particular, we introduce slack variables to represent the degree to
which each point lies outside the tube:
yi
≤
f(xi) + ϵ + ξ+
i
yi
≥
f(xi) −ϵ −ξ−
i
Given this, we can rewrite the objective as follows:
J = C
N

(ξ+
i + ξ−
This is a quadratic function of w, and must be minimized subject to the linear constraints
i
i
ˆw =

i
αixi
Once the model is trained, we can then make predictions using
ˆy(x) = ˆw0 + ˆwT x
Plugging in the deﬁnition of ˆw we get
ˆy(x) = ˆw0 +

i
αixT
i x
Finally, we can replace xT
i x with κ(xi, x) to get a kernelized solution:
ˆy(x) = ˆw0 +

i
αiκ(xi, x)
SVMs for classiﬁcation
We now discuss how to apply SVMs to classiﬁcation. We ﬁrst focus on the binary case, and
Hinge lossthis section, we replace the NLL loss with the hinge loss, deﬁned as
hinge, hence its name. The overall objective has the form
min
w,w0
N

variables ξi, one can show that this is equivalent to solving
min
w,w0,ξ
N

ξi
s.t.
We
can eliminate the primal variables w, w0 and ξi, and just solve the N dual variables, which
However, specialized algorithms, which avoid the use of generic QP solvers, have been developed
One can show that the solution has the form
ˆw =

i
αixi
called support vectors; these are points which are either incorrectly classiﬁed, or are classiﬁed
illustration.
At test time, prediction is done using
ˆy(x) = sgn(f(x)) = sgn

ˆw0 + ˆwT x

ˆy(x) = sgn


ˆw0 +
N

αiκ(xi, x)

This
depends on the sparsity level, and hence on the regularizer C.R1
R0
w
x
r = f(x)
∥w∥
x⊥
−w0
∥w∥
(a)
(b)
(a) Illustration of the geometry of a linear decision boundary in 2d. A point x is classiﬁed
w is a vector which is perpendicular to the decision boundary. The term w0 controls the distance of
the decision boundary from the origin. The signed distance of x from its orthogonal projection onto the
the soft margin principle. Points with circles around them are support vectors. We also indicate the valueThe large margin principle
goal is to derive a discriminant function f(x) which will be linear in the feature space implied
we see that
x = x⊥+ r w
||w||
where r is the distance of x from the decision boundary whose normal vector is w, and x⊥is
the orthogonal projection of x onto this boundary. Hence
f(x)
=
wT x + w0 = (wT x⊥+ w0) + rwT w
||w||
√
wT w, and r = f(x)
||w||.
We would like to make this distance r = f(x)/||w|| as large as possible, for reasons illustrated
(especially if we work in a high dimensional feature space), but intuitively, the best one to pick
is the one that maximizes the margin, i.e., the perpendicular distance to the closest point. In
addition, we want to ensure each point is on the correct side of the boundary, hence we want
max
w,w0
N
min
yi(wT xi + w0)
||w||
distance of any point to the boundary, since the k factor cancels out when we divide by ||w||.
decision boundary. We therefore want to optimize
min
w,w0
s.t.
(The fact of
The
constraint says that we want all points to be on the correct side of the decision boundary with
classiﬁer.
If the data is not linearly separable (even after using the kernel trick), there will be no feasible
min
w,w0,ξ
N

ξi
s.t.Correct log−odds
RVM y(x)
SVM y(x)
permission of Mike Tipping.
interpret 
i ξi as an upper bound on the number of misclassiﬁed points.
The parameter C is a regularization parameter that controls the number of errors we are
Probabilistic output
measure of conﬁdence in our prediction. One heuristic approach is to interpret f(x) as the
where a, b can be estimated by maximum likelihood on a separate validation set. (Using the
training set to estimate a and b leads to severe overﬁtting.) This technique was ﬁrst proposed in
However, the resulting probabilities are not particularly well calibrated, since there is nothing?
R1
R2
R3
C1
Not C1
C2
Not C2
(a)
?
R1
R2
R3
C1
C3
C2
C3
C1
C2
(b)
(Bishop 2006a).
class case, by replacing the sigmoid function with the softmax, and the Bernoulli distribution
are not on a calibrated scale and hence are hard to compare to each other.
which we train C binary classiﬁers, fc(x), where the data from class c is treated as positive,
and the data from all the other classes is treated as negative. However, this can result in regions
A common alternative is to pick ˆy(x) = arg maxc fc(x).
However, this technique may
not work either, since there is no guarantee that the different fc functions have comparable
magnitudes. In addition, each binary subproblem is likely to suffer from the class imbalance
the class which has the highest number of votes. However, this can also result in ambiguities,
It is worth remembering that all of these difficulties, and the plethora of heuristics that have
been proposed to ﬁx them, fundamentally arise because SVMs do not model uncertainty using
probabilities, so their output scores are not comparable across classes.C
γ
cv error
(a)
C
cv error
(b)
optimal error, computed using Bayes rule applied to the model used to generate the data. Based on Figure
Choosing C
SVMs for both classiﬁcation and regression require that you specify the kernel function and the
strongly with the kernel parameters. For example, suppose we are using an RBF kernel with
precision γ =
the data ﬁrst, for a spherical Gaussian kernel to make sense.
To choose C efficiently, one can develop a path following algorithm in the spirit of lars
Summary of key points
dients: the kernel trick, sparsity, and the large margin principle. The kernel trick is necessary
to prevent underﬁtting, i.e., to ensure that the feature vector is sufficiently rich that a linear
as implicitly deﬁning a potentially high dimensional feature vector.) If the original features are
ﬁces to use a linear kernel, κ(x, x′) = xT x′, which is equivalent to working with the original
features.Method
Opt. w
Opt. kernel
Sparse
Prob.
Multiclass
Section
L2VM
Convex
EB
No
Yes
Yes
Yes
L1VM
Convex
CV
Yes
Yes
Yes
Yes
RVM
Not convex
EB
Yes
Yes
Yes
Yes
SVM
Convex
CV
Yes
No
Indirectly
No
GP
EB
No
Yes
Yes
No
Comparison of various kernel based classiﬁers. EB = empirical Bayes, CV = cross validation.
See text for details.
The sparsity and large margin principles are necessary to prevent overﬁtting, i.e., to ensure
that we do not use all the basis functions. These two ideas are closely related to each other,
and both arise (in this case) from the use of the hinge loss function. However, there are other
methods of achieving sparsity (such as ℓ1), and also other methods of maximizing the margin
(such as boosting). A deeper discussion of this point takes us outside of the scope of this book.
A probabilistic interpretation of SVMs
use kernels. However, all of these approaches use a logistic or probit likelihood, as opposed to
the hinge loss used by SVMs. It is natural to wonder if one can interpret the SVM more directly
as a probabilistic model. To do so, we must interpret Cg(m) as a negative log likelihood, where
exp(−Cg(f)) + exp(−Cg(−f)) be a constant independent of f. But it turns out this is not
In particular, one can show that
i w)+) =

 ∞
√2πλi
exp

λi
	
Thus the exponential of the negative hinge loss can be represented as a Gaussian scale mixture.
interpretation of SVMs.)
Comparison of discriminative kernel methods
We have mentioned several different methods for classiﬁcation and regression based on kernels,
(GP stands for “Gaussian process”, which we discuss in•
Optimize w: a key question is whether the objective J(w) = −log p(D|w) −log p(w)
is convex or not. L2VM, L1VM and SVMs have convex objectives. RVMs do not. GPs are
Bayesian methods that do not perform parameter estimation.
•
Optimize kernel: all the methods require that one “tune” the kernel parameters, such as the
bandwidth of the RBF kernel, as well as the level of regularization. For methods based on
Gaussians, including L2VM, RVMs and GPs, we can use efficient gradient based optimizers to
maximize the marginal likelihood. For SVMs, and L1VM, we must use cross validation, which
•
Sparse: L1VM, RVMs and SVMs are sparse kernel methods, in that they only use a subset of
the training examples. GPs and L2VM are not sparse: they use all the training examples. The
principle advantage of sparsity is that prediction at test time is usually faster. In addition,
one can sometimes get improved accuracy.
•
Probabilistic: All the methods except for SVMs produce probabilistic output of the form
p(y|x). SVMs produce a “conﬁdence” value that can be converted to a probability, but such
•
Multiclass: All the methods except for SVMs naturally work in the multiclass setting, by using
a multinoulli output instead of Bernoulli. The SVM can be made into a multiclass classiﬁer,
•
Mercer kernel: SVMs and GPs require that the kernel is positive deﬁnite; the other techniques
do not.
Apart from these differences, there is the natural question: which method works best? In
a small experiment3, we found that all of these methods had similar accuracy when averaged
over a range of problems, provided they have the same kernel, and provided the regularization
constants are chosen appropriately.
Given that the statistical performance is roughly the same, what about the computational
SVMs also take
L1VM should be faster than an RVM, since an RVM requires multiple rounds of ℓ1 minimization
which is faster than ℓ1 minimization. This is reﬂected in our empirical results.
probabilistic output matters (e.g., for active learning or control problems), use a GP. The only
circumstances under which using an SVM seems sensible is the structured output case, where
(We attribute the enormous popularity of SVMs not
to their superiority, but to ignorance of the alternatives, and also to the lack of high quality
software implementing the alternatives.)
including SVMs and various non kernel methods. 
 
Boxcar
Epanechnikov
Tricube
Gaussian
A comparison of some popular smoothing kernels. The boxcar kernel has compact support
but is not smooth. The Epanechnikov kernel has compact support but is not differentiable at its boundary.
Figure generated by smoothingKernelPlot.
Kernels for building generative models
There is a different kind of kernel known as a smoothing kernel which can be used to create
as well as for creating generative models for classiﬁcation and regression by making models of
the form p(y, x).
Smoothing kernels
A smoothing kernel is a function of one argument which satisﬁes the following properties:






A simple example is the Gaussian kernel,
κ(x) ≜
(2π)
We can control the width of the kernel by introducing a bandwidth parameter h:
hκ(x
h)
We can generalize to vector valued inputs by deﬁning an RBF kernel:
κh(x) = κh(||x||)
In the case of the Gaussian kernel, this becomes
κh(x) =
D

2h2 x2
j)Although Gaussian kernels are popular, they have unbounded support. An alternative kernel,
with compact support, is the Epanechnikov kernel, deﬁned by
κ(x)
≜
can use fast nearest neighbor methods to evaluate the density.
Unfortunately, the Epanechnikov kernel is not differentiable at the boundary of its support.
κ(x)
≜
This has compact support and has two continuous derivatives at the boundary of its support.
The boxcar kernel is simply the uniform distribution:
κ(x)
≜
We will use this kernel below.
Kernel density estimation (KDE)
this case, the model becomes
N
N

N(x|xi, σ2I)
We can generalize the approach by writing
N
N

κh (x −xi)
This is called a Parzen window density estimator, or kernel density estimator (KDE), and
there is no need to pick K. The disadvantage is that the model takes a lot of memory to store,
and a lot of time to evaluate. It is also of no use for clustering tasks.
we just count how many data points land within an interval of size h around xi. On the bottom,
we use a Gaussian kernel, which results in a smoother ﬁt.
The usual way to pick h is to minimize an estimate (such as cross validation) of the frequentist(a)
(b)
(c)
(d)
generated by parzenWindowDemo2.
to infer h. DP mixtures can also be more efficient than KDE, since they do not need to store
From KDE to KNN
We can use KDE to deﬁne the class conditional densities in a generative classiﬁer. This turns
out to provide an alternative derivation of the nearest neighbors classiﬁer, which we introduced
To show this, we follow the presentation of (Bishop 2006a, p125).
In kde
with a boxcar kernel, we ﬁxed the bandwidth and count how many data points fall within theGaussian kernel regression
 
 
true
data
estimate
An example of kernel regression in 1d using a Gaussian kernel.
Figure generated by
kernelRegressionDemo, based on code by Yi Cao.
allow the bandwidth or volume to be different for each data point. Speciﬁcally, we will “grow”
a volume around x until we encounter K data points, regardless of their class label. Let the
resulting volume have size V (x) (this was previously hD), and let there be Nc(x) examples
from class c in this volume. Then we can estimate the class conditional density as follows:
p(x|y = c, D) =
Nc(x)
NcV (x)
where Nc is the total number of examples in class c in the whole data set. The class prior can
be estimated by
p(y = c|D) = Nc
N
Hence the class posterior is given by
p(y = c|x, D)
=
Nc(x)
NcV (x)
Nc
N

c′
Nc′(x)
Nc′V (x)
Nc′
N
=
Nc(x)

c′ Nc′(x) = Nc(x)
K
where we used the fact that 
c Nc(x) = K, since we choose a total of K points (regardless of
c).
Kernel regression
learning. We can also use KDE for regression. The goal is to compute the conditional expectation
f(x) = E [y|x] =


y p(y|x)dy =
)
y p(x, y)dy
)
p(x, y)dyWe can use KDE to approximate the joint density p(x, y) as follows:
N
N

κh(x −xi)κh(y −yi)
Hence
f(x)
=
N

N
)
yκh(y −yi)dy
N

N
)
κh(y −yi)dy
=

N

N
To derive this result, we used two properties of smoothing kernels. First, that they integrate to
one, i.e.,
)
)
yκh(y −yi)dy = yi. This follows by
deﬁning x = y −yi and using the zero mean property of smoothing kernels:


(x + yi)κh(x)dx =


xκh(x)dx + yi


We can rewrite the above result as follows:
f(x)
=
N

wi(x)yi
wi(x)
≜
κh(x −xi)

N
We see that the prediction is just a weighted sum of the outputs at the training points, where
the weights depend on how similar x is to the stored training points. This method is called
example, where we use a Gaussian kernel.
Note that this method only has one free parameter, namely h. One can show (Bowman and
the optimal bandwidth h is given by
h =
3N
ˆσ
We can compute a robust approximation to the standard deviation by ﬁrst computing the mean
absolute deviation
MAD = median(|x −median(x)|)
and then using

hxhy.Although these heuristics seem to work well, their derivation rests on some rather dubious
assumptions (such as Gaussianity of the true density). Furthermore, these heuristics are limited
which can handle many tuning parameters, and which is based on much more transparent
principles (maximizing the marginal likelihood).
Locally weighted regression
If we deﬁne κh(x −xi) = κ(x, xi), we can rewrite the prediction made by kernel regression as
follows
ˆf(x∗) =
N

yi
κ(x∗, xi)

N
Note that κ(x, xi) need not be a smoothing kernel.
If it is not, we no longer need the
normalization term, so we can just write
ˆf(x∗) =
N

yiκ(x∗, xi)
This model is essentially ﬁtting a constant function locally. We can improve on this by ﬁtting a
linear regression model for each point x∗by solving
min
β(x∗)
N

We can compute the paramters β(x∗) for each test case by solving the following weighted
least squares problem:
β(x∗) = (ΦT D(x∗)Φ)−1ΦT D(x∗)y
prediction has the form
ˆf(x∗) = φ(x∗)T β(x∗) = (ΦT D(x∗)Φ)−1ΦT D(x∗)y =
N

wi(x∗)yi
The term wi(x∗), which combines the local smoothing kernel with the effect of linear regression,
Exercises
√
√
2x, x2]T . (This is equivalent tousing a second order polynomial kernel.) The max margin classiﬁer has the form
s.t.
version) that w is perpendicular to the decision boundary between the two points in the 3d feature
space.
b. What is the value of the margin that is achieved by this w? Hint: recall that the margin is the distance
space, with a line separating one from the other.
decision boundary, so the inequalities will be tight.
e. Write down the form of the discriminant function f(x) = w0 + wT φ(x) as an explicit function of x.
decision boundary guaranteed to separate the classes?Gaussian processes
Introduction
In supervised learning, we observe some inputs xi and some outputs yi.
We assume that
yi = f(xi), for some unknown function f, possibly corrupted by noise. The optimal approach
is to infer a distribution over functions given the data, p(f|X, y), and then to use this to make
predictions given new inputs, i.e., to compute
p(y∗|x∗, X, y) =


p(y∗|f, x∗)p(f|X, y)df
Up until now, we have focussed on parametric representations for the function f, so that
instead of inferring p(f|D), we infer p(θ|D). In this chapter, we discuss a way to perform
Bayesian inference over functions themselves.
Our approach will be based on Gaussian processes or GPs.
A GP deﬁnes a prior over
functions, which can be converted into a posterior over functions once we have seen some data.
Although it might seem difficult to represent a distribution over a function, it turns out that we
only need to be able to deﬁne a distribution over the function’s values at a ﬁnite, but arbitrary,
set of points, say x1, . . . , xN. A GP assumes that p(f(x1), . . . , f(xN)) is jointly Gaussian, with
deemed by the kernel to be similar, then we expect the output of the function at those points
It turns out that, in the regression setting, all these computations can be done in closed form,
we must use approximations, such as the Gaussian approximation, since the posterior is no
longer exactly Gaussian.
Having properly tuned probabilistic output is important in certain applications, such as onlinex1
x2
x⋆
f ⋆
y1
y2
y⋆
i p(yi|fi). The hidden nodes
fi = f(xi) represent the value of the function at each of the data points. These hidden nodes are fully
interconnected by undirected edges, forming a Gaussian graphical model; the edge strengths represent the
covariance terms Σij = κ(xi, xj). If the test point x∗is similar to the training points x1 and x2, then
the predicted output y∗will be similar to y1 and y2.
known as kriging, which is widely used in the spatial statistics literature.
GPs for regression
In this section, we discuss GPs for regression. Let the prior on the regression function be a GP,
denoted by
f(x) ∼GP(m(x), κ(x, x′))
where m(x) is the mean function and κ(x, x′) is the kernel or covariance function, i.e.,
m(x)
=
E [f(x)]
κ(x, x′)
=
E

(f(x) −m(x))(f(x′) −m(x′))T  
We obviously require that κ() be a positive deﬁnite kernel. For any ﬁnite set of points, this
process deﬁnes a joint Gaussian:
p(f|X) = N(f|μ, K)
consider parametric models for the mean function, so the GP just has to model the residual(a)
(b)
Left: some functions sampled from a GP prior with SE kernel. Right: some samples from a GP
observation of the function evaluated at xi. Given a test set X∗of size N∗× D, we want to
predict the function outputs f∗.
If we ask the GP to predict f(x) for a value of x that it has already seen, we want the GP to
return the answer f(x) with no uncertainty. In other words, it should act as an interpolator
of the training data. This will only happen if we assume the observations are noiseless. We will
consider the case of noisy observations below.
Now we return to the prediction problem. By deﬁnition of the GP, the joint distribution has
the following form
 f
f∗
	
∼N

μ
μ∗
	
,
 K
K∗
KT
∗
K∗∗
		
where K = κ(X, X) is N ×N, K∗= κ(X, X∗) is N ×N∗, and K∗∗= κ(X∗, X∗) is N∗×N∗.
form
p(f∗|X∗, X, f)
=
N(f∗|μ∗, Σ∗)
μ∗
=
μ(X∗) + KT
Σ∗
=
K∗∗−KT
∗K−1K∗
p(f|X), where we use a squared exponential kernel, aka Gaussian kernel or RBF kernel. In
1d, this is given by
κ(x, x′) = σ2
Here ℓcontrols the horizontal length scale over which the function varies, and σ2
f controls the
vertical variation. (We discuss how to estimate such kernel parameters below.) On the right weshow samples from the posterior, p(f∗|X∗, X, f). We see that the model perfectly interpolates
the training data, and that the predictive uncertainty increases as we move further away from
the observed data.
behavior of a complex simulator, such as a weather forecasting program. (If the simulator is
stochastic, we can deﬁne f to be its mean output; note that there is still no observation noise.)
One can then estimate the effect of changing simulator parameters by examining their effect
on the GP’s predictions, rather than having to run the simulator many times, which may be
prohibitively slow. This strategy is known as DACE, which stands for design and analysis of
Predictions using noisy observations
Now let us consider the case where what we observe is a noisy version of the underlying
y).
In this case, the model is not required to
interpolate the data, but it must come “close” to the observed data. The covariance of the
observed noisy responses is
cov [yp, yq] = κ(xp, xq) + σ2
cov [y|X] = K + σ2
yIN ≜Ky
The second matrix is diagonal because we assumed the noise terms were independently added
to each observation.
is given by

y
f∗
	
∼N

Ky
K∗
KT
∗
K∗∗
		
where we are assuming the mean is zero, for notational simplicity. Hence the posterior predictive
density is
p(f∗|X∗, X, y)
=
N(f∗|μ∗, Σ∗)
μ∗
=
KT
y y
Σ∗
=
K∗∗−KT
y K∗
In the case of a single test input, this simpliﬁes as follows
p(f∗|x∗, X, y)
=
N(f∗|kT
y y, k∗∗−kT
y k∗)
where k∗= [κ(x∗, x1), . . . , κ(x∗, xN)] and k∗∗= κ(x∗, x∗).
Another way to write the
posterior mean is as follows:
f ∗= kT
y y =
N

αiκ(xi, x∗)
y y. We will revisit this expression later.(a)
(b)
(c)
by gprDemoChangeHparams, written by Carl Rasmussen.
Effect of the kernel parameters
The predictive performance of GPs depends exclusively on the suitability of the chosen kernel.
κy(xp, xq) = σ2
Here ℓis the horizontal scale over which the function changes, σ2
f controls the vertical scale of
the function, and σ2
we discuss below); now the function looks more “wiggly”. Also, the uncertainty goes up faster,input x1
input x2
output y
(a)
input x1
input x2
output y
(b)
input x1
input x2
output y
(c)
written by Carl Rasmussen.
We can extend the SE kernel to multiple dimensions as follows:
κy(xp, xq) = σ2
We can deﬁne the matrix M in several ways.
The simplest is to use an isotropic matrix,
direction than the x2 direction.
matrix as a low rank matrix plus a diagonal matrix. The columns of Λ correspond to relevantEstimating the kernel parameters
To estimate the kernel parameters, we could use exhaustive search over a discrete grid of values,
with validation loss as an objective, but this can be quite slow. (This is the approach used to
tune kernels used by SVMs.) Here we consider an empirical Bayes approach, which will allow us
to use continuous optimization methods, which are much faster. In particular, we will maximize
the marginal likelihood1
p(y|X) =


p(y|f, X)p(f|X)df
i N(yi|fi, σ2
y), the marginal likelihood is given by
The ﬁrst term is a data ﬁt term, the second term is a model complexity term, and the third term
is just a constant. To understand the tradeoff between the ﬁrst two terms, consider a SE kernel
in 1D, as we vary the length scale ℓand hold σ2
y ﬁxed. Let J(ℓ) = −log p(y|X, ℓ). For short
y y will be small. However, the model complexity
be considered “near” any others, so the log |Ky| will be large. For long length scales, the ﬁt will
right), so log |Ky| will be small.
We now discuss how to maximize the marginal likelhiood. Let the kernel parameters (also
∂
∂θj
log p(y|X)
=
y
∂Ky
∂θj
y
∂Ky
∂θj
)
=
2tr

y )∂Ky
∂θj
	
parameter to compute the gradient.
The form of ∂Ky
∂θj
depends on the form of the kernel, and which parameter we are taking
y), and then use the chain rule.
Given an expression for the log marginal likelihood and its derivative, we can estimate the
not convex, local minima can be a problem, as we illustrate below.
Example
y)
y. The two
latent Gaussian vector f. This moves us up one level of the Bayesian hierarchy, and reduces the chances of overﬁtting
(the number of kernel parameters is usually fairly small compared to a standard parametric model).characteristic lengthscale
noise standard deviation
(a)
input, x
output, y
(b)
input, x
output, y
(c)
Illustration of local minima in the marginal likelihood surface. (a) We plot the log marginal
likelihood vs σ2
y and ℓ, for ﬁxed σ2
corresponding to the lower left local minimum, (ℓ, σ2
noise. (c) The function corresponding to the top right local minimum, (ℓ, σ2
smooth and has high noise. The data was generated using (ℓ, σ2
Figure generated by gprDemoMarglik, written by Carl Rasmussen.
evidence to conﬁdently decide which is more reasonable, although the more complex model
With more data, the MAP estimate should come to dominate.
(top of panel a) corresponds to the case where the noise is very high; in this regime, the marginal
likelihood is insensitive to the length scale (indicated by the horizontal contours), since all the
the case where the length scale is very short; in this regime, the marginal likelihood is insensitive
to the noise level, since the data is perfectly interpolated. Neither of these regions would be
chosen by a good optimizer.log(length−scale)
log(magnitude)
z1
z2
(a)
log(length−scale)
log(magnitude)
(b)
log(length−scale)
log(magnitude)
(c)
Carlo, and central composite design.
Used with kind permission
of Jarno Vanhatalo.
we can compute a discrete grid of possible values, centered on the MAP estimate ˆθ (computed
as above). We can then approximate the posterior over the latent variables using
p(f|D) ∝
S

p(f|D, θs)p(θs|D)δs
In higher dimensions, a regular grid suffers from the curse of dimensionality. An obvious
Monte Carlo, whereby we place grid points at the mode, and at a distance ±1sd from the modeMultiple kernel learning
A quite different approach to optimizing kernel parameters known as multiple kernel learning.
The idea is to deﬁne the kernel as a weighted sum of base kernels, κ(x, x′) = 
j wjκj(x, x′),
and then to optimize the weights wj instead of the kernel parameters themselves.
This is
particularly useful if we have different kinds of data which we wish to fuse together.
See
Computational and numerical issues *
The predictive mean is given by f∗= kT
y y. For reasons of numerical stability, it is unwise
to directly invert Ky.
A more robust alternative is to compute a Cholesky decomposition,
Ky = LLT . We can then compute the predictive mean and variance, and the log marginal
y y = L−T L−1y. We can then compute the mean using kT
∗α in O(N) time
and the variance using k∗∗−kT
An alternative to Cholesky decomposition is to solve the linear system Kyα = y using
Another approach is to
discussion of other speedup techniques.
yI);
∗α ;
2yT α −
i log Lii −N
Sometimes it is useful to use a linear model for the mean of the process, as follows:
f(x) = βT φ(x) + r(x)
If we assume β ∼N(b, B), we can integrate these parameters out to get a new GP (O’Hagan
f(x) ∼GP

φ(x)T b, κ(x, x′) + φ(x)T Bφ(x′)
log p(yi|fi)
∂
∂fi log p(yi|fi)
i log p(yi|fi)
log sigm(yifi)
ti −πi
log Φ(yifi)
yiφ(fi)
Φ(yifi)
−
φ2
i
Φ(yifi)
p43).
Integrating out β, the corresponding predictive distribution for test inputs X∗has the following
p(f∗|X∗, X, y)
=
N(f∗, cov [f∗])
f∗
=
ΦT
∗β + KT
y (y −Φβ)
β
=
y y + B−1b)
cov [f∗]
=
K∗∗−KT
y ΦT )−1R
R
=
y Φ∗
The predictive mean is the output of the linear model plus a correction term due to the GP, and
the predictive covariance is the usual GP covariance plus an extra term due to the uncertainty
in β.
GPs meet GLMs
In this section, we extend GPs to the GLM setting, focussing on the classiﬁcation case. As with
Bayesian logistic regression, the main difficulty is that the Gaussian prior is not conjugate to
is the simplest and fastest.
Binary classiﬁcation
In the binary case, we deﬁne the model as p(yi|xi) = σ(yif(xi)), where, following (Rasmussen
Computing the posterior
Deﬁne the log of the unnormalized posterior as follows:Let J(f) ≜−ℓ(f) be the function we want to minimize. The gradient and Hessian of this are
given by
g
=
−∇log p(y|f) + K−1f
H
=
Note that W ≜−∇∇log p(y|f) is a diagonal matrix because the data are iid (conditional on
f). Expressions for the gradient and Hessian of the log likelihood for the logit and probit case
We can use IRLS to ﬁnd the MAP estimate. The update has the form
f new
=
=
At convergence, the Gaussian approximation of the posterior takes the following form:
Computing the posterior predictive
We now compute the posterior predictive. First we predict the latent function at the test case
x∗. For the mean we have
E [f∗|x∗, X, y]
=


E [f∗|f, x∗, X, y] p(f|X, y)df
=


kT
∗K−1f p(f|X, y)df
=
kT
∗K−1E [f|X, y] ≈kT
∗K−1ˆf
To compute the predictive variance, we use the rule of iterated variance:
var [f∗]
=
E [var [f∗|f]] + var [E [f∗|f]]
E [var [f∗|f]] = E

k∗∗−kT
∗K−1k∗
 
= k∗∗−kT
∗K−1k∗
var [E [f∗|f]] = var

k∗K−1f
 
= kT
∗K−1cov [f] K−1k∗
Combining these we get
var [f∗]
=
k∗∗−kT
get
var [f∗]
≈
k∗∗−kT
∗K−1k∗+ kT
=
k∗∗−kTSo in summary we have
p(f∗|x∗, X, y) = N(E [f∗] , var [f∗])
To convert this in to a predictive distribution for binary responses, we use


σ(f∗)p(f∗|x∗, X, y)df∗
discussed Bayesian logistic regression.
Computing the marginal likelihood
We need the marginal likelihood in order to optimize the kernel parameters. Using the Laplace
Hence
log p(y|X)
≈
Computing the derivatives ∂log p(y|X,θ)
∂θj
is more complex than in the regression case, since ˆf
p125).
Numerically stable computation *
To implement the above equations in a numerically stable way, it is best to avoid inverting K
B = IN + W
One can use the matrix inversion lemma to show
Hence the IRLS update becomes
f new
=
b
=
K(I −W
=
K (b −W
a2aTˆf −

i
log Lii
where we exploited the fact that
We now compute the predictive distribution. Rather than using E [f∗] = kT
∗K−1ˆf, we exploit
E [f∗] = kT
∗∇log p(y|ˆf)
To compute the predictive variance, we exploit the fact that
to get
var [f∗] = k∗∗−kT
∗W
where v = L \ (W
of test cases.
Example
hence the very sharp turns in the decision boundary. On the right, we show the predictions
data.
In this section, we consider a model of the form p(yi|xi) = Cat(yi|S(fi)), where fi =
which are a priori independent, and which may use different kernels. As before, we will use
a Gaussian approximation to the posterior. (A similar model, but using the multinomial probitW = −∇∇log p(y|f) ;
B = IN + W
L = cholesky(B) ;
b = Wf + ∇log p(y|f) ;
a = b −W
f = Ka;
2aT f −
i log Lii;
∗∇log p(y|f);
)
sigm(z)N(z|E [f∗] , var [f∗])dz;
(a)
(b)
Contours of the posterior predictive probability for the red circle class generated by a GP with
parameters, short length scale. (b) Learned parameters, long length scale. Figure generated by gpcDemo2d,
based on code by Carl Rasmussen.Computing the posterior
The unnormalized log posterior is given by
2f T K−1f + yT f −
N

log

 C

exp fic

log 2π
where
f = (f11, . . . , fN1, f12, . . . , fN2, · · · , f1C, . . . , fNC)T
and y is a dummy encoding of the yi’s which has the same layout as f. Also, K is a block
diagonal matrix containing Kc, where Kc = [κc(xi, xj)] models the correlation of the c’th
latent function.
The gradient and Hessian are given by
∇ℓ
=
−K−1f + y −π
∇∇ℓ
=
where W ≜diag(π) −ΠΠT , where Π is a CN × N matrix obtained by stacking diag(π:c)
We can use IRLS to compute the mode. The Newton step has the form
Computing the posterior predictive
mean of the latent response we have
c ˆfc = kc(x∗)T (yc −ˆπc)
We can put this in vector form by writing
E [f∗] = Q∗T(y −ˆπ)
where
Q∗=
⎛
⎜
⎝
k1(x∗)
. . .
...
. . .
kC(x∗)
⎞
⎟
⎠
response is given by
cov [f∗]
=
Σ + QT
=
diag(k(x∗, x∗)) −QTwhere Σ is a C × C diagonal matrix with Σcc = κc(x∗, x∗) −kT
c kc(x∗), and
k(x∗, x∗) = [κc(x∗, x∗)].
To compute the posterior predictive for the visible response, we need to use
p(y|x∗, X, y) ≈


Cat(y|S(f∗))N(f∗|E [f∗] , cov [f∗])df∗
Computing the marginal likelihood
Using arguments similar to the binary case, we can show that
ˆf T K−1ˆf + yTˆf −
N

log

 C

exp ˆfic

This can be optimized numerically in the usual way.
Numerical and computational issues
GPs for Poisson regression
In this section, we illustrate GPs for Poisson regression. An interesting application of this is to
the relative risk of heart attack in different regions in Finland. The data consists of the heart
the following form:
yi ∼Poi(eiri)
where ei is the known expected number of deaths (related to the population of cell i and the
overall death rate), and ri is the relative risk of cell i which we want to infer.
Since the
data counts are small, we regularize the problem by sharing information with spatial neighbors.
length scale and magnitude that are estimated from data.
right, the posterior variance. We see that the RR is higher in Eastern Finland, which is consistent
with other studies. We also see that the variance in the North is higher, since there are fewer
people living there.Posterior mean of the relative risk, FIC
 
 
(a)
Posterior variance of the relative risk, FIC
 
 
(b)
We show the relative risk of heart disease in Finland using a Poisson GP. Left: posterior mean.
Right: posterior variance. Figure generated by gpSpatialDemoLaplace, written by Jarno Vanhatalo.
Connection with other methods
There are variety of other methods in statistics and machine learning that are closely related to
Linear models compared to GPs
p(f∗|x∗, X, y)
=
N(μ, σ2)
μ
=
σ2y
xT
∗A−1XT y
σ2
=
xT
∗A−1x∗
follows
μ
=
xT
∗ΣXT (K + σ2
yI)−1y
σ2
=
xT
∗Σx∗−xT
∗ΣXT (K + σ2I)−1XΣx∗
where we have deﬁned K = XΣXT , which is of size N × N. Since the features only ever
appear in the form XΣXT , xT
∗ΣXT or xT
∗Σx∗, we can kernelize the above expression by
deﬁning κ(x, x′) = xT Σx′.
Thus we see that Bayesian linear regression is equivalent to a GP with covariance function
κ(x, x′) = xT Σx′. Note, however, that this is a degenerate covariance function, since it has at
a limited number of functions. This can result in underﬁtting, since the model is not ﬂexible
enough to capture the data. What is perhaps worse, it can result in overconﬁdence, since themodel’s prior is so impoverished that its posterior will become too concentrated. So not only is
the model wrong, it think it’s right!
Linear smoothers compared to GPs
A linear smoother is a regression function which is a linear function of the training outputs:
ˆf(x∗) =

i
wi(x∗) yi
model, where the output is a linear function of the input vector.)
that GP regession is a linear smoother, note that the mean of the posterior predictive distribution
of a GP is given by
f(x∗) = kT
∗(K + σ2
yIN)−1y =
N

yiwi(x∗)
where wi(x∗) = [(K + σ2
yIN)−1k∗]i.
In kernel regression, we derive the weight function from a smoothing kernel rather than a
Mercer kernel, so it is clear that the weight function will then have local support. In the case
of a GP, things are not as clear, since the weight function depends on the inverse of K. For
certain GP kernel functions, we can analytically derive the form of wi(x); this is known as the
N
the yi’s. More interestingly, wi(x∗) is a local function, even if the original kernel used by the GP
is not local. Futhermore the effective bandwidth of the equivalent kernel of a GP automatically
decreases as the sample size N increases, whereas in kernel smoothing, the bandwidth h needs
details.
Degrees of freedom of linear smoothers
It is clear why this method is called “linear”, but why is it called a “smoother”? This is best
explained in terms of GPs. Consider the prediction on the training set:
f = K(K + σ2
y)−1y
Now let K have the eigendecomposition K = 
N
i . Since K is real and symmetric
orthonormal. Now let y = 
N
i y. Then we can rewrite the above
equation as follows:
f =
N

λi + σ2y
uimatrix K instead of the data matrix X. In any case, the interpretation is similar: if
λi
frequency components in y are smoothed out. The effective degrees of freedom of the linear
smoother is deﬁned as
dof ≜tr(K(K + σ2
N

λi
λi + σ2y
This speciﬁes how “wiggly” the curve is.
SVMs compared to GPs
N

i αixi,
i,j αiαjxT
the SVM objective can be rewritten as
2f T f + C
N

Compare this to MAP estimation for GP classiﬁer:
2f T f −
N

log p(yi|fi)
It is tempting to think that we can “convert” an SVM into a GP by ﬁguring out what likelihood
would be equivalent to the hinge loss. However, it turns out there is no such likelihood (Sollich
to derive sparse kernel machines. We discuss the connection between these methods and GPs
below.
L1VM and RVMs compared to GPs
Sparse kernel machines are just linear models with basis function expansion of the form φ(x) =
following kernel:
κ(x, x′) =
D

αj
φj(x)φj(x′)j )). This kernel function has two interesting properties. First, it
will be highly constrained. Second, the kernel depends on the training data. This can cause the
model to be overconﬁdent when extrapolating beyond the training data. To see this, consider
a point x∗far outside the convex hull of the data. All the basis functions will have values
the predictive variance increases as we move away from the training data, as desired.
See
Neural networks compared to GPs
In the binary classiﬁcation case, a neural network is deﬁned by a logistic regression model
applied to a logistic regression model:
p(y|x, θ) = Ber

y|sigm

wT sigm(Vx)

It turns out there is an interesting connection between neural networks and Gaussian processes,
Consider a neural network for regression with one hidden layer. This has the form
p(y|x, θ) = N(y|f(x; θ), σ2)
where
f(x) = b +
H

vjg(x; uj)
where b is the offset of bias term, vj is the output weight from hidden unit j to the response
y, uj are the inputs weights to unit j from the input x, and g() is the hidden unit activation
function. This is typically the sigmoid or tanh function, but can be any smooth function.
b) v ∼'
w),
u ∼'
=
=
σ2
b +

j
σ2
vEv [g(x; uj)g(x′; uj)]
=
σ2
b + Hσ2
vEu [g(x; u)g(x′; u)]
where the last equality follows since the H hidden units are iid. If we let σ2
(since more hidden units will increase the input to the ﬁnal node, so we should scale down
the magnitude of the weights), then the last term becomes ω2Eu [g(x; u)g(x′; u)]. This is a
sum over H iid random variables. Assuming that g is bounded, we can apply the central limitinput, x
input, xʼ
(a)
input, x
output, f(x)
(b)
this kernel, using various values of σ. Figure generated by gpnnDemo, written by Chris Williams.
If we use as activation / transfer function g(x; u) = erf(u0 + 
D
) z
kernel has the form




κ(x, x′) = tanh(a + bxT x′), which is not positive deﬁnite.
some functions sampled from the corresponding GP. These are equivalent to functions which
are superpositions of erf(u0 + ux) where u0 and u are random. As σ2 increases, the variance
of u increases, so the function varies more quickly. Unlike the RBF kernel, functions sampled
same value they had at the “edge” of the data.
Now suppose we use an RBF network, which is equivalent to a hidden unit activation function
uI), one can show that the
coresponding kernel is equivalent to the RBF or SE kernel.
Smoothing splines compared to GPs *
Univariate splines
The basic idea is to ﬁt a function f by minimizing the discrepancy to the data plus a smoothing
term that penalizes functions that are “too wiggly”. If we penalize the m’th derivative of thefunction, the objective becomes
J(f) =
N



( dm
dxm f(x))2dx
in the two outermost intervals (−∞, x1] and [xN, ∞):
f(x) =


 N

+

+ I(x ̸∈I)

 N

+


 N

+

+ I(x ̸∈I)

 N

αi(x −xi)+

which is a series of truncated cubic polynomials, whose left hand sides are located at each of the
N training points. (The fact that the model is linear on the edges prevents it from extrapolating
too wildly beyond the range of the data; if we drop this requirement, we get an “unrestricted”
spline.)
We can clearly ﬁt this model using ridge regression: ˆw = (ΦT Φ + λIN)−1ΦT y, where the
Regression splines
In general, we can place the polynomials at a ﬁxed set of K locations known as knots, denoted
ξk. The result is called a regression spline. This is a parametric model, which uses basis
simplicity):
f(x) = β0 + β1x +
K

+
Choosing the number and locations of the knots is just like choosing the number and values of
penalized splines.
The connection with GPs
of the following function
f(x) = β0 + β1x + r(x)fκsp(x, x′)), where
κsp(x, x′) ≜

(x −u)+(x′ −u)+du
that regularizers don’t always make good priors.
One can generalize cubic splines to 2d input by deﬁning a regularizer of the following form:

 
 ∂2f(x)
∂x2
 ∂2f(x)
∂x1∂x2
+
∂2f(x)
∂x2
dx1dx2
One can show that the solution has the form
f(x) = β0 + βT
N

αiφi(x)
where φi(x) = η(||x −xi||), and η(z) = z2 log z2. This is known as a thin plate spline. This
is equivalent to MAP estimation with a GP whose kernel is deﬁned in (Williams and Fitzgibbon
inputs. However, in the parametric regression spline setting, where we forego the regularizer on
f, we have more freedom in deﬁning our basis functions. One way to handle multiple inputs is
to use a tensor product basis, deﬁned as the cross product of 1d basis functions. For example,
for 2d input, we can deﬁne
f(x1, x2)
=
β0 +

m
β1m(x1 −ξ1m)+ +

m
β2m(x2 −ξ2m)+
+

m
β12m(x1 −ξ1m)+(x2 −ξ2m)+
there will be too many parameters to ﬁt. One approach to this problem is to use a search
procedure to look for useful interaction terms.
This is known as MARS, which stands for
RKHS methods compared to GPs *
We can generalize the idea of penalizing derivatives of functions, as used in smoothing splines,
to ﬁt functions with a more general notion of smoothness.Mercer’s theorem says that any positive deﬁnite kernel function can be represented in terms of
eigenfunctions:
κ(x, x′) =
∞

λiφi(x)φi(x′)
The φi form an orthormal basis for a function space:
Hk = {f : f(x) =
∞

fiφi(x),
∞

i /λi < ∞}
Now deﬁne the inner product between two functions f(x) = 
∞

∞
⟨f, g⟩H ≜
∞

figi
λi
⟨κ(x1, ·), κ(x2, ·)⟩H = κ(x1, x2)
This is called the reproducing property, and the space of functions Hk is called a reproducing
kernel Hilbert space or RKHS.
Now consider an optimization problem of the form
J(f) =
2σ2y
N

H
where ||f||J is the norm of a function:
||f||H = ⟨f, f⟩H =
∞

i
λi
The intuition is that functions that are complex wrt the kernel will have large norms, because
they will need many eigenfunctions to represent them. We want to pick a simple function that
provides a good ﬁt to the data.
f(x) =
N

αiκ(x, xi)
This is known as the representer theorem, and holds for other convex loss functions besides
squared error.
We can solve for the α by substituting in f(x) = 
N
property to get
J(α) =
2σ2y
2αT KαMinimizing wrt α we ﬁnd
ˆα = (K + σ2
and hence
ˆf(x∗) =

i
ˆαiκ(x∗, xi) = kT
∗(K + σ2
yI)−1y
since the mean and mode of a Gaussian are the same, we can see that linear regresson with an
RKHS regularizer is equivalent to MAP estimation with a GP. An analogous statement holds for
the GP logistic regression case, which also uses a convex likelihood / loss function.
GP latent variable model
this section, we discuss a different way to combine kernels with probabilistic PCA. The resulting
as follows:
p(zi)
=
p(yi|zi, θ)
=
N(yi|Wzi, σ2I)
We can ﬁt this model by maximum likelihood, by integrating out the zi and maximizing W
(and σ2). The objective is given by

2tr(C−1YT Y)

in terms of the eigenvectors of YT Y.
Now we consider the dual problem, whereby we maximize Z and integrate out W. We will
use a prior of the form p(W) = 
p(Y|Z, σ2)
=
D

=

z YYT )

where Kz = ZZT + σ2I. Based on our discussion of the connection between the eigenvalues
If we use a linear kernel, we recover PCA. But we can also use a more general kernel:
Kz = K + σ2I, where K is the Gram matrix for Z. The MLE for ˆZ will no longer be available(a)
(b)
by
ℓ= −D
z YYT )
and the gradient is given by
∂ℓ
∂Zij
=
∂ℓ
∂Kz
∂Kz
∂Zij
where
∂ℓ
∂Kz
z
z
The form of ∂Kz
∂Zij will of course depend on the kernel used. (For example, with a linear kernel,
where Kz = ZZT + σ2I, we have ∂Kz
∂Z = Z.) We can then pass this gradient to any standard
optimizer, such as conjugate gradient descent.
(instead of grid search) to tune the kernel parameters, the ability to handle prior information,etc. For a discussion of some other probabilistic methods for (spectral) dimensionality reduction,
Approximation methods for large datasets
need to invert (or compute the Cholesky decomposition of) the N × N kernel matrix K. A
variety of approximation methods have been devised which take O(M 2N) time, where M is a
ExercisesAdaptive basis function models
Introduction
linear models for regression and classiﬁcation. The prediction takes the form f(x) = wT φ(x),
where we deﬁne
φ(x) = [κ(x, μ1), . . . , κ(x, μN)]
tially perform a form of template matching, whereby they compare the input x to the stored
Although this can work well, it relies on having a good kernel function to measure the
similarity between data vectors. Often coming up with a good kernel function is quite difficult.
intensities (which is what a Gaussian kernel corresponds to) does not work well. Although it is
mizing the marginal likelihood. For example, if we use the ARD kernel,
κ(x, x′) = θ0 exp
⎛
D

θj(xj −x′
⎞
⎠
such methods can be computationally expensive. Another approach, known as multiple kernel
κ(x, x′) = 
j wjκj(x, x′), and then estimates the mixing weights wj.
But this relies on
having good base kernels (and is also computationally expensive).
An alternative approach is to dispense with kernels altogether, and try to learn useful features
function model (ABM), which is a model of the form
f(x) = w0 +
M

wmφm(x)where φm(x) is the m’th basis function, which is learned from data. This framework covers all
of the models we will discuss in this chapter.
Typically the basis functions are parametric, so we can write φm(x) = φ(x; vm), where vm
such models often signiﬁcantly outperform linear models, as we will see.
Classiﬁcation and regression trees (CART)
Classiﬁcation and regression trees or CART models, also called decision trees (not to be
confused with the decision trees used in decision theory) are deﬁned by recursively partitioning
the input space, and deﬁning a local model in each resulting region of input space. This can be
represented by a tree, with one leaf per region, as we explain below.
Basics
less than some threshold t1. If yes, we then ask if x2 is less than some other threshold t2. If
yes, we are in the bottom left quadrant of space, R1. If no, we ask if x1 is less than t3. And
We can write the model in the following form
f(x) = E [y|x] =
M

M

wmφ(x; vm)
where Rm is the m’th region, wm is the mean response in this region, and vm encodes the
choice of variable to split on, and the threshold value, on the path from the root to the m’th leaf.
basis functions deﬁne the regions, and the weights specify the response value in each region.
We discuss how to ﬁnd these basis functions below.
We can generalize this to the classiﬁcation setting by storing the distribution over class labels
empirical fraction of positive examples that satisfy each conjunction of feature values, which
deﬁnes a path from the root to a leaf.R1
R4
R5
R2
R3
(a)
(b)
generated by regtreeSurfaceDemo.
shape
color
ellipse
other
blue
red
other
yes
no
there are n1 positive examples that match this path, and n0 negative examples. In this tree, most of
the leaves are “pure”, meaning they only have examples of one class or the other; the only exception is
negative red ellipses by adding a further test based on size. However, it is not always desirable to construct
trees that perfectly model the training data, due to overﬁtting.
Growing a tree
which are three popular implementations of the method.
(See dtfit for a simple Matlab
implementation.)
The split function chooses the best feature, and the best value for that feature, as follows:
(j∗, t∗) = arg
minreturn node
node.test = λx.xj∗< t∗// anonymous function;
return node;
where the cost function for a given dataset will be deﬁned below. For notational simplicity, we
to a numeric value t. The set of possible thresholds Tj for feature j can be obtained by sorting
consider splits of the form xij = ck and xij ̸= ck, for each possible class label ck. Although
fragmentation, meaning too little data might “fall” into each subtree, resulting in overﬁtting.
The function that checks if a node is worth splitting can use several stopping heuristics, such
as the following:
•
is the reduction in cost too small? Typically we deﬁne the gain of using a feature to be a
normalized measure of the reduction in cost:
Δ ≜cost(D) −
|DL|
|D| cost(DL) + |DR|
|D| cost(DR)
	
•
has the tree exceeded the maximum desired depth?
•
is the distribution of the response in either DL or DR sufficiently homogeneous (e.g., all
labels are the same, so the distribution is pure)?
•
is the number of examples in either DL or DR too small?
All that remains is to specify the cost measure used to evaluate the quality of a proposed
split. This depends on whether our goal is regression or classiﬁcation. We discuss both cases
below.
Regression cost
In the regression setting, we deﬁne the cost as follows:
cost(D) =
where y =
|D|

Alternatively, we can ﬁt a linear regression model for each leaf, using as inputs the features that
were chosen on the path from the root, and then measure the residual error.
Classiﬁcation cost
In the classiﬁcation setting, there are several ways to measure the quality of a split. First, we
ﬁt a multinoulli model to the data in the leaf satisfying the test Xj < t by estimating the
ˆπc =
|D|

I(yi = c)
where D is the data in the leaf.
Given this, there are several common error measures for
evaluating a proposed partition:
•
Misclassiﬁcation rate. We deﬁne the most probable class label as ˆyc = argmaxc ˆπc. The
corresponding error rate is then
|D|

•
Entropy, or deviance:
H (ˆπ) = −
C

ˆπc log ˆπc
Note that minimizing the entropy is equivalent to maximizing the information gain (Quinlan
infoGain(Xj < t, Y )
≜
H (Y ) −H (Y |Xj < t)
=

−

c
p(y = c) log p(y = c)


+


c
p(y = c|Xj < t) log p(c|Xj < t)


the mutual information between Xj and Y : E [infoGain(Xj, Y )] = 
k p(Xj = k)infoGain(Xj = k, Y ) =
H (Y ) −H (Y |Xj) = I (Y ; Xj). 
 
Error rate
Gini
Entropy
Node impurity measures for binary classiﬁcation. The horizontal axis corresponds to p, the
•
Gini index
C


c
ˆπc −

c
ˆπ2

c
ˆπ2
c
This is the expected error rate. To see this, note that ˆπc is the probability a random entry in
Example
the tree.Sepal length
Sepal width
 
 
setosa
versicolor
virginica
(a)
x
y
unpruned decision tree
 
 
versicolor
setosa
virginica
(b)
(a) Iris data. We only show the ﬁrst two features, sepal length and sepal width, and ignore
versicolor
setosa
setosa
virginica
versicolor
versicolor
versicolor
versicolor
versicolor
virginica
virginica
virginica
versicolor
versicolor
virginica
virginica
versicolor
versicolor
virginica
(a)
Number of terminal nodes
Cost (misclassification error)
 
 
Cross−validation
Training set
Best choice
(b)
(a) Unpruned decision tree for Iris data. (b) Plot of misclassiﬁcation error rate vs depth of
tree. Figure generated by dtreeDemoIris.
Pruning a tree
To prevent overﬁtting, we can stop growing the tree if the decrease in the error is not sufficient
to justify the extra complexity of adding an extra subtree. However, this tends to be too myopic.
feature on its own has little predictive power.
The standard approach is therefore to grow a “full” tree, and then to perform pruning. This
can be done using a scheme that prunes the branches giving the least increase in the error. Seeversicolor
setosa
virginica
versicolor
setosa
(a)
x
y
pruned decision tree
 
 
versicolor
setosa
virginica
(b)
Pruned decision tree for Iris data. Figure generated by dtreeDemoIris.
Pros and cons of trees
CART models are popular for several reasons: they are easy to interpret2, they can easily handle
mixed discrete and continuous inputs, they are insensitive to monotone transformations of the
inputs (because the split points are based on ranking the data points), they perform automatic
variable selection, they are relatively robust to outliers, they scale well to large data sets, and
However, CART models also have some disadvantages.
The primary one is that they do
not predict very accurately compared to other kinds of model.
This is in part due to the
greedy nature of the tree construction algorithm. A related problem is that trees are unstable:
small changes to the input data can have large effects on the structure of the tree, due to the
tree. In frequentist terminology, we say that trees are high variance estimators. We discuss a
solution to this below.
Random forests
One way to reduce the variance of an estimate is to average together many estimates.
For
example, we can train M different trees on different subsets of the data, chosen randomly with
which can induce a similar partition to the chosen variable at any given split; these can be used in case the chosen
variable is unobserved at test time. These are called surrogate splits. This method ﬁnds highly correlated features,
and can be thought of as learning a local joint model of the input. This has the advantage over a generative model
of not modeling the entire joint distribution of inputs, but it has the disadvantage of being entirely ad hoc. A simpler
approach, applicable to categorical variables, is to code “missing” as a new value, and then to treat the data as fully
observed.replacement, and then compute the ensemble
f(x) =
M

M fm(x)
“bootstrap aggregating”.
can result in highly correlated predictors, which limits the amount of variance reduction that is
possible. The technique known as random forests (Breiman 2001a) tries to decorrelate the base
learners by learning trees based on a randomly chosen subset of input variables, as well as a
randomly chosen subset of data cases. Such models often have very good predictive accuracy
Bagging is a frequentist concept. It is also possible to adopt a Bayesian approach to learning
inference over the space of trees (structure and parameters) using MCMC. This reduces the
variance of the predictions. We can also perform Bayesian inference over the space of ensembles
of trees, which tends to work much better. This is known as Bayesian adaptive regression
are farily slow to train, but produce high quality classiﬁers.
CART compared to hierarchical mixture of experts *
An interesting alternative to a decision tree is known as the hierarchical mixture of experts.
HME approach is a promising competitor to CART trees”. Some of the advantages include the
following:
•
The model can partition the input space using any set of nested linear decision boundaries.
•
The model makes predictions by averaging over all experts.
By contrast, in a standard
decision tree, predictions are made only based on the model in the corresponding leaf. Since
leaves often contain few training examples, this can result in overﬁtting.
•
Fitting an HME involves solving a smooth continuous optimization problem (usually using
EM), which is likely to be less prone to local optima than the standard greedy discrete
optimization methods used to ﬁt decision trees. For similar reasons, it is computationallyGeneralized additive models
A simple way to create a nonlinear model with multiple inputs is to use a generalized additive
f(x) = α + f1(x1) + · · · + fD(xD)
Here each fj can be modeled by some scatterplot smoother, and f(x) can be mapped to p(y|x)
using a link function, as in a GLM (hence the term generalized additive model).
If we use regression splines (or some other ﬁxed basis function expansion approach) for the
fj, then each fj(xj) can be written as βT
j φj(xj), so the whole model can be written as
becomes
J(α, f1, . . . , fD) =
N

⎛
⎝yi −α −
D

fj(xij)
⎞
⎠
+
D

λj


f ′′
j (tj)2dtj
Backﬁtting
We now discuss how to ﬁt the model using MLE. The constant α is not uniquely identiﬁable,
since we can always add or subtract constants to any of the fj functions. The convention is to
assume N
N
N
To ﬁt the rest of the model, we can center the responses (by subtracting ˆα), and then
iteratively update each fj in turn, using as a target vector the residuals obtained by omitting
term fj:
ˆfj := smoother({yi −

k̸=j
ˆfk(xik)}N
We should then ensure the output is zero mean using
N
N

ˆfj(xij)
then the above objective is convex (since each smoothing spline is a linear operator, as shown
In the GLM case, we need to modify the method somewhat. The basic idea is to replace the
μi = sigm(ˆα + DComputational efficiency
Each call to the smoother takes O(N) time, so the total cost is O(NDT), where T is the
approach is to combine it with a sparsity penalty, see e.g., the SpAM (sparse additive model)
Alternatively, we can use a greedy approach, such as
Multivariate adaptive regression splines (MARS)
We can extend GAMs by allowing for interaction effects. In general, we can create an ANOVA
decomposition:
f(x) = β0 +
D

fj(xj) +

j,k
fjk(xj, xk) +

j,k,l
fjkl(xj, xk, xl) + · · ·
many parameters to ﬁt.
It is common to use greedy search to decide which variables to add.
The multivariate
regression splines to represent the multidimensional regression functions. For example, for 2d
input, we might use
f(x1, x2)
=
β0 +

m
β1m(x1 −t1m)+
+

m
β2m(t2m −x2)+ +

m
β12m(x1 −t1m)+(t2m −x2)+
To create such a function, we start with a set of candidate basis functions of the form
These are 1d linear splines where the knots are at all the observed values for that variable. We
pairs in C. For example, we might initially get
f(x)
=(a)
(b)
(c)
generated by marsDemo.
We proceed in this way until the model becomes very large.
(We may impose an upper
bound on the order of interactions.) Then we prune backwards, at each step eliminating the
basis function that causes the smallest increase in the residual error, until the CV error stops
improving.
The whole procedure is closely related to CART. To see this, suppose we replace the piecewise
linear basis functions by step functions I(xj > t) and I(xj < t). Multiplying by a pair of
reﬂected step functions is equivalent to splitting a node. Now suppose we impose the constraint
that once a variable is involved in a multiplication by a candidate term, that variable gets
replaced by the interaction, so the original variable is no longer available. This ensures that a
variable can not be split more than once, thus guaranteeing that the resulting model can be
represented as a tree. In this case, the MARS growing strategy is the same as the CART growing
strategy.
Boosting
learner or a base learner. The algorithm works by applying the weak learner sequentially to
weighted versions of the data, where more weight is given to examples that were misclassiﬁed
by earlier rounds.
This weak learner can be any classiﬁcation or regression algorithm, but it is common to use a
This
By contrast, single decision trees performed very poorly.
proved that one could boost the performance (on the training set) of any weak learner arbitrarilytrain
test
Training (solid blue) and test (dotted red) error vs number of iterations. Figure generated by boostingDemo,
written by Richard Stapenhurst.
high, provided the weak learner could always perform slightly better than chance. For example,
surprising is that the test set error continues to decline even after the training set error has
reached zero (although the test set error will eventually go up). Thus boosting is very resistant
to overﬁtting. (Boosted decision stumps form the basis of a very successful face detector (Viola
digital cameras.)
In view of its stunning empirical success, statisticians started to become interested in this
how boosting could be extended to handle a variety of loss functions, including for regression,
Forward stagewise additive modeling
The goal of boosting is to solve the following optimization problem:
min
f
N

L(yi, f(xi))
If we use squared error loss, the optimal estimate is given by
f ∗(x) = argmin
f(x)
= Ey|x

= E [Y |x]Name
Loss
Derivative
f ∗
Algorithm
Squared error
yi −f(xi)
E [y|xi]
L2Boosting
Absolute error
|yi −f(xi)|
sgn(yi −f(xi))
median(y|xi)
Gradient boosting
Exponential loss
exp(−˜yif(xi))
−˜yi exp(−˜yif(xi))
πi
AdaBoost
Logloss
yi −πi
πi
LogitBoost
Some commonly used loss functions, their gradients, their population minimizers f ∗, and
y− f
loss
 
 
logloss
exp
Illustration of various loss functions for binary classiﬁcation.
The horizontal axis is the
knowing the true conditional distribution p(y|x). Hence this is sometimes called the population
minimizer, where the expectation is interpreted in a frequentist sense. Below we will see that
boosting will try to approximate this conditional expectation.
discuss that here.
An alternative convex upper bound is exponential loss, deﬁned by
L(˜y, f) = exp(−˜yf)
This will have some computational advantages over the logloss,
to be discussed below. It turns out that the optimal estimate for this loss is also f ∗(x) =zero:
∂
∂f(x)E

e−˜yf(x)|x

=
∂
=
=
Since ﬁnding the optimal f is hard, we shall tackle it sequentially. We initialise by deﬁning
f0(x) = arg min
γ
N

L(yi, f(xi; γ))
ˆπ
N
N
powerful model for our baseline, such as a GLM.
Then at iteration m, we compute
(βm, γm) = argmin
β,γ
N

and then we set
The key point is that we do not go back and adjust earlier parameters. This is why the method
is called forward stagewise additive modeling.
We continue this for a ﬁxed number of iterations M. In fact M is the main tuning parameter
of the method. Often we pick it by monitoring the performance on a separate validation set, and
then stopping once performance starts to decrease; this is called early stopping. Alternatively,
for details).
In practice, better (test set) performance can be obtained by performing “partial updates” of
the form
form of loss function. However, it is independent of the form of weak learner.
L2boosting
Suppose we used squared error loss. Then at step m the loss has the form(a)
(b)
(c)
Example of adaboost using a decision stump as a weak learner. The degree of blackness
represents the conﬁdence in the red class. The degree of whiteness represents the conﬁdence in the blue
Stapenhurst.
generality. Hence we can ﬁnd the new basis function by using the weak learner to predict rm.
we will see that this method, with a suitable choice of weak learner, can be made to give the
AdaBoost
Consider a binary classiﬁcation problem with exponential loss. At step m we have to minimize
Lm(φ)
=
N

N

wi,m exp(−β˜yiφ(xi))
can rewrite this objective as follows:
Lm
=
e−β

wi,m + eβ

˜yi̸=φ(xi)
wi,m
=
(eβ −e−β)
N

wi,mI(˜yi ̸= φ(xi)) + e−β
N

wi,m
Consequently the optimal function to add is
φm = argmin
φ
wi,mI(˜yi ̸= φ(xi))
This can be found by applying the weak learner to a weighted version of the dataset, with
weights wi,m. Subsituting φm into Lm and solving for β we ﬁnd
errmwhere
errm =
N
N
The overall update becomes
With this, the weights at the next iteration become
=
wi,me−βm ˜yiφm(xi)
=
=
wi,me2βmI(˜yi̸=φm(xi))e−βm
otherwise. Since e−βm will cancel out in the normalization step, we can drop it. The result is
An example of this algorithm in action, using decision stumps as the weak learner, is given in
Fit a classiﬁer φm(x) to the training set using weights w;
Compute errm =
N
N
;
Set wi ←wi exp[αmI(˜yi ̸= φm(xi))];
M

;
LogitBoost
The trouble with exponential loss is that it puts a lot of weight on misclassiﬁed examples, as
method very sensitive to outliers (mislabeled examples). In addition, e−˜yf is not the logarithm
estimates from f(x).
binary class label. If φm returns a probability instead, a modiﬁed algorithm, known as real AdaBoost, can be used. SeeA natural alternative is to use logloss instead. This only punishes mistakes linearly, as is clear
ﬁnal learned function, using
ef(x)
e−f(x) + ef(x) =
Lm(φ)
=
N

By performing a Newton upate on this objective (similar to IRLS), one can derive the algorithm
Compute the working response zi =
y∗
i −πi
φm = argminφ
N
2φm(x);
M

;
Boosting as functional gradient descent
Rather than deriving new versions of boosting for every different loss function, it is possible to
explain this, imagine minimizing
ˆf = argmin
f
L(f)
where f = (f(x1), . . . , f(xN)) are the “parameters”. We will solve this stagewise, using gradient
gim =
∂L(yi, f(xi))
∂f(xi)
where ρm is the step length, chosen by
ρm = argmin
ρ
This is called functional gradient descent.
In its current form, this is not much use, since it only optimizes f at a ﬁxed set of N points,
so we do not learn a function that can generalize. However, we can modify the algorithm by
ﬁtting a weak learner to approximate the negative gradient signal. That is, we use this update
γm = argmin
γ
N

N
Compute the gradient residual using rim = −

∂L(yi,f(xi))
∂f(xi)

Use the weak learner to compute γm which minimizes N
If we apply this algorithm using squared loss, we recover L2Boosting.
If we apply this
outliers than squared error loss.
Sparse boosting
Suppose we use as our weak learner the following algorithm: search over all possible variables
j(m)
=
argmin
j
N

ˆβjm
=
N
N
ij
φm(x)
=
ˆβj(m),m xj(m)It is clear that this will result in a sparse estimate, at least if M is small. To see this, let us
rewrite the update as follows:
the full regularization path for the lasso problem. The same algorithm can be used for sparse
logistic regression, by simply modifying the residual to be the appropriate negative gradient.
Now consider a weak learner that is similar to the above, except it uses a smoothing spline
instead of linear regression when mapping from xj to the residual.
The result is a sparse
variables at a time. The resulting method often works much better than MARS (Buhlmann and
Multivariate adaptive regression trees (MART)
It is quite common to use CART models as weak learners. It is usually advisable to use a shallow
tree, so that the variance is low. Even though the bias will be high (since a shallow tree is likely
to be far from the “truth”), this will compensated for in subsequent rounds of boosting.
The height of the tree is an additional tuning parameter (in addition to M, the number of
rounds of boosting, and ν, the shrinkage factor). Suppose we restrict to trees with J leaves.
If we combine the gradient boosting algorithm with (shallow) regression trees, we get a model
known as MART, which stands for “multivariate adaptive regression trees”. This actually includes
a slight reﬁnement to the basic gradient boosting algorithm: after ﬁtting a regression tree to the
the loss:
γjm = argmin
γ

where Rjm is the region for leaf j in the m’th tree, and γjm is the corresponding parameter (the
mean response of y for regression problems, or the most probable class label for classiﬁcation
problems).
Why does boosting work so well?
We have seen that boosting works very well, especially for classiﬁers.
There are two main
reasons for this. First, it can be seen as a form of ℓ1 regularization, which is known to helpWe could use ℓ1 regularization to select a subset of these. Alternatively we can use boosting,
where at each step, the weak learner creates a new φk on the ﬂy. It is possible to combine
and then prunes off irrelevant ones using ℓ1 regularization.
A Bayesian view
So far, our presentation of boosting has been very frequentist, since it has focussed on greedily
minimizing loss functions. A likelihood interpretation of the algorithm was given in (Neal and
p(y|x, θ) =
M

πmp(y|x, γm)
where each expert p(y|x, γm) is like a weak learner. We usually ﬁt all M experts at once
using EM, but we can imagine a sequential scheme, whereby we only update the parameters
for one expert at a time. In the E step, the posterior responsibilities will reﬂect how well the
existing experts explain a given data point; if this is a poor ﬁt, these data points will have
more inﬂuence on the next expert that is ﬁtted. (This view naturally suggest a way to use a
instead of mixtures of experts.)
Notice that this is a rather “broken” MLE procedure, since it never goes back to update the
parameters of an old expert. Similarly, if boosting ever wants to change the weight assigned to a
weak learner, the only way to do this is to add the weak learner again with a new weight. This
Feedforward neural networks (multilayer perceptrons)
regression models stacked on top of each other, with the ﬁnal layer being either another logistic
regression or a linear regression model, depending on whether we are solving a classiﬁcation or
regression problem. For example, if we have two layers, and we are solving a regression problem,
the model has the form
p(y|x, θ)
=
N(y|wT z(x), σ2)
z(x)
=
g(Vx) = [g(vT
Hx)]
z(x) = φ(x, V) is called the hidden layer (a deterministic function of the input), H is thexn1
xni
xnD
zn1
znj
znH
yn1
ynk
ynC
...
...
vij
wjk
A neural network with one hidden layer.
number of hidden units, V is the weight matrix from the inputs to the hidden nodes, and
linear, otherwise the whole model collapses into a large linear regression model of the form
y = wT (Vx). One can show that an MLP is a universal approximator, meaning it can model
any suitably smooth function, given enough hidden units, to any desired level of accuracy
To handle binary classiﬁcation, we pass the output through a sigmoid, as in a GLM:
p(y|x, θ) = Ber(y|sigm(wT z(x)))
We can easily extend the MLP to predict multiple outputs. For example, in the regression case,
we have
p(y|x, θ) = N(y|W φ(x, V), σ2I)
p(y|x, θ)
=
Cat(y|S(Wz(x))
Convolutional neural networks
is called feature extraction or feature construction. These hidden features are then passed as
input to the ﬁnal GLM. This approach is particularly useful for problems where the original input
features are not very individually informative. For example, each pixel in an image is not very
informative; it is the combination of pixels that tells us what objects are present. Conversely, for
a task such as document classiﬁcation using a bag of words representation, each feature (word
count) is informative on its own, so extracting “higher order” features is less important. Not
suprisingly, then, much of the work in neural networks has been motivated by visual patternA form of MLP which is particularly well suited to 1d signals like speech or text, or 2d signals
like images, is the convolutional neural network. This is an MLP in which the hidden units
have local receptive ﬁelds (as in the primary visual cortex), and in which the weights are tied
or shared across the image, in order to reduce the number of parameters. Intuitively, the effect
of such spatial parameter tying is that any useful features that are “discovered” in some portion
resulting network then exhibits translation invariance, meaning it can classify patterns no
matter where they occur inside the input image.
a kernel), adding a bias, and then passing the result through some form of nonlinearity. ThereA single pass over the data set is called an epoch. When Mike O’Neill did these experiments in
To further reduce the error rate, a standard trick is to expand the training set by including
distorted versions of the original data, to encourage the network to be invariant to small changes
that don’t affect the identity of the digit. These can be created by applying a random ﬂow ﬁeld
stochastic gradient descent, we can create these distortions on the ﬂy, rather than having to
LeNet5 has a subsampling layer between each convolutional layer, which either averages or
computes the max over each small window in the previous layer, in order to reduce the size, and
was inspired by Hubel and Wiesel’s model of simple and complex cells in the visual cortex
at that time.
The second difference between LeNet5 and the Simard architecture is that the ﬁnal layer is
actually an RBF network rather than a more standard sigmoidal or softmax layer. This model
Of course, classifying isolated digits is of limited applicability: in the real world, people usually
write strings of digits or other letters. This requires both segmentation and classiﬁcation. Le Cun
and colleagues devised a way to combine convolutional neural networks with a model similar
account of the system, which remains one of the best performing systems for this task.)(a)
(b)
(c)
(d)
(e)
(f)
Figure generated by elasticDistortionsDemo, written by Kevin Swersky.
INPUT
32x32
Convolutions
Subsampling
Convolutions
C1: feature maps 
Subsampling
S2: f. maps
C5: layer
F6: layer
Full connection
Full connection
Gaussian connections
OUTPUTgenerative model.)
Other kinds of neural networks
Other network topologies are possible besides the ones discussed above. For example, we can
have skip arcs that go directly from the input to the output, skipping the hidden layer; we
can have sparse connections between the layers; etc. However, the MLP always requires that
the weights form a directed acyclic graph.
If we allow feedback connections, the model is
known as a recurrent neural network; this deﬁnes a nonlinear dynamical system, but does
not have a simple probabilistic interpretation. Such RNN models are currently the best approach
ﬁeld network or associative memory; its probabilistic counterpart is known as a Boltzmann
A brief history of the ﬁeld
Neural networks have been the subject of great interest for many decades, due to the desire to
understand the brain, and to build learning machines. It is not possible to review the entire
history here. Instead, we just give a few “edited highlights”.output as a weighted sum of inputs passed through a threshold function, y = I(
i wixi > θ),
This is similar to a sigmoidal activation function.
Frank Rosenblatt
trons” in which they showed that such linear models, with no hidden layers, were very limited
in their power, since they cannot classify data that is not linearly separable. This considerably
reduced interest in the ﬁeld.
attention.) This spawned a decade of intense interest in these models.
system, that learned a mapping from English words to phonetic symbols which could be fed
into a speech synthesizer. An audio demo of the system as it learns over time can be found at
“babbling” and then gradually learns to pronounce English words. NETtalk learned a distributed
representation (via its hidden layer) of various sounds, and its success spawned a big debate in
psychology between connectionism, based on neural networks, and computationalism, based
on syntactic rules. This debate lives on to some extent in the machine learning community,
like” representations, or using more structured models.
provide similar prediction accuracy to neural networks while being considerably easier to train
(since they use a convex objective function). This spawned a decade of interest in kernel methods
amount of human expertise to design the right kernel function.
which provided a way, for the ﬁrst time, to learn deep networks, by training one layer at a time
The backpropagation algorithm
methods. Since MLPs have lots of parameters, they are often trained on very large data sets.
NIPS, which stands for “neural information processing systems”, it was important to ensure your paper did not contain
the word “neural network”! 
 
tanh
sigmoid
the output layer. Figure generated by tanhPlot.
We now discuss how to compute the gradient vector of the NLL by applying the chain rule of
calculus. The resulting algorithm is known as backpropagation, for reasons that will become
apparent.
For notational simplicity, we shall assume a model with just one hidden layer. It is helpful
We now convert this hidden layer to the output layer as follows. Let bn = Wzn be the
another nonlinearity, corresponding to the canonical link for the GLM. (We reserve the notation
yn, without the hat, for the output corresponding to the n’th training case.) For a regression
model, we use h(b) = b; for binary classifcation, we use h(b) = [sigm(b1), . . . , sigm(bc)]; for
We can write the overall model as follows:
xn
V
→an
W
→bn
residual errors, ˆσ2 =In the regression case, with K outputs, the NLL is given by the squared error:
J(θ) = −

n

k
In the classiﬁcation case, with K classes, the NLL is given by the cross entropy
J(θ) = −

n

k
ynk log ˆynk(θ)
Our task is to compute ∇θJ. We will derive this for each n separately; the overall gradient is
Let us start by considering the output layer weights. We have
∇wkJn = ∂Jn
∂bnk
∇wkbnk = ∂Jn
∂bnk
zn
since bnk = wT
k zn.
Assuming h is the canonical link function for the output GLM, then
∂Jn
∂bnk
≜δw
nk = (ˆynk −ynk)
which is the error signal. So the overall gradient is
∇wkJn = δw
nkzn
δw
nk.
For the input layer weights, we have
∇vjJn = ∂Jn
∂anj
∇vjanj ≜δv
njxn
where we exploited the fact that anj = vT
j xn. All that remains is to compute the ﬁrst level
nj. We have
δv
nj
=
∂Jn
∂anj
=
K

∂Jn
∂bnk
∂bnk
∂anj
=
K

δw
nk
∂bnk
∂anj
Now
bnk =

j
wkjg(anj)
so
∂bnk
∂anj
= wkjg′(anj)
where g′(a) =
d
dag(a). For tanh units, g′(a) =
d
for sigmoid units, g′(a) =
d
δv
nj
=
K

δw
nkwkjg′(anj)hence the term “backpropagation”. The key property is that we can compute the gradients locally:
each node only needs to know about its immediate neighbors. This is supposed to make the
algorithm “neurally plausible”, although this interpretation is somewhat controversial.
Putting it all together, we can compute all the gradients as follows: we ﬁrst perform a
forwards pass to compute an, zn, bn and ˆyn. We then compute the error for the output layer,
n
n . We then compute the overall gradient as follows:
∇θJ(θ) =

n
[δv
nxn, δw
n zn]
Identiﬁability
It is easy to see that the parameters of a neural network are not identiﬁable. For example, we
can change the sign of the weights going into one of the hidden units, so long as we change
the sign of all the weights going out of it; these effects cancel, since tanh is an odd function, so
tanh(−a) = −tanh(a). There will be H such sign ﬂip symmetries, leading to 2H equivalent
settings of the parameters. Similarly, we can change the identity of the hidden units without
affecting the likelihood.
There are H! such permutations.
The total number of equivalent
parameter settings (with the same likelihood) is therefore H!2H.
be a more serious problem, although with enough data, these local optima are often quite
“shallow”, and simple stochastic optimization methods can avoid them. In addition, it is common
to perform multiple restarts, and to pick the best solution, or to average over the resulting
predictions. (It does not make sense to average the parameters themselves, since they are not
identiﬁable.)
Regularization
As usual, the MLE can overﬁt, especially if the number of nodes is large. A simple way to prevent
this is called early stopping, which means stopping the training procedure when the error on
the validation set ﬁrst starts to increase. This method works because we usually initialize from
small random weights, so the model is initially simple (since the tanh and sigm functions are
nearly linear near the origin). As training progresses, the weights become larger, and the model
becomes nonlinear. Eventually it will overﬁt.
Another way to prevent overﬁtting, that is more in keeping with the approaches used elsewhere
in this book, is to impose a prior on the parameters, and then use MAP estimation. It is standard
of the prior. In the neural networks literature, this is called weight decay, since it encourages
small weights, and hence simpler models. The penalized NLL objective becomes
J(θ) = −
N

log p(yn|xn, θ) + α

ij
v2
ij +

jk
w2
jk](Note that we don’t penalize the bias terms.) The gradient of the modiﬁed objective becomes
∇θJ(θ) = [

n
δv
nxn + αv,

n
δw
n zn + αw]
If the regularization is sufficiently strong, it does not matter if we have too many hidden units
(apart from wasted computation). Hence it is advisable to set H to be as large as you can afford
As with ridge regression, it is good practice to standardize the inputs to zero mean and unit
variance, so that the spherical Gaussian prior makes sense.
Consistent Gaussian priors *
second layer weights results in the lack of a certain desirable invariance property. In particular,
model. Then we would like the model to learn to predict the same function, by suitably scaling
its internal weights and bias terms. However, the amount of scaling needed by the ﬁrst and
Therefore we need to use a different regularization strength for the ﬁrst and second layer.
Fortunately, this is easy to do — we just use the following prior:
αw
αv
αb
αc
I)
the functions steeper. Decreasing αb allows the ﬁrst layer biases to get bigger, which allows
the center of the sigmoid to shift left and right more. Decreasing αw allows the second layer
weights to get bigger, making the functions more “wiggly” (greater sensitivity to change in the
input, and hence larger dynamic range). And decreasing αc allows the second layer biases to
we will see an easier way to deﬁne priors over functions.)
Weight pruning
Since there are many weights in a neural network, it is often helpful to encourage sparsity.
responses in the training set to zero mean, to be consistent with the fact that the prior on the output bias has zero
mean.(a)
(b)
(c)
(d)
(e)
(c) Decreasing αb by
mlpPriorsDemo.Neural Network
y
h44
h43
h42
h41
h40
h34
h33
h31
h30
h24
h23
h21
h20
h14
h13
h22
h32
h12
h11
h10
x1
x0
(a)
 
 
Data
Deep Neural Net
(b)
(a) A deep but sparse neural network. The connections are pruned using ℓ1 regularization.
terms. (b) Predictions made by the model on the training set. Figure generated by sparseNnetDemo,
written by Mark Schmidt.
Another way to regularize the parameters is to encourage similar weights to share statistical
strength. But how do we know which parameters to group together? We can learn this, by using
a mixture model. That is, we model p(θ) as a mixture of (diagonal) Gaussians. Parameters that
are assigned to the same cluster will share the same mean and variance and thus will have
similar values (assuming the variance for that cluster is low). This is called soft weight sharing
p271) if you want to know the details.
An interesting way to regularize “deep” feedforward neural networks is to encourage the hidden
layers to assign similar objects to similar representations. This is useful because it is often easy
to obtain “side” information consisting of sets of pairs of similar and dissimilar objects. For
example, in a video classiﬁcation task, neighboring frames can be deemed similar, but frames
done without collecting any labels.
embedding of item xi, e.g., f(xi) = z(xi, θ), where z is the hidden layer of a neural network.
Now deﬁne a loss function L(f(xi), f(xj), Sij) that depends on the embedding of two objects,and the observed similarity measure. For example, we might want to force similar objects to
have similar embeddings, and to force the embeddings of dissimilar objects to be a minimal
distance apart:
L(fi, fj, Sij) =

where m is some minimal margin. We can now deﬁne an augmented loss function for training
the neural network:

NLL(f(xi), yi) + λ

L(f(xi), f(xj), Sij)
Such an objective can be easily optimized by stochastic gradient descent.
tion, pick a random labeled training example, (xn, yn), and take a gradient step to optimize
NLL(f(xi), yi). Then pick a random pair of similar unlabeled examples xi, xj (these can
sometimes be generated on the ﬂy rather than stored in advance), and make a gradient step to
Note that this technique is effective because it can leverage massive amounts of data. In
words. This constitutes the positive examples. To create negative examples, the middle word of
each window was replaced by a random English word (this is likely to be an “invalid” sentence
to a supervised semantic role labeling task, for which very little labeled training data is available.
Bayesian inference *
Although MAP estimation is a succesful way to reduce overﬁtting, there are still some good
reasons to want to adopt a fully Bayesian approach to “ﬁtting” neural networks:
•
larization than MAP estimation.
•
and the number of hidden units.
This is likely to be much faster than cross validation,
•
making.•
We can use online inference methods, such as the extended Kalman ﬁlter, to do online
One can adopt a variety of approximate Bayesian inference techniques in this context. In this
Parameter posterior for regression
We will use a prior of the form p(w) =
The posterior can be approximated as follows:
p(w|D, α, β)
∝
exp(−E(w))
E(w)
≜
βED(w) + αEW (w)
ED(w)
≜
N

EW (w)
≜
2wT w
where ED is the data error, EW is the prior error, and E is the overall error (negative log
around its minimum (the MAP estimate)
where A is the Hessian of E:
A = ∇∇E(wMP ) = βH + αI
where H = ∇∇ED(wMP ) is the Hessian of the data error. This can be computed exactly
in O(d2) time, where d is the number of parameters, using a variant of backpropagation (see
(Note that
diagonal approximations of H are usually very inaccurate.) In either case, using this quadratic
approximation, the posterior becomes Gaussian:
p(w|α, β, D)
≈Parameter posterior for classiﬁcation
entropy error of the form
ED(w)
≜
N

Predictive posterior for regression
The posterior predictive density is given by
p(y|x, D, α, β) =


This is not analytically tractable because of the nonlinearity of f(x, w).
Let us therefore
f(x, w) ≈f(x, wMP ) + gT (w −wMP )
where
we have
p(y|x, D, α, β) ≈N(y|f(x, wMP ), σ2(x))
where the predictive variance depends on the input x as follows:
The error bars will be larger in regions of input space where we have little training data. See
Predictive posterior for classiﬁcation
In this section, we discuss how to approximate p(y|x, D) in the case of binary classiﬁcation.
Speciﬁcally, we have
Let us make a linear approximation to this:
a(x, w) ≈aMP (x) + gT (w −wMP )
where aMP (x) = a(x, wMP ) and g = ∇xa(x, wMP ) can be found by a modiﬁed version of
backpropagation. Clearly
p(a|x, D) ≈N(a(x, wMP ), g(x)T A−1g(x))Input
Target
 
 
data
function
network
error bars
(a)
 
 
Data
Function
Prediction
Samples
(b)
The dashed green line is the true function. (a) Result of using a Laplace approximation, after performing
empirical Bayes to optimize the hyperparameters. The solid red line is the posterior mean prediction,
Figure generated by
mlpRegEvidenceDemo. (b) Result of using hybrid Monte Carlo, using the same trained hyperparameters
as in (a). The solid red line is the posterior mean prediction, and the dotted blue lines are samples from
the posterior predictive. Figure generated by mlpRegHmcDemo, written by Ian Nabney.
Hence the posterior predictive for the output is


a)bT wMP )
κ(σ2)
≜
Of course, a simpler (and potentially more accurate) alternative to this is to draw a few samples
from the Gaussian posterior and to approximate the posterior predictive using Monte Carlo.
fected, however.
ARD for neural networks
Once we have made the Laplace approximation to the posterior, we can optimize the marginal
p(θ) =
D

αv,i
I)
H

αw,j
I)
If we ﬁnd αv,i = ∞, then input feature i is irrelevant, and its weight vector v:,i is pruned out.
Similarly, if we ﬁnd αw,j = ∞, then hidden feature j is irrelevant. This is known as automaticThe software package NETLAB contains a simple example of ARD applied to a neural network,
called demard.
This demo creates some data according to a nonlinear regression function
f(x1, x2, x3) = sin(2πx1) + ϵ, where x2 is a noisy copy of x1. We see that x2 and x3 are
irrelevant for predicting the target. However, x2 is correlated with x1, which is relevant. Using
is very relevant.
Ensemble learning
Ensemble learning refers to learning a weighted combination of base models of the form
f(y|x, π) =

wmfm(y|x)
where the wm are tunable parameters. Ensemble learning is sometimes called a committee
method, since each base model fm gets a weighted “vote”.
fact, one can argue that a neural net is an ensemble method, where fm represents the m’th
hidden unit, and wm are the output layer weights. Also, we can think of boosting as kind of
ensemble learning, where the weights on the base models are determined sequentially. Below
we describe some other forms of ensemble learning.
Stacking
ˆw = argmin
w
N

L(yi,
M

wmfm(x))
However, this will result in overﬁtting, with wm being large for the most complex model. A
ˆw = argmin
w
N

L(yi,
M

wm ˆf −i
m (x))
where ˆf −i
m (x) is the predictor obtained by training on data excluding (xi, yi). This is known
robust to the case where the “true” model is not in the model class than standard BMA (Clarke
submission of the winning team (BellKor’s Pragmatic Chaos) in terms of accuracy (Sill et al.Class
C1
C2
C3
C4
C5
C6
· · ·
C15
· · ·
· · ·
...
· · ·
Each row deﬁnes a
idea is that we are trying to decode a symbol (namely the class label) which has C possible
states. We could use a bit vector of length B = ⌈log2 C⌉to encode the class label, and train
B separate binary classiﬁers to predict each bit. However, by using more bits, and by designing
the codewords to have maximal Hamming distance from each other, we get a method that is
ˆc(x) = min
c
B

|Ccb −ˆpb(x)|
random code worked just as well as the optimal code: both methods work by averaging the
results of multiple classiﬁers, thereby reducing variance.
Ensemble learning is not equivalent to Bayes model averaging
and then using this to make predictions, is to make a weighted average of the predictions made
by each model, i.e., we compute
p(y|x, D) =

p(y|x, m, D)p(m|D)
This is called Bayes model averaging (BMA), and can sometimes give better performance than
computationally infeasible (analytical integration is obviously not possible in a discrete space,
although one can sometimes use dynamic programming to perform the computation exactly,
posterior. An even simpler approximation (and the one most widely used in practice) is to just
use the MAP model.
It is important to note that BMA is not equivalent to ensemble learning (Minka 2000c). This
latter technique corresponds to enlarging the model space, by deﬁning a single new modelmodel
1st
2nd
3rd
4th
5th
6th
7th
8th
9th
10th
rf
svm
ann
knn
dt
logreg
nb
Fraction of time each method achieved a speciﬁed rank, when sorting by mean performance
which is a convex combination of base models, as follows:
p(y|x, π) =

πmp(y|x, m)
dictions using p(y|x, D) =

p(y|x, π)p(π|D)dπ. However, it is much more common to use
point estimation methods for π, as we saw above.
Experimental comparison
We have described many different methods for classiﬁcation and regression. Which one should
you use? That depends on which inductive bias you think is most appropriate for your domain.
Usually this is hard to assess, so it is common to just try several different methods, and
see how they perform empirically.
Below we summarize two such comparisons that were
carefully conducted (although the data sets that were used are relatively small). See the website
mlcomp.org for a distributed way to perform large scale comparisons of this kind. Of course,
no universally best learning method.
used to assess average test error. (This is separate from any internal CV a method may need to
use for model selection.)The methods they compared are as follows (listed in roughly decreasing order of performance,
•
•
RF: random forest
•
•
SVM: support vector machine
•
ANN: artiﬁcial neural network
•
•
•
DT: decision tree
•
LOGREG: logistic regression
•
NB: naive Bayes
old metrics just require a point estimate as output.
compared.
Obviously the results vary by dataset and by metric. Therefore just averaging the performance
does not necessarily give reliable conclusions. However, one can perform a bootstrap analysis,
which shows how robust the conclusions are to such changes.
The results are shown in
by random forests, bagged decision trees, SVMs and neural networks. However, the following
methods all did relatively poorly: KNN, stumps, single decision trees, logistic regression and
naive Bayes.
These results are generally consistent with conventional wisdom of practioners in the ﬁeld.
problems with large numbers of (mostly irrelevant) features, given small training sets.
(This
was called a “feature selection” challenge, but performance was measured in terms of predictive
accuracy, not in terms of the ability to select features.) The ﬁve datasets that were used are
problem to make it harder. These have no predictive power, but are correlated with the original
features.Dataset
Domain
Type
D
% probes
Ntrain
Nval
Ntest
Aracene
Mass spectrometry
Dense
Dexter
Text classiﬁcation
Sparse
Dorothea
Drug discovery
Sparse
Gisette
Digit recognition
Dense
Madelon
Artiﬁcial
Dense
Screened features
ARD
Method
Avg rank
Avg time
Avg rank
Avg time
HMC MLP
Boosted MLP
Bagged MLP
Boosted trees
Random forests
(HMC
methods based on bagging and boosting. Note that all of these methods are quite similar: in
each case, the prediction has the form
ˆf(x∗) =
M

wmE [y|x∗, θm]
estimated sequentially.
To improve computational and statistical performance, some feature selection was performed.
are again the winner. In second place are either random forests or boosted MLPs, depending
on the preprocessing. However, it is not clear how statistically signiﬁcant these differences are,
since the test sets are relatively small.
In terms of training time, we see that MCMC is much slower than the other methods. It would
be interesting to see how well deterministic Bayesian inference (e.g., Laplace approximation)
would perform. (Obviously it will be much faster, but the question is: how much would one losex1
partial dependence
x2
x3
x4
x5
x6
partial dependence
x7
x8
x9
x10
gression problem.
Used with kind permission of Hugh
Chipman.
in statistical performance?)
Linear models are popular in part because they are easy to interpet. However, they often are
poor predictors, which makes them a poor proxy for “nature’s mechanism”. Thus any conclusions
about the importance of particular variables should only be based on models that have good
predictive accuracy (Breiman 2001b). (Interestingly, many standard statistical tests of “goodness
of ﬁt” do not test the predictive accuracy of a model.)
Unfortunately, they are hard to interpret directly. Fortunately, there are various heuristics we can
use to “probe” such models, in order to assess which input variables are the most important.
variables.usage
relevant. The different coloured lines correspond to different numbers of trees in the ensemble.
Source:
One useful way to measure the effect of a set s of variables on the output is to compute a
as the response to xs with the other predictors averaged out:
N
N

f(xs, xi,−s)
response is roughly linear in x4 and x5, and roughly quadratic in x3. (The error bars are obtained
can use bootstrap.)
Another very useful summary computes the relative importance of predictor variables.
This can be thought of as a nonlinear, or even “model free”, way of performing variable selection,
although the technique is restricted to ensembles of trees. The basic idea, originally proposed
In particular, let vj =
M
M
where Tm is the m’th tree. If we can sample the posterior of trees, p(T1:M|D), we can easily
compute the posterior for vj. Alternatively, we can use bootstrap.
much more than the ﬁve irrelevant variables. As we increase the number M of trees, all the
variables are more likely to be chosen, reducing the sensitivity of this method, but for small M,
the method is farily diagnostic.Exercises
In this question, we ﬁt a model which can predict what torques a robot needs to apply in order to make
The mapping from x to y is highly nonlinear.
standard practice and focus on just predicting a scalar output, namely the torque for the ﬁrst joint.
Standardize the inputs so they
have zero mean and unit variance on the training set, and center the outputs so they have zero mean
on the training set. Apply the corresponding transformations to the test data. Below we will describe
various models which you should ﬁt to this transformed data. Then make predictions and compute the
standardized mean squared error on the test set as follows:
SMSE =
Ntest
Ntest
σ2
where σ2 =
Ntrain
Ntrain
a. The ﬁrst method you should try is standard linear regression.
Turn in your numbers and code.
using this method.)
c. Now try ﬁtting a feedforward neural network. Use CV to pick the number of hidden units and the
strength of the ℓ2 regularizer. What SMSE do you get? Turn in your numbers and code.Markov and hidden Markov models
Introduction
In this chapter, we discuss probabilistic models for sequences of observations, X1, . . . , XT , of
arbitrary length T. Such models have applications in computational biology, natural language
processing, time series forecasting, etc. We focus on the case where we the observations occur
at discrete “time steps”, although “time” may also refer to locations within a sequence.
Markov models
captures all the relevant information for predicting the future (i.e., we assume it is a sufficient
statistic). If we assume discrete time steps, we can write the joint distribution as follows:
p(X1:T ) = p(X1)p(X2|X1)p(X3|X2) . . . = p(X1)
T

This is called a Markov chain or Markov model.
since the same parameter is shared by multiple variables. This assumption allows us to model
an arbitrary number of variables using a ﬁxed number of parameters; such models are called
stochastic processes.
this section.
Transition matrix
written as a K × K matrix, known as the transition matrix A, where Aij = p(Xt =
one, α
β
(a)
A12
A23
A11
A22
A33
(b)
to visualize such automata by drawing a directed graph, where nodes represent states and arrows
diagram. The weights associated with the arcs are the probabilities. For example, the following
A =

α
β
	
A =
⎛
⎝
A11
A12
A22
A23
⎞
⎠
The Aij element of the transition matrix speciﬁes the probability of getting from i to j in
Aij(m + n) =
K

Aik(m)Akj(n)
In words, the probability of getting from i to j in m + n steps is just the probability of getting
from i to k in m steps, and then from k to j in n steps, summed up over all k. We can write
the above as a matrix multiplication
A(m + n) = A(m)A(n)
Hence
Thus we can simulate multiple steps of a Markov chain by “powering up” the transition matrix.SAYS IT’S NOT IN THE CARDS LEGENDARY RECONNAISSANCE BY ROLLIE
DEMOCRACIES UNSUSTAINABLE COULD STRIKE REDLINING VISITS TO PROFIT
BOOKING WAIT HERE AT MADISON SQUARE GARDEN COUNTY COURTHOUSE WHERE HE
HAD BEEN DONE IN THREE ALREADY IN ANY WAY IN WHICH A TEACHER
m.txt .
Application: Language modeling
One important application of Markov models is to make statistical language models, which are
probability distributions over sequences of words. We deﬁne the state space to be all the words
in English (or some other language). The marginal probabilities p(Xt = k) are called unigram
space) estimated from Darwin’s On The Origin Of Species.
Language models can be used for several things, such as the following:
•
Sentence completion A language model can predict the next word given the previous
words in a sentence. This can be used to reduce the amount of typing required, which is
particularly important for disabled users (see e.g., David Mackay’s Dasher system1), or uses of
mobile devices.
•
Data compression Any density model can be used to deﬁne an encoding scheme, by
assigning short codewords to more probable strings. The more accurate the predictive model,
the fewer the number of bits it requires to store the data.
•
•
Automatic essay writing One can sample from p(x1:t) to generate artiﬁcial text. This is
generates much more semantically plausible text.)Unigrams
_ a b c d e f g h i j k l m n o p q r s t u v w x y z
_
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
Bigrams
Unigram and bigram counts from Darwin’s On The Origin Of Species. The 2D picture on the
right is a Hinton diagram of the joint distribution. The size of the white squares is proportional to the
ngramPlot.
MLE for Markov language models
bility of any particular sequence of length T is given by
p(x1:T |θ)
=
=
K

T

K

K

is a sequence of length Ti, is given by
log p(D|θ)
=
N

log p(xi|θ) =

j
j log πj +

j

k
Njk log Ajk
where we deﬁne the following counts:
j ≜
N

I(xi1 = j), Njk ≜
N

Hence we can write the MLE as the normalized counts:
ˆπj =
j

j
,
ˆAjk =
Njk

k Njk
These results can be extended in a straightforward way to higher order Markov models.
parameters, corresponding to all possible word pairs. It is very unlikely we will see all of these
in our training data. However, we do not want to predict that a particular word string is totally
impossible just because we happen not to have seen it in our training text — that would be a
An alternative to using smart priors is to gather lots and lots of data. For example, Google
...
Although such an approach, based on “brute force and ignorance”, can be successful, it is
rather unsatisfying, since it is clear that this is not how humans learn (see e.g., (Tenenbaum
Empirical Bayes version of deleted interpolation
A common heuristic used to ﬁx the sparse data problem is called deleted interpolation (Chen
green ideas sleep furiously”. We would not want our model to predict that this string is impossible. Even ungrammatical
constructs should be allowed by our model with a certain probability, since people frequently violate grammatical rules,
especially in spoken language.
tails.backoff smoothing; the idea is that if fjk is too small, we “back off” to a more reliable estimate,
namely fk.
tions made by a simple hierarchical Bayesian model. Our presentation follows (McKay and Peto
Aj ∼Dir(α0m1, . . . , α0mK) = Dir(α0m) = Dir(α)
where Aj is row j of the transition matrix, m is the prior mean (satisfying 
The posterior is given by Aj ∼Dir(α + Nj), where Nj = (Nj1, . . . , NjK) is the vector
that records the number of times we have transitioned out of state j to each of the other states.
Nj + α0
= fjkNj + αmk
Nj + α0
where Ajk = E [Ajk|D, α] and
λj =
α
Nj + α0
than backing off to the empirical marginal frequencies fk, we back off to the model parameter
mk.
The only remaining question is: what values should we use for α and m? Let’s use empirical
Bayes. Since we assume each row of the transition matrix is a priori independent given α, the
p(D|α) =

j
B(Nj + α)
B(α)
where Nj = (Nj1, . . . , NjK) are the counts for leaving state j and B(α) is the generalized
beta function.
We can ﬁt this using the methods discussed in (Minka 2000e). However, we can also use the
This says that the prior probability of word k is given by the number of different contexts in
which it occurs, rather than the number of times it occurs. To justify the reasonableness of thisA Markov chain in which we put a different Dirichlet prior on every row of the transition
matrix A, but the hyperparameters of the Dirichlet are shared.
Imagine, you see, that the language, you see, has, you see, a
frequently occuring couplet ’you see’, you see, in which the second
word of the couplet, see, follows the first word, you, with very high
probability, you see. Then the marginal statistics, you see, are going
to become hugely dominated, you see, by the words you and see, with
equal frequency, you see.
for some novel context word not seen before, would turn out to be the same, since the marginal
’You’ appears in many contexts, so P(you|novel) should be high, but ’see’ only follows ’you’, so
for free, since we back off to mk not fk, and mk will be large for ’you’ and small for ’see’ by
While the above smoothing methods handle the case where the counts are small or even zero,
none of them deal with the case where the test set may contain a completely novel word. In
particular, they all assume that the words in the vocabulary (i.e., the state space of Xt) is ﬁxed
and known (typically it is the set of unique words in the training data, or in some dictionary).(a)
(b)
and hence will assign zero probability to a test sentence with an unfamiliar word. (Unfamiliar
words are bound to occur, because the set of words is an open class. For example, the set of
proper nouns (names of people and places) is unbounded.)
A standard heuristic to solve this problem is to replace all novel words with the special symbol
unk, which stands for “unknown”. A certain amount of probability mass is held aside for this
event.
A more principled solution would be to use a Dirichlet process, which can generate a countably
“accepted” as genuine words, then the system has no predictive power, since any misspelling
will be considered a new word. So the novel word has to be seen frequently enough to warrant
Stationary distribution of a Markov chain *
We have been focussing on Markov models as a way of deﬁning joint probability distributions
over sequences. However, we can also interpret them as stochastic dynamical systems, where
we “hop” from one state to another at each time step. In this case, we are often interested in the
long term distribution over states, which is known as the stationary distribution of the chain.
In this section, we discuss some of the relevant theory. Later we will consider two important
What is a stationary distribution?
be the probability of being in state j at time t. It is conventional in this context to assume that
π1(j) =

i
π0(i)Aij
or, in matrix notation,
π1 = π0AWe can imagine iterating these equations. If we ever reach a stage where
π = πA
then we say we have reached the stationary distribution (also called the invariant distribution
or equilibrium distribution). Once we enter the stationary distribution, we will never leave.

π1
π2
π3

=

π1
π2
π3

⎛
⎝
A12
A13
A21
A23
A31
A32
⎞
so
or
π1(A12 + A13) = π2A21 + π3A31
In general, we have
πi

j̸=i
Aij =

j̸=i
πjAji
In other words, the probability of being in state i times the net ﬂow out of state i must equal
the probability of being in each other state j times the net ﬂow from that state into i. These
are called the global balance equations. We can then solve these equations, subject to the
constraint that 
Computing the stationary distribution
To ﬁnd the stationary distribution, we can just solve the eigenvector equation AT v = v, and
of A and AT are the same.) Of course, since eigenvectors are unique only up to constants of
proportionality, we must normalize v at the end to ensure it sums to one.
linear system:
π1
π2
π3

⎛
⎝
−A12
−A21
−A31
−A32
⎞
⎠=since π = πA. See mcStatDist for some Matlab code.
Unfortunately, not all chains have a stationary distribution. as we explain below.
When does a stationary distribution exist? *
the above stationary distributions.
We see from this example that a necessary condition to have a unique stationary distribution
is that the state transition diagram be a singly connected component, i.e., we can get from any
state to any other state. Such chains are called irreducible.
This example motivates the following deﬁnition.
Let us say that a chain has a limiting
ij exists and is independent of i, for all j. If this holds, then
P(Xt = j) =

i
Let us now characterize when a limiting distribution exists. Deﬁne the period of state i to be
where gcd stands for greatest common divisor, i.e., the largest integer that divides all the
is aperiodic if all its states are aperiodic. One can show the following important result:
limiting distribution, which is equal to π, its unique stationary distribution.
A special case of this result says that every regular ﬁnite state chain has a unique stationary
distribution, where a regular chain is one whose transition matrix satisﬁes An
integer n and all i, j, i.e., it is possible to get from any state to any other state in n steps.
Consequently, after n steps, the chain could be in any state, no matter where it started. One
can show that sufficient conditions to ensure regularity are that the chain be irreducible (singly
all integers, or all the uncountable set of all reals), we need to generalize some of the earlierdeﬁnitions. Since the details are rather technical, we just brieﬂy state the main results without
icity, as before. But we also require that each state is recurrent. (A chain in which all states
are recurrent is called a recurrent chain.) Recurrent means that you will return to that state
where you started from. But now consider an example with an inﬁnite state space. Suppose we
it is irreducible.
It should be intuitively obvious that we require all states to be recurrent for a stationary
distribution to exist. However, this is not sufficient. To see this, consider the random walk
In this case, we can return to the origin an
inﬁnite number of times, so the chain is recurrent.
However, it takes inﬁnitely long to do
so.
This prohibits it from having a stationary distribution.
The intuitive reason is that the
distribution keeps spreading out over a larger and larger set of the integers, and never converges
we say a chain is ergodic if all its states are ergodic.
We can now state our main theorem:
bution, which is equal to π, its unique stationary distribution.
Detailed balance
Establishing ergodicity can be difficult. We now give an alternative condition that is easier to
verify.
We say that a Markov chain A is time reversible if there exists a distribution π such that
πiAij = πjAji
These are called the detailed balance equations. This says that the ﬂow from i to j must
equal the ﬂow from j to i, weighted by the appropriate source probabilities.
We have the following important result.
balance wrt distribution π, then π is a stationary distribution of the chain.X1
X2
X3
X4
X5
X6
A very small world wide web. Figure generated by pagerankDemo, written by Tim Davis.
Proof. To see this, note that

i
πiAij =

i
πjAji = πj

i
Aji = πj
and hence π = Aπ.
chain with a stationary distribution which does not satisfy detailed balance).
as input a desired distribution π and construct a transition matrix (or in general, a transition
kernel) A which satisﬁes detailed balance wrt π. Thus by sampling states from such a chain,
we will eventually enter the stationary distribution, and will visit states with probabilities given
by π.
Application: Google’s PageRank algorithm for web page ranking *
We will treat the web as a giant directed graph, where nodes represent web pages (documents)
a few designated root nodes, such as dmoz.org, the home of the Open Directory Project, and
then follows the links, storing all the pages that we encounter, until we run out of time.
Next, all of the words in each web page are entered into a data structure called an inverted
index. That is, for each word, we store a list of the documents where this word occurs. (In
practice, we store a list of hash codes representing the URLs.) At test time, when a user entersa query, we can just look up all the documents containing each word, and intersect these
lists (since queries are deﬁned by a conjunction of search terms). We can get a reﬁned search
by storing the location of each word in each document. We can then test if the words in a
document occur in the same order as in the query.
We
we can create the following inverted index, where each pair represents a document and word
location:
"a":
"is":
"it":
"what":
we retrieve the following documents:
{T0, T1} ∩{T0, T1, T2} ∩{T0, T1, T2} = {T0, T1}
If we require that the word order matches, only document T1 would be returned. More generally,
matches the query’s word order, or to other features, such as if the words occur in the title of
relevance. This is called document ranking.
So far, we have described the standard process of information retrieval. But the link structure
of the web provides an additional source of information. The basic idea is that some web pages
are more authoritative than others, so these should be ranked higher (assuming they match
the query). A web page is an authority if it is linked to by many other pages. But to protect
site to boost its apparent relevance, we will weight each incoming link by the source’s authority.
Thus we get the following recursive deﬁnition for the authoritativeness of page j, also called its
PageRank:
πj =

i
Aijπi
stationary distribution of a Markov chain.
In the simplest setting, we deﬁne Ai. as a uniform distribution over all states that i is
connected to. However, to ensure the distribution is unique, we need to make the chain into a
regular chain. This can be done by allowing each state i to jump to any other state (including
itself) with some small probability. This effectively makes the transition matrix aperiodic and
fully connected (although the adjacency matrix Gij of the web itself is highly sparse).
We discuss efficient methods for computing the leading eigenvector of this giant matrix below.(a)
(b)
We ﬁnd that the stationary distribution is
because being linked to from an inﬂuential nodehelps increase your PageRank score more than
being linked to by a less inﬂuential node.
Efficiently computing the PageRank vector
Now imagine performing a random walk on
uniformly at random. If there are no outlinks, you just jump to a random page. (These random
Hence we can solve for its unique stationary distribution using eigenvector methods.)
This
deﬁnes the following transition matrix:
Mij =
to another without following a link and cj = 
columns sum to one. Note that M = AT in our earlier notation.
We can represent the transition matrix compactly as follows. Deﬁne the diagonal matrix D
with entries
djj =
Deﬁne the vector z with components
zj =

δ
M = pGD + 1zT
The matrix M is not sparse, but it is a rank one modiﬁcation of a sparse matrix. Most of the
explicitly.
Our goal is to solve v = Mv, where v = πT . One efficient method to ﬁnd the leading
eigenvector of a large matrix is known as the power method. This simply consists of repeated
v ∝Mv = pGDv + 1zT v
It is possible to implement the power method without using any matrix multiplications, by
simply sampling from the transition matrix and counting how often you visit each state. This is
essentially a Monte Carlo approximation to the sum implied by v = Mv. Applying this to the
converge, starting from a uniform distribution. (See also the function pagerankDemo, by Tim
Davis, for an animation of the algorithm in action, applied to the small web example.) To handle
For details on how to perform this Monte Carlo power method in a parallel distributed
Web spam
page on 1000s of irrelevant web pages, thus increasing its ranking on Google’s search engine
of them that their effect added up. Businesses call this search engine optimization; Google
calls it web spam. When Google was notiﬁed of this scam (by the New York Times), it manually
downweighted JC Penney, since such behavior violates Google’s code of conduct. The result
Automatically detecting such scams relies on various techniques which are beyond the scope of
this chapter.
Hidden Markov models(a)
(b)
p(xt|zt). The corresponding joint distribution has the form
p(z1:T , x1:T )
=
p(z1:T )p(x1:T |z1:T ) =

p(z1)
T

  T

p(xt|zt)

The observations in an HMM can be discrete or continuous. If they are discrete, it is common
for the observation model to be an observation matrix:
p(xt = l|zt = k, θ) = B(k, l)
If the observations are continuous, it is common for the observation model to be a conditional
Gaussian:
p(xt|zt = k, θ) = N(xt|μk, Σk)
The resulting model is similar to a Gaussian mixture model, except the cluster membership
has Markovian dynamics.
(Indeed, HMMs are sometimes called Markov switching models
We see that we tend to get multiple observations in the same
location, and then a sudden jump to a new cluster.
Applications of HMMs
They have the advantage
mediated via the latent variables.
In particular, note that they do not assume the Markov
inside a generative classiﬁer.
However, it is more common to imbue the hidden states with some desired meaning, and to
then try to estimate the hidden states from the observations, i.e., to compute p(zt|x1:t) if we arex x .
.
.
x
A G -
-
-
C
A -
A G -
C
A G -
A A -
-
-
A A A C
A G -
-
-
C
bat
rat
cat
gnat
goat
.
.
(a)
M
M
End
Begin
M
M
M
I
I
I
I
D
D
D
(b)
Used with kind permission of Richard Durbin.
discussion of the differences between these two approaches). Below we give some examples of
applications which use HMMs in this way:
•
Automatic speech recognition.
Here xt represents features extracted from the speech
represents the language model, and the observation model p(xt|zt) represents the acoustic
•
Activity recognition. Here xt represents features extracted from a video frame, and zt is
the class of activity the person is engaged in (e.g., running, walking, sitting, etc.) See e.g.,
•
Part of speech tagging. Here xt represents a word, and zt represents its part of speechrelated tasks.
•
Gene ﬁnding. Here xt represents the DNA nucleotides (A,C,G,T), and zt represents whether
•
Protein sequence alignment. Here xt represents an amino acid, and zt represents whether
this matches the latent consensus sequence at this location. This model is called a proﬁle
If zt is a match state, then xt is equal to the t’th value of the consensus. If zt is an insert
state, then xt is generated from a uniform distribution that is unrelated to the consensus
sequence. If zt is a delete state, then xt = −. In this way, we can generate noisy copies of
we see various versions of this below. A path through the state transition diagram, shown
most probable path is D, D, I, I, I, M. This means we delete the A and G parts of the
model parameters by counting the number of such transitions, and the number of emissions
Inference in HMMs
We now discuss how to infer the hidden state sequence of an HMM, assuming the parameters
of parameter estimation.
Types of inference problems for temporal models
There are several different kinds of inferential tasks for an HMM (and SSM in general).
To
illustrate the differences, we will consider an example called the occasionally dishonest casino,
up, and zt represents the identity of the dice that is being used. Most of the time the casino
this model, we may observe data such as the following:
Example output of casinoDemo
Rolls:
Die:
LLLLLLLLLLLLLLFFFFFFLLLLLLLLLLLLLLFFFFFFFFFFFFFFFFFFLLLLLLLL
Here “rolls” refers to the observed symbol and “die” refers to the hidden state (L is loaded and
F is fair). Thus we see that the model generates a sequence of symbols, but the statistics of the





























An HMM for the occasionally dishonest casino. The blue arrows visualize the state transition
roll number
p(loaded)
filtered
(a)
roll number
p(loaded)
smoothed
(b)
roll number
Viterbi
(c)
Inference in the dishonest casino. Vertical gray bars denote the samples that we generated
using a loaded die. (a) Filtered estimate of probability of using a loaded dice. (b) Smoothed estimates. (c)
MAP trajectory. Figure generated by casinoDemo.
distribution changes abruptly every now and then. In a typical application, we just see the rolls
and want to infer which dice is being used. But there are different kinds of inference, which we
summarize below.
•
Filtering means to compute the belief state p(zt|x1:t) online, or recursively, as the data
streams in. This is called “ﬁltering” because it reduces the noise more than simply estimating
the hidden state using just the current estimate, p(zt|xt). We will see below that we can
an example.
•
for an example. By conditioning on past and future data, our uncertainty will be signiﬁcantly
mitted a crime. As he moves through the crime scene, his uncertainty is high until he ﬁnds
the key clue; then he has an “aha” moment, his uncertainty is reduced, and all the previously
confusing observations are, in hindsight, easy to explain.ILOWHULQJ
SUHGLFWLRQ
IL[HGODJ
VPRRWKLQJ
IL[HGODJ
VPRRWKLQJ

RIIOLQH

W
W
W
W
K
O
which we have data. The arrow represents the time step at which we want to perform inference. t is the
current time, T is the sequence length, ℓis the lag and h is the prediction horizon. See text for details.
•
Fixed lag smoothing is an interesting compromise between online and offline estimation; it
than ﬁltering, but incurs a slight delay. By changing the size of the lag, one can trade off
accuracy vs delay.
•
Prediction Instead of predicting the past given the future, as in ﬁxed lag smoothing, we
=


zt
It is straightforward to perform this computation: we just power up the transition matrix and
hidden states; it can be converted into a prediction about future observations using

smoothing, and prediction.
•
able state sequence.
In the context of HMMs, this is known as Viterbi decoding (seedecoding for the occasionally dishonest casino HMM. We see that the smoothed (offline)
estimate is indeed smoother than the ﬁltered (online) estimate. If we threshold the estimates
•
Posterior samples If there is more than one plausible interpretation of the data, it can be
useful to sample from the posterior, z1:T ∼p(z1:T |x1:T ). These sample paths contain much
more information than the sequence of marginals computed by smoothing.
•
Probability of the evidence We can compute the probability of the evidence, p(x1:T ),
by summing up over all hidden paths, p(x1:T ) = 
z1:T p(z1:T , x1:T ). This can be used to
clustering, for anomaly detection, etc.
The forwards algorithm
We now describe how to recursively compute the ﬁltered marginals, p(zt|x1:t) in an HMM.
The algorithm has two steps.
First comes the prediction step, in which we compute the
=

i
Next comes the update step, in which we absorb the observed data from time t using Bayes
rule:
αt(j)
≜
=
Zt
p(xt|zt = j,
where the normalization constant is given by
Zt
≜

j
vector notation, we can write the update in the following simple form:
the transition matrix, and u ⊙v is the Hadamard product, representing elementwise vectorIn addition to computing the hidden states, we can use this algorithm to compute the log
probability of the evidence:
log p(x1:T |θ) =
T

T

log Zt
(We need to work in the log domain to avoid numerical underﬂow.)
ψt(j) = p(xt|zt = j), initial state distribution π(j) = p(z1 = j);
t log Zt;
j uj;
online inference. We now discuss how to compute the smoothed marginals, p(zt = j|x1:T ),
using offline inference.
Basic idea
The key decomposition relies on the fact that we can break the chain into two parts, the past
and the future, by conditioning on zt:
x1:t)
Let αt(j) ≜p(zt = j|x1:t) be the ﬁltered belief state as before. Also, deﬁne
as the conditional likelihood of future evidence given that the hidden state at time t is j.
(Note that this is not a probability distribution over states, since it does not need to satisfy

γt(j) ≜p(zt = j|x1:T )
γt(j) ∝αt(j)βt(j)=
=

j
=

j

=

j
=

j
βt(j)ψt(j)ψ(i, j)
The base case is
Having computed the forwards and backwards messages, we can combine them to compute
The pseudo code is very similar to the forwards case; see hmmFwdBack for an implementation.
We can think of this algorithm as passing “messages” from left to right, and then from right
when we discuss belief propagation.
need to compute the expected number of transitions from state i to state j:
Nij =


computed as follows
≜
∝
∝
∝
=Time and space complexity
It is clear that a straightforward implementation of FB takes O(K2T) time, since we must
perform a K × K matrix multiplication at each step. For some applications, such as speech
recognition, K is very large, so the O(K2) term becomes prohibitive.
Fortunately, if the
transition matrix is sparse, we can reduce this substantially.
transition matrix, the algorithm takes O(TK) time.
In some cases, we can exploit special properties of the state space, even if the transition
matrix is not sparse. In particular, suppose the states represent a discretization of an underlying
backwards algorithm in O(TK log K) time. This is very useful for models with large state
In some cases, the bottleneck is memory, not time. The expected sufficient statistics needed
by EM are 
space complexity from O(KT) to O(K log T) at the cost of increasing the running time from
The Viterbi algorithm
z∗= arg max
z1:T p(z1:T |x1:T )
where the nodes are possible states at each time step, and the node and edge weights are log
probabilities. That is, the weight of a path z1, z2, . . . , zT is given by
log π1(z1) + log φ1(z1) +
T

MAP vs MPE
Before discussing how the algorithm works, let us make one important remark: the (jointly) most
probable sequence of states is not necessarily the same as the sequence of (marginally) most probable
given by the maximizer of the posterior marginals or MPM:
ˆz = (arg max
z1 p(z1|x1:T ), . . . , arg max
zT p(zT |x1:T ))






















As a simple example of the difference, consider a chain with two time steps, deﬁning the
following joint:
The advantage of the joint MAP estimate is that is is always globally consistent. For example,
suppose we are performing speech recognition and someones says “recognize speech”. This
probable interpretation of that particular window of sound, but when we add the requirement
that the data be explained by a single linguistically plausible path, this interpretation becomes
less likely.
why, note that in Viterbi, when we estimate zt, we “max out” the other variables:
z∗
t = arg max
zt
max
p(zt|x1:T ) =

For example, consider a joint distribution where we observeDetails of the algorithm
Freeman 2001b), but in general, it can lead to incorrect results if there are multiple equally
probably joint assignments. The reasons is that each node breaks ties independently and hence
may do so in a manner that is inconsistent with its neighbors. The Viterbi algorithm is therefore
product, but the backwards pass uses a traceback procedure to recover the most probable path
through the trellis of states. Essentially, once zt picks its most probable state, the previous nodes
condition on this event, and therefore they will break ties consistently.
In more detail, deﬁne
δt(j)
≜
max
This is the probability of ending up in state j at time t, given that we take the most probable
path. The key insight is that the most probable path to state j at time t must consist of the
Hence
δt(j)
=
max
i
We also keep track of the most likely previous state, for each possible state that we end up in:
at(j)
=
argmax
i
That is, at(j) tells us the most likely previous state on the most probable path to zt = j. We
initialize by setting
δ1(j) = πjφ1(j)
and we terminate by computing the most probable ﬁnal state z∗
T :
z∗
T = arg max
i
δT (i)
We can then compute the most probable sequence of states using traceback:
z∗
v and we want to query q; let n be the remaining nuisance variables.
We deﬁne the MAP estimate as x∗
q =
arg maxxq

xn p(xq, xn|xv), where we max over xq and sum over xn.
By contrast, we deﬁne the MPE or
most probable explanation as (x∗
q, x∗
n) = arg maxxq,xn p(xq, xn|xv), where we max over both xq and xn. This
n = ∅. However, if n ̸= ∅, then summing out the nuisance variables can give different results than maxing them out.
Summing out nuisance variables is more sensible, but computationally harder, because of the need to combine max and&



&



&



&



&



&



&



(a)























(b)
for a single phone. We are visualizing the state transition diagram. We assume the observations have been
applied to this model, with data sequence C1, C3, C4, C6. The columns represent time, and the rows
we can also easily work in the log domain. The key difference is that log max = max log,
whereas log  ̸=  log. Hence we can use
≜
max
=
max
i
In the case of Gaussian observation models, this can result in a signiﬁcant (constant factor)
dimensional Gaussian. This is one reason why the Viterbi algorithm is widely used in the E step
Example=
=
the latter is more probable. Hence this is the only one we have to remember. The algorithm
continues in this way until we have reached the end of the sequence. One we have reached the
Time and space complexity
The time complexity of Viterbi is clearly O(K2T) in general, and the space complexity
If the transition matrix has the form
The Viterbi algorithm returns one of the most probable paths. It can be extended to return the
list. Once can then use a discriminative method to rerank the paths based on global features
derived from the fully observed state sequence (as well as the visible features). This technique
is widely used in speech recognition. For example, consider the sentence “recognize speech”. It
is possible that the most probable interpretation by the system of this acoustic signal is “wreck
a nice speech”, or maybe “wreck a nice beach”. Maybe the correct interpretation is much lower
of the correct interpretation based on a more global context.
rather than representing qualitatively different interpretations of the data. Instead we might want
to generate a more diverse set of paths to more accurately represent posterior uncertainty. One
way to do this is to sample paths from the posterior, as we discuss below. For some other ways
Forwards ﬁltering, backwards sampling
It is often useful to sample paths from the posterior:
zs
the initial pair of states, z∗
t ∼p(zt|z∗
forwards sampling pass. An alternative is to do the forwards pass, and then perform samplingin the backwards pass. The key insight into how to do this is that we can write the joint from
right to left using
p(z1:T |x1:T ) = p(zT |x1:T )

We can then sample zt given future sampled states using
zs
t
∼
The sampling distribution is given by
=

=
∝

zt,
=
x1:t)p(zt|x1:t)
=
The base case is
zs
T ∼p(zT = i|x1:T ) = αT (i)
as we will see below.
Learning for HMMs
z1:T is observed in the training set, and then the harder case where z1:T is hidden.
Training with fully observed data
If we observe the hidden state sequences, we can compute the MLEs for A and π exactly as in
The details on how to estimate B depend on the form of the observation model.
The
situation is identical to ﬁtting a generative classiﬁer. For example, if each state has a multinoulli
represents the observed symbol, the MLE is given by
ˆBjl =
N X
jl
Nj
, N X
jl ≜
N

Ti

I(zi,t = j, xi,t = l)This result is quite intuitive: we simply add up the number of times we are in state j and we
see a symbol l, and divide by the number of times we are in state j.
ˆμk = xk
Nk
,
ˆΣk = (xx)T
k −Nk ˆμk ˆμT
k
Nk
where the sufficient statistics are given by
xk
≜
N

Ti

I(zi,t = k)xi,t
(xx)T
k
≜
N

Ti

I(zi,t = k)xi,txT
i,t
Analogous results can be derived for other kinds of distributions. One can also easily extend all
of these results to compute MAP estimates, or even full posteriors over the parameters.
If the zt variables are not observed, we are in a situation analogous to ﬁtting a mixture model.
The most common approach is to use the EM algorithm to ﬁnd the MLE or MAP parameters,
E step
It is straightforward to show that the expected complete data log likelihood is given by
Q(θ, θold)
=
K

E

k

log πk +
K

K

E [Njk] log Ajk
+
N

Ti

K

p(zt = k|xi, θold) log p(xi,t|φk)
where the expected counts are given by
E

k

=
N

p(zi1 = k|xi, θold)
E [Njk]
=
N

Ti

E [Nj]
=
N

Ti

p(zi,t = j|xi, θold)rithm on each sequence. In particular, this algorithm computes the following smoothed node
and edge marginals:
γi,t(j)
≜
ξi,t(j, k)
≜
M step
counts:
ˆAjk =
E [Njk]

k′ E [Njk′], ˆπk = E

k

N
This result is quite intuitive: we simply add up the expected number of transitions from j to k,
and divide by the expected number of times we transition from j to anything else.
For a multinoulli observation model, the expected sufficient statistics are
E [Mjl]
=
N

Ti

γi,t(j)I(xi,t = l) =
N


γi,t(j)
The M step has the form
ˆBjl = E [Mjl]
E [Nj]
This result is quite intuitive: we simply add up the expected number of times we are in state j
and we see a symbol l, and divide by the expected number of times we are in state j.
For a Gaussian observation model, the expected sufficient statistics are given by
E [xk]
=
N

Ti

γi,t(k)xi,t
E

(xx)T
k

=
N

Ti

γi,t(k)xi,txT
i,t
The M step becomes
ˆμk = E [xk]
E [Nk],
ˆΣk = E

(xx)T
k

−E [Nk] ˆμk ˆμT
k
E [Nk]
This can (and should) be regularized in the same way we regularize GMMs.
Initialization
As usual with EM, we must take care to ensure that we initialize the parameters carefully, to
minimize the chance of getting stuck in poor local optima. There are several ways to do this,
such as•
Use some fully labeled data to initialize the parameters.
•
Initially ignore the Markov dependencies, and estimate the observation parameters using the
•
Randomly initialize the parameters, use multiple restarts, and pick the best solution.
EM for GMMs, so it is common to initialize EM for HMMs using Viterbi training, which
means approximating the posterior over paths with the single most probable path. (This is not
necessarily a good idea, since initially the parameters are often poorly estimated, so the Viterbi
switch to Viterbi near convergence.)
Bayesian methods for “ﬁtting” HMMs *
EM returns a MAP estimate of the parameters. In this section, we brieﬂy discuss some methods
for Bayesian parameter estimation in HMMs. (These methods rely on material that we will cover
later in the book.)
One approach is to use variational Bayes EM (VBEM), which we discuss in general terms in
plug in the posterior mean parameters instead of the MAP estimates. The M step updates the
parameters of the conjugate posteriors, instead of updating the parameters themselves.
An alternative to VBEM is to use MCMC. A particularly appealing algorithm is block Gibbs
parameters from their posteriors, conditional on the sampled latent paths. This is simple to
implement, but one does need to take care of unidentiﬁability (label switching), just as with
Discriminative training
Sometimes HMMs are used as the class conditional density inside a generative classiﬁer. In this
case, p(x|y = c, θ) can be computed using the forwards algorithm. We can easily maximize the
joint likelihood N
However, we might like to ﬁnd the parameters that maximize the conditional likelihood
N

p(yi|xi, θ) =

i
p(yi|θ)p(xi|yi, θ)

c p(yi = c|θ)p(xi|c, θ)
This is more expensive than maximizing the joint likelihood, since the denominator couples all Cto generic gradient based methods. Nevertheless, discriminative training can result in improved
accuracies. The standard practice in speech recognition is to initially train the generative models
Model selection
In HMMs, the two main model selection issues are: how many states, and what topology to use
for the state transition diagram. We discuss both of these issues below.
Choosing the number of hidden states
Choosing the number of hidden states K in an HMM is analogous to the problem of choosing
the number of mixture components. Here are some possible solutions:
•
•
slow and is not widely used.
•
Use variational Bayes to “extinguish” unwanted components, by analogy to the GMM case
•
Use an “inﬁnite HMM”, which is based on the hierarchical Dirichlet process. See e.g., (Beal
Structure learning
The term structure learning in the context of HMMs refers to learning a sparse transition
matrix. That is, we want to learn the structure of the state transition diagram, not the structure
of the graphical model (which is ﬁxed). A large number of heuristic methods have been proposed.
Most alternate between parameter estimation and some kind of heuristic split merge method
Alternatively, one can pose the problem as MAP estimation using a minimum entropy prior,
of the form
p(Ai,:) ∝exp(−H (Ai,:))
This prior prefers states whose outgoing distribution is nearly deterministic, and hence has low
methods can be used. The trouble with this is that we might prune out all incoming transitions
Generalizations of HMMs
Many variants of the basic HMM model have been proposed. We brieﬂy discuss some of them
below.Dt
Qt
Xt
In a standard HMM, the probability we remain in state i for exactly d steps is
ii ∝exp(d log Aii)
kind of exponentially decaying function of d is sometimes unrealistic.
Markov because to predict the next state, it is not sufficient to condition on the past state: we
also need to know how long we’ve been in that state. When the state space is not observed
or an explicit duration HMM.
HSMMs are widely used in many gene ﬁnding programs, since the length distribution of
HSMMs are useful not only because they can model the waiting time of each state more
accurately, but also because they can model the distribution of a whole batch of observations at
once, instead of assuming all observations are conditionally iid. That is, they can use likelihood
duration in state k is for l time steps. This is useful for modeling data that is piecewise linear,
HSMM as augmented HMMs
ﬁgure, we have assumed the observations are iid within each state, but this is not required,
the maximum duration of any state.
When we ﬁrst enter state j, we sample Dt from the
duration distribution for that state, Dt ∼pj(·). Thereafer, Dt deterministically counts downp
p
p
p
(a)
 
 
(b)
stochastic transition to a new state.
More precisely, we deﬁne the CPDs as follows:
=
⎧
⎨
⎩
pj(d′)
otherwise
=
⎧
⎨
⎩
Ajk
otherwise
of parametric distribution, such as a Gamma distribution. If pj(d) is a geometric distribution,
this emulates a standard HMM.
this is rather inefficient, since Dt is deterministic. It is possible to marginalize Dt out, and derive
Unfortunately, all these methods take O(TK2D) time, where T is the sequence length, K is
the number of states, and D is the maximum duration of any state.
state with n new states, each with the same emission probabilities as the original state. For
multiplying by the number of possible paths we ﬁnd that the total probability of a path of length
d is
p(d) =

	need
on
the
words
phones
phones
aa
n
end
n
iy
d
dh
n
ax
iy
end
end
end
end
represents bigram word probabilities. The middle level represents the phonetic spelling of each word. The
This is equivalent to the negative binomial distribution.
backwards on this model takes O(T(KE)Fin) time, where Fin is the average number of
as gene ﬁnding, which also often uses HSMMs.) Since Fin + D ≫EFin, the expanded state
Hierarchical HMMs
automatic speech recognition. The phone and subphone models can be “called” from different
higher level contexts. We can always “ﬂatten” an HHMM to a regular HMM, but a factored
representation is often easier to interpret, and allows for more efficient inference and model
ﬁtting.
less expressive than stochastic context free grammars (SCFGs), since they only allow hierarchies
of bounded depth, but they support more efficient inference. In particular, inference in SCFGs
t
represents the state at time t and level ℓ. A state transition at level ℓis only “allowed” if theQ1
Q1
Q1
Q2
Q2
Q2
Q3
Q3
Q3
Y1
Y2
Y3
An HHMM represented as a DGM. Qℓ
t is the state at time t, level ℓ; F ℓ
level ℓhas ﬁnished (entered its exit state), otherwise F ℓ
nodes are hidden. We may optionally clamp F ℓ
to ensure all models have ﬁnished by the end of the sequence.
t
node. (The chain below
ﬁnishes when it chooses to enter its end state.) This mechanism ensures that higher level chains
evolve more slowly than lower level chains, i.e., lower levels are nested within higher levels.
A variable duration HMM can be thought of as a special case of an HHMM, where the top
level is a deterministic counter, and the bottom level is a regular HMM, which can only change
a conditional density model for sequences of the form
p(y1:T , z1:T |u1:T , θ)
where ut is the input at time t; this is sometimes called a control signal. If the inputs and
outputs are continuous, a typical parameterization would be
=
Cat(zt|S(Wiut))
p(yt|xt, zt = j, θ)
=
N(yt|Vjut, Σj)
Thus the transition matrix is a logistic regression model whose parameters depend on the
previous state. The observation model is a Gaussian whose parameters depend on the currentyt
zt
ut
(a)
x1
x2
xT
z1
z2
zT
(b)
(c)
matrix) can change, although this is not shown.
state. The whole model can be thought of as a hidden version of a maximum entropy Markov
backwards algorithm to estimate the hidden states. It is also straightforward to derive an EM
The standard HMM assumes the observations are conditionally independent given the hidden
state. In practice this is often not the case. However, it is straightforward to have direct arcs from
HMM, or a regime switching Markov model. For continuous data, the observation model
becomes
This is a linear regression model, where the parameters are chosen according to the current
observations:
L

Wj,ℓxt−ℓ+ μj, Σj)
for discrete observations.x1
x2
x3
(a)
x31
x32
x33
z31
z32
z33
x21
x22
x23
z21
z22
z23
x11
x12
x13
z11
z12
z13
(b)
E step is unchanged, as is the M step for the transition matrix. If we assume scalar observations
for notational simplicty, the M step involves minimizing

t
E

σ2(st)(yt −yT

Focussing on the w terms, we see that this requires solving K weighted least squares problems:
J(w1:K) =

j

t
γt(j)
σ2(j)(yt −yT
where γt(j) = p(zt = k|x1:T ) is the smoothed posterior marginal. This is a weighted linear
regression problem, where the design matrix has a Toeplitz form. This subproblem can be solved
is called a dynamic Bayesian multi net, since it is a mixture of different networks.
In the
regression matrices, Wj, and we can change the structure of the connections within the
components of xt by using sparse Gaussian graphical models, either directed or undirected. See
Factorial HMMchain would represent speaking style, another the words that are being spoken.
Unfortunately, conditioned on xt, all the hidden variables are correlated (due to explaining
away the common observed child xt). This make exact state estimation intractable. However,
Coupled HMM and the inﬂuence model
neighboring chains. That is, we represent the joint conditional distribution as
=

c
=
The trouble with the above model is that it requires O(CK4) parameters to specify, if there
are C chains with K states per chain, because each state depends on its own past plus the
past of its two neighbors. There is a closely related model, known as the inﬂuence model
as
=
C

where 
matrices. The αc,c′ parameter speciﬁes how much inﬂuence chain c has on chain c′. This
model only takes O(C2 + CK2) parameters to specify. Furthermore, it allows each chain to
be inﬂuenced by all the other chains, not just its nearest neighbors. (Hence the corresponding
all the previous nodes.) This has been used for various tasks, such as modeling conversational
become fully correlated even if the interaction graph is sparse. Various approximate inference
methods can be applied, as we discuss later.
Dynamic Bayesian networks (DBNs)
A dynamic Bayesian network is just a way to represent a stochastic process using a directed
on the context. Geoff Hinton (who invented the term “deep belief network”) has suggested the acronyms DyBN and
DeeBN to avoid this ambiguity.SensorValid1
FYdotDiff1
FcloseSlow1
Xdot0
Xdot1
InLane0
InLane1
LeftClr0
LeftClr1
RightClr0
RightClr1
LatAction0
LatAction1
FwdAction0
FwdAction1
Ydot0
Ydot1
Stopped0
Stopped1
BXdot1
EngStatus0
EngStatus1
BcloseFast1
FrontBackStatus0
FrontBackStatus1
BYdotDiff1
Fclr1
Bclr1
XdotSens1
YdotSens1
LeftClrSens1
RightClrSens1
TurnSignal1
FYdotDiffSens1
FclrSens1
BXdotSens1
BclrSens1
BYdotDiffSens1
slice t
evidence
The BATnet DBN. The transient nodes are only shown for the second slice, to minimize
clutter. The dotted lines can be ignored. Used with kind permission of Daphne Koller.
rather it is a network representation of a dynamical system. All of the HMM variants we have
seen above could be considered to be DBNs. However, we prefer to reserve the term “DBN”
main problem is that exact inference can be computationally expensive, because all the hidden
variables become correlated over time (this is known as entanglement — see e.g., (Koller and
tractable exact inference.
However, later we will see algorithms that can exploit the graph
structure for efficient approximate inference.
Exercises
reversed transition matrix. Then show how to compute the posterior marginals γt(i) = p(St = i|x1:T )from the backwards ﬁltered messages rt(i), the forwards ﬁltered messages αt(i), and the stationary
distribution Πt(i).
Consider an HMM where the observation model has the form
p(xt|zt = j, θ)
=


k
wjkN(xt|μjk, Σjk)
•
Draw the DGM.
•
Derive the E step.
•
Derive the M step.
full covariance matrix for KM values (where M is the number of mixture components per hidden state),
Gaussians, and to let the state inﬂuence the mixing weights but not the means and covariances. This is
•
Draw the corresponding graphical model.
•
Derive the E step.
•
Derive the M step.State space models
Introduction
A state space model or SSM is just like an HMM, except the hidden states are continuous. The
model can be written in the following generic form:
zt
=
yt
=
h(zt, ut, δt)
where zt is the hidden state, ut is an optional input or control signal, yt is the observation, g
is the observation noise at time t. We assume that all parameters of the model, θ, are known;
if not, they can be included into the hidden state, as we discuss below.
One of the primary goals in using SSMs is to recursively estimate the belief state, p(zt|y1:t, u1:t, θ).
this later in this chapter. We will also discuss how to convert our beliefs about the hidden state
words, we assume
•
The transition model is a linear function
•
The observation model is a linear function
yt = Ctzt + Dtut + δt
•
The system noise is Gaussian
•
The observation noise is Gaussian
stationary. 
 
observed
truth
(a)
 
 
observed
filtered
(b)
 
 
observed
smoothed
(c)
Illustration of Kalman ﬁltering and smoothing. (a) Observations (green cirles) are generated
by an object moving to the right (true location denoted by black squares). (b) Filtered estimated is shown
the posterior covariance. For clarity, we only plot the ellipses every other time step. (c) Same as (b), but
using offline Kalman smoothing. Figure generated by kalmanTrackingDemo.
seen any data.
Σt|t = Σt.) We can compute these quantities efficiently using the celebrated Kalman ﬁlter,
But before discussing algorithms, we discuss some important
applications.
Applications of SSMs
SSMs have many applications, some of which we discuss in the sections below. We mostly
widely used.
SSMs for object tracking
One of the earliest applications of Kalman ﬁltering was for tracking objects, such as airplanes
and missiles, from noisy measurements, such as radar. Here we give a simpliﬁed example to
illustrate the key ideas. Consider an object moving in a 2D plane. Let z1t and z2t be the
horizontal and vertical locations of the object, and ˙z1t and ˙z2t be the corresponding velocity.
zT
t =
z1t
z2t
˙z1t
˙z2t

.Let us assume that the object is moving at constant velocity, but is “perturbed” by random
Gaussian noise (e.g., due to the wind). Thus we can model the system dynamics as follows:
zt
=
⎛
⎜
⎜
⎝
z1t
z2t
˙z1t
˙z2t
⎞
⎟
⎟
⎠
=
⎛
⎜
⎜
⎝
Δ
Δ
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠+
⎛
⎜
⎜
⎝
ϵ1t
ϵ2t
ϵ3t
ϵ4t
⎞
⎟
⎟
⎠
according to Newton’s laws, but is subject to random changes in velocity.
represent our observation, which we assume is subject to Gaussian noise. We can model this as
follows:
yt
=
Ctzt + δt

y1t
y2t
	
=

	
⎛
⎜
⎜
⎝
z1t
z2t
˙z1t
˙z2t
⎞
⎟
⎟
⎠+
⎛
⎜
⎜
⎝
δ1t
δ2t
δ3t
δ4t
⎞
⎟
⎟
⎠
Finally, we need to specify our initial (prior) beliefs about the state of the object, p(z1). We
perform sequential Bayesian updating to compute p(zt|y1:t) using an algorithm known as the
at each time step (think of “blips” on a radar screen). We observe these blips and ﬁlter out
the noise by using the Kalman ﬁlter. At every step, we have p(zt|y1:t), from which we can
compute p(z1t, z2t|y1:t) by marginalizing out the dimensions corresponding to the velocities.
(This is easy to do since the posterior is Gaussian.) Our “best guess” about the location of the
see that our uncertainty goes down over time, as the effects of the initial uncertainty get “washed
out”. We also see that the estimated trajectory has “ﬁltered out” some of the noise. To obtain
Robotic SLAM
Consider a robot moving around an unknown 2d world. It needs to learn a map and keep
track of its location within that map. This problem is known as simultaneous localization andL2
Y1
Y2
YT
Y1
Y3
X1
X2
X3
. . .
XT
L1
Illustration of graphical model underlying SLAM. Li is the ﬁxed location of landmark i, xt
Robot pose
(a)
(b)
Illustration of the SLAM problem. (a) A robot starts at the top left and moves clockwise in a
circle back to where it started. We see how the posterior uncertainty about the robot’s location increases
and then decreases as it returns to a familar location, closing the loop. If we performed smoothing, this
new information would propagate backwards in time to disambiguate the entire trajectory. (b) We show the
precision matrix, representing sparse correlations between the landmarks, and between the landmarks and
the robot’s position (pose). This sparse precision matrix can be visualized as a Gaussian graphical model,
as shown.
Koller.mapping, or SLAM for short, and is widely used in mobile robotics, as well as other applications
such as indoor navigation using cellphones (since GPS does not work inside buildings).
Let us assume we can represent the map as the 2d locations of a ﬁxed set of K landmarks,
denote them by L1, . . . , LK (each is a vector in R2). For simplicity, we will assume these are
uniquely identiﬁable. Let xt represent the unknown location of the robot at time t. We deﬁne
the state space to be zt = (xt, L1:K); we assume the landmarks are static, so their motion
model is a constant, and they have no system noise. If yt measures the distance from xt to
the set of closest landmarks, then the robot can update its estimate of the landmark locations
Over time, the uncertainty in the robot’s location will increase, due to wheel slippage etc.,
but when the robot returns to a familiar location, its uncertainty will decrease again. This is
representing cov [xt|y1:t, u1:t], grow and then shrink. (Note that in this section, we assume that
a human is joysticking the robot through the environment, so u1:t is given as input, i.e., we do
t , since that
matrix correspond to absent edges in the corresponding undirected Gaussian graphical model
prior on L), so the GGM is a disconnected graph, and Λt is diagonal. However, as the robot
moves about, it will induce correlation between nearby landmarks. Intuitively this is because the
robot is estimating its position based on distance to the landmarks, but the landmarks’ locations
nodes. As a consequence of the precision matrix becoming denser, exact inference takes O(K3)
time. (This is an example of the entanglement problem for inference in DBNs.) This prevents
the method from being applied to large maps.
There are two main solutions to this problem. The ﬁrst is to notice that the correlation pattern
The remaining correlations
become weaker over time. Consequently we can dynamically “prune out” weak edges from
A second approach is to notice that, conditional on knowing the robot’s path, x1:t, the
landmark locations are independent. That is, p(L|x1:t, y1:t) = K
forms the basis of a method known as FastSLAM, which combines Kalman ﬁltering and particlext
yt
θt
(a)
time
weights
online linear regression
 
 
w0
w1
w0 batch
w1 batch
(b)
(a) A dynamic generalization of linear regression. (b) Illustration of the recursive least squares
algorithm applied to the model p(y|x, θ) = N(y|w0 + w1x, σ2). We plot the marginal posterior of w0
and w1 vs number of data points. (Error bars represent E [wj|y1:t] ±

var [wj|y1:t].) After seeing all
the data, we converge to the offline ML (least squares) solution, represented by the horizontal lines. Figure
generated by linregOnlineDemoKalman.
Online parameter learning using recursive least squares
We can perform online Bayesian inference for the parameters of various statistical models using
regression.
The basic idea is to let the hidden state represent the regression parameters, and to let the
prior to be p(θ) = N(θ|θ0, Σ0). (If we want to do online ML estimation, we can just set
Σ0 = ∞I.) Let the hidden state be zt = θ; if we assume the regression parameters do not
change, we can set At = I and Qt = 0I, so
t , and Rt = σ2, so the
N(yt|Ctzt, Rt) = N(yt|xT
Applying the Kalman ﬁlter to this model provides a way to update our posterior beliefs about
the parameters as the data streams in. This is known as the recursive least squares or RLS
algorithm.
Kalman update for the posterior mean has the formKt = ΣtCT
t .
Hence the update for the
parameters becomes
σ2 Σt|t(yt −xT
If we approximate
ηt to ensure convergence to the MLE. Furthermore, the algorithm may take multiple passes
SSM for time series forecasting *
At ﬁrst sight, it might not be apparent why SSMs are useful, since the goal in forecasting is
to predict future visible variables, not to estimate hidden states of some system. Indeed, most
time series is to create a generative model of the data in terms of latent processes, which capture
different aspects of the signal. We can then integrate out the hidden variables to compute the
posterior predictive of the visibles.
observed data. This is called a structural time series model. Below we explain some of the
basic building blocks.
Local level model
The simplest latent process is known as the local level model, which has the form
yt
=
at + ϵy
t ,
ϵy
at
=
t ,
ϵa
examples.at
yt
(a)
 
 
(b)
ssmTimeSeriesSimple.
at
bt
yt
(a)
 
 
(b)
generated by ssmTimeSeriesSimple.
Local linear trend
Many time series exhibit linear trends upwards or downwards, at least locally. We can model
this by letting the level at change by an amount bt at each step as follows:
yt
=
at + ϵy
t ,
ϵy
at
=
t ,
ϵa
bt
=
t,
ϵb
A =

	
, C =


, Q =
Qa
Qb
	at
bt
yt
c1
c1
t
c2
c2
t
c3
c3
t
 
 
Seasonality
adding a latent process consisting of a series offset terms, ct, which sum to zero (on average)
over a complete cycle of S steps:
ct = −

ct−s + ϵc
t, ϵc
ARMA models *
xt =
p

αixt−i +
q

βjwt−j + vtx1
x2
x3
x4
(a)
w1
w3
w2
x1
x2
x3
x4
(b)
w1
w3
w2
x1
x2
x3
x4
(c)
model.
Used with kind permission of Myung Choi.
hidden common causes, which induces dependencies between adjacent time steps. This models
captures correlation at short and long time scales.
ARMA approach. In addition, it allows the parameters to evolve over time, which makes the
which is analogous to the forwards algorithm for HMMs. We then consider the offline case,
The Kalman ﬁltering algorithm
models. We will represent the marginal posterior at time t by
p(zt|y1:t, u1:t) = N(zt|μt, Σt)
Since everything is Gaussian, we can perform the prediction and update steps in closed form,
as we explain below.
The resulting algorithm is the Gaussian analog of the HMM ﬁlter inPrediction step
The prediction step is straightforward to derive:
=


=
≜
≜
AtΣt−1AT
t + Qt
Measurement step
The measurement step can be computed using Bayes rule, as follows
p(zt|y1:t, ut)
=
N(zt|μt, Σt)
μt
=
Σt
=
tion and the actual observation:
rt
≜
yt −ˆyt
ˆyt
≜
and Kt is the Kalman gain matrix, given by
Kt
≜
Σt|t−1CT
t
where
St
≜
=
E


=
CtΣt|t−1CT
t + Rt
sources. Note that by using the matrix inversion lemma, the Kalman gain matrix can also be
written as
We now have all the quantities we need to implement the algorithm; see kalmanFilter for
some Matlab code.
Let us try to make sense of these equations. In particular, consider the equation for thecorrection factor, which is Kt times the error signal rt. The amount of weight placed on the
t , which
is the ratio between the covariance of the prior (from the dynamic model) and the covariance
small, and we will place little weight on the correction term. Conversely, if we have a weak prior
correction term.
Marginal likelihood
log p(y1:T |u1:T ) =

t
where
Posterior predictive
=


=
This is useful for time series forecasting.
Computational issues
There are two dominant costs in the Kalman ﬁlter: the matrix inversion to compute the Kalman
the latter cost dominates. However, in such cases, we can sometimes use sparse approximations
In cases where |yt| ≫|zt|, we can precompute Kt, since, suprisingly, it does not depend on
the actual observations y1:t (an unusual property that is speciﬁc to linear Gaussian systems).
The iterative equations for updating Σt are called the Ricatti equations, and for time invariant
sons of numerical stability. One approach is the information ﬁlter, which recursively updates
t
composition or the UtDtUt decomposition of Σt. This is much more numerically stable thanDerivation *
We now derive the Kalman ﬁlter equations. For notational simplicity, we will ignore the input
is given by
t
=
t Ct
Σt
=
t (Rt + CtΣt|t−1CT
=
μt
=
We will now massage this into the form stated earlier. Applying the second matrix inversion
t yt
=
t yt
=
Σt|t−1CT
t (Rt + CtΣt|t−1CT
t )−1yt = Ktyt
we have
=

t Σt|t−1CT

Putting the two together we get
μt
=
The Kalman smoothing algorithm
t. This is useful for online inference problems, such as tracking. However, in an offline setting,
we can wait until all the data has arrived, and then compute p(zt|y1:T ).
By conditioning
on past and future data, our uncertainty will be signiﬁcantly reduced. This is illustrated in
trajectory than for the ﬁltered trajectory. (The ellipsoids are larger at the beginning and end of
the trajectory, since states near the boundary do not have as many useful neighbors from which
to borrow information.)We now explain how to compute the smoothed estimates, using an algorithm called the
backwards algorithm for HMMs, although there are some small differences which we discuss
below.
Algorithm
Kalman ﬁltering can be regarded as message passing on a graph, from left to right. When the
messages have reached the end of the graph, we have successfully computed p(zT |y1:T ). Now
we work backwards, from right to left, sending information from the future back to the past,
and them combining the two information sources. The question is: how do we compute these
backwards equations? We ﬁrst give the equations, then the derivation.
We have
p(zt|y1:T )
=
N(μt|T , Σt|T )
μt|T
=
Σt|T
=
t
Jt
≜
Σt|tAT
and ΣT |T from the Kalman ﬁlter. Note that this backwards pass does not need access to the
data, that is, it does not need y1:T . This allows us to “throw away” potentially high dimensional
observation vectors, and just keep the ﬁltered belief states, which usually requires less memory.
Derivation *
The key idea is to leverage the Markov property, which says that zt is independent of future
p(zt|y1:T )
=


=


p(zt|y1:t,
The question is: how do we perform the integration?

zt
	
|
 μt|t
	 
Σt|t
Σt|tAT
		=
t )
We can compute the smoothed distribution for t using the rules of iterated expectation and
iterated covariance. First, the mean:
μt|T
=
E

  y1:T

=
E

  y1:T

=
E

  y1:T

=
Now the covariance:
Σt|T
=
cov

  y1:T

+ E

  y1:T

=

  y1:T

=
cov


+ E

t |y1:T

=
Jtcov


JT
t
=
t
=
t
rithm.
Note that in both the forwards and backwards passes for LDS, we always worked with normalized
distributions, either conditioned on the past data or conditioned on all the data. Furthermore,
the backwards pass depends on the results of the forwards pass. This is different from the usual
It turns out that we can rewrite the Kalman smoother in a modiﬁed form which makes it
p(zt|y1:T )
=


=


Now
sowhich is the conditional likelihood of the future data. This backwards message can be computed
makes more sense to try to approximate the smoothed distribution rather than the backwards
When using SSMs for time series forecasting, and also in some physical state estimation
problems, the observation matrix C and the transition matrix A are both known and ﬁxed, by
deﬁnition of the model. In such cases, all that needs to be learned are the noise covariances Q
the data after a few time steps. This can be encouraged by setting the initial state covariance
to be large, representing a weak prior.) Although we can estimate Q and R offline, using the
methods described below, it is also possible to derive a recursive procedure to exactly compute
Identiﬁability and numerical stability
learn A and C. However, in this case we can set Q = I without loss of generality, since an
arbitrary noise covariance can be modeled by appropriately modifying A. Also, by analogy with
factor analysis, we can require R to be diagonal without loss of generality. Doing this reduces
the number of free parameters and improves numerical stability.
Another constraint that is useful to impose is on the eigenvalues of the dynamics matrix A.
To see why this is important, consider the case of no system noise. In this case, the hidden
state at time t is given by
zt = Atz1 = UΛtU−1z1
where U is the matrix of eigenvectors for A, and Λ = diag(λi) contains the eigenvalues. IfBelow we discuss how to estimate the parameters. However, for simplicity of presentation, we
do not impose any of the constraints mentioned above.
Training with fully observed data
If we observe the hidden state sequences, we can ﬁt the model by computing the MLEs (or even
the full posteriors) for the parameters by solving a multivariate linear regression problem for
R from the residuals in predicting yt from zt.
If we only observe the output sequence, we can compute ML or MAP estimates of the parameters
Subspace methods
EM does not always give satisfactory results, because it is sensitive to the initial parameter
estimates. One way to avoid this is to use a different approach known as a subspace method
To understand this approach, let us initially assume there is no observation noise and no
fold or subspace. We can identify this subspace using PCA (see the above references for details).
Once we have an estimate of the zt’s, we can ﬁt the model as if it were fully observed. We can
either use these estimates in their own right, or use them to initialize EM.
There are various offline Bayesian alternatives to the EM algorithm, including variational Bayes
The Bayesian approach can also be used to
parameters to the state space, the model is generally no longer linear Gaussian. Consequently
we must use some of the approximate online inference methods to be discussed below.
many models are non linear. For example, most moving objects do not move in straight lines.
And even if they did, if we assume the parameters of the model are unknown and add themcommon, e.g., due to outliers, or when inferring parameters for GLMs instead of just linear
regression. For these more general models, we need to use approximate inference.
function, there are two main ways to approximate p(Y ) by a Gaussian. The ﬁrst is to use a
space of Gaussians by moment matching. We discuss each of these methods in turn. (See also
more accurate but slower to compute.)
Extended Kalman ﬁlter (EKF)
we consider models of the form
zt
=
yt
=
where the transition model g and the observation model h are nonlinear but differentiable
functions. Furthermore, we focus on the case where we approximate the posterior by a single
The extended Kalman ﬁlter or EKF can be applied to nonlinear Gaussian dynamical systems
of this form. The basic idea is to linearize g and h about the previous state estimate using
a ﬁrst order Taylor series expansion, and then to apply the standard Kalman ﬁlter equations.
(The noise variance in the equations (Q and R) is not changed, i.e., the additional error due to
we pass a Gaussian distribution p(x), shown on the bottom right, through a nonlinear function
y = g(x), shown on the top right. The resulting distribution (approximated by Monte Carlo) is
shown in the shaded gray area in the top left corner. The best Gaussian approximation to this,
computed from E [g(x)] and var [g(x)] by Monte Carlo, is shown by the solid black line. The
EKF approximates this Gaussian as follows: it linearizes the g function at the current mode, μ,
and then passes the Gaussian distribution p(x) through this linearized function. In this example,
the result is quite a good approximation to the ﬁrst and second moments of p(y), for much less
cost than an MC approximation.
In more detail, the method works as follows. We approximate the measurement model using
p(yt|zt)
≈
where Ht is the Jacobian matrix of h evaluated at the prior mode:
Hij
≜
∂hi(z)
∂zj
Ht
≜p(y)
y
p(y)
Gaussian of p(y)
Mean of p(y)
EKF Gaussian
Mean of EKF
x
Function g(x)
Taylor approx.
g(μ)
x
p(x)
p(x)
Nonlinear transformation of a Gaussian random variable. The prior p(x) is shown on the
bottom right. The function y = g(x) is shown on the top right. The transformed distribution p(y) is
a complex distribution. The solid line is the best Gaussian approximation to this; the dotted line is the EKF
approximation to this.
Used with kind permission of Sebastian
Thrun.
Similarly, we approximate the system model using
≈
where
Gij(u)
≜
∂gi(u, z)
∂zj
Gt
≜
so G is the Jacobian matrix of g evaluated at the prior mode.
Given this, we can then apply the Kalman ﬁlter to compute the posterior as follows:
=
=
GtVt−1GT
t + Qt
Kt
=
Vt|t−1HT
t (HtVt|t−1HT
μt
=
Vt
=mean 
covariance 
sigma 
points 
Actual (sampling) 
Linearized (EKF) 
( )
=
y
f x
(
)
i
iχ
ϒ = f
transformed 
sigma points 
covariance 
mean 
y
f x
P
P
=
=
( )    
y
A
A
T
x
true 
mean 
true 
covariance 
f x
( )
A
A
T
x
P
An example of the unscented transform in two dimensions.
Source: (Wan and der Merwe
Used with kind permission of Eric Wan.
We see that the only difference from the regular Kalman ﬁlter is that, when we compute the
course slower.
There are two cases when the EKF works poorly. The ﬁrst is when the prior covariance is
large. In this case, the prior distribution is broad, so we end up sending a lot of probability
mass through different parts of the function that are far from the mean, where the function has
been linearized. The other setting where the EKF works poorly is when the function is highly
which works better than the EKF in both of these settings.
Unscented Kalman ﬁlter (UKF)
The key intuition is this: it is easier
to approximate a Gaussian than to approximate a function. So instead of performing a linear
approximation to the function, and passing a Gaussian through it, instead pass a deterministically
chosen set of points, known as sigma points, through the function, and ﬁt a Gaussian to the
resulting transformed points. This is known as the unscented transform, and is sketched inThe UKF basically uses the unscented transform twice, once to approximate passing through
the system model g, and once to approximate passing through the measurement model h. We
give the details below. Note that the UKF and EKF both perform O(d3) operations per time step
order, whereas the EKF is only a ﬁrst order approximation (although both the EKF and UKF can
be extended to capture higher order terms). Furthermore, the unscented transform does not
making it simpler to implement and more widely applicable.
The unscented transform
Before explaining the UKF, we ﬁrst explain the unscented transform. Assume p(x) = N(x|μ, Σ),
and consider estimating p(y), where y = f(x) for some nonlinear function f. The unscented
x
=
!
μ, {μ + (
"
(d + λ)Σ):i}d
"
(d + λ)Σ):i}d
#
means the i’th column of matrix M.
These sigma points are propagated through the nonlinear function to yield yi = f(xi), and
the mean and covariance for y is computed as follows:
μy
=
2d

wi
myi
Σy
=
2d

wi
c(yi −μy)(yi −μy)T
where the w’s are weighting terms, given by
wi
m
=
λ
d + λ
wi
c
=
λ
wi
m
=
wi
c =
√
3σ
√
3σ.
The UKF algorithm
The UKF algorithm is simply two applications of the unscented tranform, one to computez0
=
!
"
"
#
z∗i
t
=
g(ut, z0i
μt
=
2d

wi
mz∗i
t
Σt
=
2d

wi
c(z∗i
t −μt)(z∗i
t −μt) + Qt
where γ =
√
d + λ.
The second step is to approximate the likelihood p(yt|zt) ≈N(yt|ˆyt, St) by passing the
prior N(zt|μt, Σt) through the observation model h:
z0
t
=

μt, {μt + γ(
$
Σt):i}d
$
Σt):i}d
	
y∗i
t
=
h(z0i
t )
ˆyt
=
2d

wi
my∗i
t
St
=
2d

wi
c(y∗i
t −ˆyt)(y∗i
t −ˆyt)T + Rt
Finally, we use Bayes rule for Gaussians to get the posterior p(zt|y1:t, u1:t) ≈N(zt|μt, Σt):
Σ
z,y
t
=
2d

wi
c(z∗i
t −μt)(y∗i
t −ˆyt)T
Kt
=
Σ
z,y
t
t
μt
=
μt + Kt(yt −ˆyt)
Σt
=
Σt −KtStKT
t
Assumed density ﬁltering (ADF)
imate the posterior by a distribution of a certain convenient form, such as a Gaussian. More
tractable distributions, e.g., Gaussians with a diagonal covariance matrix, or a product of discrete
Ztˆpt
qt
Predict
Update
Project
Predict
Update
Project
(a)
xt
yt
st
θt
(b)
where
Zt =


is the normalization constant and


is the one step ahead predictive distribution. If the prior is from a suitably restricted family, this
is no longer in our tractable family, ˆp(θt) ̸∈Q. So after updating we seek the best tractable
approximation by computing
q(θt) = argmin
KL (ˆp(θt)||q(θt))
to the “exact” posterior ˆp(θt), and can be thought of as projecting ˆp onto the space of tractable
If q is in the exponential family, one can show that this KL minimization can be done by
moment matching. We give some examples of this below.
is the j’th hidden variable at time t, then the exact posterior p(θt) becomes intractable to
compute because of the entanglement problem. Suppose we use a fully factored approximation
of the form q(θt) = D
j is in state k, and D is the number of variables. In this case, the moment matching operation
becomes
πtjk = ˆp(θt,j = k)of repeated approximations remains bounded (under certain assumptions about the stochasticity
of the system).
Gaussian approximation for online inference in GLMs
Now suppose q(θt) = D
parameters of the tractable approximation to the posterior are
μt,j = Eˆp [θt,j] , τt,j = varˆp [θt,j]
This method can be used to do online inference for the parameters of many statistical models.
For example, theTrueSkill system, used in Microsoft’s Xbox to rank players over time, uses this
The model has the form
p(yt|xt, θt)
=
Ber(yt|sigm(xT
=
where σ2 is some process noise which allows the parameters to change slowly over time. (This
we concentrate on the measurement update step.
Deﬁne the deterministic quantity st = θT

=
=

j
=

j
x2
The posterior for st is given by
qt(st)
=
N(st|mt, vt)
mt
=


st
Zt
vt
=


s2
t
Zt
t
Zt
=

where p(yt|st) = Ber(yt|st). These integrals are one dimensional, and so can be computed
UKF algorithm.
Then one can show that the new factored posterior over the model parameters is given by
q(θt,j)
=
N(θt,j|μt,j, τt,j)
μt,j
=
τt,j
=
aj
≜

j′ x2
Thus we see that the parameters which correspond to inputs with larger magnitude (big |xt,j|)
In this case, the measurement update can be done in closed form, without the need for numerical
integration. In either case, the algorithm only takes O(D) operations per time step, so it can
be applied to models with large numbers of parameters. And since it is an online algorithm,
it can also handle massive datasets.
Bayesian) online learning algorithms, and sometimes even outperform state of the art batch
Many systems contain both discrete and continuous hidden variables; these are known as hybrid
systems. For example, the discrete variables may indicate whether a measurement sensor is
faulty or not, or which “regime” the system is in. We will see some other examples below.
called a switching linear dynamical system (SLDS), a jump Markov linear system (JMLS),
or a switching state space model (SSSM). More precisely, we have a discrete latent variable,
the continuous variables have linear Gaussian CPDs, conditional on the discrete states:
=
Aij
=
p(yt|zt, qt = k, ut, θ)
=
N(yt|Ckzt + Dkut, Rk)yt
zt
qt
ut
(a)
(b)
A switching linear dynamical system. (a) Squares represent discrete nodes, circles represent
continuous nodes. (b) Illustration of how the number of modes in the belief state grows exponentially over
time. We assume there are two binary states.
Inference
SSM model, is intractable.
To see why, suppose qt is binary, but that only the dynamics
A depend on qt, not the observation matrix.
Our initial belief state will be a mixture of
Various approximate inference methods have been proposed for this model, such as the
following:
•
Prune off low probability trajectories in the discrete tree; this is the basis of multiple
•
Use Monte Carlo. Essentially we just sample discrete trajectories, and apply an analytical
•
Use ADF, where we approximate the exponentially large mixture of Gaussians with a smaller
A Gaussian sum ﬁlter for switching SSMs
by a mixture of K Gaussians.
This can be implemented by running K Kalman ﬁlters inb1
b2




@
@
@
R




@
@
@
R
-
-
-
-
t
t
t
t
@
@
@
R
B
B
B
B
B
BBN

















Merge
Merge
-
-
b1
t
b2
t
(a)
b1
b2
-
-
Merge
-
-
˜b1
˜b2
-
-
-
-
b1
t
b2
t
(b)
ADF for a switching linear dynamical system. (a) GPB2 method. (b) IMM method. See text
for details.
parallel. This is particularly well suited to switching SSMs. We now describe one version of this
one per discrete state:
bi
We then pass this through the K different linear models to get
bij
mixtures down to a single mixture to give
bj
t ≜p(zt, qt = j|y1:t) = πtjN(zt|μt,j, Σt,j)The optimal way to approximate a mixture of Gaussians with a single Gaussian is given by
q = arg minq KL (q||p), where p(z) = 
k πkN(z|μk, Σk) and q(z) = N(z|μ, Σ). This can
be solved by moment matching, that is,
μ
=
E [z] =

k
Σ
=
cov [z] =

k
πk

Σk + (μk −μ)(μk −μ)T 
preserves the ﬁrst two moments. Applying these equations to our model, we can go from bij
t to
bj
t as follows (where we drop the t subscript for brevity):
πj
=

i
πij
πj|i
=
πij

j′ πij′
μj
=

i
Σj
=

i
πj|i

Σij + (μij −μj)(μij −μj)T 
This algorithm requires running K2 ﬁlters at each step. A cheaper alternative is to represent
the belief state by a single Gaussian, marginalizing over the discrete switch at each step. This
is a straightforward application of ADF. An offline extension to this method, called expectation
matching), and then updating it using K different Kalman ﬁlters, one per value of qt.
See
Suppose we are tracking K objects, such as airplanes, and at time t, we observe K′ detection
events, e.g., “blips” on a radar screen.
We can have K′ < K due to occlusion or missed
detections. We can have K′ > K due to clutter or false alarms. Or we can have K′ = K. In
any case, we need to ﬁgure out the correspondence between the K′ detections ytk and the K
objects ztj. This is called the problem of data association, and it arises in many application
domains.
is the unknown mapping which speciﬁes which objects caused which observations. It speciﬁes
the “wiring diagram” for time slice t. The standard way to solve this problem is to compute
a weight which measures the “compatibility” between object j and measurement k, typically
data association heuristic). This gives us a K × K′ weight matrix. We can make this into aqt
square matrix of size N × N, where N = max(K, K′), by adding dummy background objects,
which can explain all the false alarms, and adding dummy observations, which can explain all
the missed detections. We can then compute the maximal weight bipartite matching using the
on this, we can perform a Kalman ﬁlter update, where objects that are assigned to dummy
observations do not perform a measurement update.
are many ways to do this, but perhaps the simplest and most robust methods are based on
Application: fault diagnosis
tanks of liquid, interconnected by pipes. In this example, we just have two tanks, for simplicity.
We want to estimate the pressure inside each tank, based on a noisy measurement of the ﬂow
into and out of each tank. However, the measurement devices can sometimes fail. Furthermore,
pipes can burst or get blocked; we call this a “resistance failure”. This model is widely used as
nodes represent discrete variables, such as measurement failures and resistance failures. The
remaining variables are continuous. A variety of approximate inference algorithms can be applied(a)
R1
R1
R12
R12
R2
R2
(b)
P = pressure, F = ﬂow, M = measurement, RF = resistance failure, MF = measurement failure. Based on
Application: econometric forecasting
Exercises
are in (Ghahramani and Hinton 1996b); your task is to derive these results.Undirected graphical models (Markov
random ﬁelds)
Introduction
However, for some domains, being forced to choose a direction for the edges, as required by
a DGM, is rather awkward. For example, consider modeling an image. We might suppose that
the intensity values of neighboring pixels are correlated. We can create a DAG model with a 2d
An alternative is to use an undirected graphical model (UGM), also called a Markov random
ﬁeld (MRF) or Markov network. These do not require us to specify edge orientations, and are
much more natural for some problems such as image analysis and spatial statistics. For example,
nativel UGMs (aka conditional random ﬁelds, or CRFs), which deﬁne conditional densities of the
an empirical comparison of the two approaches for an image processing task.
Conditional independence properties of UGMs
Key properties
UGMs deﬁne CI relationships via simple graph separation as follows: for sets of nodes A, B,
and C, we say xA ⊥G xB|xC iff C separates A from B in the graph G. This means that,
when we remove all the nodes in C, if there are no paths connecting any node in A to any
node in B, then the CI property holds. This is called the global Markov property for UGMs.X1
X2
X3
X4
X5
X6
X7
X8
X9
X10
X11
X12
X13
X14
X15
X16
X17
X18
X19
X20
(a)
X1
X2
X3
X4
X5
X6
X7
X8
X9
X10
X11
X12
X13
X14
X15
X16
X17
X18
X19
X20
(b)
(a) A 2d lattice represented as a DAG. The dotted red node X8 is independent of all other
(orange). (b) The same model represented as a UGM. The red node X8 is independent of the other black
nodes given its neighbors (blue nodes).
(a)
(b)
(a) A DGM. (b) Its moralized version, represented as a UGM.
The set of nodes that renders a node t conditionally independent of all the other nodes in
the graph is called t’s Markov blanket; we will denote this by mb(t). Formally, the Markov
blanket satisﬁes the following property:
t ⊥V \ cl(t)|mb(t)
where cl(t) ≜mb(t) ∪{t} is the closure of node t. One can show that, in a UGM, a node’s
Markov blanket is its set of immediate neighbors. This is called the undirected local Markov
dent given the rest if there is no direct edge between them. This is called the pairwise Markov
property. In symbols, this is written as
Using the three Markov properties we have discussed, we can derive the following CI properties
•
•G
L
P
Relationship between Markov properties of UGMs.
(a)
(b)
moralized version of (a).
•
It is obvious that global Markov implies local Markov which implies pairwise Markov. What is
is that pairwise implies global, and hence that all these Markov properties are the same, as
of this result is that it is usually easier to empirically assess pairwise conditional independence;
such pairwise CI statements can be used to construct a graph from which global CI statements
can be extracted.
We have seen that determinining CI relationships in UGMs is much easier than in DGMs, because
we do not have to worry about the directionality of the edges. In this section, we show how to
determine CI relationships for a DGM using a UGM.
It is tempting to simply convert the DGM to a UGM by dropping the orientation of the edges,
than the corresponding undirected chain A −B −C. The latter graph incorrectly states that
A ⊥C|B. To avoid such incorrect CI statements, we can add edges between the “unmarried”
parents A and C, and then drop the arrows from the edges, forming (in this case) a fully3UREDELOLVWLF0RGHOV
*UDSKLFDO0RGHOV
'LUHFWHG
8QGLUHFWHG
&KRUGDO
DGMs and UGMs can perfectly represent different sets of distributions. Some distributions
can be perfectly represented by either DGMs or UGMs; the corresponding graph must be chordal.
Unfortunately, moralization loses some CI information, and therefore we cannot use the
approach to determining if A ⊥B|C. First we form the ancestral graph of DAG G with respect
ancestors of U. We then moralize this ancestral graph, and apply the simple graph separation
Comparing directed and undirected graphical models
Which model has more “expressive power”, a DGM or a UGM? To formalize this question, recall
Now deﬁne G to be
perfect map of p if I(G) = I(p), in other words, the graph can represent all (and only) the CI
properties of the distribution. It turns out that DGMs and UGMs are perfect maps for different
a representation language.
As an example of some CI relationships that can be perfectly modeled by a DGM but not a
the arrows, we get A −C −B, which asserts A ⊥B|C and A ̸⊥B, which is incorrect. In fact,
structure. In general, CI properties in UGMs are monotonic, in the following sense: if A ⊥B|C,C
(a)
(b)
(c)
A
D
B
B
D
A
C
B
D
A
C
Used with kind permission of Daphne Koller.
on extra variables can eliminate conditional independencies due to explaining away.
As an example of some CI relationships that can be perfectly modeled by a UGM but not a
incorrectly encodes B ⊥D. In fact there is no DGM that can precisely represent all and only
the CI statements encoded by this UGM.
Some distributions can be perfectly modeled by either a DGM or a UGM; the resulting graphs
are called decomposable or chordal. Roughly speaking, this means the following: if we collapse
will be a tree. Of course, if the graph is already a tree (which includes chains as a special case),
Parameterization of MRFs
Although the CI properties of UGM are simpler and more natural than for DGMs, representing
the joint distribution for a UGM is less natural than for a DGM, as we see below.
Since there is no topological ordering associated with an undirected graph, we can’t use the chain
rule to represent p(y). So instead of associating CPDs with each node, we associate potential
functions or factors with each maximal clique in the graph.
We will denote the potential
its arguments. The joint distribution is then deﬁned to be proportional to the product of clique
potentials. Rather surprisingly, one can show that any positive distribution whose CI properties
can be represented by a UGM can be represented in this way. We state this result more formally
below.erties of an undirected graph G iff p can be represented as a product of factors, one per maximal
clique, i.e.,
p(y|θ) =
Z(θ)

ψc(yc|θc)
where C is the set of all the (maximal) cliques of G, and Z(θ) is the partition function given by
Z(θ) ≜

x

ψc(yc|θc)
then we can write p as follows:
p(y|θ) =
Z(θ)ψ123(y1, y2, y3)ψ234(y2, y3, y4)ψ35(y3, y5)
where
Z =

y
ψ123(y1, y2, y3)ψ234(y2, y3, y4)ψ35(y3, y5)
There is a deep connection between UGMs and statistical physics. In particular, there is a
model known as the Gibbs distribution, which can be written as follows:
p(y|θ) =
Z(θ) exp(−

c
E(yc|θc))
a UGM by deﬁning
ψc(yc|θc) = exp(−E(yc|θc))
We see that high probability states correspond to low energy conﬁgurations. Models of this form
are known as energy based models, and are commonly used in physics and biochemistry, as
Note that we are free to restrict the parameterization to the edges of the graph, rather than
p(y|θ)
∝
ψ12(y1, y2)ψ13(y1, y3)ψ23(y2, y3)ψ24(y2, y4)ψ34(y3, y4)ψ35(y3, y5)
∝

s∼t
ψst(ys, yt)
This form is widely used due to its simplicity, although it is not as general.
This reﬂects the fact that a lot of pioneering working in statistical physics was done by Germans.Representing potential functions
If the variables are discrete, we can represent the potential or energy functions as tables of
Rather, they represent the relative “compatibility” between the different assignments to the
potential. We will see some examples of this below.
A more general approach is to deﬁne the log potentials as a linear function of the parameters:
where φc(xc) is a feature vector derived from the values of the variables yc. The resulting log
probability has the form
log p(y|θ) =

c
For example, consider a pairwise MRF, where for each edge, we associate a feature vector of
length K2 as follows:
φst(ys, yt) = [. . . , I(ys = j, yt = k), . . .]
If we have a weight for each feature, we can convert this into a K × K potential function as
follows:
ψst(ys = j, yt = k) = exp([θT
stφst]jk) = exp(θst(j, k))
But the
To see why this is useful, suppose we are interested in making a probabilistic model of English
spelling. Since certain letter combinations occur together quite frequently (e.g., “ing”), we will
need higher order factors to capture this.
Suppose we limit ourselves to letter trigrams.
A
never occur.
An alternative approach is to deﬁne indicator functions that look for certain “special” triples,

k
sponding binary feature function. By tying the parameters across locations, we can deﬁne the
probability of a word of any length using
p(y|θ) ∝exp(

t

k
This raises the question of where these feature functions come from. In many applications,
they are created by hand to reﬂect domain knowledge (we will see examples later), but it is alsoExamples of MRFs
In this section, we show how several popular probability models can be conveniently expressed
as UGMs.
Ising model
neighboring spins tend to line up in the same direction, whereas in other kinds of magnets,
We can model this as an MRF as follows. We create a graph in the form of a 2D or 3D lattice,
clique potential:
ψst(ys, yt) =

ewst
e−wst
e−wst
ewst
	
Here wst is the coupling strength between nodes s and t. If two nodes are not connected in
state; this can be used to model ferromagnets, and is an example of an associative Markov
network. If the weights are sufficiently strong, the corresponding probability distribution will
ground states of the system.
in which not all the constraints can be satisﬁed at the same time. The corresponding probability
distribution will have multiple modes. Interestingly, computing the partition function Z(J) can
There is an interesting analogy between Ising models and Gaussian graphical models. First,
follows:
log ˜p(y) = −

s∼t
2yT Wy
(and hence high probability) if neighboring states agree.
Sometimes there is an external ﬁeld, which is an energy term which is added to each spin.
This can be modelled using a local energy term of the form −bT y, where b is sometimes calleda bias term. The modiﬁed distribution is given by
log ˜p(y)
=

s∼t
wstysyt +

s
2yT Wy + bT y
that looks similar to a Gaussian:
One very important difference is that, in the case of Gaussians, the normalization constant,
Z = |2πΣ|, requires the computation of a matrix determinant, which can be computed in
O(D3) time, whereas in the case of the Ising model, the normalization constant requires
summing over all 2D bit vectors; this is equivalent to computing the matrix permanent, which
Hopﬁeld networks
matrix, W = WT . These weights, plus the bias terms b, can be learned from training data
dressable memory. The idea is this: suppose we train on a set of fully observed bit vectors,
corresponding to patterns we want to memorize. Then, at test time, we present a partial pattern
memory based on a piece of the example itself, hence the term “associative memory”.
Since exact inference is intractable in this model, it is standard to use a coordinate descent
algorithm known as iterative conditional modes (ICM), which just sets each node to its most
likely (lowest energy) state, given all its neighbors. The full conditional can be shown to be
s,:y−s + bs)
Picking the most probable state amounts to using the rule y∗
t wstyt > bs and using
y∗
Since inference is deterministic, it is also possible to interpret this model as a recurrent
they are univariate conditional density models of the form p(y|x, θ) which can only be used for
A Boltzmann machine generalizes the Hopﬁeld / Ising model by including some hidden
nodes, which makes the model representationally more powerful. Inference in such modelsExamples of how an associative memory can reconstruct images. These are binary images
hopfieldDemo.
(a)
(b)
(c)
largest, red is smallest. Used with kind permission of Erik Sudderth. See gibbsDemoIsing for Matlab
code to produce a similar plot for the Ising model.
However, we could equally well apply Gibbs to a Hopﬁeld net and ICM to a Boltzmann machine:
on Boltzmann machines.xs
xt
ys
yt
Potts model
common to use a potential function of the following form:
ψst(ys, yt) =
⎛
⎝
eJ
eJ
eJ
⎞
⎠
a parameter of the system is called a phase transition, and has been widely studied in the
The Potts model can be used as a prior for image segmentation, since it says that neighboring
pixels are likely to have the same discrete label and hence belong to the same segment. We can
combine this prior with a likelihood term as follows:
p(y, x|θ) = p(y|J)

t
p(xt|yt, θ) =

Z(J)

s∼t
ψ(ys, yt; J)
 
t
p(xt|yt, θ)
where p(xt|yt = k, θ) is the probability of observing pixel xt given that the corresponding
segment belongs to class k. This observation model can be modeled using a Gaussian or a
The corresponding graphical model is a mix of undirected and directed edges, as shown in
ing, this combination of an undirected and directed graph is called a chain graph. However,since the xt nodes are observed, they can be “absorbed” into the model, thus leaving behind an
undirected “backbone”.
This model is a 2d analog of an HMM, and could be called a partially observed MRF. As
in an HMM, the goal is to perform posterior inference, i.e., to compute (some function of)
p(y|x, θ). Unfortunately, the 2d case is provably much harder than the 1d case, and we must
resort to approximate methods, as we discuss in later chapters.
Although the Potts prior is adequate for regularizing supervised learning problems, it is not
sufficiently accurate to perform image segmentation in an unsupervised way, since the segments
produced by this model do not accurately represent the kinds of segments one sees in natural
Gaussian MRFs
MRF of the following form:
p(y|θ)
∝

s∼t
ψst(ys, yt)

t
ψt(yt)
ψst(ys, yt)
=
2ysΛstyt)
ψt(yt)
=
2Λtty2
t + ηtyt)
(Note that we could easily absorb the node potentials ψt into the edge potentials, but we have
kept them separate for clarity.) The joint distribution can be written as follows:
p(y|θ)
∝
2yT Λy]
η = Λμ.
The zero entries in Λ are called structural zeros, since they represent the absent edges in the
graph. Thus undirected GGMs correspond to sparse precision matrices, a fact which we exploit
Comparing Gaussian DGMs and UGMs *
sparse Cholesky factorizations of covariance matrices, whereas undirected GGMs correspond to
using the Potts model as a prior for image segmentation, but the results in their paper are misleading because they didSource: (Dahlhaus and Eichler
Used with kind permission of Rainer Dahlhaus and Oxford University Press.
sparse precision matrices.
The advantage of the DAG formulation is that we can make the
without worrying about positive deﬁnite constraints. The disadavantage of the DAG formulation
is its dependence on the order, although in certain domains, such as time series, there is already
a natural ordering of the variables.
It is actually possible to combine both representations, resulting in a Gaussian chain graph.
Gaussian CPD:
then the correlation amongst the components within a time slice is most naturally modeled
using a UGM. For example, suppose we have
A1 =
⎛
⎜
⎜
⎜
⎜
⎝
⎞
⎟
⎟
⎟
⎟
⎠
, A2 =
⎛
⎜
⎜
⎜
⎜
⎝
⎞
⎟
⎟
⎟
⎟
⎠
and
Σ =
⎛
⎜
⎜
⎜
⎜
⎝
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎝
⎞
⎟
⎟
⎟
⎟
⎠x1
x2
x3
(a)
x1
x2
x3
w1
w2
(b)
Sometimes we have a sparse covariance matrix rather than a sparse precision matrix. This can
graph representing a sparse covariance matrix is called a covariance graph.) By contrast, if
A bidirected graph can be converted to a DAG with latent variables, where each bidirected
edge is replaced with a hidden variable representing a hidden common cause, or confounder,
separation.
We can combine bidirected and directed edges to get a directed mixed graphical model.
Markov logic networks *
we saw how we could expand a lattice UGM to model images of any size. What about more
complex domains, where we have a variable number of objects and relationships between them?Friends(A,A)
Smokes(A)
Smokes(B)
Friends(B,B)
Friends(A,B)
Friends(B,A)
Cancer(A)
Cancer(B)
logic as follows:
∀x.Sm(x) =⇒Ca(x)
∀x.∀y.Fr(x, y) ∧Sm(x) =⇒Sm(y)
Of course, such rules are not always true. Indeed, this brittleness is the main reason why
logical approaches to AI are no longer widely used, at least not in their pure form.
There
have been a variety of attempts to combine ﬁrst order logic with probability theory, an area
One simple approach is to take logical rules and attach weights (known as certainty factors) to
them, and then to interpret them as conditional probability distributions. For example, we might
joint distribution, because the resulting graph may not be a DAG.
An alternative approach is to treat these rules as a way of deﬁning potential functions in an
To specify the network, we ﬁrst rewrite all the rules in conjunctive normal form (CNF), also
known as clausal form. In this case, we get
¬Sm(x) ∨Ca(x)
¬Fr(x, y) ∨¬Sm(x) ∨Sm(y)
The ﬁrst clause can be read as “Either x does not smoke or he has cancer”, which is logically
to be universally quantiﬁed.)
whether the property holds or not of that object. A (logical) relation is just a function of two or more arguments (objects)
that evaluates to true or false, depending on whether the relationship holds between that set of objects or not.A common approach (as used in Prolog) is to restrict the language to Horn clauses, which are
clauses that contain at most one positive literal. Essentially this means the model is a series of
a single term.
Once we have encoded our knowledge base as a set of clauses, we can attach weights to
each one; these weights are the parameter of the model, and they deﬁne the clique potentials
as follows:
ψc(xc) = exp(wcφc(xc))
where φc(xc) is a logical expression which evaluates clause c applied to the variables xc, and
wc is the weight we attach to this clause. Roughly speaking, the weight of a clause speciﬁes
the probability of a world in which this clause is satsiﬁed relative to a world in which it is not
satisﬁed.
Now suppose there are two objects (people) in the world, Anna and Bob, which we will denote
by constant symbols A and B. We can make a ground network from the above clauses by
Note that we have not encoded the fact that Fr is a symmetric relation, so Fr(A, B) and
Fr(B, A) might have different values. Similarly, we have the “degenerate” nodes Fr(A, A) and
then the model compiler, which generates the ground network, could avoid creating redundant
nodes.)
In summary, we can think of MLNs as a convenient way of specifying a UGM template, that
can get unrolled to handle data of arbitrary size. There are several other ways to deﬁne relational
details.
Learning
In this section, we discuss how to perform ML and MAP parameter estimation for MRFs. We will
see that this is quite computationally expensive. For this reason, it is rare to perform Bayesian
Training maxent models using gradient methods
p(y|θ) =
Z(θ) exp


c
θT
c φc(y)

N

i
N

i

c
θT
c φc(yi) −log Z(θ)

optimizers. In particular, the derivative for the weights of a particular clique, c, is given by
∂ℓ
∂θc
=
N

i

φc(yi) −
∂
∂θc
log Z(θ)

expectation of the c’th feature under the model, i.e.,
∂log Z(θ)
∂θc
=
E [φc(y)|θ] =

y
φc(y)p(y|θ)
Hence the gradient of the log likelihood is
∂ℓ
∂θc
=

N

i
φc(yi)

−E [φc(y)]
In the ﬁrst term, we ﬁx y to its observed values; this is sometimes called the clamped term.
In the second term, y is free; this is sometimes called the unclamped term or contrastive
term. Note that computing the unclamped term requires inference in the model, and this must
be done once per gradient step. This makes UGM training much slower than DGM training.
The gradient of the log likelihood can be rewritten as the expected feature vector according
to the empirical distribution minus the model’s expectation of the feature vector:
∂ℓ
∂θc
= Epemp [φc(y)] −Ep(·|θ) [φc(y)]
At the optimum, the gradient will be zero, so the empirical distribution of the features will
match the model’s predictions:
Epemp [φc(y)] = Ep(·|θ) [φc(y)]
This is called moment matching. This observation motivates a different optimization algorithm
Training partially observed maxent models
In general, we can
represent such models as follows:
p(y, h|θ) =
Z(θ) exp(

c
θT
c φc(h, y))The log likelihood has the form
ℓ(θ)
=
N

i
log


hi
p(yi, hi|θ)


N

i
log

Z(θ)

hi
˜p(yi, hi|θ)


where
˜p(y, h|θ) ≜exp


c
θT
c φc(h, y)


is the unnormalized distribution. The term 
hi ˜p(yi, hi|θ) is the same as the partition function
for the whole model, except that y is ﬁxed at yi. Hence the gradient is just the expected features
where we clamp yi, but average over h:
∂
∂θc
log


hi
˜p(yi, hi|θ)


=
E [φc(h, yi)|θ]
So the overall gradient is given by
∂ℓ
∂θc
=
N

i
{E [φc(h, yi)|θ] −E [φc(h, y)|θ]}
The ﬁrst set of expectations are computed by “clamping” the visible nodes to their observed
values, and the second set are computed by letting the visible nodes be free. In both cases, we
marginalize over hi.
An alternative approach is to use generalized EM, where we use gradient methods in the M
Approximate methods for computing the MLEs of MRFs
When ﬁtting a UGM there is (in general) no closed form solution for the ML or the MAP estimate
In models where inference is intractable, learning also becomes intractable. This has motivated
dicsuss some of these alternatives below, and defer others to later sections.
Pseudo likelihood
ℓP L(θ)
≜

y
D

N
N

D

log p(yid|yi,−d, θ)
ℓML(θ) =

y,x
pemp(y log p(y|θ) =
N

log p(yi|θ)Method
Restriction
Exact MLE?
Section
Closed form
Only Chordal MRF
Exact
IPF
Only Tabular / Gaussian MRF
Exact
Low tree width
Exact
Only CRFs
No hidden variables
Approximate
Stochastic ML
-
Exact (up to MC error)
Contrastive divergence
-
Approximate
Minimum probability ﬂow
Can integrate out the hiddens
Approximate
(a)
(b)
(a) A small 2d lattice. (b) The representation used by pseudo likelihood. Solid nodes are
given all of its neighbors. This objective is generally fast to compute since each full conditional
p(yid|yi,−d, θ) only requires summing over the states of a single node, yid, in order to compute
the local normalization constant. The PL approach is similar to ﬁtting each full conditional
except that the parameters are tied between adjacent nodes.
One problem with PL is that it is hard to apply to models with hidden variables (Parise and
completely on node t, even at the expense of ignoring other potentially useful information, such
as its local evidence.
PL works as well as exact ML for fully observed Ising models, and of course PL is much faster.
Stochastic maximum likelihood
N

i
[φ(yi) −E [φ(y)]]The gradient for a partially observed MRF is similar. In both cases, we can approximate the
model expectations using Monte Carlo sampling. We can combine this with stochastic gradient
for each minibatch of size B do
Sample ys,k ∼p(y|θk) ;
S
S
for each training case i in minibatch do
gik = φ(yi) −ˆE(φ(y)) ;
B

Decrease step size η;
Typically we use MCMC to generate the samples. Of course, running MCMC to convergence
at each step of the inner loop would be extremely slow. Fortunately, it was shown in (Younes
the parameters a small amount. We call this algorithm stochastic maximum likelihood or
SML. (There is a closely related algorithm called persistent contrastive divergence which we
Feature induction for maxent models *
MRFs require a good set of features. One unsupervised way to learn such features, known as
feature induction, is to start with a base set of features, and then to continually create new
feature combinations out of old ones, greedily adding the best ones to the model. This approach
how to build unconditional probabilistic models to represent English spelling. Initially the model
has no features, which represents the uniform distribution. The algorithm starts by choosing to
add the feature
φ1(y) =

twhich checks if any letter is lower case or not. After the feature is added, the parameters are
same word without a lowercase letter in that position. Some samples from this model, generated
m, r, xevo, ijjiir, b, to, jz, gsr, wq, vf, x, ga, msmGh, pcp, d, oziVlal,
hzagh, yzop, io, advzmxnv, ijv_bolft, x, emx, kayerf, mlj, rawzyb, jp, ag,
ctdnnnbg, wgdw, t, kguv, cy, spxcq, uzflbbf, dxtkkn, cxwx, jpd, ztzh, lv,
igcump, zjcjs, lqpWiqu, cefmfhc, o, lb, fdcY, tzby, yopxmvk, by, fz„ t, govyccm,
ijyiduwfzo, 6xr, duh, ejv, pk, pjw, l, fl, w
The second feature added by the algorithm checks if two adjacent characters are lower case:
φ2(y) =

s∼t
Now the model has the form
Z exp(θ1φ1(y) + θ2φ2(y))
Some
shown below.
was, reaser, in, there, to, will, „ was, by, homes, thing, be, reloverated,
ther, which, conists, at, fores, anditing, with, Mr., proveral, the, „ ***,
intrally, of, qut, ., best, compers, ***, cluseliment, uster, of, is, deveral,
this, thise, of, offect, inatever, thifer, constranded, stater, vill, in, thase,
in, youse, menttering, and, ., of, in, verate, of, to
This approach of feature learning can be thought of as a form of graphical model structure
of the resulting graph structure. However, the resulting graphs can become densely connected,
which makes inference (and hence parameter estimation) intractable.
Iterative proportional ﬁtting (IPF) *
Consider a pairwise MRF where the potentials are represented as tables, with one parameter per
ψst(ys, yt) = exp
!
θT
#
and similarly for ψt(yt). Thus the feature vectors are just indicator functions.of the features equals the model’s expectation:
Epemp [I(ys = j, yt = k)]
=
Ep(·|θ) [I(ys = j, yt = k)]
pemp(ys = j, yt = k)
=
p(ys = j, yt = k|θ)
where pemp is the empirical probability:
pemp(ys = j, yt = k) = Nst,jk
N
=
N
N
For a general graph, the condition that must hold at the optimum is
pemp(yc) = p(yc|θ)
can show that p(yc|θ) = ψc(yc). However, even if the graph is not decomposable, we can
imagine trying to enforce this condition. This suggests an iterative coordinate ascent scheme
where at each step we compute
c
(yc) = ψt
c(yc) × pemp(yc)
p(yc|ψt)
where the multiplication is elementwise. This is known as iterative proportional ﬁtting or IPF
pc = p(yc|ψ);
ˆpc = pemp(yc);
ψc = ψc ∗ˆpc
pc ;
Example
Total
male
female
TotalSuppose we want to ﬁt a disconnected graphical model containing nodes Y1 and Y2 but with
no edge between them. That is, we want to ﬁnd vectors ψ1 and ψ2 such that M ≜ψ1ψT
where M are the model’s expected counts, and C are the empirical counts.
By moment
matching, we ﬁnd that the row and column sums of the model must exactly match the row
Total
male
female
Total
It is easy to see that this matches the required constraints. See IPFdemo2x2 for some Matlab
code that computes these numbers. This method is easily to generalized to arbitrary graphs.
Speed of IPF
IPF is a ﬁxed point algorithm for enforcing the moment matching constraints and is guaranteed
form of the model. If the graph is decomposable, then IPF converges in a single iteration, but in
general, IPF may require many iterations.
It is clear that the dominant cost of IPF is computing the required marginals under the model.
Nevertheless, coordinate descent can be slow. An alternative method is to update all the
parameters at once, by simply following the gradient of the likelihood. This gradient approach
has the further signiﬁcant advantage that it works for models in which the clique potentials may
not be fully parameterized, i.e., the features may not consist of all possible indicators for each
clique, but instead can be arbitrary. Although it is possible to adapt IPF to this setting of general
features, resulting in a method known as iterative scaling, in practice the gradient method is
Generalizations of IPF
We can use IPF to ﬁt Gaussian graphical models: instead of working with empirical counts, we
a Bayesian IPF algorithm for sampling from the posterior of the model’s parameters (see e.g.,
IPF for decomposable graphical models
There is a special family of undirected graphical models known as decomposable graphical
information.
In the case of decomposable graphical models, IPF converges in one iteration. In fact, theˆψc(yc = k) =
N
N
and for Gaussian potentials, we have
ˆμc =
N
N
, ˆΣc =

i(yic −ˆμc)(xic −ˆμc)T
N
details.
Conditional random ﬁelds (CRFs)
conditioned on input features:
p(y|x, w) =
Z(x, w)

c
ψc(yc|x, w)
A CRF can be thought of as a structured output extension of logistic regression. We will usually
ψc(yc|x, w) = exp(wT
c φ(x, yc))
where φ(x, yc) is a feature vector derived from the global inputs x and the local set of labels
yc. We will give some examples below which will make this notation clearer.
The advantage of a CRF over an MRF is analogous to the advantage of a discriminative
modeling things that we always observe. Instead we can focus our attention on modeling what
we care about, namely the distribution of labels given the data.
Another important advantage of CRFs is that we can make the potentials (or factors) of the
label smoothing between two neighboring nodes s and t if there is an observed discontinuity in
the image intensity between pixels s and t. Similarly, in natural language processing problems,
we can make the latent labels depend on global properties of the sentence, such as which
language it is written in. It is hard to incorporate global features into generative models.
The disadvantage of CRFs over MRFs is that they require labeled training data, and theyxt
yt
(a)
xt
yt
xg
(b)
xt
yt
xg
(c)
Various models for sequential data. (a) A generative directed HMM. (b) A discriminative
directed MEMM. (c) A discriminative undirected CRF.
are joint density models of the form
p(x, y|w) =
T

both xt and yt for all t, it is very easy to train such models, using techniques described in
An HMM requires specifying a generative observation model, p(xt|yt, w), which can be
difficult. Furthemore, each xt is required to be local, since it is hard to deﬁne a generative
model for the whole stream of observations, x = x1:T .
An obvious way to make a discriminative version of an HMM is to “reverse the arrows” from
p(y|x, w) =

t
where x = (x1:T , xg), xg are global features, and xt are features speciﬁc to node t. (This
partition into local and global is not necessary, but helps when comparing to HMMs.) This is
An MEMM is simply a Markov chain in which the state transition probabilities are conditioned
output setting, but it suffers from a subtle problem known (rather obscurely) as the label bias
the information ﬂow.
To understand what this means in practice, consider the part of speech (POS) tagging task.
Suppose we see the word “banks”; this could be a verb (as in “he banks at BoA”), or a noun (as
in “the river banks were overﬂowing”). Locally the POS tag for the word is ambiguous. However,(a)
(b)
(c)
(d)
(e)
Example of handwritten letter recognition. In the word ’brace’, the ’r’ and the ’c’ look very
similar, but can be disambiguated using context.
of Ben Taskar.
suppose that later in the sentence, we see the word “ﬁshing”; this gives us enough context to
infer that the sense of “banks” is “river banks”. However, in an MEMM (unlike in an HMM and
CRF), the “ﬁshing” evidence will not ﬂow backwards, so we will not be able to disambiguate
“banks”.
p(y|x, w) =
Z(x, w)
T

ψ(yt|x, w)

does not block the information from xt from reaching other yt′ nodes.
The label bias problem in MEMMs occurs because directed models are locally normalized,
all joint conﬁgurations, will ensure the model deﬁnes a valid distribution. However, this solution
comes at a price: we do not get a valid probability distribution over y until we have seen
the whole sentence, since only then can we normalize over all conﬁgurations. Consequently,
inference. Furthermore, the fact that Z depends on all the nodes, and hence all their parameters,
Applications of CRFs
CRFs have been applied to many interesting problems; we give a representative sample below.
These applications illustrate several useful modeling tricks, and will also provide motivation for
Handwriting recognition
known) labels of one’s neighbors, it is possible to use context to reduce the error rate. Note
that the node potential, ψt(yt|xt), is often taken to be a probabilistic discriminative classiﬁer,its
withdrawal
from
the
UAL
Airways
rose
after
announcing
 British
deal
ADJ
N
V
IN
V
PRP
N
IN
N
N
DT
B
I
O
O
O
B
I
O
I
POS
NP
I
B
Begin noun phrase
Within noun phrase
Not a noun phrase
Noun
Adjective
B
I
O
N
ADJ
Verb
Preposition
Possesive pronoun
Determiner (e.g., a, an, the)
V
IN
PRP
DT
KEY
A CRF for joint POS tagging and NP segmentation.
Used with kind permission of Daphne Koller.
such as a neural network or RVM, that is trained on isolated letters, and the edge potentials,
ψst(ys, yt), are often taken to be a language bigram model. Later we will discuss how to train
all the potentials jointly.
Noun phrase chunking
One common NLP task is noun phrase chunking, which refers to the task of segmenting a
sentence into its distinct noun phrases (NPs). This is a simple example of a technique known as
shallow parsing.
In more detail, we tag each word in the sentence with B (meaning beginning of a new NP), I
(meaning inside a NP), or O (meaning outside an NP). This is called BIO notation. For example,
in the following sentence, the NPs are marked with brackets:
B
I
O
O
O
B
I
O
B
I
I
(British Airways) rose after announcing (its withdrawl) from (the UAI deal)
(We need the B symbol so that we can distinguish I I, meaning two words within a single NP,
from B B, meaning two separate NPs.)
A standard approach to this problem would ﬁrst convert the string of words into a string of
POS tags, and then convert the POS tags to a string of BIOs. However, such a pipeline method
can propagate errors. A more robust approach is to build a joint probabilistic model of the
connections between adjacent labels encode the probability of transitioning between the B, I
and O states, and can enforce constraints such as the fact that B must preceed I. The features
are usually hand engineered and include things like: does this word begin with a capital letter, is
features per node.
The number of features has minimal impact on the inference time, since the features are
observed and do not need to be summed over.
(There is a small increase in the cost of Mrs.
Green
spoke
today
in
New
York
( )
Green
chairs
the
ﬁnance committee
OTH
OTH
OTH
OTH
OTH
OTH
OTH
KEY
Begin person name
Within person name
Begin location name
Within location name
Not an entitiy
OTH
Used with kind permission of Daphne Koller.
evaluating potential functions with many features, but this is usually negligible; if not, one can
use ℓ1 regularization to prune out irrelevant features.) However, the graph structure can have a
computationally intractable.
Named entity recognition
A task that is related to NP chunking is named entity extraction. Instead of just segmenting
out noun phrases, we can segment out phrases to do with people and locations.
Similar
techniques are used to automatically populate your calendar from your email messages; this is
called information extraction.
a word is a person, location, or something else. (Proper nouns are particularly difficult to deal
with because they belong to an open class, that is, there is an unbounded number of possible
names, unlike the set of nouns and verbs, which is large but essentially ﬁxed.) We can get better
a link between all occurrences of the same word, and force the word to have the same tag in
each occurence. (The same technique can also be helpful for resolving the identity of pronouns.)
We see that the graph structure itself changes depending on the input, which is an additional
advantage of CRFs over generative models.
ally more expensive than in a simple chain with local connections, for reasons explained inIllustration of a simple parse tree based on a context free grammar in Chomsky normal
form. The feature vector φ(x, y) = Ψ(x, y) counts the number of times each production rule was used.
Natural language parsing
such rule has an associated probability. The resulting model deﬁnes a probability distribution
over sequences of words. We can compute the probability of observing a particular sequence
PCFGs are generative models. It is possible to make discriminative versions which encode
the probability of a labeled tree, y, given a sequence of words, x, by using a CRF of the form
p(y|x) ∝exp(wT φ(x, y)). For example, we might deﬁne φ(x, y) to count the number of
times each production rule was used (which is analogous to the number of state transitions in
Hierarchical classiﬁcation
groups the classes into a hierarchy. We can encode the position of y within this hierarchy by
deﬁning a binary vector φ(y), where we turn on the bit for component y and for all its children.
This can be combined with input features φ(x) using a tensor product, φ(x, y) = φ(x)⊗φ(y).
This method is widely used for text classiﬁcation, where manually constructed taxnomies
information can be shared between the parameters for nearby categories, enabling generalization
across classes.Illustration of a simple label taxonomy, and how it can be used to compute a distributed
with kind permission of Yasemin Altun.
y, from the discrete sequence of amino acids, x.
We can deﬁne an energy function E(x, y), where we include various pairwise interaction
terms between nearby residues (elements of the y vector). This energy is usually deﬁned as a
weighted sum of individual energy terms, E(x, y|w) = D
energy contribution due to various electrostatic charges, hydrogen bonding potentials, etc, and
Given the model, we can compute the most probable side chain conﬁguration using y∗=
Stereo vision
and are not generated by the model. We will assume a pairwise CRF.
estimate the depth of every pixel given two images taken from slightly different angles. In this
By using some standard preprocessing techniques, one can convert depth estimation into aproblem of estimating the disparity ys between the pixel at location (is, js) in the left image
and the corresponding pixel at location (is + ys, js) in the right image. We typically assume
that corresponding pixels have similar intensity, so we deﬁne a local node potential of the form
ψs(ys|x) ∝exp

%
where xL is the left image and xR is the right image. This equation can be generalized to model
the intensity of small windows around each location. In highly textured regions, it is usually
possible to ﬁnd the corresponding patch using cross correlation, but in regions of low texture,
there will be considerable ambiguity about the correct value of ys.
We can easily add a Gaussian prior on the edges of the MRF that encodes the assumption
that neighboring disparities ys, yt should be similar, as follows:
ψst(ys, yt) ∝exp

	
The resulting model is a Gaussian CRF.
to account for the occasional large changes in disparity that occur between neighboring pixels
which are on different sides of an occlusion boundary. One gets much better results using a
truncated Gaussian potential of the form
ψst(ys, yt) ∝exp

2γ2 min

%
be imposed if disparities are signiﬁcantly different. This is called a discontinuity preserving
potential; note that such penalties are not convex. The local evidence potential can be made
robust in a similar way, in order to handle outliers due to specularities, occlusions, etc.
On the bottom left is the corresponding true disparity values. The remaining columns represent
the results using a Gaussian edge potential, and the bottom row shows the results using the
truncated potential. The latter is clearly better.
unless the model is jointly Gaussian. Consequently, it is common to discretize the variables.
metric.
Symmetry: f(a, b) = f(b, a); and Triangle inequality: f(a, b) + f(b, c) ≥f(a, c). If f satisﬁes only the ﬁrst two

	








Illustration of belief propagation for stereo depth estimation. Left column: image and true
Used with kind permission of Erik Sudderth.
CRF training
N

i
N

i

c
wT
c φc(yi, xi) −log Z(w, xi)

and the gradient becomes
∂ℓ
∂wc
=
N

i

φc(yi, xi) −
∂
∂wc
log Z(w, xi)

=
N

i
[φc(yi, xi) −E [φc(y, xi)]]
Note that we now have to perform inference for every single training case inside each gradient
step, which is O(N) times slower than the MRF case. This is because the partition function
depends on the inputs xi.
In most applications of CRFs (and some applications of MRFs), the size of the graph structure
can vary. Hence we need to use parameter tying to ensure we can deﬁne a distribution of
arbitrary size. In the pairwise case, we can write the model as follows:
p(y|x, w) =
Z(w, x) exp

wT φ(y, x)
where w = [wn, we] are the node and edge parameters, and
φ(y, x) ≜[

t
φt(yt, x),

s∼t
φst(ys, yt, x)]
are the summed node and edge features (these are the sufficient statistics).
The gradient
expression is easily modiﬁed to handle this case.
Gaussian prior, the new objective becomes
N

i
It is simple to modify the gradient expression.
Alternatively, we can use ℓ1 regularization. For example, we could use ℓ1 for the edge weights
we to learn a sparse graph structure, and ℓ2 for the node weights wn, as in (Schmidt et al.
N

i
To handle large datasets, we can use stochastic gradient descent (SGD), as described in
It is possible (and useful) to deﬁne CRFs with hidden variables, for example to allow for an
unknown alignment between the visible features and the hidden labels (see e.g., (Schnitzspan
Structural SVMs
We have seen that training a CRF requires inference, in order to compute the expected sufficient
statistics needed to evaluate the gradient. For certain models, computing a joint MAP estimate
section, we discuss a way to train structured output classiﬁers that that leverages the existence of
fast MAP solvers. (To avoid confusion with MAP estimation of parameters, we will often refer to
MAP estimation of states as decoding.) These methods are known as structural support vector
a discussion of the differences.)
SSVMs: a probabilistic view
In this book, we have mostly concentrated on ﬁtting models using MAP parameter estimation,
i.e., by minimizing functions of the form
RMAP (w) = −log p(w) −
N

log p(yi|xi, w)However, at test time, we pick the label so as to minimize the posterior expected loss (deﬁned
ˆy(x|w) = argmin
ˆy

y
L(ˆy, y)p(y|x, w)
where L(y∗, ˆy) is the loss we incur when we estimate ˆy but the truth is y∗. It therefore seems
training set:
REL(w) ≜−log p(w) +
N

log

y
L(yi, y)p(y|xi, w)

We will assume that we can write our model in the following form:
p(y|x, w)
=
exp(wT φ(x, y))
Z(x, w)
p(w)
=
exp(−E(w))
Z
where Z(x, w) = 
y exp(wT φ(x, y)). Also, let us deﬁne L(yi, y) = exp ˜L(yi, y). With
this, we can rewrite our objective as follows:
REL(w)
=
−log p(w) +

i
log

y
exp ˜L(yi, y)exp(wT φ(x, y))
Z(x, w)

=
E(w) +

i
−log Z(xi, w) + log

y
exp
!
˜L(vyi, y) + wT φ(xi, y)
#
We will now consider various bounds in order to simplify this objective. First note that for
any function f(y) we have
max

exp[f(y)] ≤log

|Y| exp

max
y
f(y)
	
= log |Y| + max
y
lower and upper bound. Hence we see that
REL(w)
∼
E(w) +
N


max
y
&
˜L(yi, y) + wT φ(xi, y)
'
−max
y
wT φ(xi, y)

However,
performing these tasks separately will only result in an optimal decision if we can compute the exact posterior. In mostis not convex in w. However, we can devise a convex upper bound by exploiting the following
f(y′) ≤log

y
exp[f(y)]
REL(w)
≤
E(w) +
N


max
y
&
˜L(yi, y) + wT φ(xi, y)
'
−wT φ(xi, yi)

RSSV M(w)
≜
N


max
y
&
˜L(yi, y) + wT φ(xi, y)
'
−wT φ(xi, yi)

2yx, this
criterion reduces to the following (by considering the two cases that y = yi and y ̸= yi):
RSV M(w)
≜
N



So we see that the SSVM criterion can be seen as optimizing an upper bound on the Bayesian
This bound will be tight (and hence
the approximation will be a good one) when ||w|| is large, since in that case, p(y|x, w) will
concentrate its mass on argmaxy p(y|x, w).
Unfortunately, a large ||w|| corresponds to a
model that is likely to overﬁt, so it is unlikely that we will be working in this regime (because we
will tune the strength of the regularizer to avoid this situation). An alternative justiﬁcation for the
SVM criterion is that it focusses effort on ﬁtting parameters that affect the decision boundary.
This is a better use of computational resources than ﬁtting the full distribution, especially when
the model is wrong.
will set the stage for the algorithms we discuss below.
on the training set using this predictor if
∀i. maxities, resulting in a total of N|Y| −N linear constraints of the following form:
For brevity, we introduce the notation
δi(y) ≜φ(xi, yi) −φ(xi, y)
If we can achieve zero loss, there will typically be multiple solution vectors w. We pick the
one that maximizes the margin, deﬁned as
γ ≜min
i
f(x, yi; w) −max
y′∈Y\y f(x, y′; w)
in the optimization problem
max
s.t.
Equivalently, we can write
min
w
s.t.
To allow for the case where zero loss cannot be achieved (equivalent to the data being inseparable
in the case of binary classiﬁcation), we relax the constraints by introducing slack terms ξi, one
per data case. This yields
min
w,ξ
N

ξi
s.t.
In the case of structured outputs, we don’t want to treat all constraint violations equally. For
example, in a segmentation problem, getting one position wrong should be punished less than
getting many positions wrong. One way to achieve this is to divide the slack variable by the size
min
w,ξ
N

ξi
s.t.
ξi
Alternatively, we can deﬁne the margin to be proportional to the loss (this is called margin
min
w,ξ
N

ξi
s.t.
constraint.) This latter approach is used in M3nets.For future reference, note that we can solve for the ξ∗
i terms as follows:
ξ∗
Substituting in, and dropping the constraints, we get the following equivalent problem:
min
w

i
max
y
(
L(yi, y) + wT φ(xi, y)
)
−wT φ(xi, yi)
Empirical risk minimization
risk, deﬁned by
R(w) + C
N
N

L(yi, f(xi, w))
where R(w) is the regularizer, and f(xi, w) = argmaxy wT φ(xi, y) = ˆyi is the prediction.
Since this objective is hard to optimize, because the loss is not differentiable, we will construct
a convex upper bound instead.
We can show that
R(w) + C
N

i
max
is such a convex upper bound. To see this, note that
L(yi, f(xi, w))
≤
L(yi, f(xi, w)) −wT φ(xi, yi) + wT φ(xi, ˆyi)
≤
max
y
L(yi, y) −wT φ(xi, yi) + wT φ(xi, y)
Computational issues
straints. This is intractable, since Y is usually exponentially large. In the case of the margin
nomial number, provided the loss function and the feature vector decompose according to a
An alternative approach is to work directly with the exponentially sized QP. This allows for
the use of more general loss functions. There are several possible methods to make this feasible.
One is to use cutting plane methods. Another is to use stochastic subgradient methods. We
discuss both of these below.Fi
C
l
l
h
S
f h
l
l
h
I
(a) We add the ﬁrst constraint; the shaded region is the new feasible set. The new minimum norm solution
is w1. (b) We add another constraint; the dark shaded region is the new feasible set. (c) We add a third
constraint.
Cutting plane methods for ﬁtting SSVMs
This method can handle general loss functions, and is implemented in the popular SVMstruct
package11. The method is based on the cutting plane method from convex optimization (Kelley
The basic idea is as follows. We start with an initial guess w and no constraints. At each
iteration, we then do the following: for each example i, we ﬁnd the “most violated” constraint
the QP solver.) We can can easily modify the algorithm to optimize the slack rescaling version
The key to the efficiency of this method is that only polynomially many constraints need to
be added, and as soon as they are, the exponential number of other constraints are guaranteed
algorithm, i.e., to compute
argmax
L(yi, y) + wT φ(xi, y)s.t.
called the separation oracle.) If the loss function has an additive decomposition of the same
form as the features, then we can fold the loss into the weight vector, i.e., we can ﬁnd a new
algorithm, such as Viterbi, on the model p(y|x, w′).
˜y = argmax
y̸=ˆy
wT φ(xi, y)
Viterbi algorithm to do decoding; the second best path will differ from the best path in a single
position, which can be obtained by changing the variable whose max marginal is closest to its
decision boundary to its second best value. We can generalize this (with a bit more work) to
For Hamming loss, L(y∗, y) = 
t I(y∗
A linear time algorithm
Although the above algorithm takes polynomial time, we can do better, and devise an algorithm
that runs in linear time, assuming we use a linear kernel (i.e., we work with the original features
φ(x, y) and do not apply the kernel trick). The basic idea, as explained in (Joachims et al.just N|Y|. Speciﬁcally, we optimize the following (assuming the margin rescaling formulation):
min
s.t.
N wT
N

N
N

Compare this to the original version, which was
min
N ξ
s.t.
N ξ∗
i .
s.t.
N wT N
N
N
N
N
N wT N
Finally, it can be shown that the number of iterations is a constant independent on N. Thus
the overall running time is linear.
Online algorithms for ﬁtting SSVMs
Although the cutting plane algorithm can be made to run in time linear in the number of data
points, that can still be slow if we have a large dataset. In such cases, it is preferable to use
online learning. We brieﬂy mention a few possible algorithms below.
The structured perceptron algorithm
A very simple algorithm for ﬁtting SSVMs is the structured perceptron algorithm (Collinsstep, we compute ˆy = argmax p(y|x) (e.g., using the Viterbi algorithm) for the current training
sample x. If ˆy = y, we do nothing, otherwise we update the weight vector using
To get good performance, it is necessary to average the parameters over the last few updates
Stochastic subgradient descent
and it does not enforce any kind of margin. An alternative approach is to perform stochastic
for binary SVMs, but can be extended to SSVMS.
Let us start by considering the objective function:
f(w) =
N

max
ˆyi

L(yi, ˆyi) + wT φ(xi, ˆyi)

Letting ˆyi be the argmax of this max. Then the subgradient of this objective function is
g(w)
=
N

In stochastic subgradient descent, we approximate this gradient with a single term, i, and then
perform an update:
Latent structural SVMs
In many applications of interest, we have latent or hidden variables h. For example, in object
not know where it is. The location of the object, or its pose, can be considered a hidden variable.
Or in machine translation, we may know the source text x (say English) and the target text y
(say French), but we typically do not know the alignment between the words.
We will extend our model as follows, to get a latent CRF:
p(y, h|x, w)
=
exp(wT φ(x, y, h))
Z(x, w)
Z(x, w)
=

y,h
exp(wT φ(x, y, h))In addition, we introduce the loss function L(y∗, y, h); this measures the loss when the “action”
that we take is to predict y using latent variables h. We could just use L(y∗, y) as before, since
h is usually a nuisance variable and not of direct interest. However, h can sometimes play a
Given the loss function, we deﬁne our objective as
REL(w)
=
−log p(w) +

i
log
⎡
⎣
y,h
exp ˜L(yi, y, h)exp(wT φ(x, y, h))
Z(x, w)
⎤
⎦
Using the same loose lower bound as before, we get
REL(w)
≤
E(w) +
N

max
y,h
&
˜L(yi, y, h) + wT φ(xi, y, h)
'
−
N

max
h
wT φ(xi, yi, h)
Unfortunately, this objective is no longer convex.
However, it is a difference of convex
g(w), where f and g are convex. The method alternates between ﬁnding a linear upper bound
pseudocode. CCCP is guaranteed to decrease the objective at every iteration, and to converge to
a local minimum or a saddle point.
Find hyperplane vt such that −g(w) ≤−g(wt) + (w −wt)T vt for all w ;
When applied to latent SSVMs, CCCP is very similar to (hard) EM. In the “E step”, we compute
whether document j is relevant to q or not. Suppose our goal is to maximize the precision at k, which is a metric widely
of relevance. This corresponds to a latent total ordering, that has to be consistent with the observed partial ordering
k
k
kthe linear upper bound by setting vt = −C N
i ), where
hi = argmax
h
wT
t φ(xi, yi, h)
In the “M step”, we estimate w using techniques for solving fully visible SSVMs. Speciﬁcally, we
minimize
N

max
y,h
(
L(yi, y, h) + wT φ(xi, y, h)
)
−C
N

wT φ(xi, yi, h∗
i )
Exercises
(Source: Jordan.)
In this question, we study the relationship between sparse matrices and sparse graphs for Gaussian
throughout.
Recall that for jointly Gaussian random variables, we know that Xi and Xj are independent iff they are
a. Suppose
Σ =
⎛
⎝
⎞
⎠
an undirected graphical model that captures as many of these independence statements (marginal and
conditional) as possible, but does not make any false independence assertions.
b. Suppose
Σ =
⎛
⎝
⎞
⎠
pendencies amongst X1, X2 and X3? Draw an undirected graphical model that captures as many of
these independence statements (marginal and conditional) as possible, but does not make any false
independence assertions.
c. Now suppose the distribution on X can be represented by the following DAG:
Let the CPDs be as follows:d. For the DAG model in the previous question: Are there any marginal independencies amongst X1, X2
and X3? What about conditional independencies? Draw an undirected graphical model that captures
as many of these independence statements as possible, but does not make any false independence
assertions (either marginal or conditional).
(Source: MacKay.)
following matrices could be the covariance matrix?
A =
⎛
⎝
⎞
⎠, B =
⎛
⎝
⎞
⎠, C =
⎛
⎝
⎞
⎠, D =
⎛
⎝
⎞
b. Which of the above matrices could be inverse covariance matrix?
above matrices could be the covariance matrix?
d. Which of the above matrices could be the inverse covariance matrix?
follows
⎛
⎝
⎞
⎛
⎝
⎞
⎠


given a data set with N training instances. Assume for simplicity that the cost of computing a single
feature over a single instance in our data set is constant, as is the cost of computing the expected value
of each feature once we compute a marginal over the variables in its scope. Assume that it takes c time
to compute all the marginals for each data case. Also, assume that we need r iterations for the gradient
process to converge.
•
•
Consider an Ising model
p(x1, . . . , xn|θ) =
Z(θ)

exp(Jijxixj)
n

exp(hixi)where x−i are all nodes except i, and nbi are the neighbors of i in the graph. Hint: you answer should
expression for p(xi|x−i, θ) in this case. (This result can be used when applying Gibbs sampling to the
model.)Exact inference for graphical models
Introduction
variables (assumed discrete) and v are the visible variables. This algorithm can be modiﬁed
is to generalize these exact inference algorithms to arbitrary graphs. The resulting methods apply
to both directed and undirected graphical models. We will describe a variety of algorithms, but
detailed exposition of exact inference techniques for discrete directed graphical models.
Belief propagation for trees
The
algorithm.
Serial protocol
We initially assume (for notational simplicity) that the model is a pairwise MRF (or CRF), i.e.,
p(x|v)
=
Z(v)

ψs(xs)

(s,t)∈E
ψs,t(xs, xt)
where ψs is the local evidence for node s, and ψst is the potential for edge s −t. We will
consider the case of models with higher order cliques (such as directed trees) later on.
One way to implement BP for undirected trees is as follows. Pick an arbitrary node and call it
the root, r. Now orient all edges away from r (intuitively, we can imagine “picking up the graph”
and child. Now we send messages up from the leaves to the root (the collect evidence phase)
and then back down from the root (the distribute evidence phase), in a manner analogous tos1
s2
s
u1
u2
u
t
root
v−
st
(a)
s1
s2
s
u1
u2
u
t
root
st
(b)
to compute the belief state at node t. We will initially condition the belief only on evidence that
is at or below t in the graph, i.e., we want to compute bel−
t (xt) ≜p(xt|v−
t ). We will call this a
children, summarizing what they think t should know about the evidence in their subtrees, i.e.,
we have computed m−
st), where v−
st is all the evidence on the downstream
bel−
t (xt) ≜p(xt|v−
Zt
ψt(xt)

m−
where ψt(xt) ∝p(xt|vt) is the local evidence for node t, and Zt is the local normalization
constant. In words, we multiply all the incoming messages from our children, as well as the
incoming message from our local evidence, and then normalize.
How do we compute the messages themselves? Consider computing m−
of t’s children. Assume, by recursion, that we have computed bel−
s (xs) = p(xs|v−
s ). Then we
can compute the message as follows:
m−

xs
ψst(xs, xt)bel−
s (xs)
Essentially we convert beliefs about xs into beliefs about xt by using the edge potential ψst.
We continue in this way up the tree until we reach the root. Once at the root, we have “seen”
all the evidence in the tree, so we can compute our local belief state at the root using
belr(xr) ≜p(xr|v) = p(xt|v−
r ) ∝ψr(xr)

m−
This completes the end of the upwards pass, which is analogous to the forwards pass in an
HMM. As a “side effect”, we can compute the probability of the evidence by collecting thenormalization constants:
p(v) =

t
Zt
We can now pass messages down from the root. For example, consider node s, with parent t,
st is all the evidence on the upstream
bels(xs) ≜p(xs|v) ∝bel−
s (xs)

How do we compute these downward messages? For example, consider the message from t
sent it:
st) =

xt
ψst(xs, xt) belt(xt)
m−
Rather than dividing out the message sent up to t, we can plug in the equation of belt to get

xt
ψst(xs, xt)ψt(xt)

m−

In other words, we multiply together all the messages coming into t from all nodes except for
the recipient s, combine together, and then pass through the edge potential ψst. In the case of
a chain, t only has one child s and one parent p, so the above simpliﬁes to

xt
The version of BP in which we use division is called belief updating, and the version in
Parallel protocol
So far, we have presented a serial version of the algorithm, in which we send messages up
to the root and back. This is the optimal approach for a tree, and is a natural extension of
now consider a parallel version of BP. This gives equivalent results to the serial version but is
less efficient when implemented on a serial machine.The basic idea is that all nodes receive messages from their neighbors in parallel, they then
updates their belief states, and ﬁnally they send new messages back out to their neighbors.
This process repeats until convergence. This kind of computing architecture is called a systolic
array, due to its resemblance to a beating heart.
absorbs messages from all its neighbors using
bels(xs) ∝ψs(xs)

Then, in parallel, each node sends messages to each of its neighbors:
=

xs
⎛
⎝ψs(xs)ψst(xs, xt)

⎞
⎠
sent by the recipient, and then passing through the ψst potential.
At iteration T of the algorithm, bels(xs) represents the posterior belief of xs conditioned on
the evidence that is T steps away in the graph. After D(G) steps, where D(G) is the diameter
of the graph (the largest distance between any two pairs of nodes), every node has obtained
information from all the other nodes. Its local belief state is then the correct posterior marginal.
steps.
a node can only send a message once it has received messages from all its other neighbors.
This means we must start with the leaf nodes, which only have one neighbor. The messages
then propagate up to the root and back. We can also update the nodes in a random order.
The only requirement is that each node get updated D(G) times. This is just enough time for
information to spread throughout the whole tree.
Similar parallel, distributed algorithms for solving linear systems of equations are discussed
version of BP, and the Jacobi algorithm is analogous to the parallel version of BP.
Gaussian BP *
Now consider the case where p(x|v) is jointly Gaussian, so it can be represented as a Gaussian
will assume the following node and edge potentials:
ψt(xt)
=
2Attx2
t + btxt)
ψst(xs, xt)
=
2xsAstxt)
so the overall model has the form
2xT Ax + bT x)Note that by completing the square, the local evidence can be rewritten as a Gaussian:
t )
Below we describe how to use BP to compute the posterior node marginals,
t )
If the graph is a tree, the method is exact. If the graph is loopy, the posterior means may still
Although the precision matrix A is often sparse, computing the posterior mean requires
computation in O(D) time instead of O(D3). This is related to various methods from linear
Since the model is jointly Gaussian, all marginals and all messages will be Gaussian. The
key operations we need are to multiply together two Gaussian factors, and to marginalize out a
variable from a joint Gaussian factor.
For multiplication, we can use the fact that the product of two Gaussians is Gaussian:
=
λ
=
λ1 + λ2
μ
=
where
C =
.
λ
exp
	
For marginalization, we have the following result:


exp(−ax2 + bx)dx =
"
bels(xs)
=
ψs(xs)

s )
λs
=
ℓs +

λts
μs
=
s
⎛
⎝ℓsms +

⎞
⎠=


xs
⎛
⎝ψst(xs, xt)ψs(xs)

⎞
⎠dxs
=


xs
ψst(xs, xt)fs\t(xs)dxs
where fs\t(xs) is the product of the local evidence and all incoming messages excluding the
message from t:
fs\t(xs)
≜
ψs(xs)

=
s\t)
λs\t
≜
ℓs +

λus
μs\t
≜
s\t
⎛
⎝ℓsms +

⎞
⎠
=


xs
exp(−xsAstxt)
/
ψst(xs,xt)
/
fs\t(xs)
dxs
=


xs
exp

(−λs\tx2

dxs + const
∝
exp


∝
st )
λst
=
A2
μst
=
One can generalize these equations to the case where each node is a vector, and the messages
If we apply the
resulting algorithm to a linear dynamical system, we recover the Kalman smoothing algorithm
Other BP variants *
In this section, we brieﬂy discuss several variants of the main algorithm.xt
vt
ψt
ft
with the max operator. We can then compute the local MAP marginal of each node. However,
details.
Sampling from a tree
It is possible to draw samples from a tree structured model by generalizing the forwards ﬁltering
Computing posteriors on sets of variables
=
“mega node”, and then multiplying all the incoming messages as well as all the local factorsCoherence
Diﬃculty
Grade
Intelligence
SAT
Letter
Happy
Job
(a)
Coherence
Diﬃculty
Grade
Intelligence
SAT
Letter
Happy
Job
(b)
The variable elimination algorithm
We have seen how to use BP to compute exact marginals on chains and trees. In this section,
we discuss an algorithm to compute p(xq|xv) for any kind of graph.
a typical student. The corresponding joint has the following form:
P(C, D, I, G, S, L, J, H)
=
P(C)P(D|C)P(I)P(G|I, D)P(S|I)P(L|G)P(J|L, S)P(H|G, J)
Note that the forms of the CPDs do not matter, since all our calculations will be symbolic.
However, for illustration purposes, we will assume all variables are binary.
Before proceeding, we convert our model to undirected form. This is not required, but it
makes for a more uniﬁed presentation, since the resulting method can then be applied to both
have nothing to do with graphical models). Since the computational complexity of inference in
DGMs and UGMs is, generally speaking, the same, nothing is lost in this transformation from a
To convert the DGM to a UGM, we simply deﬁne a potential or factor for every CPD, yielding
p(C, D, I, G, S, L, J, H)
=
Suppose we want to infer y and we observe x1 and x2, but not x3 and x4. It is clear that we can safely remove
x3 and x4, since 
apply this process recursively. Since potential functions do not necessary sum to one, we cannot use this trick in theSince all the potentials are locally normalized, since they are CPDs, there is no need for a
that share a child must get “married”, by adding an edge between them; this process is known
clique that can “store” the CPDs of each family.
to all the variables (except for J), adding up the probability of each joint instantiation:
p(J)
=

L

S

G

H

I

D

C
p(C, D, I, G, S, L, J, H)
p(J)
=

L,S,G,H,I,D,C
p(C, D, I, G, S, L, J, H)
=

L,S,G,H,I,D,C
ψC(C)ψD(D, C)ψI(I)ψG(G, I, D)ψS(S, I)ψL(L, G)
×ψJ(J, L, S)ψH(H, G, J)
=

L,S
ψJ(J, L, S)

G
ψL(L, G)

H
ψH(H, G, J)

I
ψS(S, I)ψI(I)
×

D
ψG(G, I, D)

C
ψC(C)ψD(D, C)
together all the terms in the scope of the 
C operator to create the temporary factor
τ ′
Then we marginalize out C to get the new factor
τ1(D) =

C
τ ′
Next we multiply together all the terms in the scope of the 
D operator and then marginalize
out to create
τ ′
=
ψG(G, I, D)τ1(D)
τ2(G, I)
=

D
τ ′
L

S
ψJ(J, L, S)

G
ψL(L, G)

H
ψH(H, G, J)

I
ψS(S, I)ψI(I)

D
ψG(G, I, D)

C
ψC(C)ψD(D, C)

	


τ1(D)

L

S
ψJ(J, L, S)

G
ψL(L, G)

H
ψH(H, G, J)

I
ψS(S, I)ψI(I)

D
ψG(G, I, D)τ1(D)

	


τ2(G,I)

L

S
ψJ(J, L, S)

G
ψL(L, G)

H
ψH(H, G, J)

I
ψS(S, I)ψI(I)τ2(G, I)

	


τ3(G,S)

L

S
ψJ(J, L, S)

G
ψL(L, G)

H
ψH(H, G, J)

	


τ4(G,J)
τ3(G, S)

L

S
ψJ(J, L, S)

G
ψL(L, G)τ4(G, J)τ3(G, S)

	


τ5(J,L,S)

L

S
ψJ(J, L, S)τ5(J, L, S)

	


τ6(J,L)

L
τ6(J, L)

	


τ7(J)
Next we multiply together all the terms in the scope of the 
I operator and then marginalize
out to create
τ ′
=
ψS(S, I)ψI(I)τ2(G, I)
τ3(G, S)
=

I
τ ′
And so on.
The above technique can be used to compute any marginal of interest, such as p(J) or
p(J, H). To compute a conditional, we can take a ratio of two marginals, where the visible
variables have been clamped to their known values (and hence don’t need to be summed over).
For example,

In general, we can write
p(xq|xv)
=
p(xq, xv)
p(xv)
=

xh p(xh, xq, xv)

xh

x′
q p(xh, x′q, xv)dence.
See variableElimination for a simple Matlab implementation of this algorithm, which
works for arbitrary graphs, and arbitrary discrete factors. But before you go too crazy, please
The generalized distributive law *
Abstractly, VE can be thought of as computing the following expression:
p(xq|xv) ∝

x

c
ψc(xc)
It is understood that the visible variables xv are clamped, and not summed over.
VE uses
avoid redundant computation.
However, there are other tasks we might like to solve for any given graphical model. For
example, we might want the MAP estimate:
x∗= argmax
x

c
ψc(xc)
Fortunately, essentially the same algorithm can also be used to solve this task: we just replace
sum with max. (We also need a traceback step, which actually recovers the argmax, as opposed
two binary operations called “+” and “×”, which satisfy the following three axioms:
(a × b) + (a × c) = a × (b + c)
for all triples (a, b, c) from K.
This framework covers an extremely wide range of important applications, including constraint
Computational complexity of VE
The running time of VE is clearly exponential in the size of the largest factor, since we have sum
over all of the corresponding variables. Some of the factors come from the original model (and
are thus unavoidable), but new factors are created in the process of summing out. For example,Domain
+
×
Name
(−∞, ∞]
(min, ∞)
{T, F}
(∨, F)
(∧, T)
Boolean satisﬁability
Some commutative semirings.

D

C
ψD(D, C)

H

L

S
ψJ(J, L, S)

I
ψI(I)ψS(S, I)

G
ψG(G, I, D)ψL(L, )ψH(H, G, J)

	


τ1(I,D,L,J,H)

D

C
ψD(D, C)

H

L

S
ψJ(J, L, S)

I
ψI(I)ψS(S, I)τ1(I, D, L, J, H)

	


τ2(D,L,S,J,H)

D

C
ψD(D, C)

H

L

S
ψJ(J, L, S)τ2(D, L, S, J, H)

	


τ3(D,L,J,H)

D

C
ψD(D, C)

H

L
τ3(D, L, J, H)

	


τ4(D,J,H)

D

C
ψD(D, C)

H
τ4(D, J, H)

	


τ5(D,J)

D

C
ψD(D, C)τ5(D, J)

	


τ6(D,J)

D
τ6(D, J)

	


τ7(J)
present together in any factor.
The order in which we perform the summation is known as the elimination order. This
can have a large impact on the size of the intermediate factors that are created. For example,
the largest factors are τ1(I, D, L, J, H) and τ2(D, L, S, J, H), which are much bigger.
We can determine the size of the largest factor graphically, without worrying about the actual
numerical values of the factors. When we eliminate a variable Xt, we connect it to all variablesCoherence
Diﬃculty
Grade
Intelligence
SAT
Letter
Happy
Job
(a)
Coherence
Diﬃculty
Grade
Intelligence
SAT
Letter
Happy
Job
(b)
Coherence
Diﬃculty
Grade
Intelligence
SAT
Letter
Happy
Job
(c)
Example of the elimination process, in the order C, D, I, etc. When we eliminate I (ﬁgure
that share a factor with Xt (to reﬂect the new temporary factor τ ′
t). The edges created by this
Let G(≺) be the (undirected) graph induced by applying variable elimination to G using
elimination ordering ≺. The temporary factors generated by VE correspond to maximal cliques
in the graph G(≺). For example, with ordering (C, D, I, H, G, S, L), the maximal cliques are
as follows:
{C, D}, {D, I, G}, {G, L, S, J}, {G, J, H}, {G, I, S}
It is clear that the time complexity of VE is

K|c|
where C are the cliques that are created, |c| is the size of the clique c, and we assume for
notational simplicity that all the variables have K states each.
Let us deﬁne the induced width of a graph given elimination ordering ≺, denoted w(≺), as
Obviously we would like to minimize the running time, and hence the induced width. Let us
deﬁne the treewidth of a graph as the minimal induced width.
w ≜min
≺
maxIt is also possible to derive approximate
In some cases, the optimal elimination ordering is clear. For example, for chains, we should
work forwards or backwards in time. For trees, we should work from the leaves to the root.
and trees takes O(V K2) time. This is one reason why Markov chains and Markov trees are so
widely used.
Unfortunately, for other graphs, the treewidth is large. For example, for an m × n 2d lattice,
Of course, just because VE is slow doesn’t mean that there isn’t some smarter algorithm out
A weakness of VE
dence on treewidth) is that it is inefficient if we want to compute multiple queries conditioned
is that FB caches the messages computed on the forwards pass, so it can reuse them later.
up to x1 (the messages correspond to the τ factors created by VE). Similarly we can compute
p(x2|v), p(x3|v) and then p(x4|v). We see that some of the messages used to compute the
The question is: how can we combine the efficiency of BP on trees with the generality of VE?
The junction tree algorithm *
The junction tree algorithm or JTA generalizes BP from trees to arbitrary graphs. We sketch
Creating a junction tree
edges as we go, according to a given elimination ordering. The resulting graph will be a chordal;

;

;

;

;

;

;

;

;

;

;

;

;

;

;

;


D


E

P
[

P
[

P
[

P
[

P
[

P
[

P
[

P
[

P
[

P
[

P
[

P
[

P
[

P
[

P
[

Sending multiple messages along a tree. (a) X1 is root. (b) X2 is root. (c) X4 is root. (d) All






(a)






(b)Having created a chordal graph, we can extract its maximal cliques. In general, ﬁnding max
cliques is computationally hard, but it turns out that it can be done efficiently from this special
{C, D}, {G, I, D}, {G, S, I}, {G, J, S, L}, {H, G, J}
Note that if the original graphical model was already chordal, the elimination process would not
models decomposable, since they break into little pieces deﬁned by the cliques.
It turns out that the cliques of a chordal graph can be arranged into a special kind of
tree known as a junction tree. This enjoys the running intersection property (RIP), which
means that any subset of nodes containing a given variable forms a connected component.
tree nodes, so they can share information about this variable. A similar situation holds for all
the other variables.
One can show that if a tree that satisﬁes the running intersection property, then applying
BP to this tree (as we explain below) will return the exact values of p(xc|v) for each node c
in the tree (i.e., clique in the induced graph). From this, we can easily extract the node and
edge marginals, p(xt|v) and p(xs, xt|v) from the original model, by marginalizing the clique
Message passing on a junction tree
Having constructed a junction tree, we can use it for inference. The process is very similar
the belief updating form (which involves division), also known as the Hugin (named after a
sketch how the Hugin algorithm works.
We assume the original model has the following form:
Z

ψc(xc)
where C(G) are the cliques of the original graph.
On the other hand, the tree deﬁnes a
distribution of the following form:
p(x) =


variables and any other nuisance variables that lie on the path between them, multiply in messages onto the boundary
of the mega node, and then marginalize out the internal nuisance variables. This internal marginalization may requireGrade
Letter
Job
Happy
Coherence
SAT
G,J,S,L
H,G,J
G,S,I
G,I,D
C,D
G,J
G,S
G,I
D
Intelligence
Difﬁculty
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
(a)
(b)
(c)
tree. An edge between nodes s and t is labeled by the intersection of the sets on nodes s and t; this is
Daphne Koller.
where C(T) are the nodes of the junction tree (which are the cliques of the chordal graph), and
we ﬁnd a clique in the tree c′ ∈C(T) which contains it, c′ ⊇c. We then multiply ψc onto ψc′
by computing ψc′ = ψc′ ψc. After doing this for all the cliques in the original graph, we have

ψc(xc) =

ψc(xc)
its parent j the following message:

Ci\Sij
ψi(Ci)
That is, we marginalize out the variables that node i “knows about” which are irrelevant to j,
and then we send what is left over. Once a node has received messages from all its children, it
updates its belief state using
ψi(Ci) ∝ψi(Ci)
At the root, ψr(Cr) represents p(xCr|v), which is the posterior over the nodes in clique
Cr conditioned on all the evidence. Its normalization constant is p(v)/Z0, where Z0 is the
was a DGM.)
children j the following message:

Ci\Sij ψi(Ci)
We divide out by what j sent to i to avoid double counting the evidence. This requires that we
its parent, it can compute its ﬁnal belief state using
An equivalent way to present this algorithm is based on storing the messages inside the
separator potentials. So on the way up, sending from i to j we compute the separator potential
ψ∗
ij(Sij) =

Ci\Sij
ψi(Ci)
and then update the recipient potential:
ψ∗
j (Cj) ∝ψj(Cj)ψ∗
ij(Sij)
ψij(Sij)
On the way down, from i to j, we compute the separator potential
ψ∗∗
ij (Sij) =

Ci\Sij
ψ∗
i (Ci)
and then update the recipient potential:
ψ∗∗
j (Cj) ∝ψ∗
j (Cj)ψ∗∗
ij (Sij)
ψ∗
ij(Sij)
This process is known as junction tree calibration.
Its
correctness follows from the fact that each edge partitions the evidence into two distinct groups,
plus the fact that the tree satisﬁes RIP, which ensures that no information is lost by only
performing local computations.
Example: jtree algorithm on a chain
It is interesting to see what happens if we apply this process to a chain structured graph such asX1 X2
X1
X2
X1 Y1
X2 Y2
X2
X2 X3
X3
X3 Y3
X3
X3 X4
X4
X4 Y4
Next we send messages from left to right.

p(yt|Xt), which corresponds to its local evidence. When combined with the updated clique
Thus the messages in the forwards pass are the ﬁltered belief states αt, and the clique potentials
αt ,
where γt(k) = p(xt = k|v1:T ) and αt(k) = p(xt = k|v1:t). By multiplying by this message,
we “swap out” the old αt message and “swap in” the new γt message. We see that the backwards
further discussion of this difference.
Computational complexity of JTA
and space, where |C| is the number of cliques and w is the treewidth of the graph, i.e., the
steps remain unchanged. Only the message computation differs. We just need to deﬁne how
to multiply, divide, and marginalize Gaussian potential functions. This is most easily done in
algorithm takes O(|C|w3) time and O(|C|w2) space. When applied to a chain structured graph,Q1
Qn
Q4
Q3
Q2
C1
A1
X
A2
Cm
C3
C2
. . .
. . .
binary random variables. The Ct variables are deterministic functions of the Qs’s, and compute the truth
value of each clause. The At nodes are a chain of AND gates, to ensure that the CPT for the ﬁnal x node
has bounded size. The double rings denote nodes with deterministic CPDs.
Used with kind permission of Daphne Koller.
JTA generalizations *
We have seen how to use the JTA algorithm to compute posterior marginals in a graphical model.
There are several possible generalizations of this algorithm, some of which we mention below.
All of these exploit graph decomposition in some form or other. They only differ in terms of
•
•
•
Computing posterior samples. The collect pass is the same as usual, but in the distribute
details.
•
•
Computational intractability of exact inference in the worst case
of a graph. Since the treewidth can be O(number of nodes) in the worst case, this means these
algorithms can be exponential in the problem size.
rithm out there. Unfortunately, this seems unlikely, since it is easy to show that exact inferenceMethod
Restriction
Section
Chains, D or LG
Belief propagation
Trees, D or LG
Variable elimination
Low treewidth, D or LG, single query
Junction tree algorithm
Low treewidth, D or LG
Loopy belief propagation
Approximate, D or LG
Convex belief propagation
Approximate, D or LG
Mean ﬁeld
Gibbs sampling
Approximate
Summary of some methods that can be used for inference in graphical models. “D” means
The term “single query” refers to the restriction that VE only computes one marginal p(xq|xv) at a time.
variational mean ﬁeld only applies to models where the likelihood is in the exponential family, and the
evidence, so inference in this model implicitly solves the SAT problem.
compute the normalizing constant Z, we have to count how many satisfying assignments there
since, intuitively speaking, it only requires ﬁnding one satisfying assignment, not counting all of
them.
Approximate inference
Many popular probabilistic models support efficient exact inference, since they are based on
chains, trees or low treewidth graphs.
But there are many other models for which exact
summary of coming attractions.
For the most part, these methods do not come with any
guarantee as to their accuracy or running time. Theoretical computer scientists would therefore
describe them as heuristics rather than approximation algorithms. In fact, one can prove that
binary variables, and each clause consists of the conjunction of three variables (or their negation). The goal is to ﬁnd a
satisfying assignment, which is a set of values for the Qi variables such that the expression evaluates to true.
inference in models with discrete hidden states.it is not possible to construct polynomial time approximation schemes for inference in general
heuristic methods often perform well in practice.
Exercises
i.e.,


x6


x5


x4


x3


x2


x1
If we use the variable elimination algorithm, we will create new intermediate factors. What is the largest
intermediate factor?
b. Add an edge to the original MRF between every pair of variables that end up in the same factor. (These
are called ﬁll in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this
graph?


x6


x5


x3


x2


x1


x4
If we use the variable elimination algorithm, we will create new intermediate factors. What is the largest
intermediate factor?
d. Add an edge to the original MRF between every pair of variables that end up in the same factor. (These
are called ﬁll in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this
graph?
measure of blood pressure, which is low if you are healthy and high if you are unhealthy. We deﬁne the
CPDs as follows
p(G1)
=
p(G2|G1)
=

p(G3|G1)
=

=
=
etc.X1
X2
X3
G1
G2
G3
A simple DAG representing inherited diseases.
What is the posterior belief on G1, i.e.,
intuitively.
Consider an MRF with a 2D m × n lattice graph structure, so each hidden node, Xij, is connected to its
because the maximum cliques of the corresponding triangulated graph have size O(max{m, n}). Suppose
m ≪n i.e., the lattice is short and fat.
a. How can one efficiently perform exact inference (using a deterministic algorithm) in such models? (By
exact inference, I mean computing marginal probabilities P(Xij|⃗y) exactly, where ⃗y is all the evidence.)
Give a brief description of your method.
b. What is the asymptotic complexity (running time) of your algorithm?
this case, how can one efficiently exactly compute (using a deterministic algorithm) the MAP estimate
arg maxx P(x|y), where x is the joint assignment to all hidden nodes?Variational inference
Introduction
We have now seen several algorithms for computing (functions of) a posterior distribution. For
discrete graphical models, we can use the junction tree algorithm to perform exact inference,
However, this takes time exponential in the treewidth of the
graph, rendering exact inference often impractical. For the case of Gaussian graphical models,
exact inference is cubic in the treewidth. However, even this can be too slow if we have many
variables. In addition, the JTA does not work for continuous random variables outside of the
case.
exact posterior p(x|D) in closed form, provided the prior p(x) is conjugate to the likelihood,
examples of this. (Note that in this chapter, x represent the unknown variables, whereas in
discussed the Gaussian approximation, which is useful for inference in two node models of the
to logistic regression.)
The Gaussian approximation is simple. However, some posteriors are not naturally modelled
using Gaussians. For example, when inferring multinomial parameters, a Dirichlet distribution is
a better choice, and when inferring states in a discrete graphical model, a categorical distribution
is a better choice.
In this chapter, we will study a more general class of deterministic approximate inference
distribution from some tractable family, and then to try to make this approximation as close
as possible to the true posterior, p∗(x) ≜p(x|D). This reduces inference to an optimization
for speed. The bottom line is that variational inference often gives us the speed beneﬁts of MAP
estimation but the statistical beneﬁts of the Bayesian approach.Variational inference
Suppose p∗(x) is our true but intractable distribution and q(x) is some approximation, chosen
from some tractable family, such as a multivariate Gaussian or a factored distribution.
We
assume q has some free parameters which we want to optimize so as to make q “similar to” p∗.
An obvious cost function to try to minimize is the KL divergence:
KL (p∗||q) =

x
p∗(x) log p∗(x)
q(x)
However, this is hard to compute, since taking expectations wrt p∗is assumed to be intractable.
A natural alternative is the reverse KL divergence:
KL (q||p∗) =

x
q(x) log q(x)
p∗(x)
ing a suitable form for q). We discuss the statistical differences between these two objectives in
p(x|D) pointwise is hard, since it requires evaluating the intractable normalization constant
Z = p(D).
However, usually the unnormalized distribution ˜p(x) ≜p(x, D) = p∗(x)Z is
tractable to compute. We therefore deﬁne our new objective function as follows:
J(q)
≜
KL (q||˜p)
where we are slightly abusing notation, since ˜p is not a normalized distribution. Plugging in the
deﬁnition of KL, we get
J(q)
=

x
q(x) log q(x)
˜p(x)
=

x
q(x) log
q(x)
Zp∗(x)
=

x
q(x) log q(x)
p∗(x) −log Z
=
KL (q||p∗) −log Z
Since Z is a constant, by minimizing J(q), we will force q to become close to p∗.
(negative log likelihood):
is referred to as the energy functional), which is a lower bound on the log likelihood of the
data:
Since this bound is tight when q = p∗, we see that variational inference is closely related to EMAlternative interpretations of the variational objective
There are several equivalent ways of writing this objective that provide different insights. One
formulation is as follows:
J(q) = Eq [log q(x)] + Eq [−log ˜p(x)] = −H (q) + Eq [E(x)]
which is the expected energy (recall E(x) = −log ˜p(x)) minus the entropy of the system. In
Another formulation of the objective is as follows:
J(q)
=
Eq [log q(x) −log p(x)p(D|x)]
=
Eq [log q(x) −log p(x) −log p(D|x)]
=
Eq [−log p(D|x)] + KL (q(x)||p(x))
This is the expected NLL, plus a penalty term that measures how far the approximate posterior
is from the exact prior.
We can also interpret the variational objective from the point of view of information theory
details.
Forward or reverse KL? *
Since the KL divergence is not symmetric in its arguments, minimizing KL (q||p) wrt q will give
different behavior than minimizing KL (p||q). Below we discuss these two different methods.
projection. By deﬁnition, we have
KL (q||p) =

x
q(x) ln q(x)
p(x)
p.
KL (p||q) =

x
p(x) ln p(x)
q(x)
p.
distribution is multimodal, using the forwards KL is a bad idea (assuming q is constrained to
between the two peaks. In such contexts, the reverse KL is not only more tractable to compute,
but also more sensible statistically.(a)
(b)
(c)
Illustrating forwards vs reverse KL on a bimodal distribution. The blue curves are the contours
of the true distribution p. The red curves are the contours of the unimodal approximation q. (a) Minimizing
(a)
(b)
Illustrating forwards vs reverse KL on a symmetric Gaussian.
The blue curves are the
contours of the true distribution p. The red curves are the contours of a factorized approximation q. (a)
by KLpqGauss.
an elongated 2d Gaussian and the approximating distribution is a product of two 1d Gaussians.
μ =
μ1
μ2
	
, Λ =

Λ11
Λ12
Λ21
Λ22
	
show that the solution has the form
q(x)
=
m1
=
m2
=
function of the distribution q, whereas the regular energy is a function of the state vector x.compact: its variance is controlled by the direction of smallest variance of p. In fact, it is
factorized, results in an approximation that is overconﬁdent.
the optimal solution when minimizing the forward KL wrt a factored approximation is to set q
to be the product of marginals. Thus the solution has the form
For the rest of this chapter, and for most of the next, we will focus on minimizing KL (q||p).
KL (p||q).
One can create a family of divergence measures indexed by a parameter α ∈R by deﬁning
the alpha divergence as follows:
Dα(p||q) ≜



	
the Hellinger distance, deﬁned by
DH(p||q) ≜

 !
p(x)
dx
Note that
"
The mean ﬁeld method
approximation of the form
q(x) =

i
qi(xi)
Our goal is to solve this optimization problem:
min
q1,...,qD KL (q||p)
derive a coordinate descent method, where at each step we make the following update:
log qj(xj) = E−qj [log ˜p(x)] + constModel
Section
Ising model
Factorial HMM
Univariate Gaussian
Linear regression
Logistic regression
Mixtures of Gaussians
Latent Dirichlet allocation
Some models in this book for which we provide detailed derivations of the mean ﬁeld inference
algorithm.
where ˜p(x) = p(x, D) is the unnormalized posterior and the notation E−qj [f(x)] means to
take the expectation over f(x) with respect to all the variables except for xj. For example, if
we have three variables, then
E−q2 [f(x)] =

x1

x3
q(x1)q3(x3)f(x1, x2, x3)
where sums get replaced by integrals where necessary.
When updating qj, we only need to reason about the variables which share a factor with xj,
constant term. Since we are replacing the neighboring values by their mean value, the method
of sending sampled values between neighboring nodes, we send mean values between nodes.
This tends to be more efficient, since the mean can be used as a proxy for a large number of
samples. (On the other hand, mean ﬁeld messages are dense, whereas samples are sparse; this
can make sampling more scalable to very large models.)
Of course, updating one distribution at a time can be slow, since it is a form of coordinate
descent. Several methods have been proposed to speed up this basic approach, including using
It is important to note that the mean ﬁeld method can be used to infer discrete or continuous
latent quantities, using a variety of parametric forms for qi, as we will see below.
This is
in contrast to some of the other variational methods we will encounter later, which are more
in this book.
Derivation of the mean ﬁeld update equations
Recall that the goal of variational inference is to minimize the upper bound J(q) ≥−log p(D).
Equivalently, we can try to maximize the lower bound
L(q) ≜−J(q) =

x
q(x) log ˜p(x)
q(x) ≤log p(D)
We will do this one term at a time.If we write the objective singling out the terms that involve qj, and regarding all the other
terms as constants, we get
L(qj)
=

x

i
qi(xi)

log ˜p(x) −

k
log qk(xk)

=

xj

x−j
qj(xj)

i̸=j
qi(xi)

log ˜p(x) −

k
log qk(xk)

=

xj
qj(xj)

x−j

i̸=j
qi(xi) log ˜p(x)
−

xj
qj(xj)

x−j

i̸=j
qi(xi)
⎡
⎣
k̸=j
log qk(xk) + qj(xj)
⎤
⎦
=

xj
qj(xj) log fj(xj) −

xj
qj(xj) log qj(xj) + const
where
log fj(xj)
≜

x−j

i̸=j
qi(xi) log ˜p(x) = E−qj [log ˜p(x)]
So we average out all the hidden variables except for xj. Thus we can rewrite L(qj) as follows:
L(qj) = −KL (qj||fj)
We can maximize L by minimizing this KL, which we can do by setting qj = fj, as follows:
qj(xj)
=
Zj
exp

E−qj [log ˜p(x)]

We can usually ignore the local normalization constant Zj, since we know qj must be a
normalized distribution. Hence we usually work with the form
log qj(xj) = E−qj [log ˜p(x)] + const
The functional form of the qj distributions will be determined by the type of variables xj, as
discrete random variable, then qj will be a discrete distribution; if xj is a continuous random
variable, then qj will be some kind of pdf. We will see examples of this below.
Example: mean ﬁeld for the Ising model
pixel values of the “clean” image. We have a joint model of the form
p(x, y)
=
p(x)p(y|x)where the prior has the form
p(x)
=
Z0
exp(−E0(x))
E0(x)
=
−
D


Wijxixj
and the likelihood has the form
p(y|x) =

i
p(yi|xi) =

i
exp(−Li(xi))
Therefore the posterior has the form
p(x|y)
=
Z exp(−E(x))
E(x)
=
E0(x) −

i
Li(xi)
We will now approximate this by a fully factored approximation
q(x) =

i
q(xi, μi)
ﬁrst write out log ˜p(x) = −E(x), dropping terms that do not involve xi:
log ˜p(x) = xi

Wijxj + Li(xi) + const
This only depends on the states of the neighboring nodes. Now we take expectations of this wrt

j̸=i qj(xj) to get
qi(xi) ∝exp
⎛
⎝xi

⎞
⎠
Thus we replace the states of the neighbors by their average values. Let
mi =

be the mean ﬁeld inﬂuence on node i.
i
i
The
approximate marginal posterior is given by
=
i
i =
i = sigm(2ai)
ai
≜
i −L−
i ) 
 
(a)
 
 
(b)
 
 
(c)
Example of image denoising using mean ﬁeld (with parallel updates and a damping factor
We show
isingImageDenoiseDemo.
site i:
μi
=
=
eai
eai + e−ai −
e−ai
e−ai + eai = tanh(ai)
Hence the update equation becomes
μi = tanh
⎛
⎝
i −L−
i )
⎞
⎠
We can turn the above equations in to a ﬁxed point algorithm by writing
μt
i = tanh
⎛
⎝
j
i −L−
i )
⎞
⎠
It is usually better to use damped updates of the form
μt
i
+ λ tanh
⎛
⎝
j
i −L−
i )
⎞
⎠
don’t use damping, we tend to get “checkerboard” artefacts.)
Structured mean ﬁeld *
Assuming that all the variables are independent in the posterior is a very strong assumption that
can lead to poor results. Sometimes we can exploit tractable substructure in our problem, soy1
y2
y3
(a)
(b)
(c)
that we can efficiently handle some kinds of dependencies. This is called the structured mean
of variables together, and we update them simultaneously. (This follows by simply treating all
work in this area.
Example: factorial HMM
Suppose there are M chains, each of length T, and suppose each hidden node has K states.
The model is deﬁned as follows
p(x, y) =

m

t
p(x1m = k|x0m) = p(x1m = k) = πmk, is the initial state distribution for chain m, and
p(yt|xt) = N

yt|
M

Wmxtm, Σ


Even though each chain is a priori independent, they become coupled in the posterior due
to having an observed common child, yt. The junction tree algorithm applied to this graph
good performance).We can write the exact posterior in the following form:
p(x|y)
=
Z exp(−E(x, y))
E(x, y)
=
T


yt −

m
Wmxtm

T

yt −

m
Wmxtm


−

m
xT
1m ˜πm −
T


m
xT
where ˜Am ≜log Am and ˜πm ≜log πm (both interpreted elementwise).
q(x|y)
=
Zq
M

q(x1m|ξ1m)
T

q(x1m|ξ1m)
=
K

(ξ1mkπmk)x1mk
=
K

⎛
⎝ξtmk
K

⎞
⎠
xtmk
We see that the ξtmk parameters play the role of an approximate local evidence, averaging out
the effects of the other chains. This is contrast to the exact local evidence, which couples all
the chains together.
We can rewrite the approximate posterior as q(x) =
Zq exp(−Eq(x)), where
Eq(x)
=
−
T

M

xT
tm˜ξtm −
M

xT
1m ˜πm −
T

M

xT
where ˜ξtm = log ξtm. We see that this has the same temporal factors as the exact posterior,
but the local evidence term is different. The objective function is given by
KL (q||p) = E [E] −E [Eq] −log Zq + log Z
form
ξtm
=
exp

WT
	
δm
≜
diag(WT
mΣ−1Wm)
˜ytm
≜
yt −
M

ℓ̸=m
WℓE [xt,ℓ]The ξtm parameter plays the role of the local evidence, averaging over the neighboring chains.
these approximate local evidence terms to compute q(xt,m|y1:T ) for each m and t.
The update cost is O(TMK2) for a full “sweep” over all the variational parameters, since we
as a fully factorized approximation, but is much more accurate.
Variational Bayes
of the model are known. Now suppose we want to infer the parameters themselves. If we
make a fully factorized (i.e., mean ﬁeld) approximation, p(θ|D) ≈
k q(θk), we get a method
that there are no latent variables. If we want to infer both latent variables and parameters, and
we make an approximation of the form p(θ, z1:N|D) ≈q(θ) 
i qi(zi), we get a method known
Example: VB for a univariate Gaussian
will use a conjugate prior of the form
However, we will use an approximate factored posterior of the form
out” automatically during the derivation (and conveniently, they turn out to be Gaussian and
Gamma respectively).
You might wonder why we would want to do this, since we know how to compute the
There are two reasons.
First, it is a useful
pedagogical exercise, since we can compare the quality of our approximation to the exact
form p(μ, λ) = N(μ|μ0, τ0)Ga(λ|a0, b0), for which exact inference is no longer possible.
(a distribution) instead of a point estimate. However, the term “ensemble learning” is also used to describe methods
such as boosting, so we prefer the term VB.Target distribution
The unnormalized log posterior has the form
log ˜p(μ, λ)
=
log p(μ, λ, D) = log p(D|μ, λ) + log p(μ|λ) + log p(λ)
=
N
N

=
=
N

+ const
N ), where
κ0 + N
derive this below.
=
=
−λ

N


+ const
aN
=
bN
=

N

Computing the expectations
To implement the updates, we have to specify how to compute the various expectations. Since
N ), we have
Eq(μ) [μ]
=
μN
Eq(μ)

μ2
=
κN
+ μ2
N
Since q(λ) = Ga(λ|aN, bN), we have
Eq(λ) [λ]
=
aN
bN
We can now give explicit forms for the update equations. For q(μ) we have
μN
=
κ0 + N
κN
=
(κ0 + N)aN
bN
and for q(λ) we have
aN
=
bN
=
b0 + κ0(E

μ2
+ μ2
N


x2
i + E

μ2
−2E [μ] xi

iteratively. (In fact, one can solve for the ﬁxed points of κN and bN analytically, but we don’t
do this here in order to illustrate the iterative updating scheme.)
Illustration
The green contours represent the
approximation over several iterations. We see that the ﬁnal approximation is reasonably close to
the exact solution. However, it is more “compact” than the true distribution. It is often the case
discussion of this point.
Lower bound *
In VB, we are maximizing L(q), which is a lower bound on the log marginal likelihood:
L(q) ≤log p(D) = log

 

It is very useful to compute the lower bound itself, for three reasons. First, it can be used to
assess convergence of the algorithm. Second, it can be used to assess the correctness of one’sμ
λ
 
 
exact
vb
(a)
μ
λ
 
 
exact
vb
(b)
μ
λ
 
 
exact
vb
(c)
μ
λ
 
 
exact
vb
(d)
(a)
code: as with EM, if the bound does not increase monotonically, there must be a bug. Third,
the bound can be used as an approximation to the marginal likelihood, which can be used for
Bayesian model selection.
Unfortunately, computing this lower bound involves a fair amount of tedious algebra. We
work out the details for this example, but for other models, we will just state the results without
proof, or even omit discussion of the bound altogether, for brevity.
For this model, L(q) can be computed as follows:
L(q)
=

 

q(μ, λ) log p(D, μ, λ)
=
E [log p(D|μ, λ)] + E [log p(μ|λ)] + E [log p(λ)]
−E [log q(μ)] −E [log q(λ)]
where all expectations are wrt q(μ, λ). We recognize the last two terms as the entropy of a
Gaussian and the entropy of a Gamma distribution, which are given by
H

N )

=
H (Ga(aN, bN))
=where ψ() is the digamma function.
To compute the other terms, we need the following facts:
E [log x|x ∼Ga(a, b)]
=
ψ(a) −log(b)
E [x|x ∼Ga(a, b)]
=
a
b
E

x|x ∼N(μ, σ2)

=
μ
E

x2|x ∼N(μ, σ2)

=
μ + σ2
For the expected log likelihood, one can show that
Eq(μ,λ) [log p(D|μ, λ)]
=
−N
E [λ]q(λ)
N

Eq(μ)

=
−N
−NaN
2bN

κN
	
where x and ˆσ2 are the empirical mean and variance.
Eq(λ) [log p(λ)]
=
=
aN
bN
+ a0 log b0 −log Γ(a0)
Eq(μ,λ) [log p(μ|λ)]
=
2Eq(μ,λ)


=
−κ0
aN
bN
κN

Putting it altogether, one can show that
L(q)
=
κN
+ log Γ(aN) −aN log bN + const
This quantity monotonically increases after each VB update.
Example: VB for linear regression
ridge regression known as the evidence procedure. In particular, we assumed a likelihood ofa technique known as automatic relevancy determination.
In this section, we derive a VB algorithm for this model.
We follow the presentation of
p(w, λ, α)
=
We choose to use the following factorized approximation to the posterior:
q(w, α, λ) = q(w, λ)q(α)
posterior is
q(w, α, λ)
=
N)Ga(α|aα
N, bα
N)
where
N
=
A + XX
wN
=
VNXT y
N
=
N
=
NAwN)
aα
N
=
aα
bα
N
=
bα
N
N
wT
NwN + tr(VN)
	
A
=
⟨α⟩I = aα
N
bα
N
I
This method can be extended to the ARD case in a straightforward way, by using the following
priors:
p(w)
=
p(α)
=
D

Ga(αj|aα
Nj) instead ofaα
NI. The posterior for α has the form
q(α)
=

j
Ga(αj|aα
N, bα
Nj)
aα
N
=
aα
bα
Nj
=
bα
N
N
w2
N,j + (VN)jj
	
for a single data case, we have
p(y|x, D)
=
T (y|wT
N
N
N)
The exact marginal likelihood, which can be used for model selection, is given by
p(D) =

 
 

We can compute a lower bound on log p(D) as follows:
L(q)
=
−N
N

N
N
(yi −wT
i VNxi
	
N
N
N
−log Γ(aα
N) −aα
N log bα
N
In the ARD case, the last line becomes
D


−log Γ(aα
N) −aα
N log bα
Nj

see that VB gives similar results to EB, but the precise behavior depends on the sample size.
uninformative prior, aα
α = aα
N
bα
N
=
N
N wT
NwN + tr(VN))M
P(M|D)
(a)
M
P(M|D)
(b)
M
P(M|D)
(c)
M
P(M|D)
(d)
generated by linregEbModelSelVsN.
ˆα =
D
E [wT w] =
D
wT
NwN + tr(VN)
N terms, these are the same.
In hindsight this is perhaps not that
surprising, since EB is trying to maximize log p(D), and VB is trying to maximize a lower
bound on log p(D).
Variational Bayes EM
PCA, HMMs, etc. There are now two kinds of unknowns: parameters, θ, and latent variables, zi.
infer the posterior over the latent variables, p(zi|xi, θ), and in the M step, we compute a point
informed by all N data cases, whereas zi is only informed by xi; this makes a MAP estimate ofθ more reasonable than a MAP estimate of zi.
However, VB provides a way to be “more Bayesian”, by modeling uncertainty in the parameters
θ as well in the latent variables zi, at a computational cost that is essentially the same as EM.
This method is known as variational Bayes EM or VBEM. The basic idea is to use mean ﬁeld,
where the approximate posterior has the form
p(θ, z1:N|D) ≈q(θ)q(z) = q(θ)

i
q(zi)
The second factorization follows from the model, since the latent variables are iid conditional
In VBEM, we alternate between updating q(zi|D) (the variational E step) and updating q(θ|D)
eter posterior using a delta function, q(θ|D) ≈δˆθ(θ).
The variational E step is similar to a standard E step, except instead of plugging in a MAP
estimate of the parameters and computing p(zi|D, ˆθ), we need to average over the parameters.
Roughly speaking, this can be computed by plugging in the posterior mean of the parameters
instead of the MAP estimate, and then computing p(zi|D, θ) using standard algorithms, such
The details depend on the form of the model; we give some examples below.
The variational M step is similar to a standard M step, except instead of computing a point
tics. This process is usually very similar to MAP estimation in regular EM. Again, the details on
how to do this depend on the form of the model.
The principle advantage of VBEM over regular EM is that by marginalizing out the parameters,
we can compute a lower bound on the marginal likelihood, which can be used for model
it treats parameters as “ﬁrst class citizens”, just like any other unknown quantity, whereas EM
makes an artiﬁcial distinction between parameters and latent variables.
Example: VBEM for mixtures of Gaussians *
Let us consider how to “ﬁt” a mixture of Gaussians using VBEM. (We use scare quotes since we
are not estimating the model parameters, but inferring a posterior over them.) We will follow
Fortunately, as with EM, one gets used to it after a bit of practice. (As usual with math, simply
reading the equations won’t help much, you should really try deriving these results yourself (or
try some of the exercises) if you want to learn this stuff in depth.)
The variational posterior
The likelihood function is the usual one for Gaussian mixture models:
p(z, X|θ) =

i

k
πzik
k )zikWe will assume the following factored conjugate prior
p(θ) = Dir(π|α0)

k
of the prior; we assume all the prior parameters are the same for all clusters. For the mixing
weights, we usually use a symmetric prior, α0 = α01.
The exact posterior p(z, θ|D) is a mixture of KN distributions, corresponding to all possible
labelings z. We will try to approximate the volume around one of these modes. We will use the
standard VB approximation to the posterior:
p(θ, z1:N|D) ≈q(θ)

i
q(zi)
At this stage we have not speciﬁed the forms of the q functions; these will be determined by
the form of the likelihood and prior. Below we will show that the optimal form is as follows:
q(z, θ)
=
q(z|θ)q(θ) =

i
Cat(zi|ri)


Dir(π|α)

k

will derive the update equations for these variational parameters.
Derivation of q(z) (variational E step)
The form for q(z) can be obtained by looking at the complete data log joint, ignoring terms that
do not involve z, and taking expectations of what’s left over wrt all the hidden variables except
for z. We have
log q(z)
=
Eq(θ) [log p(x, z, θ)] + const
=

i

i
zik log ρik + const
where we deﬁne
log ρik
≜
2Eq(θ) [log |Λk|] −D
2Eq(θ)

(xi −μk)T Λk(xi −μk)

Using the fact that q(π) = Dir(π), we have
log ˜πk ≜E [log πk] = ψ(αk) −ψ(

k′
αk′)the fact that
to get
log ˜Λk ≜E [log |Λk|] =
D

ψ
	
Finally, for the expected value of the quadratic form, we get
E

(xi −μk)T Λk(xi −μk)

k
+ νk(xi −mk)T Λk(xi −mk)
Putting it altogether, we get that the posterior responsibility of cluster k for datapoint i is
rik
∝
˜πk ˜Λ
k exp

−D
2βk
−νk
	
Compare this to the expression used in regular EM:
rEM
ik
∝
ˆπk| ˆΛ|
k exp

	
Derivation of q(θ) (variational M step)
Using the mean ﬁeld recipe, we have
log q(θ)
=
log p(π) +

k
log p(μk, Λk) +

i
Eq(z) [log p(zi|π)]
+

k

i
k ) + const
We see this factorizes into the form
q(θ)
=
q(π)

k
q(μk, Λk)
For the π term, we have
log q(π)
=

k
log πk +

k

i
rik log πk + const
Exponentiating, we recognize this as a Dirichlet distribution:
q(π)
=
Dir(π|α)
αk
=
α0 + Nk
Nk
=

i
rikiter
lower bound on log marginal likelihood
variational Bayes objective for GMM on old faithful data
The steep parts of the
curve correspond to places where the algorithm ﬁgures out that it can increase the bound by “killing
moving the clusters around. Figure generated by mixGaussVbDemoFaithful.
q(μk, Λk)
=
βk
=
β0 + Nk
mk
=
(β0m0 + Nkxk)/βk
k
=
+ NkSk +
β0Nk
β0 + Nk
(xk −m0)(xk −m0)T
νk
=
xk
=
Nk

i
rikxi
Sk
=
Nk

i
rik(xi −xk)(xi −xk)T
Lower bound on the marginal likelihood
The algorithm is trying to maximize the following lower bound
L =

z


q(z, θ) log p(x, z, θ)
fortunately, deriving the bound is a bit messy, because we need to compute expectations of the
unnormalized log posterior as well as entropies of the q distribution. We leave the details (whichPosterior predictive distribution
We showed that the approximate posterior has the form
q(θ) = Dir(π|α)

k
Consequently the posterior predictive density can be approximated as follows, using the results
p(x|D)
≈

z


=

k


=

k
αk

Mk
=
Lk
we would get a weighted sum of Gaussian distributions.
Model selection using VBEM
The simplest way to select K when using VB is to ﬁt several models, and then to use the
variational lower bound to the log marginal likelihood, L(K) ≤log p(D|K), to approximate
p(K|D):
p(K|D) =
eL(K)

K′ eL(K′)
However, the lower bound needs to be modiﬁed somewhat to take into account the lack of
volume occupied by the parameter posterior, it will only do so around one of the local modes.
With K components, there are K! equivalent modes, which differ merely by permuting the
labels. Therefore we should use log p(D|K) ≈L(K) + log(K!).
Automatic sparsity inducing effects of VBEM
Although VB provides a reasonable approximation to the marginal likelihood (better than BIC
value of K being considered. A faster alternative is to ﬁt a single model, where K is set large,
the mixing weights π has “spikes” near the corners of the simplex, encouraging a sparse mixing
weight vector.(a)
(b)
We visualize the posterior mean parameters at various stages of the VBEM algorithm applied
to a mixture of Gaussians model on the Old Faithful data. Shading intensity is proportional to the mixing
(a)
(b)
components get “killed off”. Figure generated by mixGaussVbDemoFaithful.
˜πk
=
exp[Ψ(αk)]
exp[Ψ(
k′ αk′)]
more and more empty over successive iterations, whereas the popular clusters get more and
more members. This is called the rich get richer phenomenon; we will encounter it again inget extinguished. This provides an efficient alternative to performing a discrete search over the
number of clusters.
Variational message passing and VIBES
just compute each node’s full conditional, and average out the neighbors. This is very similar
work. Fortunately it is possible to derive a general purpose set of update equations that work for
any DGM for which all CPDs are in the exponential family, and for which all parent nodes have
time, in a manner similar to Gibbs sampling. This is known as variational message passing or
This is a VB analog to BUGS, which is a popular generic program for Gibbs sampling discussed
continuous (e.g., when performing “Bayesian learning”). For models where all the hidden nodes
are discrete, more accurate approximate inference algorithms can be used, as we discuss in
Local variational bounds *
So far, we have been focusing on mean ﬁeld inference, which is a form of variational inference
based on minimizing KL (q||˜p), where q is the approximate posterior, assumed to be factorized,
and ˜p is the exact (but unnormalized) posterior. However, there is another kind of variational
inference, where we replace a speciﬁc term in the joint distribution with a simpler function, to
simplify computation of the posterior. Such an approach is sometimes called a local variational
approximation, since we are only modifying one piece of the model, unlike mean ﬁeld, which
is a global approximation. In this section, we study several examples of this method.
Motivating applications
Before we explain how to derive local variational bounds, we give some examples of where this
is useful.
Variational logistic regression
Consider the problem of how to approximate the parameter posterior for multiclass logisticaccurate approximation to the posterior, since it has tunable parameters. Another advantage is
that the variational approach monotonically optimizes a lower bound on the likelihood of the
data, as we will see.
To see why we need a bound, note that the likelihood can be written as follows:
p(y|X, w)
=
N

exp

yT
i ηi −lse(ηi)

where ηi = Xiwi = [xT
i w1, . . . , xT
lse(ηi) ≜log

M

eηim


The main problem is that this likelihood is not conjugate to the Gaussian prior. Below we discuss
Gaussian posteriors.
One important application of Bayesian inference for logistic regression is where we have multiple
related classiﬁers we want to ﬁt.
In this case, we want to share information between the
parameters for each classiﬁer; this requires that we maintain a posterior distibution over the
parameters, so we have a measure of conﬁdence as well as an estimate of the value. We can
embed the above variational method inside of a larger hierarchical model in order to perform
Discrete factor analysis
Another situation where variational bounds are useful arises when we ﬁt a factor analysis
model to discrete data. This model is just like multinomial logistic regression, except the input
variables are hidden factors. We need to perform inference on the hidden variables as well as
the regression weights. For simplicity, we might perform point estimation of the weights, and
just integrate out the hidden variables. We can do this using variational EM, where we use the
Correlated topic model
A topic model is a latent variable model for text documents and other forms of discrete data; see
a more powerful model, known as the correlated topic model, uses a Gaussian prior, which can
the lse function. However, we can use our variational bounds in the context of a variational EM
algorithm, as we will see later.All of the above examples require dealing with multiplying a Gaussian prior by a multinomial
lse(ηi)
=
g(ψi)
=
exp[ψi −lse(ψi)] = S(ψi)
H(ψi)
=
diag(g(ψi)) −g(ψi)g(ψi)T
holds. An upper bound to lse can be found by replacing the Hessian matrix H(ψi) with a

IM −
M

Note that Ai is independent of ψi; however, we still write it as Ai (rather than dropping the
term. The upper bound on lse therefore becomes
lse(ηi)
≤
2ηT
i Aiηi −bT
i ηi + ci
Ai
=

IM −
M + 11M1T
M

bi
=
Aiψi −g(ψi)
ci
=
2ψT
i Aiψi −g(ψi)T ψi + lse(ψi)
We can use the above result to get the following lower bound on the softmax likelihood:
log p(yi = c|xi, w)
≥

yT
2wT XiAiXiw + bT
i Xiw −ci

c
i (bi + yi)
Then we can get a “Gaussianized” version of the observation model:
p(yi|xi, w)
≥
i )
where f(xi, ψi) is some function that does not depend on w. Given this, it is easy to compute
the posterior q(w) = N(mN, VN), using Bayes rule for Gaussians. Below we will explain how
to update the variational parameters ψi.Applying Bohning’s bound to multinomial logistic regression
can deﬁne the goal of variational inference as maximizing
L(q)
≜
−KL (q(w)||p(w|D)) + Eq
 N

log p(yi|xi, w)

=
−KL (q(w)||p(w|D)) + Eq
 N

yT
i ηi −lse(ηi)

=
−KL (q(w)||p(w|D)) +
N

yT
i Eq [ηi] −
N

Eq [lse(ηi)]
where q(w) = N(w|mN, VN) is the approximate posterior.
The ﬁrst term is just the KL
divergence between two Gaussians, which is given by
−KL (N(m0, V0)||N(mN, VN))
=


where DM is the dimensionality of the Gaussian, and we assume a prior of the form p(w) =
N

yT
i Eq [ηi] =
N

yT
i ˜mi
where ˜mi ≜XimN.
The ﬁnal term can be lower bounded by taking expectations of our
quadratic upper bound on lse as follows:
−
N

i ˜mi −ci
where ˜Vi ≜XiVNXT
i . Putting it altogether, we have
LQJ(q)
≥


2DM +
N

yT
i ˜mi −ci
This lower bound combines Jensen’s inequality (as in mean ﬁeld inference), plus the quadratic
lower bound due to the lse term, so we write it as LQJ.
We will use coordinate ascent to optimize this lower bound. That is, we update the variational
posterior parameters VN and mN, and then the variational likelihood parameters ψi. We leavethe detailed derivation as an exercise, and just state the results. We have
VN
=

V0 +
N

XT
i AiXi

mN
=
Vn

N

XT
i (yi + bi)


ψi
=
˜mi = XimN
We can exploit the fact that Ai is a constant matrix, plus the fact that Xi has block structure,
to simplify the ﬁrst two terms as follows:
VN
=

V0 + A ⊗
N

xixT
i

mN
=
Vn

N

(yi + bi) ⊗xi


bound
i ) ;

IM −
M

;

+ n
i AXi
ψ := XmN;
Ψ := reshape(m, M, N);
G := exp(Ψ −lse(Ψ));
B := AΨ −G;
b := (B) ;
mN := VN


;
Bounds for the sigmoid function(a)
(b)
Quadratic lower bounds on the sigmoid (logistic) function. In solid red, we plot sigm(x) vs
becomes
≤
2aη2 −bη + c
a
=
b
=
c
=
It is possible to derive an alternative quadratic bound for this case, as shown in (Jaakkola and
≤
λ(ξ)
≜
2ξ


To facilitate comparison with Bohning’s bound, let us rewrite the JJ bound as a quadratic form
as follows
≤
2a(ξ)η2 −b(ξ)η + c(ξ)
a(ξ)
=
b(ξ)
=
c(ξ)
=
The JJ bound has an adaptive curvature term, since a depends on ξ. In addition, it is tight atIf we wish to use the JJ bound for binary logistic regression, we can make some small
ai is not constant when using the JJ bound, unlike when using the Bohning bound, means we
cannot compute VN outside of the main loop, making the method a constant factor slower.
Next we note that Xi = xT
i , so the updates for the posterior become
N
=
N

λ(ξi)xixT
i
mN
=
VN

N



Finally, to compute the update for ξi, we isolate the terms in LQJ that depend on ξi to get
L(ξ)
=
N

(
i Eq

wwT 
xi −ξ2
i )
)
+ const
Optimizing this wrt ξi gives the equation
i Eq

wwT 
xi −ξ2
i )
i E

wwT 
xi −ξ2
(ξnew
i
i (VN + mNmT
N)xi
Although the JJ bound is tighter than the Bohning bound, sometimes it is not tight enough
in order to estimate the posterior covariance accurately. A more accurate approach, which uses
number of pieces, the bound can be made arbitrarily tight.
There are several other bounds and approximations to the multiclass lse function which we
can use, which we brieﬂy summarize below. Note, however, that all of these require numerical
optimization methods to compute mN and VN, making them more complicated to implement.
Product of sigmoids
log

 K

eηk


≤α +
K

It then applies the JJ bound to the term on the right.Jensen’s inequality
Eq [lse(ηi)]
=
Eq

log

M

exp(xT
i wc)


≤
log

M

Eq

exp(xT
i wc)



≤
log

M

exp(xT
2xT
i VN,ccxi)


Multivariate delta method
method, which is a way to approximate moments of a function using a Taylor series expansion.
m we have
f(w)
≈
where g and H are the gradient and Hessian evaluated at m. If q(w) = N(w|m, V), we have
Eq [f(w)]
≈
2tr[HV]
If we use f(w) = lse(Xiw), we get
2tr[XiHXT
i V]
Variational inference based on upper bounds
So far, we have been concentrating on lower bounds. However, sometimes we need to use an
case of Ising models, the resulting MRF is not pairwise, but contains higher order interactions.
This makes the standard mean ﬁeld updates intractable. In particular, they turn out to involve
computing an expression which requires evaluating
E


= E

−log sigm(wT
i xpa(i))

sigmoid function so as to make this update tractable, resulting in a monotonically convergent
inference procedure.Exercises
Compute a Laplace approximation of p(μ, log σ|D) for a Gaussian, using an uninformative prior p(μ, log σ) ∝
posterior is
log p(μ, ℓ|D) = −n log σ −
a. Show that the ﬁrst derivatives are
∂
∂μ log p(μ, ℓ|D)
=
n(y −μ)
σ2
∂
∂ℓlog p(μ, ℓ|D)
=
σ2
b. Show that the Hessian matrix is given by
H
=
%
∂μ2 log p(μ, ℓ|D)
∂μ∂ℓlog p(μ, ℓ|D)
∂ℓ2 log p(μ, ℓ|D)
∂ℓ2 log p(μ, ℓ|D)
&
=

−n
σ2
−2n y−μ
σ2
−2n y−μ
σ2

c. Use this to derive a Laplace approximation to the posterior p(μ, ℓ|D).
L
=
E [ln p(x|z, μ, Λ)] + E [ln p(z|π)] + E [ln p(π)] + E [ln p(μ, Λ)]
−E [ln q(z)] −E [ln q(π)] −E [ln q(μ, Λ)]where
E [ln p(x|z, μ, Λ)]
=


k
Nk
(
k
−νktr(SkLk)
−νk(xk −mk)T Lk(xk −mk) −D ln(2π)
)
E [ln p(z|π)]
=


i


k
rik ln ˜πk
E [ln p(π)]
=


k
ln ˜πk
E [ln p(μ, Λ)]
=


k
 
βk
−β0νk(mk −m0)T Lk(mk −m0)
*
E [ln q(z)]
=


i


k
rik ln rik
E [ln q(π)]
=


k
E [ln q(μ, Λ)]
=


k
 βk
2π

−D
*
where the normalization constant for the Dirichlet and Wishart is given by
Cdir(α)
≜
Γ(
k αk)

k Γ(αk)
CW i(L, ν)
≜
ΓD(α)
≜
D

where ΓD(ν) is the multivariate Gamma function. Finally, the entropy of the Wishart is given by
H (Wi(L, ν))
=
E [ln |Λ|] + νD
Show that
exp(E [log πk]) =
exp(Ψ(αk))
exp(Ψ(
k′ αk′))
where π ∼Dir(α).distribution p(x, y). Show that to minimize the forwards KL KL (p||q) we should set q(x) = p(x) and
q(y) = p(y), i.e., the optimal approximation is a product of marginals
Now consider the following joint distribution, where the rows represent y and the columns x.
x
Show that the reverse KL KL (q||p) for this p has three distinct minima. Identify those minima and
evaluate KL (q||p) at each of them. What is the value of KL (q||p) if we set q(x, y) = p(x)p(y)?
Consider the binary FA model:
p(xi|zi, θ)
=
D

Ber(xij|sigm(wT
j zi + βj)) =
D

Ber(xij|sigm(ηij))
ηi
=
˜
W˜zi
˜zi
≜
˜
W
≜
(W, β)
p(zi)
=
also unknown, as in binary factor analysis. Show how to ﬁt this model using variational Bayes, making an
approximation to the posterior of the form q(x, z, W) = N
q(wi) will be Gaussian, and q(zi) will be a truncated univariate Gaussian.More variational inference
Introduction
of marginal distributions. This allows us to use different parametric forms for each variable,
which is particularly useful when performing Bayesian inference for the parameters of statistical
models (such as the mean and variance of a Gaussian or GMM, or the regression weights in a
In this chapter, we discuss a slightly different kind of variational inference. The basic idea is
to minimize J(q) = KL (q||˜p), where ˜p is the exact but unnormalized posterior as before, but
where we no longer require q to be factorized. In fact, we do not even require q to be a globally
valid joint distribution. Instead, we only require that q is locally consistent, meaning that the
joint distribution of two adjacent nodes agrees with the corresponding marginals (we will deﬁne
this more precisely below).
In addition to this new kind of inference, we will discuss approximate methods for MAP
state estimation in discrete graphical models. It turns out that algorithms for solving the MAP
problem are very similar to some approximate methods for computing marginals, as we will see.
Loopy belief propagation: algorithmic issues
There is a very simple approximate inference algorithm for discrete (or Gaussian) graphical
models known as loopy belief propagation or LBP. The basic idea is extremely simple: we
even if it is not a tree). This method is simple and efficient, and often works well in practice,
In the next section, we analyse this algorithm in terms of variational inference.
A brief history
When applied to loopy graphs, BP is not guaranteed to give correct results, and may not even
converge. Indeed, Judea Pearl, who invented belief propagation for trees, wrote the following
When loops are present, the network is no longer singly connected and local propagationschemes will invariably run into trouble . . . If we ignore the existence of loops and
permit the nodes to continue communicating with each other as if the network were
singly connected, messages may circulate indeﬁnitely around the loops and the process
may not converge to a stable equilibrium . . . Such oscillations do not normally occur in
probabilistic networks . . . which tend to bring all messages to some stable equilibrium as
time goes on. However, this asymptotic equilibrium is not coherent, in the sense that it
Despite these reservations, Pearl advocated the use of belief propagation in loopy networks as
investigates the quality of the approximation when it is applied to a particular loopy belief
network.
could be viewed as an instance of BP applied to a certain kind of graph. This was an important
observation since turbo codes have gotten very close to the theoretical lower bound on coding
efficiency proved by Shannon. (Another approach, known as low density parity check or LDPC
then, the method has been widely used in many different applications.
LBP on pairwise models
We now discuss how to apply LBP to an undirected graphical model with pairwise factors (we
discuss the directed case, which can involve higher order factors, in the next section).
The
See
discuss issues such as convergence and accuracy of this method shortly.
Send message on each edge
xs
!
ψs(xs)ψst(xs, xt) 
#
;
Update belief of each node bels(xs) ∝ψs(xs) (solid black squares), and each parity factor is connected to three bits. Each parity factor has the form
each hidden node are not shown. (b) A larger example of a random LDPC code. We see that this graph is
number of nodes. This gives us a hint as to why loopy BP works so well on such graphs. (Note, however,
that some error correcting code graphs have short loops, so this is not the full explanation.)
Source:
Used with kind permission of Martin Wainwright.
LBP on a factor graph
some nodes have more than one parent), it is useful to use a representation known as a factor
graph.
We explain this representation below, and then describe how to apply LBP to such
models.
Factor graphs
directed and undirected models, and which simpliﬁes certain message passing algorithms. More
precisely, a factor graph is an undirected bipartite graph with two kinds of nodes. Round nodes
represent variables, square nodes represent factors, and there is an edge from each variable to
function
f(x1, x2, x3, x4) = f124(x1, x2, x4)f234(x2, x3, x4)
the function
f(x1, x2, x3, x4) = f14(x1, x4)f12(x1, x2)f34(x3, x4)f23(x2, x3)f24(x2, x4)(a)
(b)
(c)
(a) A simple UGM. (b) A factor graph representation assuming one potential per maximal
clique. (c) A factor graph representation assuming one potential per edge.
x4
x5
x3
x1
x2
(a)
x4
p(x4|x3)
x3
p(x5|x3)
x5
p(x3|x1, x2)
x1
x2
p(x1)
p(x2)
(b)
We can also convert a DGM to a factor graph: just create one factor per CPD, and connect that
factorization:
f(x1, x2, x3, x4, x5) = f1(x1)f2(x2)f123(x1, x2, x3)f34(x3, x4)f35(x3, x5)
where we deﬁne f123(x1, x2, x3) = p(x3|x1, x2), etc. If each node has at most one parent (and
hence the graph is a chain or simple tree), then there will be one factor per edge (root nodes
can have their prior CPDs absorvbed into their children’s factors). Such models are equivalent
to pairwise MRFs.Message passing on a bipartite factor graph. Square nodes represent factors, and circles
represent variables.
Used with kind permission of Brendan
Frey.
BP on a factor graph

and factors to variables:

y
f(x, y)

Here nbr(x) are all the factors that are connected to variable x, and nbr(f) are all the variables
we can compute the ﬁnal beliefs as a product of incoming messages:
bel(x) ∝

In the following sections, we will focus on LBP for pairwise models, rather than for factor
graphs, but this is just for notational simplicity.
Convergence
LBP does not always converge, and even when it does, it may converge to the wrong answers.
This raises several questions: how can we predict when convergence will occur? what can we do
to increase the probability of convergence? what can we do to increase the rate of convergence?
We brieﬂy discuss these issues below. We then discuss the issue of accuracy of the results at
convergence.Synchronous
Asynchronous
No smoothing
True
Time (seconds)
% of messages converged
Time (seconds)
Time (seconds)
Time (seconds)
Time (seconds)
Time (seconds)
(a)
(d)
(b)
(e)
(c)
(f)
sychronous (few nodes converge), dashed = undamped asychnronous (half the nodes converge), solid =
line = truth, dashed = sychronous, solid = damped asychronous.
Used with kind permission of Daphne Koller.
When will LBP converge?
The details of the analysis of when LBP will converge are beyond the scope of this chapter, but
we brieﬂy sketch the basic idea. The key analysis tool is the computation tree, which visualizes
The key insight is that T iterations of LBP is equivalent to exact computation in a computation
the inﬂuence of the leaves on the root will diminish over time, and convergence will occur. See
(Wainwright and Jordan 2008b) and references therein for more information.Used with kind permission of Martin
Wainwright.
Making LBP converge
Although the theoretical convergence analysis is very interesting, in practice, when faced with a
model where LBP is not converging, what should we do?
One simple way to reduce the chance of oscillation is to use damping. That is, instead of
sending the message M k
ts, we send a damped message of the form
˜
M k
ts
(xs)
we see that damped updating results in convergence much more often than undamped updating.
It is possible to devise methods, known as double loop algorithms, which are guaranteed to
of the resulting marginals is usually not much greater than with standard LBP. (Indeed, oscillating
marginals is sometimes a sign that the LBP approximation itself is a poor one.) Consequently,
version of BP that is widely used.
Increasing the convergence rate: message scheduling
Even if LBP converges, it may take a long time. The standard approach when implementing
LBP is to perform synchronous updates, where all nodes absorb messages in parallel, and then
parallel using
where E is the number of edges, and fst(m) is the function that computes the message forlinear systems of equations. We can apply the same idea to LBP, using updates of the form
i
= fi

j
: j < i}, {mk
j : j > i}

earlier in the ordering, and using old messages (iteration k) from edges later in the ordering.
This raises the question of what order to update the messages in. One simple idea is to use
that (damped) asynchronous updating results in convergence much more often than synchronous
updating.
sweep on one tree at a time, keeping all the other messages ﬁxed.
This is known as tree
However, we can do even better by using an adaptive ordering. The intuition is that we should
proposed a technique known as residual belief propagation, in which messages are scheduled
to be sent according to the norm of the difference from their previous value. That is, we deﬁne
the residual of new message mst at iteration k to be
r(s, t, k) = || log mst −log mk
st||∞= max
i
| log mst(i)
mk
st(i)|
We can store messages in a priority queue, and always send the one with highest residual. When
a message is sent from s to t, all of the other messages that depend on mst (i.e., messages of
method converges more often, and much faster, than using sychronous updating, asynchronous
updating with a ﬁxed order, and the TRP approach.
use an upper bound on the residual of a message instead of the actual residual. This means
that messages are only computed if they are going to be sent; they are not just computed for
the purposes of evaluating the residual. This was observed to be about ﬁve times faster than
residual BP, although the quality of the ﬁnal results is similar.
Accuracy of LBP
the method converges, the means are exact, although the variances are not (typically the beliefs
are over conﬁdent).Other speedup tricks for LBP *
There are several tricks one can use to make BP run faster. We discuss some of them below.
Fast message computation for large state spaces
The cost of computing each message in BP (whether in a tree or a loopy graph) is O(Kf),
it represents the discretization of some underlying continuous space, so O(K2) per message
is too expensive.
Fortunately, for certain kinds of pairwise potential functions of the form
The key insight is that message computation is just convolution:
M k
st(xt) =

xs
ψ(xs −xt)h(xs)
where h(xs) = ψs(xs) 
vs
potential, we can compute the convolution in O(K) time by sequentially convolving with a
messages in O(K) time. However, this only works if ψ(z) = exp(−E(z)) and where E(z)
has one the following forms: quadratic, E(z) = z2; truncated linear, E(z) = min(c1|z|, c2); or
A method which is speciﬁc to 2d lattice structures, which commonly arise in computer vision,
where one of the core problems is the fast solution of linear systems of equations; this is
equivalent to MAP estimation in a Gaussian MRF. In the computer vision context, (Felzenszwalb
messages at the level below; when we reach the bottom level, just a few iterations of standard BP
process.
The beliefs at the coarse level are computed over a small number of large blocks. The local
the pixels in the block. The pairwise potential is based on the discrepancy between labels of
neighboring blocks, taking into account their size. We can then run LBP at the coarse level,
and then use this to initialize the messages one level down.
Note that the model is still aCascades
putationally cheap ﬁltering step. In fact, one can create a hierarchy of models which tradeoff
speed and accuracy. This is called a computational cascade. In the case of chains, one can
Loopy belief propagation: theoretical issues *
This paper is sometimes called “the monster” (by its own authors!) in view of its length and
technical difficulty. This section just sketches some of the main results.
To simplify the presentation, we focus on the special case of pairwise UGMs with discrete
potentials (which includes DGMs), but this makes the notation more complex (see (Koller and
UGMs represented in exponential family form
We assume the distribution has the following form:
p(x|θ, G)
=
Z(θ) exp
⎧
⎨
⎩

θs(xs) +

(s,t)∈E
θst(xs, xt)
⎫
⎬
⎭
where graph G has nodes V and edges E. (Henceforth we will drop the explicit conditioning
exponential family form as follows:
p(x|θ)
=
Z(θ) exp(−E(x))
E(x)
≜
−θT φ(x)
and φ(x) = ({I(xs = j)}, {I(xs = j, xt = k)}) are all the node and edge indicator functions
The mean of the sufficient statistics are known as the mean parameters of the model, and are
given by
μ = E [φ(x)] = ({p(xs = j)}s, {p(xs = j, xt = k)}s̸=t) = ({μs;j}s, {μst;jk}s̸=t)
itself.written as
p(x) =
Z(θ) exp
⎧
⎨
⎩

θsxs +

(s,t)∈E
θstxsxt
⎫
⎬
⎭
Hence we can use the following minimal parameterization
The marginal polytope
G is the structure of the graph deﬁning the UGM. This is deﬁned to be the set of all mean
parameters for the given model that can be generated from a valid probability distribution:
M(G) ≜{μ ∈Rd : ∃p
s.t.
μ =

x

x
For example, consider an Ising model. If we have just two nodes connected as X1 −X2,
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠
⎛
⎝
μ1
μ2
μ12
⎞
⎠≥
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠
Since M(G) is obtained by taking a convex combination of the φ(x) vectors, it can also be
written as the convex hull of the feature set:
M(G) = conv{φ1(x), . . . , φd(x)}
The marginal polytope will play a crucial role in the approximate inference algorithms we
discuss in the rest of this chapter.(a)
(b)
(c)
(a) Illustration of the marginal polytope for an Ising model with two variables. (b) Cartoon
illustration of the set MF (G), which is a nonconvex inner bound on the marginal polytope M(G). MF (G)
is used by mean ﬁeld. (c) Cartoon illustration of the relationship between M(G) and L(G), which is used
by loopy BP. The set L(G) is always an outer bound on M(G), and the inclusion M(G) ⊂L(G) is strict
(deﬁned by facets), or as the convex hull of the vertices. L(G) actually has fewer facets than M(G), despite
|V | is the number of variables, and |E| is the number of edges. By contrast, M(G) has O(|X||V |) facets.
On the other hand, L(G) has more vertices than M(G), despite the picture, since L(G) contains all the
Used with kind permission of Martin Wainwright.
Exact inference as a variational optimization problem
maximizes the energy functional
L(q) = −KL (q||p) + log Z = Eq [log ˜p(x)] + H (q) ≤log Z
where ˜p(x) = Zp(x) is the unnormalized posterior. If we write log ˜p(x) = θT φ(x), and we
let q = p, then the exact energy functional becomes
max
H (μ)). Since the KL divergence is zero when p = q, we know that
max
This is a way to cast exact inference as a variational optimization problem.
are maximizing this over a convex set. However, the marginal polytope M(G) has exponentially
many facets. In some cases, there is structure to this polytope that can be exploited by dynamic
Most of the existing deterministic approximate inference schemes that have been proposed in
the literature can be seen as different approximations to the marginal polytope, as we explain
below.Mean ﬁeld as a variational optimization problem
our new more abstract framework. This will help us compare it to other approximate methods
which we discuss below.
First, let F be an edge subgraph of the original graph G, and let I(F) ⊆I be the subset of
for the full model, and deﬁne the canonical parameter space for the submodel as follows:
In other words, we require that the natural parameters associated with the sufficient statistics
α outside of our chosen class to be zero.
For example, in the case of a fully factorized
approximation, F0, we remove all edges from the graph, giving
our tractable subgraph.
Next, we deﬁne the mean parameter space of the restricted model as follows:
This is called an inner approximation to the marginal polytope, since MF (G) ⊆M(G). See
local optima. By contrast, some of the approximations we will consider later will be convex.
We deﬁne the entropy of our approximation H (μ(F)) as the entropy of the distribution
μ deﬁned on submodel F. Then we deﬁne the mean ﬁeld energy functional optimization
problem as follows:
max
In the case of the fully factorized mean ﬁeld approximation for pairwise UGMs, we can write
this objective as follows:
max
μ∈Pd


xs
θs(xs)μs(xs) +

(s,t)∈E

xs,xt
θst(xs, xt)μs(xs)μt(xt) +

H (μs)
optimized using coordinate ascent, since it is easy to optimize a scalar concave function over P
μs(xs) ∝exp(θs(xs)) exp
⎛
⎝

xt
μt(xt)θst(xs, xt)
⎞
⎠
LBP as a variational optimization problem
In this section, we explain how LBP can be viewed as a variational inference problem.(a) Illustration of pairwise UGM on binary nodes, together with a set of pseudo marginals
that are not globally consistent. (b) A slice of the marginal polytope illustrating the set of feasible edge
(Wainwright and Jordan 2008a).
Used with kind permission of Martin Wainwright.
An outer approximation to the marginal polytope
If we want to consider all possible probability distributions which are Markov wrt our model, we
infeasible to optimize over. A standard strategy in combinatorial optimization is to relax the
a vector τ that only satisﬁes the following local consistency constraints:

xs
τs(xs)
=

xt
τst(xs, xt)
=
τs(xs)
ization constraint. We then deﬁne the set
The set L(G) is also a polytope, but it only has O(|V | + |E|) constraints. It is a convex outer
The picture shows a set of pseudo node and edge marginals, which satisfy the local consistency
requirements. However, they are not globally consistent. To see why, note that τ12 implies
shows that L(G) contains points that are not in M(G).
We claim that M(G) ⊆L(G), with equality iff G is a tree.
To see this, ﬁrst considerconstraints, hence M(G) ⊆L(G).
the normalization and marginalization constraints. However, any tree can be represented in the
form

μs(xs)

(s,t)∈E
μst(xs, xt)
μs(xs)μt(xt)
Hence satsifying normalization and local consistency is enough to deﬁne a valid distribution for
example of this fact.
The entropy approximation
M(T) as follows:
H (μ)
=

Hs(μs) −

(s,t)∈E
Ist(μst)
Hs(μs)
=
−

Ist(μst)
=

(xs,xt)∈Xs×Xt
μs(xs)μt(xt)
Note that we can rewrite the mutual information term in the form Ist(μst) = Hs(μs)+Ht(μt)−
Hst(μst), and hence we get the following alternative but equivalent expression:
H (μ)
=
−


(s,t)∈E
Hst(μst)
where ds is the degree (number of neighbors) for node s.
don’t have a tree:
HBethe(τ)
=

Hs(τs) −

(s,t)∈E
Ist(τst)
We deﬁne the Bethe free energy as
FBethe(τ) ≜−

θT τ + HBethe(τ)

We deﬁne the Bethe energy functional as the negative of the Bethe free energy.The LBP objective
Combining the outer approximation L(G) with the Bethe approximation to the entropy, we get
the following Bethe variational problem (BVP):
min
τ∈L(G) FBethe(τ) = max
τ∈L(G) θT τ + HBethe(τ)
The space we are optimizing over is a convex set, but the objective itself is not concave (since
HBethe is not concave). Thus there can be multiple local optima of the BVP.
The value obtained by the BVP is an approximation to log Z(θ). In the case of trees, the
approximation is exact, and in the case of models with attractive potentials, the approximation
Message passing and Lagrange multipliers
In this subsection, we will show that any ﬁxed point of the LBP algorithm deﬁnes a stationary
point of the above constrained objective. Let us deﬁne the normalization constraint at Css(τ) ≜
xs τs(xs), and the marginalization constraint as Cts(xs; τ) ≜τs(xs) −
xt τst(xs, xt)
L(τ, λ; θ)
≜
θT τ + HBethe(τ) +

s
λssCss(τ)
+

s,t

xs
λts(xs)Cts(xs; τ) +

xt
λst(xt)Cst(xt; τ)

log τs(xs)
=
λss + θs(xs) +

λts(xs)
log τst(xs, xt)
˜τs(xs)˜τt(xt)
=
θst(xs, xt) −λts(xs) −λst(xt)
where we have deﬁned ˜τs(xs) ≜
straint implies ˜τs(xs) = τs(xs), we get
log τst(xs, xt)
=
λss + λtt + θst(xs, xt) + θs(xs) + θt(xt)
+

λus(xs) +

λut(xt)
To make the connection to message passing, deﬁne Mts(xs) = exp(λts(xs)). With this
notation, we can rewrite the above equations (after taking exponents of both sides) as follows:
τs(xs)
∝
exp(θs(xs))

Mts(xs)
τst(xs, xt)
∝
exp (θst(xs, xt) + θs(xs) + θt(xt))
×

Mus(xs)

Mut(xt)We see that this is
equivalent to the usual expression for the node and edge marginals in LBP.
To derive an equation for the messages in terms of other messages (rather than in terms of
λts), we enforce the marginalization condition 
xt τst(xs, xt) = τs(xs). Then one can show
that
Mts(xs) ∝

xt
⎡
⎣exp {θst(xs, xt) + θt(xt)}

Mut(xt)
⎤
⎦
We see that this is equivalent to the usual expression for the messages in LBP.
Loopy BP vs mean ﬁeld
It is interesting to compare the naive mean ﬁeld (MF) and LBP approximations. There are several
obvious differences. First, LBP is exact for trees whereas MF is not, suggesting LBP will in general
and edge marginals, whereas MF only optimizes over node marginals, again suggesting LBP will
free energy approximations will be the same in both cases.
What is less obvious, but which nevertheless seems to be true, is that the MF objective has
many more local optima than the LBP objective, so optimizing the MF objective seems to be
or random initial conditions often leads to poor results, whereas optimizing BP from uniform
initial messages often leads to good results. Furthermore, initializing MF with the BP marginals
also leads to good results (although MF tends to be more overconﬁdent than BP), indicating that
the problem is caused not by the inaccuracy of the MF approximation, but rather by the severe
on the partition function, unlike BP, which is useful when using it as a subroutine inside a
learning algorithm.
Also, MF is easier to extend to other distributions besides discrete and
distributions, which have a single type, rather than needing to deﬁne pairwise distributions,
which may need to have two different types.
Extensions of belief propagation *
In this section, we discuss various extensions of LBP.
Generalized belief propagation
We can improve the accuracy of loopy BP by clustering together nodes that form a tight loop.
of continuous random variables. It is possible that similar ideas could be adapted to the discrete case, although there
may be no reason to do this, given that LBP already works well in the discrete case.Used with kind permission of Martin Wainwright.
a continuum of approximations, from LBP all the way to exact inference.

x1,x2
τ1245(x1, x2, x4, x5) = τ45(x4, x5),

x6
τ56(x5, x6) = τ5(x5), . . .
Furthermore, we approximate the entropy as follows:
HKikuchi(τ) ≜

c(g)Hg(τg)
where Hg(τg) is the entropy of the joint (pseudo) distribution on the vertices in set g, and c(g)
is called the overcounting number of set g. These are related to Mobious numbers in set
theory. Rather than giving a precise deﬁnition, we just give a simple example. For the graph in
HKikuchi(τ)
=
[H1245 + H2356 + H4578 + H5689]
−[H25 + H45 + H56 + H58] + H5
Putting these two approximations together, we can deﬁne the Kikuchi free energy3 as follows:
FKikuchi(τ) ≜−

θT τ + HKikuchi(τ)
Our variational problem becomes
min
τ∈Lt(G) FKikuchi(τ) =
max
τ∈Lt(G) θT τ + HKikuchi(τ)
Just as with the Bethe free energy, this is not a concave objective. There are several possible
algorithms for ﬁnding a local optimum of this objective, including a message passing algorithm
known as generalized belief propagation. However, the details are beyond the scope of this
LBP, but at increased computational cost (because of the need to handle clusters of nodes). This
cost, plus the complexity of the approach, have precluded it from widespread use.
Convex belief propagation
approximation to the marginal polytope. The Bethe and Kikuchi energy functionals are not
concave, but they are maximized over a convex outer approximation to the marginal polytope.
Consequently, for both MF and LBP, the optimization problem has multiple optima, so the
a concave objective maximized over a convex set, it is natural to try to come up with an
appproximation which involves a concave objective being maximized over a convex set.
We now describe one method, known as convex belief propagation. This involves working
with a set of tractable submodels, F, such as trees or planar graphs. For each model F ⊂G,
the entropy is higher, H (μ(F)) ≥H (μ(G)), since F has fewer constraints. Consequently, any
convex combination of such subgraphs will have higher entropy, too:
H (μ(G)) ≤

ρ(F)H (μ(F)) ≜H(μ, ρ)
deﬁne the convex free energy as
FConvex(μ, ρ) ≜−


We deﬁne the concave energy functional as the negative of the convex free energy. We discuss
how to optimize ρ below.
Having deﬁned an upper bound on the entropy, we now consider a convex outerbound on
the marginal polytope of mean parameters. We want to ensure we can evaluate the entropy of
any vector τ in this set, so we restrict it so that the projection of τ onto the subgraph G lives
in the projection of M onto F:
This is a convex set since each M(F) is a projection of a convex set. Hence we deﬁne our
problem as
min
τ∈L(G;F) FConvex(τ, ρ) =
max
mum. We give a speciﬁc example below.b
e
f
b
e
f
b
e
f
b
e
f
2008b).
Used with kind permission of Martin Wainwright.
Consider the speciﬁc case where F is all spanning trees of a graph. For any given tree, the
all trees, note that the terms 
F ρ(F)H(μ(F)s) for single nodes will just be Hs, since node s
appears in every tree, and 
ρst = Eρ [I((s, t) ∈E(T))], known as the edge appearance probability. Hence we have the
following upper bound on the entropy:
H (μ) ≤

Hs(μs) −

(s,t)∈E
ρstIst(μst)
The edge appearance probabilities live in a space called the spanning tree polytope. This
example of a graph and three of its spanning trees. Suppose each tree has equal weight under
ﬁnd a distribution ρ, or equivalently edge probabilities in the spanning tree polytope, that make
the above bound as tight as possible. An algorithm to do this is described in (Wainwright et al.
means enforcing normalization and local consistency. Since we have to do this for every tree,
we are enforcing normalization and local consistency on every edge. Hence L(G; F) = L(G).
So our ﬁnal optimization problem is as follows:
max
τ∈L(G)
⎧
⎨

Hs(τs) −

(s,t)∈E(G)
ρstIst(τst)
⎫
⎬
⎭
for all edges (s, t), this problem is strictly concave with a unique maximum.
How can we ﬁnd this global optimum? As for LBP, there are several algorithms, but perhaps the
simplest is a modiﬁcation of belief propagation known as tree reweighted belief propagation,also called TRW or TRBP for short. The message from t to s is now a function of all messages
sent from other neighbors v to t, as before, but now it is also a function of the message sent
from s to t. Speciﬁcally
Mts(xs)
∝

xt
exp
ρst
θst(xs, xt) + θt(xt)
	 
At convergence, the node and edge pseudo marginals are given by
τs(xs)
∝
exp(θs(xs))

[Mvs(xs)]ρvs
τst(xs, xt)
∝
ϕst(xs, xt)


ϕst(xs, xt)
≜
exp
ρst
θst(xs, xt) + θs(xs) + θt(xt)
	
equivalent to standard LBP on trees, when the method is of course exact.
In general, this message passing scheme is not guaranteed to converge to the unique global
ensure convergence.
It is also possible to produce a convex version of the Kikuchi free energy, which one can
optimize with a modiﬁed version of generalized belief propagation. See (Wainwright and Jordan
bound on the true entropy, wee see that the TRBP objective is an upper bound on log Z. Using
the fact that Ist = Hs + Ht −Hst, we can rewrite the upper bound as follows:

st
ρstHst(τst) +

s
csHs(τs) ≤log Z(θ)
t ρst.
Expectation propagation
sages are approximated. It is a generalization of the assumed density ﬁltering (ADF) algorithm,
ment matching, which locally optimizes KL (p||q) for a single term. From this, we derived the
message to send to the next time step.ADF works well for sequential Bayesian updating, but the answer it gives depends on the
order in which the data is seen. EP essentially corrects this ﬂaw by making multiple passes over
the data (thus EP is an offline or batch inference algorithm).
EP as a variational inference problem
We now explain how to view EP in terms of variational inference. We follow the presentation of
Suppose the joint distribution can be written in exponential family form as follows:
p(x|θ, ˜θ) ∝f0(x) exp(θT φ(x))
dI

exp(˜θ
T
i Φi(x))
size dT and dI intractable terms ˜θi, each of size b.
For example, consider the problem of inferring an unknown vector x, when the observation
used to represent outliers, for example.) Minka (who invented EP) calls this the clutter problem.
More formally, we assume an observation model of the form
write our model in the required form as follows:
p(x|y1:N)
∝
N

p(yi|x)
=
exp

2xT Σ−1x
	
exp

 N

log p(yi|x)


This matches our canonical form where f0(x) exp(θT φ(x)) corresponds to exp

2xT Σ−1x

,
The exact inference problem corresponds to
max
where M(φ, Φ) is the set of mean parameters realizable by any probability distribution as seen
through the eyes of the sufficient statistics:
M(φ, Φ) = {(μ, ˜μ) ∈RdT × RdIb : (μ, ˜μ) = E [(φ(X), Φ1(X), . . . , ΦdI(X))]}
As it stands, it is intractable to perform inference in this distribution. For example, in our
clutter example, the posterior contains 2N modes. But suppose we incorporate just one of the
p(x|θ, ˜θi)
∝
f0(x) exp(θT φ(x)) exp(˜θ
T
i Φi(x))In our clutter example, this becomes
p(x|θ, ˜θi)
=
exp

2xT Σ−1x
	
fashion. First, we approximate the convex set M(φ, Φ) with another, larger convex set:
L(φ, Φ) ≜{(τ, ˜τ) : τ ∈M(φ), (τ, ˜τ i) ∈M(φ, Φi)}
where M(φ) = {μ ∈RdT : μ = E [φ(X)]} and M(φ, Φi) = {(μ, ˜μi) ∈RdT × Rb :
approximation:
Hep(τ, ˜τ) ≜H (τ) +
dI

[H (τ, ˜τ i) −H (τ)]
Then the EP problem becomes
max
Optimizing the EP objective using moment matching
times to yield ηi = τ. The augmented set of parameters we need to optimize is now
(τ, (ηi, ˜τ i)dI
subject to the constraints that ηi = τ and (ηi, ˜τ i) ∈M(φ; Φi). Let us associate a vector of
becomes
L(τ; λ)
=
di


˜τ T
i ˜θi + H ((ηi, ˜τ i)) −H (ηi) + λT
i (τ −ηi)

the form
q(x|θ, λ) ∝f0(x) exp{(θ +
dI

λi)T φ(x)}
i φ(x) terms represents an approximation to the i’th intractable term using the sufficient
statistics from the base distribution, as we will see below. Similarly, by solving ∇(ηi,˜τ i)L(τ; λ) =
qi(x|θ, ˜θi, λ) ∝f0(x) exp{(θ +

j̸=i
λj)T φ(x) + ˜θ
T
i Φi(x)}This corresponds to removing the approximation to the i’th term, λi, from the base distribution,
that τ = Eq [φ(X)] and ηi = Eqi [φ(X)] are equal. In other words, we get the following
moment matching constraints:


q(x|θ, λ)φ(x)dx =


qi(x|θ, ˜θi, λ)φ(x)dx
to convergence: pick a term i; compute qi (corresponding to removing the old approximation
matching equation Eqi [φ(X)] = Eq [φ(X)]. (Note that this particular optimization scheme is
not guaranteed to converge to a ﬁxed point.)
An equivalent way of stating the algorithm is as follows. Let us assume the true distribution
is given by
Z

i
fi(x)
We approximate each fi by ˜fi and set
Z

i
˜fi(x)
Now we repeat the following until convergence:
q−i(x) = q(x)
˜fi(x)
This can be implemented by substracting off the natural parameters of ˜fi from q.
min
qnew(x) KL
Zi
fi(x)q−i(x)||qnew(x)
	
This can be done by equating the moments of qnew(x) with those of qi(x) ∝q−i(x)fi(x).
The corresponding normalization constant has the form
Zi =


q−i(x)fi(x)dx
˜fi(x) = Zi
qnew(x)
q−i(x)After convergence, we can approximate the marginal likelihood using
p(D) ≈

 
i
˜fi(x)dx
We will give some examples of this below which will make things clearer.
EP for the clutter problem
choose to approximate the posterior by a spherical Gaussian, q(x) = N(m, vI). We set f0(x)
to be the prior; this can be held ﬁxed. The factor approximations will be “Gaussian like” terms
of the form
˜fi(x) = siN(x|mi, viI)
Note, however, that in the EP updates, the variances may be negative! Thus these terms should
be interpreted as functions, but not necessarily probability distributions.
(If the variance is
negative, it means the that ˜fi curves upwards instead of downwards.)
First we remove ˜fi(x) from q(x) by division, which yields q−i(x) = N(m−i, v−iI), where
−i
=
i
m−i
=
i
(m −mi)
The normalization constant is given by
Next we compute qnew(x) by computing the mean and variance of q−i(x)fi(x) as follows:
m
=
m−i + ρi
v−i
v
=
v−i −ρi
v2
−i
ρi
=
Zi
where D is the dimensionality of x and ρi can be interpreted as the probability that yi is not
clutter.
Finally, we compute the new factor ˜fi whose parameters are given by
i
=
−i
mi
=
−i (m −m−i)
si
=
ZiAt convergence, we can approximate the marginal likelihood as follows:
p(D)
≈
N

c
≜
mT m
v
−
N

mT
i mi
vi
In (Minka 2001d), it is shown that, at least on this example, EP gives better accuracy per unit
of CPU time than VB and MCMC.
LBP is a special case of EP
We now show that loopy belief propagation is a special case of EP, where the base distribution
contains the node marginals and the “intractable” terms correspond to the edge potentials. We
base distribution takes the form

exp(θs(xs))
The entropy of this distribution is simply

s
H (τ s)
If we add in the u −v edge, the Φuv augmented distribution has the form
p(x|θ1:m, θuv) ∝

exp(θs(xs))

exp(θuv(xu, xv))
Since this graph is a tree, the exact entropy of this distribution is given by
=

s
H (τ s) −I(˜τ uv)
mation to the entropy of the full distribution is given by
Hep(τ, ˜τ)
=
H (τ) +

(u,v)∈E
=

s
H (τ s) +

(u,v)∈E

s
H (τ s) −I(˜τ uv) −

s
H (τ s)

=

s
H (τ s) −

(u,v)∈E
I(˜τ uv)
which is precisely the Bethe approximation to the entropy.a factored distribution. This is therefore equivalent to the set of all distributions which satisfy
the set M(φ, Φuv) for a single u−v edge. This is equivalent to the marginal polytope M(Guv),
where Guv is the graph with the single u −v edge added. Since this graph corresponds to a
tree, this set also satisﬁes the marginalization conditions

xv
τuv(xu, xv) = τu(xu),

xu
τuv(xu, xv) = τv(xv)
Since L(φ, Φ) is the union of such sets, as we sweep over all edges in the graph, we recover
the same set as L(G).
We have shown that the Bethe approximation is equivalent to the EP approximation. We now
show how the EP algorithm reduces to LBP. Associated with each intractable term i = (u, v)
q(x|θ, λ)
∝

s
exp(θs(xs))

(u,v)∈E
exp(λuv(xv) + λvu(xu))
=

s
exp
⎛
⎝θs(xs) +

λts(xs)
⎞
⎠
quv(x|θ, λ)
∝
q(x|θ, λ) exp (θuv(xu, xv) −λuv(xv) −λvu(xu))
We now need to update τu(xu) and τv(xv) to enforce the moment matching constraints:
(Eq [xs] , Eq [xt]) = (Equv [xs] , Equv [xt])
step along the u −v edge (in both directions), where the messages are given by Muv(xv) =
exp(λuv(xv)), and Mvu(xu) = exp(λvu(xu)). Once we have updated q, we can derive the
The above analysis suggests a natural extension, where we make the base distribution be a
tree structure instead of a fully factored distribution. We then add in one edge at a time, absorb
its effect, and approximate the resulting distribution by a new tree. This is known as tree EP
kinds of structured base distributions, we can derive algorothms that outperform generalization
Ranking players using TrueSkill
We now present an interesting application of EP to the problem of ranking players who competeW
W
W
G
G
\
\
(a)
)
)
)
K
K
N
N
G
G
\
\









(b)dynamical model, p(st
i
) = N(st
i
, γ2). In any given game, we deﬁne the performance
of player i to be pi, which has the conditional distribution p(pi|si) = N(pi|si, β2). We then
deﬁne the performance of a team to be the sum of the performance of its constituent players.
t2 = p2 + p3. Finally, we assume that the outcome of a game depends on the difference in


To simplify the presentation of the algorithm, we will ignore the dynamical model and assume
a common static factored Gaussian prior, N(μ0, σ2
p(s)
=

i
N(si|μ0, σ2)
p(dg|s)
=
p(yg|dg)
=
I(yg = sign(dg))
where ig is the ﬁrst player of game g, and jg is the second player. This is represented in
N(si|μ0, σ2
kg(dg, yg) = I(yg = sign(dg)).
Since the likelihood term (yg|dg) is not conjugate to the Gaussian priors, we will have to
perform approximate inference. Thus even when the graph is a tree, we will need to iterate.
longer be a tree.) We will represent all messages and marginal beliefs by 1d Gaussians. We will
penalized maximum likelihood.
Bradley Terry model of ranking.variables si are uniform, i.e.,
m0
and similarly m0
qt(si)
=
f(si)

g
i, λt
i)
λt
i
=
λ0 +

g
ηt
i = η0 +

g
mt
qt(sig)
mt
mt
qt(sjg)
mt
where the division is implemented by subtracting the natural parameters as follows:
λt
sig −λt
ηt
sig −ηt
and similarly for sjg.
mt
=

 

hg(dg, sig, sjg)mt
=

 

N(sjg|μt
=
N(dg|μt
vt
=
μt
=
μt
qt(dg)
∝
mt
=
N(dg|μt
≈
N(dg|μt
g, vt
g)Ψ function
(a)
Λ function
(b)
by trueskillPlot.
(Note that the upward message from the kg factor is constant.) We can ﬁnd these parameters
by moment matching as follows:
μt
g
=

σt


vt
g
=
vt


σt


Ψ(x)
≜
Φ(x)
Λ(x)
≜
Ψ(x)(Ψ(x) + x)
large positive number. That means we expect, based on the current estimate of the skills,
surprised that ig is the winner, which is reﬂected in the fact that the update factor for the
and variance becomes quite large.
mt
=
qt(dg)
mt
λt
=
λt
g −λt
ηt
g −ηt(a)
(b)
that ig is the winner, and jg is the loser. Then we have
mt
=

 

hg(dg, sig, sjg)mt
=
N(sig|μt
vt
=
μt
=
μt
And similarly
mt
=

 

hg(dg, sig, sjg)mt
=
N(sjg|μt
vt
=
μt
=
μt
prior factor, we will see that the posterior mean of sig goes up. Similarly, the posterior mean
of sjg goes down.
It is straightforward to combine EP with ADF to perform online inference, which is necessary
for most practical applications.We see that the method has correctly inferred the rank ordering of the players.
Other applications of EP
The TrueSkill model was developed by researchers at Microsoft. They and others have extended
EP has also been used for a variety of other models, such as Gaussian process classiﬁcation
MAP state estimation
In this section, we consider the problem of ﬁnding the most probable conﬁguration of variables
form:
x∗= arg max

θi(xi) +

θf(xf) = arg max
pairwise ones.) Note that the partition function Z(θ) plays no role in MAP estimation.
Linear programming relaxation
We can rewrite the objective in terms of the variational parameters as follows:
arg max
s), where x∗
s is the optimal assigment of node s. So instead
facets that is exponential in the number of nodes.
A standard strategy in combinatorial optimization is to relax the constraints. In this case,live inside a convex outer bound L(G). Having deﬁned this relaxed constraint set, we have
max
max
τ∈L(G) θT τ
If the solution is integral, it is exact; if it is fractional, it is an approximation. This is called
constraints that are enforced are those that correspond to consistency on a tree, which is a
How should we actually perform the optimization? We can use a generic linear programming
package, but this is often very slow. Fortunately, in the case of graphical models, it is possible to
devise specialised distributed message passing algorithms for solving this optimization problem,
as we explain below.
One
heuristic way to proceed would be to consider the zero temperature limit of the probability
message passing methods used to solve the inference problem so that they solve the MAP
estimation problem instead.
In particular, in the zero temperature limit, the sum operator
In more detail, let
A(θ) ≜
max
Now consider an inverse temperature β going to inﬁnity. We have
lim
β→+∞
A(βθ)
β
=
lim
β→+∞
β
max
μ∈M(G)
(
)
=
max
μ∈M(G)

lim
β→+∞
β H (μ)
%
=
max
It is the concavity of the objective function that allows us to interchange the lim and max
operators (see (Wainwright and Jordan 2008b, p274) for details).
Now consider the Bethe approximation, which has the form maxτ∈L(G) θT τ + HBethe(τ).
We showed that loopy BP ﬁnds a local optimum of this objective. In the zero temperature limit,
loopy BP does not solve this LP relaxation unless the graph is a tree (Wainwright and Jordan
2008b, p211). The reason is that Bethe energy functional is not concave (except on trees), so wesolve the above LP relaxation.
than the standard parallel updates. The idea is to pick an arbitrary node ordering X1, . . . , XN.
We then consider a set of trees which is a subsequence of this ordering. At each iteration, we
shown that this monotonically minimizes a lower bound on the energy, and thus is guaranteed
to converge to the global optimum of the LP relaxation.
Graphcuts
In this section, we show how to ﬁnd MAP state estimates, or equivalently, minimum energy
known as graphcuts and is very widely used, especially in computer vision applications.
We will start by considering the case of MRFs with binary nodes and a restricted class of
potentials; in this case, graphcuts will ﬁnd the exact global optimum. We then consider the
case of multiple states per node, which are assumed to have some underlying ordering; we can
approximately solve this case by solving a series of binary subproblems, as we will see.
Graphcuts for the generalized Ising model
Let us start by considering a binary MRF where the edge energies have the following form:
Euv(xu, xv) =

if xu = xv
λst
if xu ̸= xv
(since we are trying to minimize energy). Since we are free to add any constant we like to the
overall energy without affecting the MAP state estimate, let us rescale the local energy terms
=
=
Having constructed the graph, we compute a minimal s −t cut. This is a partition of the nodes
into two sets, Xs, which are nodes connected to s, and Xt, which are nodes connected to t. We
The best takez1
z4
z2
z3
s
t
contribute to the cost of the cut (for bidirected edges, we only count one of the costs). Here the min cut
Used with kind permission of Daphne
Koller.
pick the partition which minimizes the sum of the cost of the edges between nodes on different
sides of the partition:
cost(Xs, Xt) =

cost(xu, sv)
Minimizing the cost in this graph is equivalent to minimizing the energy in the MRF. Hence
Graphcuts for binary MRFs with submodular potentials
We now discuss how to extend the graphcuts construction to binary MRFs with more general
kinds of potential functions. In particular, suppose each pairwise energy satisﬁes the following
condition:
MRF or associative MRF, since the model “wants” neighboring states to be the same.
is, the extra value of adding one more element to a set is reduced if the set is already large. More formally, we say that
If −f is submodular, then f is supermodular.To apply graphcuts to a binary MRF with submodular potentials, we construct the pairwise
edge weights as follows:
E′
we construct new local edge weights as follows: ﬁrst we initialize E′(u) = E(u), and then for
each edge pair (u, v), we update these values as follows:
E′
=
E′
E′
=
E′
We now construct a graph in a similar way to before. Speciﬁcally, if E′
E′
xu −xv with cost E′
Graphcuts for nonbinary metric MRFs
We now discuss how to use graphcuts for approximate MAP estimation in MRFs where each
form a metric. We call such a model a metric MRF. For example, suppose the states have a
natural ordering, as commonly arises if they are a discretization of an underlying continuous
space. In this case, we can deﬁne a metric of the form E(xs, xt) = min(δ, ||xs −xt||) or a
One version of graphcuts is the alpha expansion. At each step, it picks one of the available
labels or states and calls it α; then it solves a binary subproblem where each variable can choose
precisely, we deﬁne a new MRF on binary nodes, and we deﬁne the energies of this new model,
relative to the current assignment x, as follows:
E′
E′
To optimize E′ using graph cuts (and thus ﬁgure out the optimal alpha expansion move),
we require that the energies be submodular. Plugging in the deﬁnition we get the following
constraint:
Eu,v(xu, xv) + Eu,v(α, α) ≤Eu,v(xu, α) + Eu,v(α, xv)
triangle inequality. Thus we can apply the alpha expansion move to any metric MRF.(a) initial labeling
(b) standard move
(c) α-β-swap
(d) α-expansion
just ﬂips the label of one pixel. (c) An α −β swap allows all nodes that are currently labeled as α to
be relabeled as β if this decreases the energy. (d) An α expansion allows all nodes that are not currently
labeled as α to be relabeled as α if this decreases the energy.
Used with kind permission of Ramin Zabih.
At each step of alpha expansion, we ﬁnd the optimal move from amongst an exponentially
large set; thus we reach a strong local optimum, of much lower energy than the local optima
found by standard greedy label ﬂipping methods such as iterative conditional modes. In fact,
one can show that, once the algorithm has converged, the energy of the resulting solution is at
most 2c times the optimal energy, where
c = max
(u,v)∈E
maxα̸=β Euv(α, β)
minα̸=β Euv(α, β)
approximation.
call them α and β. All the nodes currently labeled α can change to β (and vice versa) if this
of models than the α-expansion version, it is theoretically not as powerful. Indeed, in various
Experimental comparison of graphcuts and BP
techniques for this class of problems. Some of the results, for the problem of stereo depth
of speed, graphcuts is the fastest, with TRW a close second. Other algorithms, such as ICM,TRW
TRW
Energy
Running Time (s)
Energy
Running Time (s)
Energy minimization on a CRF for stereo depth estimation. Top row: two input images along
Bottom left: results are for the Teddy image (shown in top row). Bottom right: results are for the Tsukuba
Used with kind
permission of Daphne Koller.
gence to evaluate the optimal energy. It turns out that for many of the images in the stereo
if we add these constraints to the model, the graph either becomes too dense (making BP slow),
One way around this is to generate a diverse set of local modes, using repeated applications
model, which uses global features, to rerank the solutions.(b) Ground truth
(c) Swap algorithm
(d) Expansion algorithm
(e) Normalized correlation
(f) Simulated annealing
pixels, from the University of Tsukuba. (The corresponding right image is similar, but not shown.) (b)
swap, (d) α expansion, (e) normalized cross correlation, (f) simulated annealing.
Used with kind permission of Ramin Zabih.
Dual decomposition
We are interested in computing
p∗= max

θi(xi) +

θf(xf)
where F represents a set of factors. We will assume that we can tractably optimize each local
factor, but the combination of all of these factors makes the problem intractable. One way to
proceed is to optimize each term independently, but then to introduce constraints that force all
the local estimates of the variables’ values to agree with each other. We explain this in morex1
x2
x3
x4
θf(x1, x2)
θh(x2, x4)
θk(x3, x4)
θg(x1, x3)
(a)
x1
x2
x3
x4
xf
xf
xg
xg
xh
xh
xk
xk
θk(xk
θh(xh
θf(xf
=
=
=
=
=
=
=
=
θg(xg
(b)
copy of each variable for each factor it participates in.
Used
with kind permission of David Sontag.
Basic idea
Let us duplicate the variables xi, once for each factor, and then force them to be equal.
Speciﬁcally, let xf
f = {xf
p∗= max
x,xf

θi(xi) +

θf(xf
f)
s.t.
xf
Let us now introduce Lagrange multipliers, or dual variables, δfi(k), to enforce these constraints.
The Lagrangian becomes
L(δ, x, xf)
=

θi(xi) +

θf(xf
f)
+



ˆxi
δfi(ˆxi)
!
I(xi = ˆxi) −I(xf
i = ˆxi)
#
p∗= max
x,xf L(δ, x, xf)
s.t.
xf
since if the constraints hold, the last term is zero. We can get an upper bound by dropping the
consistency constraints, and just optimizing the following upper bound:
L(δ)
≜
max
x,xf L(δ, x, xf)
=

i
max
xi
⎛
⎝θi(xi) +

δfi(xi)
⎞
⎠+

f
max
xf
⎛
⎝θf(xf) −

δfi(xi)
⎞x1
δf2(x2)
δf1(x1)
δk4(x4)
δk3(x3)
δg1(x1)
+
−
−
−
−
δf1(x1)
δg3(x3)
δg1(x1)
−
−
δh2(x2)
δh4(x4)
−
−
+
x3
δg3(x3)
δk3(x3)
x4
+
δk4(x4)
δh4(x4)
+
x2
δf2(x2)
δh2(x2)
θf(x1, x2)
θh(x2, x4)
θk(x3, x4)
θg(x1, x3)
x3
x4
x4
x2
x2
x1
x1
x3
Illustration of dual decomposition.
Used with
kind permission of David Sontag.
This objective is tractable to optimize, since each xf term is decoupled. Furthermore, we see
that L(δ) ≥p∗, since by relaxing the consistency constraints, we are optimizing over a larger
space. Furthermore, we have the property that
min
δ
L(δ) = p∗
Minimizing this upper bound is known as dual decomposition or Lagrangian relaxation
that L(δ) is the dual to the same LP relaxation we saw before. We will discuss several possible
optimization algorithms below.
The main advantage of dual decomposition from a practical point of view is that it allows
one to mix and match different kinds of optimization algorithms in a convenient way.
For
example, we can combine a grid structured graph with local submodular factors to perform
image segmentation, together with a tree structured model to perform pose estimation (see
Theoretical guarantees
What can we say about the quality of the solutions obtained in this way? To understand this, let
us ﬁrst introduce some more notation:
θ
δ
i (xi)
≜
θi(xi) +

δfi(xi)
θ
δ
f(xf)
≜
θf(xf) −

δfi(xi)This represents a reparameterization of the original problem, in the sense that

i
θi(xi) +

f
θf(xf) =

i
θ
δ
i (xi) +

f
θ
δ
f(xf)
and hence
L(δ) =

i
max
δ
i (xi) +

f
max
δ
f(xf)
mizing assignments to the singleton terms agrees with the assignments to the factor terms, i.e.,
so that x∗
δ∗
i (xi) and x∗
δ∗
f (xf). In this case, we have
L(δ∗) =

i
θ
δ∗
i (x∗
i ) +

f
θ
δ∗
f (x∗
f) =

i
θi(x∗
i ) +

f
θf(x∗
f)
Now since

i
θi(x∗
i ) +

f
θf(x∗
f) ≤p∗≤L(δ∗)
we conclude that L(δ∗) = p∗, so x∗is the MAP assignment.
So if we can ﬁnd a solution where all the subproblems agree, we can be assured that it is the
global optimum. This happens surprisingly often in practical problems.
Subgradient descent
δ
i (xi)
δ
f(xf) have multiple optima. One approach is to use subgradient descent. This updates all
fi (xi) = δt
fi(xi) −αtgt
fi(xi)
(See
One can show that the gradient is given by the following sparse vector.
First let xs
δt
i (xi) and xf
δt
if xf
i ̸= xs
i (so factor f disagrees with the local term on how to set variable i), we set gfi(xs
i) =
δt
i (xs
δt
i (xf
i ),
bringing them closer to agreement. Similarly, the subgradient update will decrease the value of
θ
δt
f (xf
δt
f (xs
i, xf\i).
To compute the gradient, we need to be able to solve subproblems of the following form:
argmax
xf
θ
δt
f (xf) = argmax
xf
⎡
⎣θf(xf) −

δt
fi(xi)
⎤
⎦master.) Obviously if the scope of factor f is small, this is simple. For example, if each factor is
pairwise, and each variable has K states, the cost is just K2. However, there are some kinds of
global factors that also support exact and efficient maximization, including the following:
•
Graphical models with low tree width.
•
This
is useful for data association problems, where we must match up a sensor reading with
an unknown source.
•
•
Cardinality constraints.
For example, we might have a factor over a large set of binary
variables that enforces that a certain number of bits are turned on; this can be useful in
•
Factors which are constant for all but a small set S of distinguished values of xf. Then we
Coordinate descent
coordinate descent. By choosing the size of the blocks, we can trade off convergence speed with
ease of the local optimization problem.
Algorithmically, this is similar to belief propagation on a factor graph. In particular, we deﬁne
variable i to factor f. These messages can be computed as follows (see (Globerson and Jaakkola
=
θi(xi) +

g̸=f
=
|f| max
xf\i
⎡
⎣θf(xf) +

⎤
⎦
ifunction of all messages coming into f, and f’s local factor:
potential:
The key computational bottleneck is computing the max marginals of each factor, where we
max out all the variables from xf except for xi, i.e., we need to be able to compute the following
max marginals efficiently:
max
xf\i h(xf\i, xi),
h(xf\i, xi) ≜θf(xf) +

δjf(xj)
can solve this efficiently for low treewidth graphical models using message passing; we can also
method.
Coordinate descent is a simple algorithm that is often much faster at minimizing the dual than
gradient descent, especially in the early iterations. It also reduces the objective monotonically,
and does not need any step size parameters. Unfortunately, it is not guaranteed to converge to
the global optimum, since L(δ) is convex but not strictly convex (which implies there may be
more than one globally optimizing value). One way to ensure convergence is to replace the max
Recovering the MAP assignment
cases arise when there are fractional assignments with the same optimal value as the MAP
estimate.)
δ∗
i
has a unique maximum, x∗
locally decodable to x∗. One can show than in this case, the LP relaxation is unique and its
solution is indeed x∗. If many, but not all, of the nodes are uniquely decodable, we can “clamp”
the uniquely decodable ones to their MAP value, and then use exact inference algorithms to
ﬁgure out the optimal assignment to the remaining variables. Using this method, (Meltzer et al.
Another approach is to use the upper bound provided by the dual in a branch and boundExercises
i = β, and
x′
i = xi is unchanged f xi ̸= α and xi ̸= β.
b. Deﬁne an energy function over the new variables such that E′(t) = E(x) + const.
c. Show that E′ is submodular if E is a semimetric.
(Source: Daphne Koller.). Let X be a pairwise metric Markov random ﬁeld over a graph G = (V, E).
Suppose that the variables are nonbinary and that the node potentials are nonnegative. Let A denote the
x⋆in general, the α-expansion algorithm provides a method for ﬁnding assignments ˆx that are locally
optimal with respect to a large set of transformations, i.e., the possible α-expansion moves.
Despite the fact that α-expansion only produces a locally optimal MAP assignment, it is possible to prove
that the energy of this assignment is within a known factor of the energy of the globally optimal solution
x⋆. In fact, this is a special case of a more general principle that applies to a wide variety of algorithms,
that the solutions obtained by the algorithm are ‘strong local minima’, i.e., local minima with respect to
a large set of potential moves, then it is possible to derive bounds on the (global) suboptimality of these
solutions, and the quality of the bounds will depend on the nature of the moves considered. (There is a
precise deﬁnition of ‘large set of moves’.)
Consider the following approach to proving the suboptimality bound for α-expansion.
s =
α}, i.e., the set of nodes labelled α in the global minimum. Let x′ be an assignment that is equal to
x⋆on V α and equal to ˆx elsewhere; this is an α-expansion of ˆx. Verify that E(x⋆) ≤E(ˆx) ≤E(x′).
b. Building on the previous part, show that E(ˆx) ≤2cE(x⋆), where c = max(s,t)∈E
 maxα̸=β εst(α,β)
minα̸=β εst(α,β)

and E denotes the energy of an assignment.
Hint. Think about where x′ agrees with ˆx and where it agrees with x⋆.
(Source: Daphne Koller.). Two important problems in computer vision are that of parsing articulated objects
(e.g., the human body), called pose estimation, and segmenting the foreground and the background, called
segmentation. Intuitively, these two problems are linked, in that solving either one would be easier if the
solution to the other were available. We consider solving these problems simultaneously using a joint
inference in this model.
pixel i is in the foreground. Denote the full set of these variables by x = (xi).In addition, suppose we have an undirected tree structure T = (V′, E′) on the parts. For each body
part, we have a discrete set of candidate poses that the part can be in, where each pose is characterized
by parameters specifying its position and orientation. (These candidates are generated by a procedure
external to the algorithm described here.) Deﬁne yjk to be a binary variable indicating whether body part
for each part. Note that in order to describe a valid conﬁguration, y must satisfy the constraint that
K
Suppose we have the following energy function on pixels:
E1(x) =




(i,j)∈E
viewed as the energy for a pairwise metric MRF with respect to G.
We then have the following energy function for parts:
E2(y) =


θp(yp) +


(p,q)∈E′
θpq(yp, yq).
Since each part candidate yjk is assumed to come with a position and orientation, we can compute a
binary mask in the image plane. The mask assigns a value to each pixel, denoted by {wi
wi
energy function relating the parts and the pixels:
E3(x, y) =




K


jk.
In other words, this energy term only penalizes the case where a part candidate is active but the pixel
underneath is labeled as background.
Formulate the minimization of E1 + E2 + E3 as an integer program and show how you can use dual
decomposition to solve the dual of this integer program. Your solution should describe the decomposition
into slaves, the method for solving each one, and the update rules for the overall algorithm. Brieﬂy justify
your design choices, particularly your choice of inference algorithms for the slaves.Monte Carlo inference
Introduction
So far, we discussed various deterministic algorithms for posterior inference.
ods enjoy many of the beneﬁts of the Bayesian approach, while still being about as fast as
ity (e.g., they usually assume conjugate priors and exponential family likelihoods, although see
thermore, although they are fast, their accuracy is often limited by the form of the approximation
which we choose.
In this chapter, we discuss an alternative class of algorithms based on the idea of Monte Carlo
(unweighted) samples from the posterior, xs ∼p(x|D), and then use these to compute any
quantity of interest, such as a posterior marginal, p(x1|D), or the posterior of the difference of
two quantities, p(x1 −x2|D), or the posterior predictive, p(y|D), etc. All of these quantities
S
S
By generating enough samples, we can achieve any desired level of accuracy we like. The main
issue is: how do we efficiently generate samples from a probability distribution, particularly in
samples. In the next chapter, we discuss an iterative method known as Markov Chain Monte
Carlo, or MCMC for short, which produces dependent samples but which works well in high
dimensions. Note that sampling is a large topic. The reader should consult other books, such as
Sampling from standard distributions
These methods are often used as subroutines by more complex methods.
Using the cdfx
u
F
Sampling using an inverse CDF. Figure generated by sampleCdf.
be its inverse. Then we have the following result.
Proof.
=
(applying F to both sides)
=
F(x)
where the ﬁrst line follows since F is a monotonic function, and the second line follows since
U is uniform on the unit interval.
Hence we can sample from any univariate distribution, for which we can evaluate its inverse
“slide along” the x axis until you intersect the F curve, and then “drop down” and return the
illustration.
For example, consider the exponential distribution
The cdf is
whose inverse is the quantile function
λ
sampling from the uniform and then transforming the results using −ln(u)/λ.We now describe a method to sample from a Gaussian. The idea is we sample uniformly from
a unit radius circle, and then use the change of variables formula to derive samples from a
spherical 2d Gaussian. This can be thought of as two samples from a 1d Gaussian.
z2
πI(z inside circle). Now deﬁne
xi = zi
r2
p(x1, x2) = p(z1, z2)| ∂(z1, z2)
∂(x1, x2)| =

√
2x2
 
√
2x2

Hence x1 and x2 are two independent samples from a univariate Gaussian. This is known as
To sample from a multivariate Gaussian, we ﬁrst compute the Cholesky decomposition of its
cov [y] = Lcov [x] LT = L I LT = Σ
Rejection sampling
pling, which we now explain.
Basic idea
In rejection sampling, we create a proposal distribution q(x) which satisifes Mq(x) ≥˜p(x),
for some constant M, where ˜p(x) is an unnormalized version of p(x) (i.e., p(x) = ˜p(x)/Zp
for some possibly unknown constant Zp). The function Mq(x) provides an upper envelope for
˜p. We then sample x ∼q(x), which corresponds to picking a random x location, and then
envelope. If u >
˜p(x)
the acceptance region is shown shaded, and the rejection region is the white region between
the shaded zone and the upper envelope.
We now prove that this procedure is correct. Let                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          


























x
uMq(x  )
Accept region
Reject region
x  ~ q(x)
(i)
(i)
(i)
(i)
p(x  )
Mq(x  )
(a)
 
 
target p(x)
comparison function Mq(x)
(b)
(a) Schematic illustration of rejection sampling.
Then the cdf of the accepted points is given by
=
P(x accepted)
=
 
I((x, u) ∈S0)q(x)dudx
 
I((x, u) ∈S)q(x)dudx =
 x0
−∞˜p(x)dx
 ∞
−∞˜p(x)dx
which is the cdf of p(x), as desired.
How efficient is this method?
Since we generate with probability q(x) and accept with
probability
˜p(x)
Mq(x), the probability of acceptance is
p(accept) =


˜p(x)
M


˜p(x)dx
Hence we want to choose M as small as possible while still satisfying Mq(x) ≥˜p(x).
Example
Ga(x|α, λ) =
One can show that if Xi
iid
∼Expon(λ), and Y = X1 + · · · + Xk, then Y ∼Ga(k, λ). For(a)
f(x) half−gaussian
(b)
samples from f(x) (by ARS)
(c)
(a) Idea behind adaptive rejection sampling. We place piecewise linear upper (and lower)
Figure generated by
Daniel Eaton.
p(x)
q(x)
=
Ga(x|α, λ)
=
Γ(k)λα
This ratio attains its maximum when x = α −k. Hence
M =
Ga(α −k|α, λ)
based on the Cauchy distribution.)
Application to Bayesian statistics
Suppose we want to draw (unweighted) samples from the posterior, p(θ|D) = p(D|θ)p(θ)/p(D).
We can use rejection sampling with ˜p(θ) = p(D|θ)p(θ) as the target distribution, q(θ) = p(θ)
as our proposal, and M = p(D|ˆθ), where ˆθ = arg max p(D|θ) is the MLE; this was ﬁrst
˜p(θ)
Mq(θ) = p(D|θ)
p(D|ˆθ)
Thus samples from the prior that have high likelihood are more likely to be retained in the
posterior. Of course, if there is a big mismatch between prior and posterior (which will be the
case if the prior is vague and the likelihood is informative), this procedure is very inefficient. We
discuss better algorithms later.
Adaptive rejection sampling
We now describe a method that can automatically come up with a tight upper envelope q(x)
to any log concave density p(x). The idea is to upper bound the log density with a piecewisebased on a ﬁxed grid over the support of the distribution. We then evaluate the gradient of the
log density at these locations, and make the lines be tangent at these points.
Since the log of the envelope is piecewise linear, the envelope itself is piecewise exponential:
where xi are the grid points. It is relatively straightforward to sample from this distribution. If
the sample x is rejected, we create a new grid point at x, and thereby reﬁne the envelope. As
the number of grid points is increased, the tightness of the envelope improves, and the rejection
it can be applied to unnormalized distributions.
Rejection sampling in high dimensions
It is clear that we want to make our proposal q(x) as close as possible to the target distribution
p(x), while still being an upper bound. But this is quite hard to achieve, especially in high
dimensions.
pI) using as a proposal
qI). Obviously we must have σ2
p in order to be an upper bound. In D
both p and q are normalized), which decreases exponentially fast with dimension. For example,
This is a fundamental weakness of rejection sampling.
from high dimensional distributions. Sometimes this uses (adaptive) rejection sampling as a
Importance sampling
We now describe a Monte Carlo method known as importance sampling for approximating
integrals of the form
I = E [f] =


f(x)p(x)dx
Basic idea
The idea is to draw samples x in regions which have high probability, p(x), but also where
|f(x)| is large. The result can be super efficient, meaning it needs less samples than if we
were to sample from the exact distribution p(x). The reason is that the samples are focussed
on the important parts of space. For example, suppose we want to estimate the probability of
proposal of the form q(x) ∝f(x)p(x) than to sample from p(x) itself.
Importance sampling samples from any proposal, q(x). It then uses these samples to estimatethe integral as follows:
E [f] =


f(x)p(x)
S
S

wsf(xs) = ˆI
where ws ≜p(xs)
q(xs) are the importance weights. Note that, unlike rejection sampling, we use all
the samples.
How should we choose the proposal? A natural criterion is to minimize the variance of the
estimate ˆI = 
s wsf(xs). Now
varq(x) [f(x)w(x)] = Eq(x)


−I2
Since the last term is independent of q, we can ignore it. By Jensen’s inequality, we have the
following lower bound:
Eq(x)




|f(x)|p(x)dx
The lower bound is obtained when we use the optimal importance distribution:
q∗(x) =
|f(x)|p(x)

|f(x′)|p(x′)dx′
When we don’t have a particular target function f(x) in mind, we often just try to make
q(x) as close as possible to p(x). In general, this is difficult, especially in high dimensions, but
it is possible to adapt the proposal distribution to improve the approximation. This is known as
Handling unnormalized distributions
It is frequently the case that we can evaluate the unnormalized target distribution, ˜p(x), but not
its normalization constant, Zp. We may also want to use an unnormalized proposal, ˜q(x), with
possibly unknown normlization constant Zq. We can do this as follows. First we evaluate
E [f] = Zq
Zp


f(x) ˜p(x)
Zp
S
S

˜wsf(xs)
where ˜ws ≜˜p(xs)
˜q(xs) is the unnormalized importance weight. We can use the same set of samples
Zp
Zq
Zq


˜p(x)dx =


˜p(x)
S
S

˜ws
Hence
ˆI =
S

s ˜wsf(xs)
S

s ˜ws
=
S

wsf(xs)where
ws ≜
˜ws

s′ ˜ws′
are the normalized importance weights. The resulting estimate is a ratio of two estimates, and
Importance sampling for a DGM: likelihood weighting
We now describe a way to use importance sampling to generate samples from a distribution
If we have no evidence, we can sample from the unconditional joint distribution of a DGM
p(x) as follows: ﬁrst sample the root nodes, then sample their children, then sample their
children, etc. This is known as ancestral sampling. It works because, in a DAG, we can always
topologically order the nodes so that parents preceed children. (Note that there is no equivalent
easy method for sampling from an unconditional undirected graphical model.)
Now suppose we have some evidence, so some nodes are “clamped” to observed values, and
we want to sample from the posterior p(x|D). If all the variables are discrete, we can use the
following simple procedure: perform ancestral sampling, but as soon as we sample a value that
is inconsistent with an observed value, reject the whole sample and start again. This is known
Needless to say, logic sampling is very inefficient, and it cannot be applied when we have
before, conditional on their parents. But don’t sample observed variables; instead we just use
their observed values. This is equivalent to using a proposal of the form
q(x) =

t̸∈E
p(xt|xpa(t))

δx∗
t (xt)
where E is the set of observed nodes, and x∗
t is the observed value for node t. We should
therefore give the overall sample an importance weight as follows:
w(x) = p(x)
q(x) =

t̸∈E
p(xt|xpa(t))
p(xt|xpa(t))

p(xt|xpa(t))
=

p(xt|xpa(t))
Sampling importance resampling (SIR)
We can draw unweighted samples from p(x) by ﬁrst using importance sampling (with proposal
q) to generate a distribution of the form
p(x) ≈

swhere ws are the normalized importance weights.
We then sample with replacement from
Let this procedure induce a
distribution denoted by ˆp. To see that this is valid, note that
=

s


s ˜p(xs)/q(xs)
→

q(x)q(x)dx
 ˜p(x)
q(x)q(x)dx
=


˜p(x)dx
=


weighted approximation of the form
S′
S′

δxs(x)
Note that we typically take S′ ≪S.
p(θ|D) = p(D|θ)p(θ)/p(D). We can use importance sampling with ˜p(θ) = p(D|θ)p(θ) as
the unnormalized posterior, and q(θ) = p(θ) as our proposal. The normalized weights have the
form
ws =
˜p(θs)/q(θs)

s′ ˜p(θs′)/q(θs′) =
p(D|θs)

s′ p(D|θs′)
We can then use SIR to sample from p(θ|D).
Of course, if there is a big discrepancy between our proposal (the prior) and the target (the
posterior), we will need a huge number of importance samples for this technique to work reliably,
since otherwise the variance of the importance weights will be very large, implying that most
discuss particle ﬁltering.)
Particle ﬁltering
Particle ﬁltering (PF) is a Monte Carlo, or simulation based, algorithm for recursive BayesianSequential importance sampling
The basic idea is to appproximate the belief state (of the entire state trajectory) using a weighted
set of particles:
p(z1:t|y1:t) ≈
S

ˆws
where ˆws
t is the normalized weight of sample s at time t. From this representation, we can
easily compute the marginal distribution over the most recent state, p(zt|y1:t), by simply
entire trajectories has various implications which we will discuss later.)
We update this belief state using importance sampling.
If the proposal has the form
q(zs
ws
t ∝p(zs
q(zs
which can be normalized as follows:
ˆws
t =
ws
t

s′ ws′
t
We can rewrite the numerator recursively as follows:
p(z1:t|y1:t)
=
=
∝
where we have made the usual Markov assumptions.
We will restrict attention to proposal
densities of the following form:
so that we can “grow” the trajectory by adding the new state zt to the end. In this case, the
importance weights simplify to
ws
t
∝
p(yt|zs
t)p(zs
t|zs
q(zs
t|zs
=
ws
p(yt|zs
t)p(zs
t|zs
q(zs
t|zs
most recent part of the trajectory and observation sequence, rather than the whole history, in
order to compute the new sample. In this case, the weight becomes
ws
t ∝ws
p(yt|zs
t)p(zs
t|zs
q(zs
t|zsHence we can approximate the posterior ﬁltered density using
p(zt|y1:t) ≈
S

ˆws
t (zt)
The basic algorithm is now very simple: for each old sample s, propose an extension using
zs
t ∼q(zt|zs
this basic algorithm does not work very well, as we discuss below.
The degeneracy problem
The basic sequential importance sampling algorithm fails after a few steps because most of
the particles will have negligible weight. This is called the degeneracy problem, and occurs
time), using a myopic proposal distribution.
We can quantify the degree of degeneracy using the effective sample size, deﬁned by
Seff ≜
S
t ]
where w∗s
t
= p(zs
t|y1:t)/q(zs
t|zs
be computed exactly, since we don’t know the true posterior, but we can approximate it using
ˆSeff =
S
If the variance of the weights is large, then we are wasting our resources updating particles with
low weight, which do not contribute much to our posterior estimate.
There are two main solutions to the degeneracy problem: adding a resampling step, and using
a good proposal distribution. We discuss both of these in turn.
The resampling step
The main improvement to the basic SIS algorithm is to monitor the effective sampling size,
and whenever it drops below a threshold, to eliminate particles with low weight, and then
to create replicates of the surviving particles. (Hence PF is sometimes called survival of the
t }S
replacement S times from the weighted distribution
p(zt|y1:t) ≈
S

ˆws
t (zt)
where the probability of choosing particle j for replication is wj
t. (This is sometimes called
so we set the new weights to wsSURSRVDO
ZHLJKWLQJ
UHVDPSOH
]
W
_\

W


]
W
_\

W


\
W
_]
W


]
W
_\

W


Illustration of particle ﬁltering.
nomial resampling, which computes
(K1, . . . , KS) ∼Mu(S, (w1
t , . . . , wS
t ))
We then make Ks copies of zs
t. Various improvements exist, such as systematic resampling
residual resampling, and stratiﬁed sampling, which can reduce the variance of the weights.
of the state is required, it should be computed before the resampling step, since this will result
in lower variance.)
Draw zs
t ∼q(zt|zs
Compute weight ws
t ∝ws
p(yt|zs
t)p(zs
t|zs
q(zs
t|zs
;
t =
ws
t

s′ ws′
t
;
S
Resample S indices π ∼wt;
z:
t = zπ
t ;
ws
Although the resampling step helps with the degeneracy problem, it introduces problems of
its own. In particular, since the particles with high weight will be selected many times, there is
a loss of diversity amongst the population. This is known as sample impoverishment. In theextreme case of no process noise (e.g., if we have static but unknown parameters as part of the
state space), then all the particles will collapse to a single point within a few iterations.
To mitigate this problem, several solutions have been proposed.
particles,
p(zt|y1:t) ≈
S

ws
t κ(zt −zs
t)
where κ is some smoothing kernel. We then sample from this smoothed distribution. This is
static parameters, add some artiﬁcial process noise. (If this is undesirable, other algorithms must
The proposal distribution
The simplest and most widely used proposal distribution is to sample from the prior:
q(zt|zs
In this case, the weight update simpliﬁes to
ws
t ∝ws
t−1p(yt|zs
t)
This can be thought of a “generate and test” approach: we sample values from the dynamic
This
is the approach used in the condensation algorithm (which stands for “conditional density
However, if the likelihood is
narrower than the dynamical prior (meaning the sensor is more informative than the motion
model, which is often the case), this is a very inefficient approach, since most particles will be
assigned very low weight.
It is much better to actually look at the data yt when generating a proposal. In fact, the
optimal proposal distribution has the following form:
q(zt|zs
p(yt|zs
If we use this proposal, the new weight is given by
ws
t ∝ws
t−1p(yt|zs


p(yt|z′
t)p(z′
t|zs
t
This proposal is optimal since, for any given zs
t takes the same value
regardless of the value drawn for zs
t. Hence, conditional on the old values z.
true weights var [w∗s
t ], is zero.In general, it is intractable to sample from p(zt|zs
to compute the predictive density p(yt|zs
proposal distribution can be used. The ﬁrst setting is when zt is discrete, so the integral becomes
a sum. Of course, if the entire state space is discrete, we can use an HMM ﬁlter instead, but
in some cases, some parts of the state are discrete, and some continuous. The second setting
is when p(zt|zs
This occurs when the dynamics are nonlinear but the
tion to p(zt|zs
models. Unlike MCMC, we do not need to worry about the proposals being reversible.
Application: robot localization
Consider a mobile robot wandering around an office environment. We will assume that it already
has a map of the world, represented in the form of an occupancy grid, which just speciﬁes
whether each grid cell is empty space or occupied by an something solid like a wall. The goal
is for the robot to estimate its location. This can be solved optimally using an HMM ﬁlter, since
we are assuming the state space is discrete. However, since the number of states, K, is often
very large, the O(K2) time complexity per update is prohibitive. We can use a particle ﬁlter as
a sparse approximation to the belief state. This is known as Monte Carlo localization, and is
so it can only sense distance to obstacles. It starts out with a uniform prior, reﬂecting the fact
that the owner of the robot may have turned it on in an arbitrary location. (Figuring out where
you are, starting from a uniform prior, is called global localization.) After the ﬁrst scan, which
indicates two walls on either side, the belief state is shown in (b). The posterior is still fairly
broad, since the robot could be in any location where the walls are fairly close by, such as a
end of the corridor. However, due to symmetry, it is not sure if it is in location I (the true
location) or location II. (This is an example of perceptual aliasing, which refers to the fact that
out precisely where it is. The whole process is analogous to someone getting lost in an office
building, and wandering the corridors until they see a sign they recognize.
Application: visual object tracking
copter) in a video sequence. The method uses a simple linear motion model for the centroid
of the object, and a color histogram for the likelihood model, using Bhattacharya distance to
compare histograms. The proposal distribution is obtained by sampling from the likelihood. SeeRoom A
Room B
Start
Room C
I
II
II
I
(a) Path and reference poses
Illustration of Monte Carlo localization.
Used
with kind permission of Sebastian Thrun.
same color as the target object. (d) shows that the system is confused between the grey of the
helicopter and the grey of the building. The posterior is bimodal. The green ellipse, representing
the posterior mean and covariance, is in between the two modes. (e) shows that the probability
mass has shifted to the wrong mode: the system has lost track. (f) shows the particles spread
out over the gray building; recovery of the object is very unlikely from this state using this(a)
(b)
(c)
(d)
(e)
(f)
Example of particle ﬁltering applied to visual object tracking, based on color histograms.
in the background. See text for details. Figure generated by pfColorTrackerDemo, written by Sebastien
Paris.
proposal.
We see that the method is able to keep track for a fairly long time, despite the presence
of clutter. However, eventually it loses track of the object. Note that since the algorithm is
an option. The simplest way to improve performance is to use more particles. An alternative
is to perform tracking by detection, by running an object detector over the image every fewApplication: time series forecasting
noise changes over time. Particle ﬁltering is widely used in such settings. See e.g., (Doucet et al.
In some models, we can partition the hidden variables into two kinds, qt and zt, such that
we can analytically integrate out zt provided we know the values of q1:t. This means we only
have sample q1:t, and can represent p(zt|q1:t) parametrically. Thus each particle s represents
a value for qs
The advantage of this approach is that we reduce the dimensionality of the space in which
we are sampling, which reduces the variance of our estimate. Hence this technique is known
method is best explained using a speciﬁc example.
A canonical example for which RBPF can be applied is the switching linear dynamical system
p(zt|y1:t, qs
If we propose from the prior, q(qt = k|qs
ws
t ∝ws
t−1p(yt|qt = k, qs
t−1Ls
t,k
where
Ls
tk
=


p(yt|qt = k, zt,
qs
The quantity Ls
tk is the predictive density for the new observation yt conditioned on qt = k and
the history qs
If K is small, we can compute the optimal proposal distribution, which is
p(qt = k|y1:t, qs
=
ˆps
=
ˆps
ˆps
=
Ls
tkp(qt = k|qs

k′ Ls
tk′p(qt = k′|qsk ∼p(qt|qs
qs
t := k;
(μs
t, Σs
t, Ls
tk) = KFupdate(μs
ws
t = ws
t−1Lk
ts;
t =
ws
t

s′ ws′
t
;
S
Resample S indices π ∼wt;
q:
t = qπ
t , μ:
t = μπ
t , Σ:
t = Σπ
t , ;
ws
where we use the following shorthand:
ˆps
We then sample from p(qt|qs
ws
t ∝ws
t−1p(yt|qs

k

Ls
tkp(qt = k|qs

actually sampled for qt, we can compute these weights ﬁrst, and use them to decide which
In more detail, the idea is this. We pass each sample in the prior through all K models
to get K posteriors, one per sample. The normalization constants of this process allow us to
old particle s that is chosen, we sample one new state qs
t = k, and use the corresponding
particle is chosen using the latest information, yt.
A further improvement can be obtained by exploiting the fact that the state space is discrete.
Application: tracking a maneuvering target
One application of SLDS is to track moving objects that have piecewise linear dynamics. For
example, suppose we want to track an airplane or missile; qt can specify if the object is ﬂying
normally or is taking evasive action. This is called maneuvering target tracking.(μs
tk, Σs
tk, Lk
ts) = KFupdate(μs
ws
t = ws
k Lk
tsp(qt = k|qs
t =
ws
t

s′ ws′
t
;
Compute optimal proposal p(k|qs
Ls

k′ Ls
Sample k ∼p(k|qs
qs
t = k, μs
t = μs
tk, Σs
t = Σs
tk;
ws
Method
misclassiﬁcation rate
MSE
Time (seconds)
PF
RBPF
so the system will turn in different directions depending on the discrete state.
colored symbols denote the discrete state, and the location of the symbol denotes the (x, y)
from the prior. The colored symbols denote the MAP estimate of the state, and the location of
the symbol denotes the MMSE (minimum mean square error) estimate of the location, which is
particles, using the optimal proposal distribution. A more quantitative comparison is shown in
discrete states. We see that the particle ﬁlter estimate of the belief state (second column) is not
as accurate as the RBPF estimate (third column) in the beginning, although after the ﬁrst few
observations performance is similar for both methods. In (b), we plot the posterior over the x
locations. For simplicity, we use the PF estimate, which is a set of weighted samples, but we
could also have used the RBPF estimate, which is a set of weighted Gaussians.data
(a)
(b)
(c)
(a) A maneuvering target.
The colored symbols represent the hidden discrete state.
(b)
Particle ﬁlter estimate. (c) RBPF estimate. Figure generated by rbpfManeuverDemo, based on code by
Nando de Freitas.
Application: Fast SLAM
for mobile robotics. The main problem with the Kalman ﬁlter implementation is that it is cubic
in the number of landmarks.
are independent. (We assume the landmarks don’t move, so we drop the t subscript). That is,
p(z|q1:t, y1:t) = L
robot’s trajectory, q1:t, and we run L independent 2d Kalman ﬁlters inside each particle. This
takes O(L) time per particle. Fortunately, the number of particles needed for good performance
is quite small (this partly depends on the control / exploration policy), so the algorithm is
essentially linear in the number of particles. This technique has the additional advantage thattruth
(a)
x1,t
PF
t
p(x1,t|y1:t)
(b)
(a) Discrete state.
The system starts in state
by rbpfManeuverDemo, based on code by Nando de Freitas.
it is easy to use sampling to handle the data association ambiguity, and that it allows for other
representations of the map, such as occupancy grids. This idea was ﬁrst suggested in (Murphy
the technique FastSLAM. See rbpfSlamDemo for a simple demo in a discrete grid world.
Exercises
Show how to use a Cauchy proposal to perform rejection sampling from a Gamma distribution. Derive the
optimal constant M, and plot the density and its upper envelope.
zt
=
yt
=
variance) proposal distribution. Hint: use Bayes rule for Gaussians.Markov chain Monte Carlo (MCMC)
inference
Introduction
and importance sampling. The trouble with these methods is that they do not work well in high
is Markov chain Monte Carlo or MCMC. In a survey by SIAM News1, MCMC was placed in the
X whose stationary distribution is the target density p∗(x) of interest (this may be a prior or a
posterior). That is, we perform a random walk on the state space, in such a way that the fraction
of time we spend in each state x is proportional to p∗(x). By drawing (correlated!) samples
x0, x1, x2, . . . , from the chain, we can perform Monte Carlo integration wrt p∗. We give the
details below.
The MCMC algorithm has an interesting history.
It was discovered by physicists working
on the atomic bomb at Los Alamos during World War II, and was ﬁrst published in the open
An extension was published in
A special case (Gibbs
wildly popular in Bayesian statistics, and is becoming increasingly popular in machine learning.
is applicable to a broader range of models, such as models whose size or structure changes
depending on the values of certain variables (e.g., as happens in matching problems), or models
we pass around distributions.
Thus sampling passes sparse messages, whereas variational inference passes denseGibbs sampling
is the MCMC analog of coordinate descent.
Basic idea
The idea behind Gibbs sampling is that we sample each variable in turn, conditioned on the
values of all the other variables in the distribution. That is, given a joint sample xs of all the
•
∼p(x1|xs
•
, xs
•
)
This readily generalizes to D variables. If xi is a visible variable, we do not sample it, since its
value is already known.
The expression p(xi|x−i) is called the full conditional for variable i. In general, xi may only
depend on some of the other variables. If we represent p(x) as a graphical model, we can infer
the dependencies by looking at i’s Markov blanket, which are its neighbors in the graph. Thus
to sample xi, we only need to know the values of i’s neighbors. In this sense, Gibbs sampling
is a distributed algorithm. However, it is not a parallel algorithm, since the samples must be
generated sequentially.
initial samples until the Markov chain has burned in, or entered its stationary distribution. We
Example: Gibbs sampling for the Ising model
p(xt|x−t, θ) ∝

ψst(xs, xt) 
 
(a)
 
 
(b)
 
 
(c)
ﬁeld inference. Figure generated by isingImageDenoiseDemo.
=


=
exp[J 
exp[J 
=
exp[Jηt]
exp[Jηt] + exp[−Jηt] = sigm(2Jηt)
where J is the coupling strength, ηt ≜
function. It is easy to see that ηt = xt(at −dt), where at is the number of neighbors that agree
with (have the same sign as) t, and dt is the number of neighbors who disagree. If this number
is equal, the “forces” on xt cancel out, so the full conditional is uniform.
We can combine an Ising prior with a local evidence term ψt. For example, with a Gaussian
observation model, we have ψt(xt) = N(yt|xt, σ2). The full conditional becomes
=
=
sigm

	
Now the probability of xt entering each state is determined both by compatibility with its
neighbors (the Ising prior) and compatibility with the data (the local likelihood term).
averaging the samples) is somewhat “blurrier”, due to the fact that mean ﬁeld tends to beExample: Gibbs sampling for inferring the parameters of a GMM
It is straightforward to derive a Gibbs sampling algorithm to “ﬁt” a mixture model, especially
if we use conjugate priors. We will focus on the case of mixture of Gaussians, although the
results are easily extended to other kinds of mixture models. (The derivation, which follows from
p(x, z, μ, Σ, π)
=
p(x|z, μ, Σ)p(z|π)p(π)
K

p(μk)p(Σk)
=

 N

K



×
Dir(π|α)
K

N(μk|m0, V0)IW(Σk|S0, ν0)
We use the same prior for each mixture component. The full conditionals are as follows. For
the discrete indicators, we have
p(zi = k|xi, μ, Σ, π)
∝
πkN(xi|μk, Σk)
p(π|z)
=
Dir({αk +
N

I(zi = k)}K
p(μk|Σk, z, x)
=
N(μk|mk, Vk)
k
=
k
mk
=
Nk
≜
N

I(zi = k)
xk
≜
N
Nk
p(Σk|μk, z, x)
=
IW(Σk|Sk, νk)
Sk
=
S0 +
N

I(zi = k)(xi −μk)(xi −μk)T
νk
=
ν0 + Nk
See gaussMissingFitGibbs for some Matlab code. (This code can also sample missing
values for x, if necessary.)Label switching
Although it is simple to implement, Gibbs sampling for mixture models has a fundamental
unidentiﬁable, since we can arbitrarily permute the hidden labels without affecting the likelihood
all modes, we would ﬁnd E [μk|D] is the same for all k (assuming a symmetric prior). This is
called the label switching problem.
This problem does not arise in EM or VBEM, which just “lock on” to a single mode. However,
it arises in any method that visits multiple modes. In 1d problems, one can try to prevent this
problem by introducing constraints on the parameters to ensure identiﬁability, e.g., μ1 < μ2 <
overwhelm the prior and cause label switching anyway. Furthermore, this technique does not
global label permutation to apply to each sample that minimizes some loss function (Stephens
Perhaps the best solution is simply to “not ask” questions that cannot be uniquely identiﬁed.
For example, instead of asking for the probability that data point i belongs to cluster k, ask
for the probability that data points i and j belong to the same cluster. The latter question is
invariant to the labeling. Furthermore, it only refers to observable quantities (are i and j grouped
together or not), rather than referring to unobservable quantities, such as latent clusters. This
approach has the further advantage that it extends to inﬁnite mixture models, discussed in
deﬁned, but the notion of a partitioning of the data is well deﬁned
Collapsed Gibbs sampling *
In some cases, we can analytically integrate out some of the unknown quantities, and just
sample the rest. This is called a collapsed Gibbs sampler, and it tends to be much more
efficient, since it is sampling in a lower dimensional space.
participate in the Markov chain; consequently we can draw conditionally independent samples
θs ∼p(θ|zs, D), which will have much lower variance than samples drawn from the joint state
theorem:
some scalar function. Then
This theorem guarantees that the variance of the estimate created by analytically integrating
estimate.α
π
z1
zi
zN
x1
xi
xN
θK
β
(a)
α
z1
zi
zN
x1
xi
xN
β
(b)
(a) A mixture model. (b) After integrating out the parameters.
variance, it is only worth doing if the integrating out can be done quickly, otherwise we will not
be able to produce as many samples per second as the naive method. We give an example of
this below.
Example: collapsed Gibbs for ﬁtting a GMM
Consider a GMM with a fully conjugate prior. In this case we can analytically integrate out the
full conditionals as follows:
p(zi = k|z−i, x, α, β)
∝
p(zi = k|z−i, α,β)p(x|zi = k, z−i,
α, β)
∝
p(zi = k|z−i, α)p(xi|x−i, zi = k, z−i, β)
p(x−i|
zi = k, z−i, β)
∝
p(zi = k|z−i, α)p(xi|x−i, zi = k, z−i, β)
ﬁrst term can be obtained by integrating out π. Suppose we use a symmetric prior of the form
p(z1, . . . , zN|α) =
Γ(α)
Γ(N + α)
K

Γ(Nk + α/K)
Γ(α/K)Hence
p(zi = k|z−i, α)
=
p(z1:N|α)
p(z−i|α) =
×
Γ(Nk + α/K)
Γ(Nk,−i + α/K)
=
Γ(N + α)
Γ(Nk,−i + α/K)
= Nk,−i + α/K
where Nk,−i ≜
xΓ(x).
xi given all the other data and all the assignments, we use the fact that
p(xi|x−i, z−i, zi = k, β) = p(xi|D−i,k)
where D−i,k = {xj : zj = k, j ̸= i} is all the data assigned to cluster k except for xi. If we
efficiently update these predictive likelihoods by caching the sufficient statistics for each cluster.
To compute the above expression, we remove xi’s statistics from its current cluster (namely zi),
and then evaluate xi under each cluster’s posterior predictive. Once we have picked a new
cluster, we add xi’s statistics to this new cluster.
(We update the nodes in random order to improve the mixing time, as
GMMs, both the naive sampler and collapsed sampler take O(NKD) time per step.
Remove xi’s sufficient statistics from old cluster zi ;
Compute pk(xi) ≜p(xi|{xj : zj = k, j ̸= i}) ;
Compute p(zi = k|z−i, D) ∝(Nk,−i + α/K)pk(xi);
Sample zi ∼p(zi|·) ;
Add xi’s sufficient statistics to new cluster zi
vertical axis is the data log probability at each iteration, computed using
log p(D|z, θ) =
N

log [πzip(xi|θzi)]
the data and the current assignment z.
vanilla sampler. Occasionally, however, both methods can get stuck in poor local modes. (NoteIteration
log p(x | π, θ)
 
 
Standard Gibbs Sampler
Rao−Blackwellized Sampler
(a)
Iteration
log p(x | π, θ)
 
 
Standard Gibbs Sampler
Rao−Blackwellized Sampler
(b)
Used with kind permission of Erik
Sudderth.
SES
Math Score
(a)
Sample Size
Slope
(b)
SES
Math Score
(c)
schools. The extreme slopes tend to correspond to schools with smaller sample sizes. (c) Predictions from
by multilevelLinregDemo, written by Emtiyaz Khan.
to MC samples in a single run.)
Gibbs sampling for hierarchical GLMs
borrowing of statistical strength. One of the most natural way to solve such problems is to use
a way to perform approximate inference in such models using variational methods. Here we
discuss how to use Gibbs sampling.
To explain the method, consider the following example. Suppose we have data on studentsxij
yij
wj
σ2
μw
Σw
Nj
J
response variable we want to predict for student i in school j. This prediction can be based on
school and student speciﬁc covariates, xij. Since the quality of schools varies, we want to use
a separate parameter for each school. So our model becomes
yij = xT
ijwj + ϵij
We could ﬁt each wj separately, but this can give poor results if the sample size of a given
positive, but there are a few “errant” cases where the slope is negative. It turns out that the lines
Thus we may not necessarily trust these ﬁts.
We can get better results if we construct a hierarchical Bayesian model, in which the wj are
this model, the schools with small sample size borrow statistical strength from the schools with
larger sample size, because the wj’s are correlated via the latent common parents (μw, Σw). (It
wj would be conditionally independent, and there would be no information sharing between
them.)
μw
∼
N(μ0, V0)
Σw
∼
σ2
∼
Given this, it is simple to show that the full conditionals needed for Gibbs sampling have thep(wj|Dj, θ)
=
N(wj|μj, Σj)
j
=
μj
=
For the overall mean:
p(μw|w1:J, Σw)
=
N(μ|μN, ΣN)
N
=
μN
=
J

j wj. For the overall covariance:
p(Σw|μw, w1:J)
=
=

j
(wj −μw)(wj −μw)T
For the noise variance:
p(σ2|D, w1:J)
=
SSR(w1:J)
=
J

Nj

(yij −wT
The light gray lines plot the mean of the posterior predictive distribution for each school:
E [yj|xij] = xT
ij ˆwj
where
S
S

w(s)
j
The dark gray line in the middle plots the prediction using the overall mean parameters, xT
ij ˆμw.
We see that the method has regularized the ﬁts quite nicely, without enforcing too much
uniformity. (The amount of shrinkage is controlled by Σw, which in turns depends on the
BUGS and JAGS
One reason Gibbs sampling is so popular is that it is possible to design general purpose software
that will work for almost any model. This software just needs a model speciﬁcation, usually
in the form a directed graphical model (speciﬁed in a ﬁle, or created with a graphical user
interface), and a library of methods for sampling from different kinds of full conditionals. (ThisSampling”. BUGS is very widely used in biostatistics and social science. Another more recent,
This uses a similar model speciﬁcation language to BUGS.
model {
y[i,j] ~ dnorm(y.hat[i,j], tau.y)
y.hat[i,j] <- inprod(W[j, ], X[i, j, ])
}
}
W[j,] ~ dmnorm(mu, SigmaInv)
}
SigmaInv
~ dwish(S0[,], eta0)
mu ~ dmnorm(mu0, V0inv)
}
We can then just pass this model to BUGS or JAGS, which will generate samples for us. See
the webpages for details.
written code, especially for complex models.
There has been some work on automatically
still typically requires human expertise.
The Imputation Posterior (IP) algorithm
sampling in which we group the variables into two classes: hidden variables z and parameters
θ. This should sound familiar: it is basically an MCMC version of EM, where the E step gets
replaced by the I step, and the M step gets replaced the P step. This is an example of a more
general strategy called data augmentation, whereby we introduce auxiliary variables in order
Blocking Gibbs sampling
site updating). If the variables are highly correlated, it will take a long time to move away















will move very slowly through the state space. In particular, the size of the moves is controlled
by the variance of the conditional distributions. If this is ℓin the x1 direction, and the support
independent sample.
In some cases we can efficiently sample groups of variables at a time. This is called blocking
and can make much bigger moves through the state space.
Metropolis Hastings algorithm
Although Gibbs sampling is simple, it is somewhat restricted in the set of models to which it
can be applied. For example, it is not much help in computing p(w|D) for a logistic regression
model, since the corresponding graphical model has no useful Markov structure. In addition,
Gibbs sampling can be quite slow, as we mentioned above.
Fortunately, there is a more general algorithm that can be used, known as the Metropolis
Hastings or MH algorithm, which we describe below.
Basic idea
The basic idea in MH is that at each step, we propose to move from the current state x to a
new state x′ with probability q(x′|x), where q is called the proposal distribution (also called
the kernel). The user is free to use any kind of proposal they want, subject to some conditions
which we explain below. This makes MH quite a ﬂexible method. A commonly used proposal is
a symmetric Gaussian distribution centered on the current state, q(x′|x) = N(x′|x, Σ); this is
we use a proposal of the form q(x′|x) = q(x′), where the new state is independent of the old
state, we get a method known as the independence sampler, which is similar to importance
Having proposed a move to x′, we then decide whether to accept this proposal or not
according to some formula, which ensures that the fraction of time spent in each state is
proportional to p∗(x). If the proposal is accepted, the new state is x′, otherwise the new stateis the same as the current state, x (i.e., we repeat the sample).
If the proposal is symmetric, so q(x′|x) = q(x|x′), the acceptance probability is given by the
following formula:
p∗(x) )
We see that if x′ is more probable than x, we deﬁnitely move there (since p∗(x′)
x′ is less probable, we may still move there anyway, depending on the relative probabilities. So
instead of greedily moving to only more probable states, we occasionally allow “downhill” moves
of time we spend in each state x is proportional to p∗(x).
If the proposal is asymmetric, so q(x′|x) ̸= q(x|x′), we need the Hastings correction, given
by the following:
r
=
α
=
p∗(x′)q(x|x′)
p∗(x)q(x′|x) = p∗(x′)/q(x′|x)
p∗(x)/q(x|x′)
This correction is needed to compensate for the fact that the proposal distribution itself (rather
than just the target distribution) might favor certain states.
An important reason why MH is a useful algorithm is that, when evaluating α, we only need to
Z ˜p(x),
where ˜p(x) is an unnormalized distribution and Z is the normalization constant. Then
α = (˜p(x′)/Z) q(x|x′)
(˜p(x)/Z) q(x′|x)
so the Z’s cancel. Hence we can sample from p∗even if Z is unknown. In particular, all we
have to do is evaluate ˜p pointwise, where ˜p(x) = p∗(x)Z.
Gibbs sampling is a special case of MH
particular, it is equivalent to using MH with a sequence of proposals of the form
q(x′|x) = p(x′
i|x−i)I(x′
−i = x−i)
That is, we move to a new state where xi is sampled from its full conditional, but x−i is left
unchanged.
α
=
p(x′)q(x|x′)
p(x)q(x′|x) = p(x′
i|x′
−i)p(x′
−i)p(xi|x′
−i)
p(xi|x−i)p(x−i)p(x′
i|x−i)
=
p(x′
i|x−i)p(x−i)p(xi|x−i)
p(xi|x−i)p(x−i)p(x′Deﬁne x = xs;
Sample x′ ∼q(x′|x);
Compute acceptance probability
α = ˜p(x′)q(x|x′)
˜p(x)q(x′|x)
Set new sample to

x′
if u < r
xs
where we exploited the fact that x′
−i = x−i, and that q(x′|x) = p(x′
i|x−i).
many other kinds of proposals we can use, as we discuss below.
Proposal distributions
For a given target distribution p∗, a proposal distribution q is valid or admissible if it gives
Formally, we can write this as
supp(p∗) ⊆∪xsupp(q(·|x))
state space, and hence is a valid proposal for any continuous state space.
Of course, in practice, it is important that the proposal spread its probability mass in just the
1D Gaussians using a random walk proposal, q(x′|x) = N(x′|x, v). This is a somewhat tricky
target distribution, since it consists of two well separated modes. It is very important to set the
variance of the proposal v correctly: If the variance is too low, the chain will only explore one
will be rejected, and the chain will be very sticky, i.e., it will stay in the same state for a long
explore the support of the target distribution. We discuss how to tune the proposal below.
One big advantage of Gibbs sampling is that one does not need to choose the proposalSamples
Iterations
(a)
Samples
Iterations
(b)
Samples
Iterations
(c)
An example of the Metropolis Hastings algorithm for sampling from a mixture of two 1D
leads to a good approximation of the true distribution (shown in red). Figure generated by mcmcGmmDemo.
Based on code by Christophe Andrieu and Nando de Freitas.
is obviously not exploring the posterior. So having a high acceptance is not the ultimate goal.
We can increase the amount of exploration by increasing the variance of the Gaussian kernel.
initial runs, used to tune the proposal, are called pilot runs.w0
w1
(a)
w0 intercept
(b)
w1 slope
(c)
(a) Joint posterior of the parameters for 1d logistic regression when applied to some SAT data.
(b) Marginal for the offset w0. (c) Marginal for the slope w1. We see that the marginals do not capture the
fact that the parameters are highly correlated. Figure generated by logregSatMhDemo.
Gaussian proposals
If we have a continuous state space, the Hessian H at a local mode ˆw can be used to deﬁne
the covariance of a Gaussian proposal distribution. This approach has the advantage that the
Hessian models the local curvature and length scales of each dimension; this approach therefore
+ Hl, so
the asymptotically optimal Gaussian proposal has the form
q(w′|w) = N

D

+ XT DX
details.
logistic regression model ﬁt to some SAT data. We initialize the chain at the mode, computed
using IRLS, and then use the above random walk Metropolis sampler.
If you cannot afford to compute the mode or its Hessian XDX, an alternative approach,
q(w′|w) = N

w,

π2 XT X
Mixture proposals
If one doesn’t know what kind of proposal to use, one can try a mixture proposal, which is a
convex combination of base proposals:
q(x′|x) =
K

wkqk(x′|x)
where wk are the mixing weights. As long as each qk is individually valid, the overall proposal
will also be valid.
The most efficient proposals depend not just on the previous hidden state, but also the visible
and then train a discriminative classiﬁer to predict p(x|f(D)), where f(D) are some features
extracted from the visible data.
person in a visual object detector), so it is hard to predict the entire state vector, p(x|f(D)).
such as the location of just the face of a person. We can then use a proposal of the form
q(x′|x, D) = π0q0(x′|x) +

k
πkqk(x′
k|fk(D))
component of the state space. For added efficiency, the discriminative proposals should suggest
joint changes to multiple variables, but this is often hard to do.
The overall procedure is a form of generate and test: the discriminative proposals q(x′|x)
generate new hypotheses, which are then “tested” by computing the posterior ratio p(x′|D)
p(x|D) , to
see if the new hypothesis is better or worse. By adding an annealing step, one can modify
the algorithm to ﬁnd posterior modes; this is called simulated annealing, and is described in
not need to ensure the proposal distribution is reversible.
Adaptive MCMC
One can change the parameters of the proposal as the algorithm is running to increase efficiency.
This is called adaptive MCMC. This allows one to start with a broad covariance (say), allowing
large moves through the space until a mode is found, followed by a narrowing of the covariance
to ensure careful exploration of the region around the mode.
However, one must be careful not to violate the Markov property; thus the parameters of the
proposal should not depend on the entire history of the chain. It turns out that a sufficient
condition to ensure this is that the adaption is “faded out” gradually over time. See e.g., (AndrieuInitialization and mode hopping
deterministic constraints, ﬁnding such a legal conﬁguration may be a hard problem in itself. It
is therefore common to initialize MCMC methods at a local mode, found using an optimizer.
tion time to perform multiple restarts of an optimizer, and to average over these modes, rather
than exploring similar points around a local mode. However, in continuous state spaces, the
each mode, in order to visit enough posterior probability mass.
Why MH works *
To prove that the MH procedure generates samples from p∗, we have to use a bit of Markov
The MH algorithm deﬁnes a Markov chain with the following transition matrix:
p(x′|x) =
 q(x′|x)r(x′|x)
if x′ ̸= x
q(x|x) + 
otherwise
This follows from a case analysis: if you move to x′ from x, you must have proposed it (with
probability q(x′|x)) and it must have been accepted (with probability r(x′|x)); otherwise you
stay in state x, either because that is what you proposed (with probability q(x|x)), or because
you proposed something else (with probability q(x′|x)) but it was rejected (with probability
balance if
p(x′|x)p∗(x) = p(x|x′)p∗(x′)
We also showed that if a chain satisﬁes detailed balance, then p∗is its stationary distribution.
Our goal is to show that the MH algorithm deﬁnes a transition function that satisﬁes detailed
is an invariant distribution wrt the Markov transition kernel q.)
ergodic and irreducible, then p∗is its unique limiting distribution.
Proof. Consider two states x and x′. Either
p∗(x)q(x′|x) < p∗(x′)q(x|x′)
or
p∗(x)q(x′|x) > p∗(x′)q(x|x′)
We will ignore ties (which occur with probability zero for continuous distributions). Without loss
of generality, assume that p∗(x)q(x′|x) > p∗(x′)q(x|x′). Hence
α(x′|x) = p∗(x′)q(x|x′)Now to move from x to x′ we must ﬁrst propose x′ and then accept it. Hence
p(x′|x) = q(x′|x)r(x′|x) = q(x′|x)p∗(x′)q(x|x′)
p∗(x)q(x′|x) = p∗(x′)
p∗(x) q(x|x′)
Hence
p∗(x)p(x′|x) = p∗(x′)q(x|x′)
The backwards probability is
p(x|x′) = q(x|x′)r(x|x′) = q(x|x′)
p∗(x)p(x′|x) = p∗(x′)p(x|x′)
irreducible.
Suppose we have a set of models with different numbers of parameters, e.g., mixture models in
which the number of mixture components is unknown. Let the model be denoted by m, and
sample all the parameters from the product space M
more parsimonious to sample in the union space X = ∪M
about parameters for the currently active model.
sionality. The trouble is that when we compute the MH acceptance ratio, we are comparing
densities deﬁned in different dimensionality spaces, which is meaningless. It is like trying to
jump MCMC or RJMCMC, is to augment the low dimensional space with extra random variables
so that the two spaces have a common measure.
Unfortunately, we do not have space to go into details here. Suffice it to say that the method
can be made to work in theory, although it is a bit tricky in practice. If, however, the continuous
parameters can be integrated out (resulting in a method called collapsed RJMCMC), much of the
difficulty goes away, since we are just left with a discrete state space, where there is no need
models. They sample basis functions from a ﬁxed set of candidates (e.g., centered on the data
points), and integrate out the other parameters analytically. This provides a Bayesian alternative
to using RVMs or SVMs.(a)
(b)
Speed and accuracy of MCMC
In this section, we discuss a number of important theoretical and practical issues to do with
MCMC.
chain has “forgotten” where it started from will the samples be coming from the chain’s stationary
distribution. Samples collected before the chain has reached its stationary distribution do not
come from p∗, and are usually thrown away. The initial period, whose samples will be ignored,
Suppose we
It is difficult to diagnose when the chain has burned in, an issue we discuss in more detail
below. (This is one of the fundamental weaknesses of MCMC.) As an interesting example of what
This suggests that the model likes(a)
(b)
of Erik Sudderth.
prior for image segmentation. Indeed, this was suggested in the original Gibbs sampling paper
However, it turns out that if you run the chain long enough, you get isolated speckles, as
convergence can lead to erroneous conclusions.
Mixing rates of Markov chains *
The amount of time it takes for a Markov chain to converge to the stationary distribution, and
forget its initial state, is called the mixing time. More formally, we say that the mixing time
after t steps. The mixing time of the chain is deﬁned as
τϵ ≜max
x0 τϵ(x0)
The mixing time is determined by the eigengap γ = λ1 −λ2, which is the difference of thex2
x4
x7
x5
x6
x1
x3
A Markov chain with low conductance. The dotted arcs represent transitions with very low
probability.
Used with kind permission of Daphne
Koller.
ﬁrst and second eigenvalues of the transition matrix. In particular, one can show that
γ log n
ϵ )
where n is the number of states. Since computing the transition matrix can be hard to do,
estimate the mixing time.
An alternative approach is to examine the geometry of the state space. For example, consider
is connected via a narrow “bottleneck”. (If they were completely disconnected, the chain would
not be ergodic, and there would no longer be a unique stationary distribution.) We deﬁne the
conductance φ of a chain as the minimum probability, over all subsets of states, of transitioning
from that set to its complement:
φ ≜
min

p∗(S)
,
One can show that
φ2 log n
ϵ )
Hence chains with low conductance have high mixing time. For example, distributions with
well in such cases, and more advanced algorithms, such as parallel tempering, are necessary
Practical convergence diagnostics
Computing the mixing time of a chain is in general quite difficult, since the transition matrix
is usually very hard to compute. In practice various heuristics have been proposed to diagnoseis, the method may claim the chain has converged when in fact it has not. This is a ﬂaw common
to all convergence diagnostics, since diagnosing convergence is computationally intractable in
One of the simplest approaches to assessing when the method has converged is to run
multiple chains from very different overdispersed starting points, and to plot the samples of
some variables of interest. This is called a trace plot. If the chain has mixed, it should have
“forgotten” where it started from, so the trace plots should converge to the same distribution,
and thus overlap with each other.
We show the traceplot for x which was sampled from a
mixture of two 1D Gaussians using four different methods: MH with a symmetric Gaussian
where it started. The results for the other methods indicate that the chains rapidly converge to
proposal is very evident. This reduces the computational efficiency, as we discuss below, but
not the statistical validity.)
Estimated potential scale reduction (EPSR)
We can assess convergence more quantitatively as follows. The basic idea is to compare the
variance of a quantity within each chain to its variance across chains. More precisely, suppose
S
S

C
C

y·c
B ≜
S
C

C
C


S


We can now construct two estimates of the variance of y. The ﬁrst estimate is W: this should
underestimate var [y] if the chains have not ranged over the full posterior. The second estimate
is
S
S B
This is an estimate of var [y] that is unbiased under stationarity, but is an overestimate if the
From this, we can deﬁne the
following convergence diagnostic statistic, known as the estimated potential scale reduction
or EPSR:
ˆR ≜
ˆV
W(a)
(b)
(c)
(d)
Traceplots for MCMC samplers. Each color represents the samples from a different starting
sampling. Figure generated by mcmcGmmDemo.
∞limit.
untrustworthy.
Accuracy of MCMC(a)
(b)
(c)
gibbs
(d)
to estimate the mean of f(X), for some function f, where X ∼p(). Denote the true mean by
f ∗≜E [f(X)]
A Monte Carlo estimate is given by
S
S

fswhere fs ≜f(xs) and xs ∼p(x). An MCMC estimate of the variance of this estimate is given
by
VarMCMC[f]
=
E

=
E
⎡
⎣
S
S

(fs −f ∗)
⎦
=
S2 E
 S


S2

s̸=t
E [(fs −f ∗)(ft −f ∗)]
=
S2

s̸=t
E [(fs −f ∗)(ft −f ∗)]
where the ﬁrst term is the Monte Carlo estimate of the variance if the samples weren’t correlated,
and the second term depends on the correlation of the samples. We can measure this as follows.
ρt ≜
S−t
S−t
S
samplers for the Gaussian mixture model. We see that the ACF of the Gibbs sampler (bottom
is “worth” more than each MH sample.
A simple method to reduce the autocorrelation is to use thinning, in which we keep every
n’th sample. This does not increase the efficiency of the underlying sampler, but it does save
space, since it avoids storing highly correlated samples.
We can estimate the information content of a set of samples by computing the effective
sample size (ESS) Seff, deﬁned by
Seff ≜
VarMC(f)
VarMCMC(f)
that of the other samplers (in this example).
How many chains?
A natural question to ask is: how many chains should we run? We could either run one long
chain to ensure convergence, and then collect samples spaced far apart, or we could run many
short chains, but that wastes the burnin time.
In practice it is common to run a medium
after discarding the ﬁrst half of the samples. If we initialize at a local mode, we may be able toModel
Goal
Method
Reference
Probit
MAP
Gradient
Probit
MAP
EM
Probit
Post
EP
Probit
Post
Probit
Post
Gibbs with ARS
Probit
Post
MH using IRLS proposal
Logit
MAP
Gradient
Logit
Post
Logit
Post
Summary of some possible algorithms for estimation and inference for binary classiﬁcation
problems using Gaussian priors. Abbreviations: Aux. = auxiliary variable sampling, ARS = adaptive rejection
reweighted least squares, KS = Kolmogorov Smirnov, MAP = maximum a posteriori, MH = Metropolis
Hastings, Post = posterior.
Auxiliary variable MCMC *
Sometimes we can dramatically improve the efficiency of sampling by introducing dummy
auxiliary variables, in order to reduce correlation between the original variables. If the original
variables are denoted by x, and the auxiliary variables by z, we require that 
z p(x, z) = p(x),
and that p(x, z) is easier to sample from than just p(x). If we meet these two conditions,
we can sample in the enlarged model, and then throw away the sampled z values, thereby
recovering samples from p(x). We give some examples below.
Auxiliary variable sampling for logistic regression
this had the form
zi
≜
wT xi + ϵi
ϵi
∼
=
p(w|D) is Gaussian and p(zi|xi, yi, w) is truncated Gaussian, both of which are easy to sample
from.
Now let us discuss how to derive an auxiliary variable Gibbs sampler for logistic regression.
pLogistic(ϵ) =
e−ϵ=

 ∞
−wT xi

 wT xi
−∞
as required.
We can derive an auxiliary variable Gibbs sampler by sampling from p(z|w, D) and p(w|z, D).
Unfortunately, sampling directly from p(w|z, D) is not possible. One approach is to deﬁne
A simpler approach is to approximate the logistic distribution by the Student distribution
inference. In particular, we write
λi
∼
ϵi
∼
i )
zi
≜
wT xi + ϵi
=
the ν parameter. There is no convenient conjugate prior, but we can consider a ﬁnite range of
possible values and evaluate the posterior as follows:
p(ν|λ) ∝p(ν)
N

i
Furthermore, if we deﬁne V0 = v0I, we can sample v0 as well. For example, suppose we use
D
j).
This can be interleaved with the other Gibbs sampling steps, and provides an appealing Bayesian
alternative to cross validation for setting the strength of the regularizer.
of these methods can also be extended to the multinomial logistic regression case. For details,
Slice sampling
Consider sampling from a univariate, but multimodal, distribution ˜p(x). We can sometimes
improve the ability to make large moves by adding an auxiliary variable u. We deﬁne the joint
distribution as follows:
ˆp(x, u) =

otherwisex
x
x
u
(i)
f(x  )
(i)
(a)
(b)
(a) Illustration of the principle behind slice sampling.
Given a previous sample xi, we
Freitas. (b) Slice sampling in action. Figure generated by sliceSamplingDemo1d.
Intercept
Slope
Posterior density
(a)
(b)
Binomial regression for 1d data. (a) Grid approximation to posterior. (b) Slice sampling
approximation. Figure generated by sliceSamplingDemo2d.
where Zp =

˜p(x)dx. The marginal distribution over x is given by


ˆp(x, u)du =

 ˜p(x)
Zp
du = ˜p(x)
Zp
= p(x)
so we can sample from p(x) by sampling from ˆp(x, u) and then ignoring u. The full conditionals
have the form
p(u|x)
=
p(x|u)
=
UA(x)
where A = {x : ˜p(x) ≥u} is the set of points on or above the chosen height u.
This
corresponds to a slice through the distribution, hence the term slice sampling (Neal 2003a).
In practice, it can be difficult to identify the set A. So we can use the following approach:test to see if each end point lies within the slice. If it does, we keep extending in that direction
until it lies outside the slice. This is called stepping out. A candidate value x′ is then chosen
shrink the region such that x′ forms one end and such that the region still contains xs. Then
another sample is drawn. We continue in this way until a sample is accepted.
To apply the method to multivariate distributions, we can sample one extra auxiliary variable
for each dimension.
The advantage of slice sampling over Gibbs is that it does not need
The advantage of slice
does require a speciﬁcation of the width of the stepping out interval).
illustrates its behavior on a slightly harder problem, namely binomial logistic regression. The
model has the form
yi ∼Bin(ni, logit(β1 + β2xi))
is infeasible.
Swendsen Wang
Consider an Ising model of the following form:
Z

e
fe(xe)

eJ
e−J
e−J
eJ
	
, where J is the edge strength. Gibbs sampling in such models can be slow when
J is large in absolute value, because neighboring states can be highly correlated. The Swendsen
Suppose we introduce auxiliary binary variables, one per edge.
These are called bond
variables, and will be denoted by z. We then deﬁne an extended model p(x, z) of the form
Z′

e
ge(xe, ze)

e−J
e−J
e−J
e−J
	
,

eJ −e−J
eJ −e−J
	Illustration of the Swendsen Wang algorithm on a 2d grid. Used with kind permission of
Kevin Tang.
and hence that 
z p(x, z) = p(x). So if we can sample from this extended model, we can just
throw away the z samples and get valid x samples from the original distribution.
Fortunately, it is easy to apply Gibbs sampling to this extended model. The full conditional
p(z|x) factorizes over the edges, since the bond variables are conditionally independent given
the node variables. Furthermore, the full conditional p(ze|xe) is simple to compute: if the
could be turned on (because their corresponding nodes are in the same state) are represented
represented by solid edges.
To sample p(x|z), we proceed as follows. Find the connected components deﬁned by the
graph induced by the bonds that are turned on. (Note that a connected component may consist
of a singleton node.) Pick one of these components uniformly at random. All the nodes in eachdenotes the selected connected component, and we choose to force all nodes within in to enter
the white state.
The validity of this algorithm is left as an exercise, as is the extension to handle local evidence
It should be intuitively clear that Swendsen Wang makes much larger moves through the state
space than Gibbs sampling. In fact, SW mixes much faster than Gibbs sampling on 2d lattice
For large T, the nodes are roughly independent, so both methods work equally well. However,
as T approaches a critical temperature Tc, the typical states of the system have very long
correlation lengths, and Gibbs sampling takes a very long time to generate independent samples.
As the temperature continues to drop, the typical states are either all on or all off. The frequency
with which Gibbs sampling moves between these two modes is exponentiall small. By contrast,
SW mixes rapidly at all temperatures.
there are exponentially many modes, even at low temperature. SW does not work very well in
this setting, since it tries to force many neighboring variables to have the same state. In fact,
In this section, we brieﬂy mention a way to perform MCMC sampling for continuous state
case in neural network models, for example.
The basic idea is to think of the parameters as a particle in space, and to create auxiliary
The two main parameters that the user must specify are how many leapfrog steps to take
way to set them automatically). This method can be combined with stochastic gradient descent
Annealing methods
Many distributions are multimodal and hence hard to sample from. However, by analogy to the
way metals are heated up and then cooled down in order to make the molecules align, we can
imagine using a computational temperature parameter to smooth out a distribution, gradually
cooling it to recover the original “bumpy” distribution. We ﬁrst explain this idea in more detail
in the context of an algorithm for MAP estimation. We then discuss extensions to the sampling
case.x
y
(a)
x
y
(b)
Simulated annealing
Hastings algorithm for generating samples from a probability distribution, which we discussed
The method is inspired by statistical physics. The key quantity is the Boltzmann distribution,
which speciﬁes that the probability of being in any particular state x is given by
p(x) ∝exp(−f(x)/T)
where f(x) is the “energy” of the system and T is the computational temperature. As the
its minimum energy (most probable) state.
local optima). As the temperature cools, the largest peaks become larger, and the smallest peaks
disappear. By cooling slowly enough, it is possible to “track” the largest peak, and thus ﬁnd the
global optimum. This is an example of a continuation method.
We can generate an algorithm from this as follows. At each step, sample a new state according
kinds of local moves must be deﬁned.
Having proposed a new state, we compute
α = exp ((f(x) −f(x′))/T)
more probable), we will deﬁnitely accept it, but it it has higher energy (is less probable), we mighttemperature vs iteration
(a)
energy vs iteration
(b)
iteration. (b) Energy vs iteration. Figure generated by saDemoPeaks.
x
y
(a)
x
y
(b)
The rate at which the temperature changes over time is called the cooling schedule.
It
provably ﬁnd the global optimum.
However, it is not clear what “sufficient slowly” means.
In practice it is common to use an exponential cooling schedule of the following form:
can get stuck in a local maximum, but cooling too slowly just wastes time. The best cooling
schedule is difficult to determine; this is one of the main drawbacks of simulated annealing.
using a random walk proposal.
We see that the method stochastically reduces the energy
distribution over time.
We see that most of the samples are concentrated near the global
maximum. When the algorithm has converged, we just return the largest value found.Annealed importance sampling
bines ideas from simulated annealing and importance sampling in order to draw independent
samples from difficult (e.g., multimodal) distributions.
Suppose we want to sample from p0(x) ∝f0(x), but we cannot do so easily; for example,
this might represent a multimodal posterior. Suppose however that there is an easier distribution
which we can sample from, call it pn(x) ∝fn(x); for example, this might be the prior. We
can now construct a sequence of intermediate distributions than move slowly from pn to p0 as
follows:
scheme used by simulated annealing which has the form fj(x) = f0(x)βj; this makes it hard
to sample from pn.) Furthermore, suppose we have a series of Markov chains Tj(x, x′) (from x
to x′) which leave each pj invariant. Given this, we can sample x from p0 by ﬁrst sampling a
sample z0 ∼T1(z1, ·). Finally we set x = z0 and give it weight
f2(z1)
f0(z0)
f1(z0)
This can be shown to be correct by viewing the algorithm as a form of importance sampling
space:
where ˜Tj is the reversal of Tj:
˜Tj(z, z′) = Tj(z′, z)pj(z′)/pj(z) = Tj(z′, z)fj(z′)/fj(z)
It is clear that 
sequences to recover the original ditribution.
Now consider the proposal distribution deﬁned by the algorithm:
Parallel tempering
Another way to combine MCMC and annealing is to run multiple chains in parallel at different
temperatures, and allow one chain to sample from another chain at a neighboring temperature.
In this way, the high temperature chain can make long distance moves through the state space,
and have this inﬂuence lower temperature chains. This is known as parallel tempering. SeeApproximating the marginal likelihood
The marginal likelihood p(D|M) is a key quantity for Bayesian model selection, and is given by
p(D|M) =


Unfortunately, this integral is often intractable to compute, for example if we have non conjugate
In this section, we brieﬂy discuss some ways to
approximate this expression using Monte Carlo.
extensive review.
The candidate method
There is a simple method for approximating the marginal likelihood known as the Candidate
p(D|M) = p(D|θ, M)p(θ|M)
p(θ|D, M)
the denominator as well. This posterior is often approximated using MCMC.
The ﬂaw with this method is that it relies on the assumption that p(θ|D, M) has marginalized
over all the modes of the posterior, which in practice is rarely possible. Consequently the method
Harmonic mean estimate
of MCMC, as follows:
S
S

p(D|θs)
each sample. The theoretical correctness of this expression follows from the following identity:




p(D|θ)
p(D|θ)p(θ)
p(D)
p(D)


p(D)
Unfortunately, in practice this method works very poorly. Indeed, Radford Neal called this “the
drawn from the posterior. But the posterior is often very insensitive to the prior, whereas the
marginal likelihood is not. We only mention this method in order to warn against its use. We
present a better method below.Annealed importance sampling
functions. Notice that Z0 =

f0(x)dx =

f(z)dz, and Zn =

fn(x)dx =

g(z)dz. Hence
Z0
Zn
=

f(z)dz

g(z)dz =
 f(z)
g(z)g(z)dz

g(z)dz
= Eq
f(z)
g(z)

S
S

ws
If fn is a prior and f0 is the posterior, we can estimate Zn = p(D) using the above equation,
provided the prior has a known normalization constant Z0. This is generally considered the
method of choice for evaluating difficult partition functions.
Exercises
als p(x1|x2) and p(x2|x1). Implement the algorithm and plot the 1d marginals p(x1) and p(x2) as
histograms. Superimpose a plot of the exact marginals.
where the structure of the graph simpliﬁes once we have assigned values to some of the nodes.) Hence,
Modify the code in gibbsDemoIsing to draw samples from a Potts prior at different temperatures, as in
μ
∼
N(μ0, γ2
∼
σ2
∼We can set η = (μ0, γ0, η0, τ0, ν0, σ0) to uninformative values. Given this model speciﬁcation, show that
=
=
=
,
)
p(σ2|θ1:D, D)
=
D


D


Nj


Hint: we canClustering
Introduction
Clustering is the process of grouping similar objects together. There are two kinds of inputs we
based clustering has the advantage that it is applicable to “raw”, potentially noisy data. We will
see examples of both below.
ing, also called partitional clustering, where we partition the objects into disjoint sets; and
hierarchical clustering, where we create a nested tree of partitions. We will discuss both of
these below. Not surprisingly, ﬂat clusterings are usually faster to create (O(ND) for ﬂat vs
most hierarchical clustering algorithms are deterministic and do not require the speciﬁcation of
K, the number of clusters, whereas most ﬂat clustering algorithms are sensitive to the initial
conditions and require some model selection method for K. (We will discuss how to choose K
in more detail below.)
The ﬁnal distinction we will make in this chapter is whether the method is based on a
know about them; second, they often contain good ideas, which can be used to speed up
inference in a probabilistic models.
Measuring (dis)similarity
between objects i and j. Subjectively judged dissimilarities are seldom distances in the strict
require D to be a true distance matrix, but many do not. If we have a similarity matrix S, we
can convert it to a dissimilarity matrix by applying any monotonically decreasing function, e.g.,
D = max(S) −S.
The most common way to deﬁne dissimilarity between objects is in terms of the dissimilarityof their attributes:
Δ(xi, xi′) =
D

Δj(xij, xi′j)
Some common attribute dissimilarity functions are as follows:
•
Squared (Euclidean) distance:
•
Squared distance strongly emphasizes large differences (because differences are squared). A
more robust alternative is to use an ℓ1 distance:
Δj(xij, xi′j) = |xij −xi′j|
This is also called city block distance, since, in 2D, the distance can be computed by
counting how many rows and columns we have to move horizontally and vertically to get
from xi to xi′.
•
j xijxi′j,
and hence 
So clustering based on correlation
(similarity) is equivalent to clustering based on squared distance (dissimilarity).
•
For ordinal variables, such as {low, medium, high}, it is standard to encode the values as
any dissimilarity function for quantitative variables, such as squared distance.
•
features gives
Δ(xi, xi) =
D

I(xij ̸= xi′j)
This is called the hamming distance.
Evaluating the output of clustering methods *
The validation of clustering structures is the most difficult and frustrating part of cluster
analysis. Without a strong effort in this direction, cluster analysis will remain a black art
accessible only to those true believers who have experience and great courage. — Jain$$$
$$%
$%%
%%&
$$
&&&
Clustering is an unupervised learning technique, so it is hard to evaluate the quality of the output
of any given method. If we use probabilistic models, we can always evaluate the likelihood of
a test set, but this has two drawbacks: ﬁrst, it does not directly assess any clustering that is
we discuss some performance measures not based on likelihood.
Intuitively, the goal of clustering is to assign points that are similar to the same cluster,
and to ensure that points that are dissimilar are in different clusters. There are several ways
However, these internal criteria may be of limited use. An alternative is to rely on some external
form of data with which to validate the method. For example, suppose we have labels for each
can induce a set of labels and vice versa.) Then we can compare the clustering with the labels
using various metrics which we describe below. We will use some of these metrics later, when
we compare clustering methods.
Purity
Let Nij be the number of objects in cluster i that belong to class j, and let Ni = C
over class labels for cluster i. We deﬁne the purity of a cluster as pi ≜maxj pij, and the
overall purity of a clustering as
purity ≜

i
Ni
N pi
of clusters.
Rand index
Let U = {u1, . . . , uR} and V = {v1, . . . , VC} be two different partitions of the N data points,
i.e., two different (ﬂat) clusterings. For example, U might be the estimated clustering and Vcontaining the following numbers: TP is the number of pairs that are in the same cluster in
both U and V (true positives); TN is the number of pairs that are in the different clusters in
both U and V (true negatives); FN is the number of pairs that are in the different clusters in
U but the same cluster in V (false negatives); and FP is the number of pairs that are in the
same cluster in U but different clusters in V (false positives). A common summary statistic is
the Rand index:
R ≜
TP + TN
TP + FP + FN + TN
of “positives” (i.e., pairs of objects put in the same cluster, regardless of label) is
TP + FP =

	
+

	
+

	
Of these, the number of true positives is given by
TP =

	
+

	
+

	
+

	

	
pairs labeled C and

	
pairs
AR ≜
index −expected index
max index −expected index
i.e., the two partitions are picked at random subject to having the original number of classes
and objects in each, and then the expected value of TP + TN is computed. This model can
be used to compute the statistical signiﬁcance of the Rand index.
The Rand index weights false positives and false negatives equally. Various other summary
One can compute their frequentist sampling distribution, and hence their statistical signiﬁcance,
using methods such as bootstrap.
Mutual information
Another way to measure cluster quality is to compute the mutual information between U and
N
be the probability that
a randomly chosen object belongs to cluster ui in U and vj in V . Also, let pU(i) = |ui|/N
be the be the probability that a randomly chosen object belongs to cluster ui in U; deﬁnepV (j) = |vj|/N similarly. Then we have
I(U, V ) =
R

C

pUV (i, j) log pUV (i, j)
pU(i)pV (j)
Unfortunately, the maximum value can be
achieved by using lots of small clusters, which have low entropy.
To compensate for this,
we can use the normalized mutual information,
NMI(U, V ) ≜
I(U, V )
Dirichlet process mixture models
The simplest approach to (ﬂat) clustering is to use a ﬁnite mixture model, as we discussed in
to just using some heuristic algorithm.
The principle problem with ﬁnite mixture models is how to choose the number of components
choose K at all.
In this section, we discuss inﬁnite mixture models, in which we do not impose any a priori
(DP). This allows the number of clusters to grow as the amount of data increases. It will also
prove useful later when we discuss hiearchical clustering.
brief review of the DP and its application to mixture modeling, based on the presentation in
From ﬁnite to inﬁnite mixture models
follows:
p(xi|zi = k, θ)
=
p(xi|θk)
p(zi = k|π)
=
πk
p(π|α)
=
Dir(π|(α/K)1K)
The form of p(θk|λ) is chosen to be conjugate to p(xi|θk). We can write p(xi|θk) as xi ∼
is the prior.(a)
(b)
Two different representations of a ﬁnite mixture model. Left: traditional representation.
Right: representation where parameters are samples from G, a discrete measure. The picture on the right
height of the spikes reﬂects the mixing weights πk. This weighted sum of delta functions is G. We then
x1 and x2, from N(θ1, σ2) and N(θ2, σ2).
permission of Erik Sudderth.
parameter used to generate observation xi; these parameters are sampled from distribution G,
which has the form
G(θ) =
K

where π ∼Dir( α
prior probability for that cluster.
If we sample from this model, we will always (with probability one) get exactly K clusters,
with data points scattered around the cluster centers. We would like a more ﬂexible model,
that can generate a variable number of clusters. Furthermore, the more data we generate, the
more likely we should be to see a new cluster. The way to do this is to replace the discrete
distribution G with a random probability measure. Below we will show that the Dirichlet
process, denoted G ∼DP(α, H), is one way to do this.
with more clusters as the amount of data increases. The resulting samples look much more like
real data than samples from a ﬁnite mixture model.
Of course, working with an “inﬁnite” model sounds scary. Fortunately, as we show below,
even though this model is potentially inﬁnite, we can perform inference using an amount of
computation that is not only tractable, but is often much less than that required to ﬁt a set(a)
(b)
(c)
(d)
(e)
(f)
Some samples from a Dirichlet process mixture model of 2D Gaussians, with concentration
different run. We also show the model parameters as ellipses, which are sampled from a vague NIW base
of ﬁnite mixture models for different K. The intuitive reason is that we can get evidence that
certain values of K are appropriate (have high posterior support) long before we have been able
to estimate the parameters, so we can focus our computational efforts on models of appropriate
complexity. Thus going to the inﬁnite limit can sometimes be faster. This is especially true
when we have multiple model selection problems to solve.(a)
(b)
(c)
regions.
Used with kind permission of Erik Sudderth.
The Dirichlet process
thermore, the GP is consistently deﬁned, so that p(f(x1)) can be derived from p(f(x1), f(x2)),
etc.

(G(T1), . . . , G(TK)) has a joint Dirichlet distribution
Dir(αH(T1), . . . , αH(TK))
for any ﬁnite partition (T1, . . . , TK) of Θ. If this is the case, we write G ∼DP(α, H), where
distribution over all the cells, p(G(T1), . . . , G(TK)), is Dirichlet, so the marginals in each cell
are beta distributed:
Beta(αH(Ti), α

j̸=i
H(Tj))
The DP is consistently deﬁned in the sense that if T1 and T2 form a partition of ˜T1, then
G(T1) + G(T2) and G( ˜T1) both follow the same beta distribution.
Recall that if π ∼Dir(α), and z|π ∼Cat(π), then we can integrate out π to get the
constraint; we say that the DP is a neutral process. Other stochastic processes can be deﬁned that do not have this
property, but they are not so computationally convenient.β1
π1
π2
π3
π4
π5
β2
β3
β4
β5
(a)
(b)
Illustration of the stick breaking construction. (a) We have a unit length stick, which we
break at a random point β1; the length of the piece we keep is called π1; we then recursively break off
pieces of the remaining stick, to generate π2, π3, . . ..
Used with
where α0 = 
given one observation is given by
The DP generalizes this to arbitrary partitions. If G ∼DP(α, H), then p(θ ∈Ti) = H(Ti) and
the posterior is
posterior is given by
G|θ1, . . . , θN, α, H ∼DP

α + N,
α + N

αH +
N

δθi



Thus we see that the DP effectively deﬁnes a conjugate prior for arbitrary measurable spaces.
The concentration parameter α is like the effective sample size of the base measure H.
Stick breaking construction of the DP
Our discussion so far has been very abstract. We now give a constructive deﬁnition for the DP,
Let π = {πk}∞
process:
βk
∼
πk
=
βk


πl)This is often denoted by
π ∼GEM(α)
α. Furthermore, the size of the πk components decreases on average.
Now deﬁne
G(θ) =
∞

As a consequence of this construction, we see that samples from a DP are discrete with
probability one. In other words, if you keep sampling it, you will get more and more repetitions
values, represented by ellipses with thick borders. This is our ﬁrst indication that the DP might
be useful for clustering.
The Chinese restaurant process (CRP)
Working with inﬁnite dimensional sticks is problematic. However, we can exploit the clustering
property to draw samples form a GP, as we now show.
α + N

αH(θ) +
K



It is much more convenient to work with discrete variables zi which specify which value of
α + N

αI(z = k∗) +
K

NkI(z = k)


where k∗represents a new cluster index that has not yet been used. This is called the Chinese
restaurant process or CRP, based on the seemingly inﬁnite supply of tables at certain Chinese
restaurants. The analogy is as follows: The tables are like clusters, and the customers are like
observations. When a person enters the restaurant, he may choose to join an existing table with
probability proportional to the number of people already sitting at this table (the Nk); otherwise,(a)
(b)
Two views of a DP mixture model. Left: inﬁnite number of clusters parameters, θk, and
Used with kind permission of Erik Sudderth.
he may choose to sit at a new table k∗. The result is a distribution over partitions of the
integers, which is like a distribution of customers to tables.
The fact that currently occupied tables are more likely to get new customers is sometimes
bution of cluster sizes induced by this prior process; it is basically a power law. The number
complexity will indeed grow logarithmically with dataset size. More ﬂexible priors over cluster
Applying Dirichlet processes to mixture modeling
The DP is not particularly useful as a model for data directly, since data vectors rarely repeat
exactly.
However, it is useful as a prior for the parameters of a stochastic data generating
mechanism, such as a mixture model. To create such a model, we follow exactly the same setup
π
∼
GEM(α)
zi
∼
π
θk
∼
H(λ)
xi
∼
F(θzi)
thus xi will be generated close to an existing datapoint.Fitting a DP mixture model
p(zi = k|z−i, x, α, λ)
∝
p(zi = k|z−i, α)p(xi|x−i, zi = k, z−i, λ)
By exchangeability, we can assume that zi is the last customer to enter the restaurant. Hence
the ﬁrst term is given by
p(zi|z−i, α) =

αI(zi = k∗) +
K

Nk,−iI(zi = k)


where K is the number of clusters used by z−i, and k∗is a new cluster. Another way to write
this is as follows:
p(zi = k|z−i, α)
=
Nk,−i
if k has been seen before
α
if k is a new cluster
To compute the second term, p(xi|x−i, zi = k, z−i, λ), let us partition the data x−i into
clusters based on z−i. Let x−i,c = {xj : zj = c, j ̸= i} be the data assigned to cluster c. If
zi = k, then xi is conditionally independent of all the data points except those assigned to
cluster k. Hence we have
p(xi|x−i, z−i, zi = k, λ)
=
p(xi|x−i,k, λ) = p(xi, x−i,k|λ)
p(x−i,k|λ)
where
p(xi, x−i,k|λ)
=


p(xi|θk)
⎡
⎣

p(xj|θk)
⎤
is the marginal likelihood of all the data assigned to cluster k, including i, and p(x−i,k|λ) is an
analogous expression excluding i. Thus we see that the term p(xi|x−i, z−i, zi = k, λ) is the
posterior preditive distribution for cluster k evaluated at xi.
If zi = k∗, corresponding to a new cluster, we have
p(xi|x−i, z−i, zi = k∗, λ) = p(xi|λ) =


which is just the prior predictive distribution for a new cluster evaluated at xi.
similar to collapsed Gibbs for ﬁnite mixtures except that we have to consider the case zi = k∗.
the induced posterior over K, seems reasonable. The method tends to rapidly discover a good
clustering. By contrast, Gibbs sampling (and EM) for a ﬁnite mixture model often gets stuck inRemove xi’s sufficient statistics from old cluster zi ;
Compute pk(xi) = p(xi|x−i(k));
Set Nk,−i = dim(x−i(k)) ;
Compute p(zi = k|z−i, D) =
Nk,−i
Compute p∗(xi) = p(xi|λ);
Compute p(zi = ∗|z−i, D) =
α
Normalize p(zi|·);
Sample zi ∼p(zi|·) ;
Add xi’s sufficient statistics to new cluster zi ;
If any cluster is empty, remove it and decrease K;
poor local optima (not shown). This is because the DPMM is able to create extra redundant
time, the DPMM converges more rapidly than a ﬁnite mixture model.
A variety of other ﬁtting methods have been proposed. (Daume 2007a) shows how one can use
A star search and beam search to quickly ﬁnd an approximate MAP estimate. (Mansinghka et al.
version of beam search. This can be more efficient than Gibbs sampling, particularly for large
For the DP, the value of α
does not have much impact on predictive accuracy, but it does affect the number of clusters.
One approach is to put a Ga(a, b) prior for α, and then to from its posterior, p(α|K, N, a, b),
Similarly, for the base distribution, we can either sample the
Affinity propagation
Mixture models, whether ﬁnite or inﬁnite, require access to the raw N × D data matrix, and
need to specify a generative model of the data. An alternative approach takes as input an N ×N
similarity matrix, and then tries to identify examplars, which will act as cluster centers. The
minima. Here we describe an alternative approach called affinity propagation (Frey and Dueck
The idea is that each data point must choose another data point as its exemplar or centroid;
some data points will choose themselves as centroids, and this will automatically determine the(a)
(b)
(c)
(d)
Whye Teh.
The goal is to maximize the following function
S(c) =
N

s(i, ci) +
N

δk(c)
The ﬁrst term measures the similarity of each point to its centroid. The second term is a penalty
term that is −∞if some data point i has chosen k as its exemplar (i.e., ci = k), but k has not
chosen itself as an exemplar (i.e., we do not have ck = k). More formally,
δk(c) =
 −∞
otherwise
The objective function can be represented as a factor graph. We can either use N nodes,Iteration
log p(x | π, θ)
 
 
Dirichlet Process Mixture
Finite Mixture
(a)
Iteration
log p(x | π, θ)
 
 
Dirichlet Process Mixture
Finite Mixture
(b)
Comparison of collapsed Gibbs samplers for a DP mixture (dark blue) and a ﬁnite mixture
starting values.
Used with kind permission of Erik Sudderth.
c1
c2
A
cN
c3
N
s(N, )
…
k
…
s(i, )
…
…
ci
Factor graphs for affinity propagation. Circles are variables, squares are factors. Each ci node
Frey.
i thinks k would make a good exemplar, compared to all the other exemplars i has looked at.
reduced to a scalar message, ai←k, known as the availability. This is a measure of how strongly
k believes it should an exemplar for i, based on all the other data points k has looked at.
As usual with loopy BP, the method might oscillate, and convergence is not guaranteed.


























































































































































































































































































































































































































































































































































































































































































































































































































































	

	


	


	


	


	


	


	








 !



 !
Example of affinity propagation. Each point is colored coded by how much it wants to be
an exemplar (red is the most, green is the least). This can be computed by summing up all the incoming
of Brendan Frey.
However, by using damping, the method is very reliable in practice. If the graph is densely
The number of clusters can be controlled by scaling the diagonal terms S(i, i), which reﬂect
2d data, where the negative Euclidean distance was used to measured similarity. The S(i, i)
Spectral clustering
An alternative view of clustering is in terms of graph cuts. The idea is we create a weighted
undirected graph W from the similarity matrix S, typically by using the nearest neighbors of
each point; this ensures the graph is sparse, which speeds computation. If we want to ﬁnd a
partition into K clusters, say A1, . . . , AK, one natural criterion is to minimize
K

W(Ak, Ak)where Ak = V \Ak is the complement of Ak, and W(A, B) ≜
problem is easy to solve. Unfortunately the optimal solution often just partitions off a single
data point from the rest. To ensure the sets are reasonably large, we can deﬁne the normalized
cut to be
K

cut(Ak, Ak)
vol(Ak)
where vol(A) ≜
the graph into K clusters such that nodes within each cluster are similar to each other, but are
different to nodes in other clusters.
result turns into an eigenvector problem known as spectral clustering (see e.g., (Shi and Malik
Going into the details would take us too far aﬁeld, but below we give a very brief summary,
Graph Laplacian
diaogonal matrix containing the weighted degree of each node. We deﬁne the graph Laplacian
as follows:
L ≜D −W
This matrix has various important properties.
Because each row sums to zero, we have
f T Lf
=
f T Df −f T Wf =

i
i −

i,j
fifjwij
=
⎛
⎝
i

i,j
fifjwij +

j
j
⎞

i,j
following result.
1A1, . . . , 1AK, where Ak are the K connected components of the graph.Hence f is constant for all vertices which are connected by a path in the graph. Now suppose
will have K indicator functions, which “select out” the connected components.
This suggests the following algorithm.
Compute the ﬁrst K eigenvectors uk of L.
Let
to them to recover the connected components. Now assign point i to cluster k iff row i of Y
was assigned to cluster k.
In reality, we do not expect a graph derived from a real similarity matrix to have isolated
connected components — that would be too easy. But it is reasonable to suppose the graph is
a small “perturbation” from such an ideal. In this case, one can use results from perturbation
theory to show that the eigenvectors of the perturbed Laplacian will be close to these ideal
largest eigenvectors of W; these are equivalent to the smallest eigenvectors of I −W. This is
similar to the above method, which computes the smallest eigenvectors of L = D −W. See
KPCA.
Normalized graph Laplacian
In practice, it is important to normalize the graph Laplacian, to account for the fact that some
nodes are more highly connected than others. There are two comon ways to do this. One
row sums to one:
Lrw ≜D−1L = I −D−1W
The eigenvalues and eigenvectors of L and Lrw are closely related to each other (see (von
again spanned by the indicator vectors 1Ak. This suggests the following algorithm: ﬁnd the
the smallest K eigenvectors of Lsym, create U, normalize each row to unit norm by creating
"
(
k u2
There is an interesting connection between Ncuts and random walks on a graph (Meilak−means clustering
x
y
(a)
spectral clustering
x
y
(b)
can be interpreted as the probability of going from i to j.
If the graph is connected and
Ncut(A, A) = p(A|A) + p(A|A)
This means that we are looking for a cut such that a random walk rarely makes transitions from
A to A or vice versa.
Example
job of clustering, since it implicitly assumes each cluster corresponds to a spherical Gaussian.
Next we try spectral clustering. We deﬁne a similarity matrix using the Gaussian kernel. We
compute the ﬁrst two eigenvectors of the Laplacian. From this we can infer the clustering in
Since the method is based on ﬁnding the smallest K eigenvectors of a sparse matrix, it takes
Hierarchical clustering
Mixture models, whether ﬁnite or inﬁnite, produce a “ﬂat” clustering. Often we want to learn a
hierarchical clustering, where clusters can be nested inside each other.(a)
(b)
(a)
Hierarchical Clustering of Profiles
(b)
Hierarchical clustering applied to the yeast gene expression data. (a) The rows are permuted
according to a hierarchical clustering scheme (average link agglomerative clustering), in order to bring
Figure generated by hclustYeastDemo.
details below.
Note that agglomerative and divisive clustering are both just heuristics, which do not optimize
produce in any formal sense. Furthermore, they will always produce a clustering of the input
data, even if the data has no structure at all (e.g., it is random noise). Later in this section we
will discuss a probabilistic version of hierarchical clustering that solves both these problems.Mark j and k as unavailable: S ←S \ {j, k};
Update dissimilarity matrix d(i, ℓ);
(a)
(b)
(c)
Illustration of (a) Single linkage. (b) Complete linkage. (c) Average linkage.
Agglomerative clustering
Agglomerative clustering starts with N groups, each initially containing one object, and then at
each step it merges the two most similar groups until there is a single group, containing all the
takes O(KND) time, and then apply hierarchical clustering to the estimated cluster centers.
The merging process can be represented by a binary tree, called a dendrogram, as shown
and every time two groups are merged, we join them in the tree. The height of the branches
represents the dissimilarity between the groups that are being joined. The root of the tree (which
is at the top) represents a group containing all the data. If we cut the tree at any given height,
There are actually three variants of agglomerative clustering, depending on how we deﬁne
the dissimilarity between groups of objects. These can give quite different results, as shown insingle link
(a)
complete link
(b)
average link
(c)
Hierarchical clustering of yeast gene expression data. (a) Single linkage. (b) Complete linkage.
(c) Average linkage. Figure generated by hclustYeastDemo.Single link
In single link clustering, also called nearest neighbor clustering, the distance between two
groups G and H is deﬁned as the distance between the two closest members of each group:
dSL(G, H) =
min
The tree built using single link clustering is a minimum spanning tree of the data, which
is a tree that connects all the objects in a way that minimizes the sum of the edge weights
(distances). To see this, note that when we merge two clusters, we connect together the two
closest members of the clusters; this adds an edge between the corresponding nodes, and this
is guaranteed to be the “lightest weight” edge joining these two clusters. And once two clusters
have been merged, they will never be considered again, so we cannot create cycles.
As a
Complete link
In complete link clustering, also called furthest neighbor clustering, the distance between
two groups is deﬁned as the distance between the two most distant pairs:
dCL(G, H) =
max
Single linkage only requires that a single pair of objects be close for the two groups to
be considered close together, regardless of the similarity of the other members of the group.
Thus clusters can be formed that violate the compactness property, which says that all the
observations within a group should be similar to each other. In particular if we deﬁne the
we can see that single linkage can produce clusters with large diameters. Complete linkage
represents the opposite extreme: two groups are considered close only if all of the observations
in their union are relatively similar. This will tend to produce clusterings with small diameter,
i.e., compact clusters.
Average link
In practice, the preferred method is average link clustering, which measures the average
distance between all pairs:
davg(G, H) =
nGnH


i′∈H
di,i′
Average link clustering represents a compromise between single and complete link clustering.
It tends to produce relatively compact clusters that are relatively far apart. However, since itinvolves averaging of the di,i′’s, any change to the measurement scale can change the result. In
contrast, single linkage and complete linkage are invariant to monotonic transformations of di,i′,
since they leave the relative ordering the same.
Divisive clustering
Divisive clustering starts with all the data in a single cluster, and then recursively divides each
are used. One approach is pick the cluster with the largest diameter, and split it in two using the
Another method is to build a minimum spanning tree from the dissimilarity graph, and then
to make new clusters by breaking the link corresponding to the largest dissimilarity.
(This
actually gives the same results as single link agglomerative clustering.)
follows.
We then
dG
nG

i′∈G
di,i′
We remove the most dissimilar object and put it in its own cluster H:
i∗= arg max
i , G = G \ {i∗}, H = {i∗}
We now continue to move objects from G to H until some stopping criterion is met. Speciﬁcally,
we pick a point i∗to move that maximizes the average dissimilarity to each i′ ∈G but minimizes
the average dissimilarity to each i′ ∈H:
dH
i =
nH

i′∈H
di,i′, i∗= arg max
i −dH
i
We continue to do this until dG
i −dH
i
is negative. The ﬁnal result is that we have split G into
on any other node in the tree. For example, we might choose to split the node G whose average
dissimilarity is highest, or whose maximum dissimilarity (i.e., diameter) is highest. We continue
all clusters are singletons.
Divisive clustering is less popular than agglomerative clustering, but it has two advantages.
First, it can be faster, since if we only split for a constant number of levels, it takes just O(N)
time. Second, the splitting decisions are made in the context of seeing all the data, whereasChoosing the number of clusters
It is difficult to choose the “right” number of clusters, since a hierarchical clustering algorithm
will always create a hierarchy, even if the data is completely random. But, as with choosing K
dendrogram (represent the dissimilarity between merged groups) between natural clusters and
we will present a Bayesian approach to hierarchical clustering that nicely solves this problem.
Bayesian hierarchical clustering
There are several ways to make probabilistic models which produce results similar to hierarchical
and takes comparable time, whereas several of the other techniques referenced above are much
slower. However, it uses Bayesian hypothesis tests to decide which clusters to merge (if any),
hypothesis tests are closely related to the calculations required to do inference in a Dirichlet
process mixture model, as we will see. Furthermore, the input to the model is a data matrix,
not a dissimilarity matrix.
The algorithm
Let D = {x1, . . . , xN} represent all the data, and let Di be the set of datapoints at the leaves
of the substree Ti. At each step, we compare two trees Ti and Tj to see if they should be
The probability of a merge is given by
rij
≜
p(Dij|Tij)
p(Dij|Tij)
=
assumed to come from the same model, and hence

 ⎡
⎣
p(xn|θ)
⎤
pseudocode, assuming p(Mij) is uniform. When ﬁnished, we can cut the tree at points wherefor each pair of clusters i, j do
Compute p(Dij|Tij)
Find the pair Di and Dj with highest merge probability rij;
Delete Di, Dj ;
The connection with Dirichlet process mixture models
In this section, we will establish the connection between BHC and DPMMs. This will in turn
p(Dk)
=

p(v)p(Dv)
p(v)
=
αmv mv
l )
Γ(α)
p(Dv)
=
mv

p(Dv
l )
where V is the set of all possible partitions of Dk, p(v) is the probability of partition v, mv is
the number of clusters in partition v, nv
l is the number of points in cluster l of partition v, Dv
l
are the points in cluster l of partition v, and nk are the number of points in Dk.
is similar to p(Dk) given above, except for the fact that it only sums over partitions which are
of data points for balanced binary trees, but this is obviously a subset of all possible partitions.)
In this way, we can use the BHC algorithm to compute a lower bound on the marginal likelihood
of the data from a DPMM. Furthermore, we can interpret the algorithm as greedily searching
given size at each step.
j. This is equal to the probability of cluster Dk coming from the DPMM, relative to all other
partitions of Dk consistent with the current tree. This can be computed as follows: initialize
dk = αΓ(nk) + didj, and πk = αΓ(nk)
dk
, where i and j are k’s left and right children.Data Set
Single Linkage
Complete Linkage
Average Linkage
BHC
Synthetic
Newsgroups
Spambase
Digits
Fglass
Purity scores for various hierarchical clustering schemes applied to various data sets. The
Used with kind permission of Katherine Heller.
gradients of the form ∂p(Dk|Tk)
∂λ
through the tree, and thus perform an empirical Bayes estimate
Experimental results
see that BHC did much better than the other methods on all datasets except the forensic glass
one.
on the top few nodes of these two trees. BHC splits off clusters concerning sports from clusters
concerning cars and space. AHC keeps sports and cars merged together. Although sports and
cars both fall under the same “rec” newsgroup heading (as opposed to space, that comes under
the “sci” newsgroup heading), the BHC clustering still seems more reasonable, and this is borne
out by the quantitative purity scores.
Clustering datapoints and features
So far, we have been concentrating on clustering datapoints.
But each datapoint is often
described by multiple features, and we might be interested in clustering them as well. Below we
describe some methods for doing this.(a)
(b)
is rec.sport.baseball, green is rec.sport.hockey, and magenta is sci.space). Top: average linkage hierarchical
clustering. Bottom: Bayesian hierarchical clustering. Each of the leaves is labeled with a color, according
to which newsgroup that document came from. We see that the Bayesian method results in a clustering
that is more consistent with these labels (which were not used during model ﬁtting).
Used with kind permission of Katherine Heller.All Data
Game
Team 
Play
Car
Space 
NASA
Baseball
  Pitch 
    Hit
NHL
Hockey 
Round
Car
Dealer 
Drive
Space
NASA 
Orbit
(a)
All Data
Car
Baseball 
Engine
Pitcher
Boston 
Ball
Car
Player 
Space
Quebec
Jet 
Boston
Vehicle
Dealer 
Driver
Team
Game 
Hockey
(b)
Biclustering
Clustering the rows and columns is known as biclustering or coclustering. This is widely used
in bioinformatics, where the rows often represent genes and the columns represent conditions.
It can also be used for collaborative ﬁltering, where the rows represent users and the columns
represent movies.
A variety of ad hoc methods for biclustering have been proposed; see (Madeira and Oliveira
assume the data are iid across samples and across features within each block:
p(x|r, c, θ) =

i

j
p(xij|ri, cj, θ) = p(xij|θri,cj)
number of clusters for the rows and columns, we can use a Dirchlet process, as in the inﬁnite
Gibbs sampling.
horses, etc. The features represent properties of the habitat (jungle, tree, coastal), or anatomical
properties (has teeth, quadrapedal), or behavioral properties (swims, eats meat), etc. The model,
clusters. For example, it discovered a bicluster that represents the fact that mammals tend to
have aquatic features.
The problem with biclustering is that each object (row) can only belong to one cluster. Intuitively,
an object can have multiple roles, and can be assigned to different clusters depending on whichO1
killer whale, blue whale, humpback, seal, walrus, dolphin
O2
antelope, horse, giraffe, zebra, deer
O3
monkey, gorilla, chimp
O4
hippo, elephant, rhino
O5
grizzly bear, polar bear
F1
ﬂippers, strain teeth, swims, arctic, coastal, ocean, water
F2
hooves, long neck, horns
F3
hands, bipedal, jungle, tree
F4
bulbous body shape, slow, inactive
F5
meat teeth, eats meat, hunter, ﬁerce
F6
walks, quadrapedal, ground
F1
O1
O2
O3
O4
O5
clusters. The original data matrix is shown, partitioned according to the discovered clusters. From Figure
(a)
riv
xij
β
cj
θjk
α
γ
(b)
block are assumed to generated iid; however, each column can have a different distributional form, which
permission of Jennifer Dy. (b) Corresponding DGM.
subset of features you use. For example, in the animal dataset, we may want to group the
animals on the basis of anatomical features (e.g., mammals are warm blooded, reptiles are not),
or on the basis of behavioral features (e.g., predators vs prey).
We now present a model that can capture this phenomenon.features. We will use a Dirichlet process prior for p(c), which allows V to grow automatically.
Then for each partition of the columns (i.e., each view), call it v, we partition the rows, again
the i’th row belongs in view v. Finally, having partitioned the rows and columns, we generate
the data: we assume all the rows and columns within a block are iid. We can deﬁne the model
more precisely as follows:
p(c, r, D)
=
p(c)p(r|c)p(D|r, c)
p(c)
=
DP(c|α)
p(r|c)
=
V (c)

DP(rv|β)
p(D|r, c, θ)
=
V (c)


⎡
⎣
K(rv)




⎤
⎦
p(D|r, c, γ)
=
V (c)


K(rv)

Beta(nj,k,v + γ, nj,k,v + γ)
Beta(γ, γ)
where nj,k,v = 
column for view v and for row cluster k. Similarly, nj,k,v counts how many features are off.
informative way, but results are more sensitive to the other two parameters, since α controls
the number of column partitions, and β controls the number of row partitions. Hence a more
features. The ﬁgures shows the (approximate) MAP partition. The ﬁrst partition of the columns
This
partition of the columns contains features that are treated as noise, with no apparent structure
(except for the single row labeled “frog”). The third partition of the columns contains ecological
features like “dangerous”, “carnivorous”, “lives in water”, etc. This divides the animals into prey,
land predators, sea predators and air predators. Thus each animal (row) can belong to a different
between the cardinality of v and cj. In other words, the number of row partitions we need to specify (the number of
views, indexed by v) depends on the number of column partitions (clusters) that we have.lives in lakes
is an amphibian
is a rodent
is tall
is a fish
is slimy
has horns
has hooves
is a feline
roars
has fins
has webbed feet
eats nuts
is smooth
lives in trees
is large
lives in cold climates
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
is ferocious
is dangerous
is a carnivore
is a predator
lives in water
flies
is long
eats leaves
eats animals
lives in grass
eats fish
lives in hot climates
Leopard
Alligator
Python
Seal
Dolphin
Frog
Jellyfish
Octopus
Penguin
Finch
Seagull
Owl
Eagle
Dragonfly
Bat
Grasshopper
Ant
Bee
Sheep
Monkey
Iguana
Ostrich
has bones
lays eggs
is warm−blooded
is a mammal
squawks
has a beak
has a tongue
is green
has a spinal cord
is a lizard
has antennae
has flippers
has paws
has a large brain
has a tail
is furry
eats mice
eats rodents
has a snout
is brown
makes loud noises
has teeth
has feet
is smart
travels in groups
Leopard
Sheep
Seal
Dolphin
Monkey
Bat
Alligator
Iguana
Frog
Python
Finch
Ostrich
Seagull
Owl
Penguin
Eagle
Grasshopper
Ant
Bee
Jellyfish
Octopus
Dragonfly
A
B
C
Frog
MAP estimate produced by the crosscat system when applied to a binary data matrix of
animals (rows) by features (columns). See text for details.
with kind permission of Vikash Mansingkha.
cluster depending on what set of features are considered. Uncertainty about the partitions can
be handled by sampling.
It is interesting to compare this model to a standard inﬁnite mixture model.
While the
and cluster the rows only using the relevant features. Note, however, that it does not need a
separate “background” model, since everything is modelled using the same mechanism. This is
useful, since one’s person’s noise is another person’s signal. (Indeed, this symmetry may explainGraphical model structure learning
Introduction
tions between variables. In this chapter, we discuss how to learn the structure of the graphical
model itself. That is, we want to compute p(G|D), where G is the graph structure, represented
as an V × V adjacency matrix.
edge discovery and density estimation. The former just requires a graph topology, whereas the
latter requires a fully speciﬁed model.
The main obstacle in structure learning is that the number of possible graphs is exponential in
is prohibitively large: even if we could afford to compute it, we could not even store it. So we
will seek appropriate summaries of the posterior. These summary statistics depend on our task.
If our goal is knowledge discovery, we may want to compute posterior edge marginals,
represents our conﬁdence in its presence. By setting a threshold, we can generate a sparse
crete optimization methods such as heuristic search. However, in the case of trees, we can
ﬁnd the globally optimal graph structure quite efficiently using exact methods, as we discuss in
If density estimation is our only goal, it is worth considering whether it would be more
appropriate to learn a latent variable model, which can capture correlation between the visible
easier to learn and, perhaps more importantly, they can be applied (for prediction purposes)
much more efficiently, since they do not require performing inference in a learned graph with
potentially high treewidth. The downside with such models is that the latent factors are often
unidentiﬁable, and hence hard to interpret. Of course, we can combine graphical model structure
learning and latent variable learning, as we will show later in this chapter.
In some cases, we don’t just want to model the observed correlation between variables;
instead, we want to model the causal structure behind the data, so we can predict the effects
of manipulating variables. This is a much more challenging task, which we brieﬂy discuss inbible
christian
god
jesus
case
fact
children
government
religion
computer
science
university
course
earth
orbit
evidence
human
world
jews
law
president
rights
state
war
gun
israel
launch
nasa
shuttle
space
lunar
moon
mission
solar
clarity, the graph has been cropped, so we only show a subset of the nodes and edges. Figure generated
by relevanceNetworkNewsgroupDemo.
Structure learning for knowledge discovery
for learning graph structures which can be used to visualize one’s data. The resulting models do
not constitute consistent joint probability distributions, so they cannot be used for prediction,
and they cannot even be formally evaluated in terms of goodness of ﬁt. Nevertheless, these
methods are a useful ad hoc tool to have in one’s data visualization toolbox, in view of their
speed and simplicity.
Relevance networks
A relevance network is a way of visualizing the pairwise mutual information between multiple
random variables: we simply choose a threshold and draw an edge from node i to node j if
ij), where
visualize the interaction between genes. The trouble with biological examples is that they are
However, relevance networks suffer from a major problem: the graphs are usually very dense,
since most variables are dependent on most other variables, even after thresholding the MIs.
For example, suppose X1 directly inﬂuences X2 which directly inﬂuences X3 (e.g., these formaids
health
baseball
fans
games
hit
league
players
bible
god
jesus
orbit
bmw
engine
honda
cancer
disease
patients
car
dealer
drive
driver
gun
insurance
oil
card
graphics
video
case
children
food
christian
mission
religion
science
computer
course
data
solar
problem
evidence
medicine
disk
files
memory
scsi
space
display
image
server
doctor
help
dos
windows
format
earth
mars
moon
email
phone
power
msg
fact
hockey
team
ftp
human
vitamin
water
number
version
nhl
puck
season
win
won
government
jews
president
rights
war
law
system
israel
research
state
launch
shuttle
lunar
mac
pc
studies
nasa
satellite
program
question
university
software
technology
world
connected.
A better approach is to use graphical models, which represent conditional independence,
rather than dependence. In the above example, X1 is conditionally independent of X3 given
than relevance networks, and hence are a more useful way of visualizing interactions between
multiple variables.
Dependency networks
A simple and efficient way to learn a graphical model structure is to independently ﬁt D sparse
can then visualize the resulting sparse graph. The advantage over relevance networks is that
redundant variables will not be selected as inputs.
We can use any kind of sparse regression or classiﬁcation method to ﬁt each CPD. (Heckermancan recover the true graph structure, assuming the data was generated from a sparse Gaussian
graphical model.
of the words present in these estimated Markov blankets represent fairly natural associations
mated statistical dependencies seem less intuitive, such as baseball:windows and bmw:christian.
We can gain more insight if we look not only at the sparsity pattern, but also the values of the
•
•
•
•
•
Words in italic red have negative weights, which represents a dissociative relationship.
For
example, the model reﬂects that baseball:windows is an unlikely combination. It turns out that
In addition to visualizing the data, a dependency network can be used for inference. However,
the only algorithm we can use is Gibbs sampling, where we repeatedly sample the nodes with
missing values from their full conditionals. Unfortunately, a product of full conditionals does
not, in general, constitute a representation of any valid joint distribution (Heckerman et al.
sometimes give reasonable results if there is not much missing data, and it is a useful method
an initialization technique for more complex structure learning methods that we discuss below.
Learning tree structures
For the rest of this chapter, we focus on learning fully speciﬁed joint probability models, which
can be used for density estimation, prediction and knowledge discovery.
start by considering the special case of trees. Trees are special because we can learn their
structure efficiently, as we disuscs below, and because, once we have learned the tree, we can(a)
(b)
(c)
An undirected tree and two equivalent directed trees.
Directed or undirected tree?
Before continuing, we need to discuss the issue of whether we should use directed or undirected
trees. A directed tree, with a single root node r, deﬁnes a joint distribution as follows:
p(x|T) =

p(xt|xpa(t))
p(x1, x2, x3, x4|T)
=
p(x1)p(x2|x1)p(x3|x2)p(x4|x2)
=
p(x2)p(x1|x2)p(x3|x2)p(x4|x2)
We see that the choice of root does not matter: both of these models are equivalent.
To make the model more symmetric, it is preferable to use an undirected tree. This can be
represented as follows:
p(x|T) =

p(xt)

(s,t)∈E
p(xs, xt)
p(xs)p(xt)
we have
p(x1, x2, x3, x4|T) = p(x1)p(x2)p(x3)p(x4)
p(x1, x2)p(x2, x3)p(x2, x4)
p(x1)p(x2)p(x2)p(x3)p(x2)p(x4)
To see the equivalence with the directed representation, let us cancel terms to get
p(x1, x2, x3, x4|T)
=
p(x1, x2)p(x2, x3)
p(x2)
p(x2, x4)
p(x2)
=
p(x1)p(x2|x1)p(x3|x2)p(x4|x2)
=
p(x2)p(x1|x2)p(x3|x2)p(x4|x2)
where p(xt|xs) = p(xs, xt)/p(xs).
Thus a tree can be represented as either an undirected or directed graph: the number of
parameters is the same, and hence the complexity of learning is the same. And of course,
inference is the same in both representations, too.
The undirected representation, which is
symmetric, is useful for structure learning, but the directed representation is more convenient
for parameter learning.log p(D|θ, T)
=

t

k
Ntk log p(xt = k|θ)
+

s,t

j,k
Nstjk log
p(xs = j, xt = k|θ)
p(xs = j|θ)p(xt = k|θ)
where Nstjk is the number of times node s is in state j and node t is in state k, and Ntk is
the number of times node t is in state k. We can rewrite these counts in terms of the empirical
MLEs, this becomes
log p(D|θ, T)
N
=


k
pemp(xt = k) log pemp(xt = k)
+

(s,t)∈E(T )
I(xs, xt|ˆθst)
distribution:
I(xs, xt|ˆθst) =

j

k
pemp(xs = j, xt = k) log
pemp(xs = j, xt = k)
pemp(xs = j)pemp(xt = k)
learning structure.
Thus the tree topology that maximizes the likelihood can be found by
computing the maximum weight spanning tree, where the edge weights are the pairwise mutual
There are several algorithms for ﬁnding a max spanning tree (MST). The two best known are
Prim’s algorithm and Kruskal’s algorithm. Both can be implemented to run in O(E log V ) time,
ﬁrst term is the cost of computing the sufficient statistics.
The connections that are learned seem intuitively reasonable.
Finding the MAP forest
Since all trees have the same number of parameters, we can safely used the maximum likelihood
score as a model selection criterion without worrying about overﬁtting. However, sometimes we
may want to ﬁt a forest rather than a single tree, since inference in a forest is much faster than
in a tree (we can run belief propagation in each tree in the forest in parallel). The MLE criterion
will never choose to omit an edge. However, if we use the marginal likelihood or a penalized
likelihood (such as BIC), the optimal solution may be a forest. Below we give the details for the
marginal likelihood case.aids
baseball
hit
bible
bmw
cancer
car
dealer
engine
honda
card
graphics
video
case
children
christian
computer
course
data
disease
disk
drive
memory
system
display
server
doctor
dos
scsi
driver
earth
god
email
ftp
phone
oil
evidence
fact
question
fans
files
format
windows
food
msg
water
image
games
jesus
religion
government
power
president
rights
state
war
gun
health
insurance
medicine
help
hockey
nhl
human
israel
jews
launch
law
league
lunar
mac
mars
patients
studies
mission
moon
nasa
number
orbit
satellite
solar
vitamin
pc
software
players
problem
program
space
puck
research
science
season
shuttle
technology
university
team
version
world
win
won
permission of Mark Schmidt. (A topologically equivalent tree can be produced using chowliuTreeDemo.)
Dirichlet prior for the CPTs. The resulting expression can be written as follows:
log p(D|T) =

log


N


t
score(Nt,pa(t))
where Nt,pa(t) are the counts (sufficient statistics) for node t and its parents, and score is
information). Then we can rewrite the objective as follows:
log p(D|T) =

t
score(t|pa(t)) =

t
wpa(t),t +

t
The last term is the same for all trees T, so we can ignore it. Thus ﬁnding the most probable
tree amounts to ﬁnding a maximal branching in the corresponding weighted directed graph.and hence the weight matrix is symmetric. In this case, the maximal branching is the same
as the maximal weight forest. We can apply a slightly modiﬁed version of the MST algorithm
and negative edge weights. Now let G′ be a graph obtained by omitting all the negative edges
from G. This cannot reduce the total weight, so we can ﬁnd the maximum weight forest of G
by ﬁnding the MST for each connected component of G′. We can do this by running Kruskal’s
algorithm directly on G′: there is no need to ﬁnd the connected components explicitly.
Mixtures of trees
A single tree is rather limited in its expressive power. Later in this chapter we discuss ways to
learn more general graphs. However, the resulting graphs can be expensive to do inference in.
mixture component may have a different tree topology. This is like an unsupervised version of
E step, we compute the responsibilities of each cluster for each data point, and in the M step,
In fact, it is possible to create an “inﬁnite mixture of trees”, by integrating out over all possible
perform exact Bayesian inference of posterior edge marginals etc. However, it is not tractable to
Learning DAG structures
In this section, we discuss how to compute (functions of) p(G|D), where G is constrained to be
a DAG. This is often called Bayesian network structure learning. In this section, we assume
there is no missing data, and that there are no hidden variables. This is called the complete
data assumption. For simplicity, we will focus on the case where all the variables are categorical
For notational simplicity, we will often assume Kt = K, so all nodes have the same number of
Markov equivalence
In this section, we discuss some fundamental limits to our ability to learn DAG structures from
data.X1
X2
X3
X4
X5
X1
X2
X3
X4
X5
X1
X2
X3
X4
X5
G1
G2
G3
Three DAGs. G1 and G3 are Markov equivalent, G2 is not.
represent the same set of CI statements, namely
X ⊥Z|Y, X ̸⊥Z
We say these graphs are Markov equivalent, since they encode the same set of CI assumptions.
That is, they all belong to the same Markov equivalence class.
assumptions.
One can prove the following theorem.
We can represent a Markov equivalence class using a single partially directed acyclic graph
(PDAG), also called an essential graph or pattern, in which some edges are directed and some
undirected. The undirected edges represent reversible edges; any combination is possible so
The signiﬁcance of the above theorem is that, when we learn the DAG structure from data,
we will not be able to uniquely identify all of the edge directions, even given an inﬁnite amount
of data. We say that we can learn DAG structure “up to Markov equivalence”. This also cautions
us not to read too much into the meaning of particular edge orientations, since we can often
change them without changing the model in any observable way.Z
Y
X
Z
Y
X
Z
Y
X
Z
Y
X
Z
Y
X
Z
Y
X
≡
≡
PDAG representation of Markov equivalent DAGs.
Exact structural inference
In this section, we discuss how to compute the exact posterior over graphs, p(G|D), ignoring
for now the issue of computational tractability.
Deriving the likelihood
Assuming there is no missing data, and that all CPDs are tabular, the likelihood can be written
as follows:
p(D|G, θ)
=
N

V
Cat(xit|xi,pa(t), θt)
=
N

V
Ct

Cat(xit|θtc)I(xi,pa(t)=c)
=
N

V
Ct

Kt

θ
tck
=
V
Ct

Kt

θNtck
tck
where Ntck is the number of times node t is in state k and its parents are in state c. (Technically
these counts depend on the graph structure G, but we drop this from the notation.)
Deriving the marginal likelihood
Of course, choosing the graph with the maximum likelihood will always pick a fully connected
graph (subject to the acyclicity constraint), since this maximizes the number of parameters. To
avoid such overﬁtting, we will choose the graph with the maximum marginal likelihood, p(D|G);
the magic of the Bayesian Occam’s razor will then penalize overly complex graphs.
To compute the marginal likelihood, we need to specify priors on the parameters. We will
make two standard assumptions.
First, we assume global prior parameter independence,
which means
p(θ) =
V
p(θt)Second, we assume local prior parameter independence, which means
p(θt) =
Ct

p(θtc)
for each t. It turns out that these assumtions imply that the prior for each row of each CPT
p(θtc) = Dir(θtc|αtc)
marginal likelihood of any DAG as follows:
p(D|G)
=
V
Ct


 ⎡
⎣

i:xi,pa(t)=c
Cat(xit|θtc)
⎤
=
V
Ct

B(Ntc + αtc)
B(αtc)
=
V
Ct

Γ(Ntc)
Γ(Ntc + αtc)
Kt

Γ(Ntck + αG
tck)
Γ(αG
ijk)
=
V
score(Nt,pa(t))
where Ntc = 
k Ntck, αtc = 
k αtck, Nt,pa(t) is the vector of counts (sufficient statistics) for
node t and its parents, and score() is a local scoring function deﬁned by
score(Nt,pa(t)) ≜
Ct

B(Ntc + αtc)
B(αtc)
We say that the marginal likelihood decomposes or factorizes according to the graph structure.
Setting the prior
equivalence, which is sometimes considered desirable. This property says that if G1 and G2 are
the only prior that satisﬁes likelihood equivalence and parameter independence is the Dirichlet
prior, where the pseudo counts have the form
αtck = α p0(xt = k, xpa(t) = c)
tribution. This is called the BDe prior, which stands for Bayesian Dirichlet likelihood equivalent.invoked an additional assumption called parameter modularity, which says that if node Xt
has the same parents in G1 and G2, then p(θt|G1) = p(θt|G2). With this assumption, we
can always derive αt for a node t in any other graph by marginalizing the pseudo counts in
tions. In this case, we have
αtck =
α
KtCt
since p0(xt = k, xpa(t) = c) =
KtCt . Thus if we sum the pseudo counts over all Ct × Kt
entries in the CPT, we get a total equivalent sample size of α. This is called the BDeu prior,
where the “u” stands for uniform. This is the most widely used prior for learning Bayes net
Simple worked example
X1
X2
Hence the posterior probabilites, under a
Example: analysis of the college plans dataset
decision of high school students about whether to attend college. Speciﬁcally, the variables are
as follows:•
Sex Male or female
•
SES Socio economic status: low, lower middle, upper middle or high.
•
IQ Intelligence quotient: discretized into low, lower middle, upper middle or high.
•
PE Parental encouragment: low or high
•
CP College plans: yes or no.
of the probability mass, so the posterior is extremely peaked.
decision about whether to go to college, which makes sense. Also, sex inﬂuences college plans
only indirectly through parental encouragement, which also makes sense. However, the direct
link from socio economic status to IQ seems surprising; this may be due to a hidden common
variables.
The K2 algorithm
Suppose we know a total ordering of the nodes. Then we can compute the distribution over
parents for each node independently, without the risk of introducing any directed cycles: weIf we just return the best set of parents for each node, we get the the K2 algorithm (Cooper
gamma model, and thus derive a different exact expression for the marginal likelihood. See
and Gaussian nodes, as long as the discrete nodes always have discrete parents; this is called a
conditional Gaussian DAG. Again, we can compute the marginal likelihood in closed form. See
In the general case (i.e., everything except Gaussians and CPTs), we need to approximate the
marginal likelihood. The simplest approach is to use the BIC approximation, which has the form

t
log p(Dt|ˆθt) −KtCt
log N
Scaling up to larger graphs
The main challenge in computing the posterior over DAGs is that there are so many possible
the following recurrence:
f(D) =
D

D
i
	
2i(D−i)f(D −i)
generally forced to use approximate methods, some of which we review below.
Approximating the mode of the posterior
Consequently, we must settle for ﬁnding a locally optimal MAP DAG. The most common
method is greedy hill climbing: at each step, the algorithm proposes small changes to the
boring graph which most increases the posterior.
cal maximum.
It is important that the method only proposes local changes to the graph,
case, we need to approximate the marginal likelhood as we discuss below.aids
baseball
fans
league
bible
bmw
cancer
health
car
dealer
driver
engine
honda
insurance
oil
card
graphics
case
children
food
water
christian
jesus
religion
computer
science
course
data
problem
disease
medicine
patients
disk
dos
files
mac
memory
program
space
display
doctor
help
version
windows
drive
format
scsi
earth
image
mars
moon
email
phone
power
evidence
fact
msg
studies
god
ftp
vitamin
number
games
hockey
nhl
puck
season
team
win
won
government
jews
law
president
rights
war
gun
hit
human
israel
research
state
launch
lunar
pc
software
server
video
mission
orbit
solar
nasa
satellite
shuttle
players
question
university
system
technology
world
since this enables the change in marginal likelihood (and hence the posterior) to be computed
in constant time (assuming we cache the sufficient statistics).
This is because all but one
We can initialize the search from the best tree, which can be found using exact methods
We can use techniques such as multiple random restarts to increase the chance of ﬁnding a
good local maximum. We can also use more sophisticated local search methods, such as genetic
algorithms or simulated annealing, for structure learning.
Approximating other functions of the posterior
If our goal is knowledge discovery, the MAP DAG can be misleading, for reasons we discussed innodes.
An approximate method is to sample DAGs from the posterior, and then to compute the
This exploits the fact that, if a total ordering of the nodes is known, we can
select the parents for each node independently, without worrying about cycles, as discussed in
(collapsed) MCMC, but this time with a parallel tempering MCMC algorithm.
Learning DAG structure with latent variables
Sometimes the complete data assumption does not hold, either because we have missing data,
p(D|G) =

 
h

h


where h represents the hidden or missing data.
In general this is intractable to compute.
For example, consider a mixture model, where
we don’t observe the cluster label.
In this case, there are KN possible completions of the
data (assuming we have K clusters); we can evaluate the inner integral for each one of these
assignments to h, but we cannot afford to evaluate all of the integrals. (Of course, most of these
integrals will correspond to hypotheses with little posterior support, such as assigning single
data points to isolated clusters, but we don’t know ahead of time the relative weight of these
assignments.)
In this section, we discuss some ways for learning DAG structure when we have latent variables
Approximating the marginal likelihood when we have missing data
The simplest approach is to use standard structure learning methods for fully visible DAGs,
methods for approximating the marginal likelihood. However, these are usually too slow to use
inside of a search over models. Below we mention some faster deterministic approximations.
BIC approximation
A simple approximation is to use the BIC score, which is given by
BIC(G) ≜log p(D|ˆθ, G) −log N
dim(G)
where dim(G) is the number of degrees of freedom in the model and ˆθ is the MAP or ML
estimate. However, the BIC score often severely underestimates the true marginal likelihoodDenote the expected sufficient statistics of the data by D = D(ˆθ); in the case of discrete
variables, we just “ﬁll in” the hidden variables with their expectation. We then use the exact
p(D|G) ≈p(D|G) =


smaller, since it does not sum over all values of h. To correct for this, we ﬁrst write
log p(D|G) = log p(D|G) + log p(D|G) −log p(D|G)
and then we apply a BIC approximation to the last two terms:
log p(D|G) −log p(D|G)
≈

log p(D|ˆθ, G) −N

−

log p(D|ˆθ, G) −N

=
log p(D|ˆθ, G) −log p(D|ˆθ, G)
Putting it altogether we get
log p(D|G) ≈log p(D|G) + log p(D|ˆθ, G) −log p(D|ˆθ, G)
likelihood. The second term p(D|ˆθ, G), which involves an exponential sum (thus matching the
“dimensionality” of the left hand side) can be computed using an inference algorithm. The ﬁnal
Variational Bayes EM
An even more accurate approach is to use the variational Bayes EM algorithm. Recall from
p(θ, z1:N|D) ≈q(θ)q(z) = q(θ)

i
q(zi)
where zi are the hidden variables in case i. In the E step, we update the q(zi), and in the
M step, we update q(θ). The corresponding variational free energy provides a lower bound on
much better approximation to the true log marginal likelihood (as estimated by a slow annealed
importance sampling procedure) than either BIC or CS. In fact, one can prove that the variational
bound will always be more accurate than CS (which in turn is always more accurate than BIC).SES
H
SEX
PE
IQ
CP
H
PE
low
low
high
high
H
SEX
male
female
male
female
SES
low
low
high
high
SES
low
low
low
low
high
high
high
high
PE
low
high
low
high
low
high
low
high
IQ
low
low
high
high
low
low
high
high
Used with kind permission of David Heckerman.
Example: college plans revisited
Recall that if we ignore the
possibility of hidden variables there was a direct link from socio economic status to IQ in the
MAP DAG. Heckerman et al. decided to see what would happen if they introduced a hidden
variable H, which they made a parent of both SES and IQ, representing a hidden common cause.
They also considered a variant in which H points to SES, IQ and PE. For both such cases, they
likely than the second most probable model with a hidden variable. So again the posterior is
very peaked.
These results suggests that there is indeed a hidden common cause underlying both the
interpret this to mean that the hidden variable represents “parent quality” (possibly a genetic
structures in the graph, and thus without affecting the likelihood; this underscores the difficulty
in interpreting hidden variables.
Interestingly, the hidden variable model has the same conditional independence assumptions
sible to distinguish between these hypotheses by merely looking at the empirical conditional
which takes parsimony into account (and not just conditional independence), we can discoverthe possible existence of hidden factors. This is the basis of much of scientiﬁc and everday
Structural EM
One way to perform structural inference in the presence of missing data is to use a standard
estimate the marginal likelihood. However, this approach is very efficient, because the marginal
likelihood does not decompose when we have missing data, and nor do its approximations.
For example, if we use the CS approximation or the VBEM approximation, we have to perform
inference in every neighboring model, just to evaluate the quality of a single move!
structural EM algorithm. The basic idea is this: instead of ﬁtting each candidate neighboring
the score of all the neighbors. Although this might be a bad approximation to the marginal
likelihood, it can be a good enough approximation of the difference in marginal likelihoods
between different models, which is all we need in order to pick the best neighbor.
More precisely, deﬁne D(G0, ˆθ0) to be the data ﬁlled in using model G0 with MAP parameters
ˆθ0. Now deﬁne a modiﬁed BIC score as follows:
scoreBIC(G, D) ≜log p(D|ˆθ, G) −log N
dim(G) + log p(G) + log p(ˆθ|G)
where we have included the log prior for the graph and parameters. One can show (Friedman
1997b) that if we pick a graph G which increases the BIC score relative to G0 on the expected
data, it will also increase the score on the actual data, i.e.,
To convert this into an algorithm, we proceed as follows. First we initialize with some graph
practice, this means when we ask for the expected counts for any particular family, we perform
inference using our current model. (If we know which counts we will need, we can precompute
all of them, which is much faster.) We then evaluate the BIC score of all of our neighbors using
data again, and repeat. For increased speed, we may choose to only reﬁt the model every few
steps, since small changes to the structure hopefully won’t invalidate the parameter estimates
One interesting application is to learn a phylogenetic tree structure. Here the observed leaves
are the DNA or protein sequences of currently alive species, and the goal is to infer the topology
of the tree and the values of the missing internal nodes. There are many classical algorithms for
Another interesting application of this method is to learn sparse mixture models (Barash and















































	












	

	

	







"


























!
 
 
!
!
!
!

! 
"
"

for a different way to perform this task, using regular EM and a set of bits, one per feature, that
are free to change across data cases.)
Discovering hidden variables
topology by ﬁtting a series of different models and computing the one with the best marginal
likelihood. How can we automate this process?
then its children are likely to be densely connected. This suggest the following heuristic (Elidan
Unfortunately, this technique does not work too well, since structure learning algorithms are
biased against ﬁtting models with densely connected cliques.
Another useful intuition comes from clustering. In a ﬂat mixture model, also called a latent
class model, the discrete latent variable provides a compressed representation of its children.
Thus we want to create hidden variables with high mutual information with their children.
model. They propose a greedy local search algorithm to learn such structures, based on adding
or deleting hidden nodes, adding or deleting edges, etc. (Note that learning the optimal latentprogram
earth
launch
lunar
mars
mission
moon
nasa
orbit
satellite
shuttle
solar
space
technology
h1
h17
card
computer
data
disk
display
dos
drive
driver
email
files
format
ftp
graphics
help
image
mac
memory
number
pc
phone
problem
research
science
scsi
server
software
system
university
version
video
windows
h9
h10
h11
h12
h16
h18
h19
h22
h23
h24
h25
h26
case
children
course
evidence
fact
government
gun
human
israel
law
power
president
question
rights
state
war
world
h2
h3
h4
h20
baseball
fans
games
hit
hockey
league
nhl
players
puck
season
team
win
won
h5
h6
h7
aids
cancer
disease
doctor
food
health
medicine
msg
patients
studies
vitamin
water
h13
h21
bmw
car
dealer
engine
honda
insurance
oil
h15
bible
christian
jesus
religion
h8
h14
god
jews
have multiple meanings, and get connected to different latent variables, representing different “topics”. For
example, the word “win” can refer to a sports context (represented by h5) or the Microsoft Windows context
models based on agglomerative hierarchical clustering. Rather than go into details, we just give
exactly two children, for speed reasons. Nevertheless, we see interpretable clusters arising. For
that inference in latent trees is exact and takes time linear in the number of nodes.
has better predictive accuracy than other approaches, such as mixture models, or trees where
all the observed data is forced to be at the leaves. Interestingly, one can show that this method
can recover the exact latent tree structure, providing the data is generated from a tree. SeeGoogle’s rephil model.
Leaves represent presence or absence of words.
Internal nodes
with kind permission of Brian Milch.
the same. Furthermore, if the observed variables are Gaussian, the hidden variables must be
Gaussian also.
Case study: Google’s Rephil
In this section, we describe a huge DGM called Rephil, which was automatically learned from
the presence or absence of words or compounds (such as “New York City”) in a text document
means each edge can be augmented with a hidden variable specifying if the link was activated
or not; if the link is not active, then the parent cannot turn the child on. (A very similar model
Parameter learning is based on EM, where the hidden activation status of each edge needs
a more probabilistically sound version of the method, developed by Uri Lerner et al. The summary below is based on
notes by Brian Milch (who also works at Google).
semantic keywords from web pages.
These keywords play a role analogous to the words that users type in when
gives an overview.idea that “nodes that ﬁre together should wire together”. To implement this, we run inference
add an edge from parent to child if the link can signiﬁcantly increase the probability of the
child. Links that are not activated very often are pruned out. We initialize with one cluster per
“document” (corresponding to a set of semantically related phrases). We then merge clusters A
and B if A explains B’s top words and vice versa. We can also discard clusters that are used
too rarely.
Exact inference in this model is obviously infeasible. However note that most leaves will be
off, since most words do not occur in a given query; such leaves can be analytically removed, as
links from the words that are on up to their parents to get a candidate set of concepts. We
then perform iterative conditional modes to ﬁnd a good set of local maxima. At each step of
ICM, each node sets its value to its most probable state given the values of its neighbors in its
Markov blanket. This continues until it reaches a local maximum. We can repeat this process
milliseconds!
Structural equation models *
edges represent correlated Gaussian noise. Such models are also called path diagrams. SEMs
are widely used, especially in economics and social science. It is common to interpret the edge
directions in terms of causality, where directed cycles are interpreted is in terms of feedback
Gaussian, as we show below. There is nothing inherently “causal” about it at all. (We discuss
We can deﬁne an SEM as a series of full conditionals as follows:
xi = μi +

j̸=i
wijxj + ϵi
Hence the joint distribution is given by p(x) = N(μ, Σ) where
Σ = (I −W)−1Ψ(I −W)−T
in addition, Ψ is diagonal, then the model is equivalent to a Gaussian DGM, as discussed inY3
Z3
Z1
Z1
Y1
Y2
Z1 feedback loop.
due to a hidden common cause.
When using structural equation models, it is common to partition the variables into latent
following model:
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
X1
X2
X3
X4
X5
X6
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
Z1
Z2
Z3
Y1
Y2
Y3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
w13
w21
w32
w41
w52
w63
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
Z1
Z2
Z3
Y1
Y2
Y3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
+
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
ϵ1
ϵ2
ϵ3
ϵ4
ϵ5
ϵ6
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where
Ψ =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
Ψ11
Ψ22
Ψ33
Ψ44
Ψ45
Ψ54
Ψ55
Ψ66
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
terms in Ψ.
Z’s, there are the extra hidden variables implied by the presence of the bidirected edges.)
The standard practice in the SEM community, as exempliﬁed by the popular commercialbuild the structure by hand, to estimate the parameters by maximum likelihood, and then to
methods. However, one can also use Bayesian inference for the parameters (see e.g., (Dunson
Gaussian DAGs, many of the techniques we have been discussing in this section can be applied.
noise has a diagonal covariance (hence no bidirected edges). In an SEM, the covariance of the
latent Gaussian has a sparse Cholesky decomposition (at least if W is acyclic), and the observed
noise might have a full covariance matrix.
variables (possibly noisily observed), we can make some of the observations be discrete (e.g., by
using probit links), and so on.
Learning causal DAGs
Causal models are models which can predict the effects of interventions to, or manipulations
of, a system. For example, an electronic circuit diagram implicitly provides a compact encoding
of what will happen if one removes any given component, or cuts any wire. A causal medical
model might predict that if I continue to smoke, I am likely to get lung cancer (and hence if
I cease smoking, I am less likely to get lung cancer). Causal claims are inherently stronger,
yet more useful, than purely associative claims, such as “people who smoke often have lung
cancer”.
to use a DAG to do causal reasoning. Finally, we brieﬂy discuss how to learn the structure of
Causal interpretation of DAGs
so if we manipulate A, then B will change. This is known as the causal Markov assumption.
(Of course, we have not deﬁned the word “causes”, and we cannot do that by appealing to a
We will also assume that all relevant variables are included in the model, i.e., there are no
unknown confounders, reﬂecting hidden common causes. This is called the causal sufficiency
assumption. (If there are known to be confounders, they should be added to the model, although
confounders explicitly.)
Assuming we are willing to make the causal Markov and causal sufficiency assumptions, we
can use DAGs to answer causal questions. The key abstraction is that of a perfect intervention;
this represents the act of setting a variable to some known value, say setting Xi to xi. A real
world example of such a perfect intervention is a gene knockout experiment, in which a gene
is “silenced”. We need some notational convention to distinguish this from observing that XiX
X
G
happens to have value xi. We use Pearl’s do calculus notation (as in the verb “to do”) and write
do(Xi = xi) to denote the event that we set Xi to xi. A causal model can be used to make
inferences of the form p(x|do(Xi = xi)), which is different from making inferences of the form
p(x|Xi = xi).
To understand the difference between conditioning on interventions and conditioning on
probably a smoker (since nicotine causes yellow stains):
However, if I intervene and paint your ﬁngers yellow, I am no longer licensed to infer this, since
I have disrupted the normal causal mechanism. Thus
bution by a DGM, and then cut the arcs coming into any nodes that were set by intervention.
intervened on from being sent back up to their parents. Having perform this surgery, we can
then perform probabilistic inference in the resulting “mutilated” graph in the usual way to reason
about the effects of interventions. We state this formally as follows.
for sets of nodes i, j, we can perform surgical intervention on the Xj nodes and then use standard
probabilistic inference in the mutilated graph.
We can generalize the notion of a perfect intervention by adding interventions as explicit
action nodes to the graph. The result is like an inﬂuence diagram, except there are no utilityx
y
Illustration of Simpson’s paradox. Figure generated by simpsonsParadoxGraph.
can then deﬁne the CPD p(Xi|do(Xi)) to be anything we want. We can also allow an action to
affect multiple nodes. This is called a fat hand intervention, a reference to someone trying to
change a single component of some system (e.g., an electronic circuit), but accidently touching
a way to model this using augmented DAGs).
Using causal DAGs to resolve Simpson’s paradox
In this section, we assume we know the causal DAG. We can then do causal reasoning by
show how causal reasoning can help resolve a famous paradox, known as Simpon’s paradox.
Simpson’s paradox says that any statistical relationship between two variables can be reversed
by including additional factors in the analysis. For example, suppose some cause C (say, taking
a drug) makes some effect E (say getting better) more likely
P(E|C)
>
P(E|¬C)
and yet, when we condition on the gender of the patient, we ﬁnd that taking the drug makes
the effect less likely in both females (F) and males (¬F):
P(E|C, F)
<
P(E|¬C, F)
P(E|C, ¬F)
<
P(E|¬C, ¬F)
This seems impossible, but by the rules of probability, this is perfectly possible, because the
event space where we condition on (¬C, F) or (¬C, ¬F) can be completely different to the
event space when we just condition on ¬C. The table of numbers below shows a concrete
Combined
Male
Female
E
¬E
Total
Rate
E
¬E
Total
Rate
E
¬E
Total
Rate
C
¬C
TotalFrom this table of numbers, we see that
>
<
<
dots represent the data for females, and the crosses represent the data for males. Within each
subgroup, we see that the effect decreases as we increase the cause.
arises is that we are interpreting the statements causally, but we are not using proper causal
reasoning when performing our calculations. The statement that the drug C causes recovery E
is
P(E|do(C))
>
P(E|do(¬C))
whereas the data merely tell us
P(E|C)
>
P(E|¬C)
This is not a contradiction. Observing C is positive evidence for E, since more males than
females take the drug, and the male recovery rate is higher (regardless of the drug).
Thus
Nevertheless, we are left with a practical question: should we use the drug or not? It seems
like if we don’t know the patient’s gender, we should use the drug, but as soon as we discover
if they are male or female, we should stop using it. Obviously this conclusion is ridiculous.
To answer the question, we need to make our assumptions more explicit. Suppose reality can
need to adjust for (i.e., condition on) the confounding variable F. This is necessary because
each value of F separately, to make sure the relationship between C and E is not affected by
any value of F.
Suppose that for each value of F, taking the drug is harmful, that is,
p(E|do(C), F)
<
p(E|do(¬C), F)
p(E|do(C), ¬F)
<
p(E|do(¬C), ¬F)
Then we can show that taking the drug is harmful overall:
p(E|do(C)) < p(E|do(¬C))
that drugs have no effect on gender
p(F|do(C)) = p(F|do(¬C)) = p(F)
Now using the law of total probability,
p(E|do(C))
=
p(E|do(C), F)p(F|do(C)) + p(E|do(C), ¬F)p(¬F|do(C))
=
p(E|do(C), F)p(F) + p(E|do(C), ¬F)p(¬F)C
F
E
Treatment
Gender
Recovery
(a)
C
F
E
Treatment
Blood Pressure
Recovery
(b)
Two different models uses to illustrate Simpson’s paradox. (a) F is gender and is a confounder
for C and E. (b) F is blood pressure and is caused by C.
Similarly,
p(E|do(¬C))
=
p(E|do(¬C), F)p(F) + p(E|do(¬C), ¬F)p(¬F)
conclude that
p(E|do(C)) < p(E|do(¬C))
the probability of the effect.
Now consider a different version of this example. Suppose we keep the data the same but
this case, we can no longer assume
p(F|do(C)) = p(F|do(¬C)) = p(F)
and the above proof breaks down. So p(E|do(C)) −p(E|do(¬C)) may be positive or negaitve.
F. That is, conditioning on F might block one of the causal pathways. In other words, by
effect of one of the two pathways by which the drug operates to bring about recovery.
Thus we see that different causal assumptions lead to different causal conclusions, and hence
different courses of action. This raises the question on whether we can learn the causal model
from data. We discuss this issue below.
Learning causal DAG structures
In this section, we discuss some ways to learn causal DAG structures.Learning from observational data
data. It is natural to ask whether these methods can recover the “true” DAG structure that was
used to generate the data. Clearly, even if we have inﬁnite data, an optimal method can only
(partially directed acylic graph), but not the complete DAG structure, because all DAGs which are
Markov equivalent have the same likelihood.
There are several algorithms (e.g., the greedy equivalence search method of (Chickering
true Markov equivalence class as the sample size goes to inﬁnity, assuming we observe all the
variables. However, we also have to assume that the generating distribution p is faithful to
the generating DAG G. This means that all the conditional indepence (CI) properties of p are
exactly captured by the graphical structure, so I(p) = I(G); this means there cannot be any CI
properties in p that are due to particular settings of the parameters (such as zeros in a regression
matrix) that are not graphically explicit. For this reason, a faithful distribution is also called a
stable distribution.
Suppose the assumptions hold and we learn a PDAG. What can we do with it? Instead of
recovering the full graph, we can focus on the causal analog of edge marginals, by computing
the magnitude of the causal effect of one node on another (say A on B). If we know the DAG, we
the magnitude of the causal effect of A on B in each DAG; ﬁnally, take the minimum of these
effects as the lower bound. It is usually computationally infeasible to compute all DAGs in the
equivalence class, but fortunately one only needs to be able to identify the local neighborhood
chance.
Learning from interventional data
tional data, where certain variables have been set, and the consequences have been measured.
were perturbed, and their phosphorylation status was measured using a technique called ﬂow
It is straightforward to modify the standard Bayesian scoring criteria, such as the marginal
likelihood or BIC score, to handle learning from mixed observational and experimental data: we(a)
B2cAMP
f
erk
akt
jnk
Psitect
AKT inh
U0126
PMA
p38
G06967
mek12
raf
pkc
pip3
plcy
pip2
pka
Present
Missing
Int. edge
(b)
activating or inhibiting chemicals.
(b) A directed graphical model representing dependencies between
various proteins (blue circles) and various experimental interventions (pink ovals), which was inferred fromjust compute the sufficient statistics for a CPD’s parameter by skipping over the cases where that
modify the counts as follows:
Ntck ≜

i:xit not set
I(xi,t = k, xi,pa(t) = c)
The justiﬁcation for this is that in cases where node t is set by force, it is not sampled from its
iﬁed scoring criterion can be combined with any of the standard structure learning algorithms.
as to reduce the posterior uncertainty as quickly as possible (a form of active learning).
The preceeding method assumes the interventions are perfect. In reality, experimenters can
rarely control the state of individual molecules. Instead, they inject various stimulant or inhibitor
chemicals which are designed to target speciﬁc molecules, but which may have side effects. We
can model this quite simply by adding the intervention nodes to the DAG, and then learning
a larger augmented DAG structure, with the constraint that there are no edges between the
intervention nodes, and no edges from the “regular” nodes back to the intervention nodes.
In particular, we plot the median graph, which
These were computed using the exact
the same structure as the optimal MAP model, argmaxG p(G|D), which was computed using
Learning undirected Gaussian graphical models
Learning the structured of undirected graphical models is easier than learning DAG structure
because we don’t need to worry about acyclicity. On the other hand, it is harder than learning
kind of local search methods (both greedy search and MCMC sampling) we used to learn DAG
structures, because the cost of evaluating each neighboring graph is too high, since we have to
reﬁt each model from scratch (there is no way to incrementally update the score of a model).
In this section, we discuss several solutions to this problem, in the context of Gaussian
random ﬁelds or undirected Gaussian graphical models (GGM)s. We consider structure learning
MLE for a GGM
Before discussing structure learning, we need to discuss parameter estimation.
The task ofN
N
covariance matrix. (For notational simplicity, we assume we have already estimated ˆμ = x.)
One can show that the gradient of this is given by
challenging (albeit still a convex constraint). One approach is to add a penalty term to the
Interestingly, one can show that the MLE must satisfy the following property: Σst = Sst if
matrix completion of S, since it retains as many of the entries in S as possible, corresponding
absent edges; the remaining entries in Σ are ﬁlled in so as to maximize the likelihood.
adjacency matrix, representing the cyclic structure, X1 −X2 −X3 −X4 −X1, and the following
empirical covariance matrix:
G =
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠, S =
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠
The MLE is given by
Σ =
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠, Ω =
⎛
⎜
⎜
⎝
⎞
⎟
⎟
and the free elements in Σ, both of which correspond to absent edges, have been highlighted.
Graphical lasso
We now discuss one way to learn a sparse GRF structure, which exploits the fact that there is a
suggests that we can learn a sparse graph structure by using an objective that encourages zeros
penalized NLL:(a)
(b)
(c)
(d)
Glasso.
coordinate descent algorithm similar to the shooting algorithm for lasso. See ggmLassoHtf for
algorithm.)
data.
However, we are ignoring the fact that the data was sampled under intervention.
In
value. These represent a range of plausible hypotheses about the connectivity of these proteins.
advantage that it can easily model the interventional nature of the data, but the disadvantage
that it cannot model the feedback loops that are known to exist in this biological pathway (see
only one DAG is incidental: we could easily use BIC to pick the “best” UGM, and conversely, wecould easily display several DAG structures, sampled from the posterior.
Bayesian inference for GGM structure *
Although the graphical lasso is reasonably fast, it only gives a point estimate of the structure.
perform posterior inference in the space of graphs, i.e., to compute p(G|D). We can then extract
DAGs. In this section, we discuss how to do this.
Here we have the same
dichotomy, but we are presenting them in the opposite order.
If the graph is decomposable, and if we use conjugate priors, we can compute the marginal
additions and removals. This means that we can perform relatively efficient stochastic local
However, the restriction to decomposable graphs is rather limiting if one’s goal is knowledge
discovery, since the number of decomposable graphs is much less than the number of general
scale to large models because they use an expensive Monte Carlo approximation to the marginal
in general.
not need to know the cliques of the graph. A further speedup is obtained by just using a
diagonal Laplace approximation, which is more accurate than BIC, but has essentially the same
cost. This, plus the lack of restriction to decomposable graphs, enables fairly fast stochastic
search methods to be used to approximate p(G|D) and its mode. This approach signiﬁcantly
outperfomed graphical lasso, both in terms of predictive accuracy and structural recovery, for a
comparable computational cost.
the total hypothesis space.The graphical lasso and variants is inhertently limited to data that is jointly Gaussian, which is
but still continuous, data in a fairly simple fashion. The basic idea is to estimate a set of D
univariate monotonic transformations fj, one per variable j, such that the resulting transformed
data is jointly Gaussian.
If this is possible, we say the data belongs to the nonparametric
transforming the data, we can compute the correlation matrix and then apply glasso in the usual
way. One can show, under various assumptions, that this is a consistent estimator of the graph
Learning undirected discrete graphical models
The problem of learning the structure for UGMs with discrete variables is harder than the
Gaussian case, because computing the partition function Z(θ), which is needed for parameter
estimation, has complexity comparable to computing the permanent of a matrix, which in
Since stochastic local search is not tractable for general discrete UGMs, below we mention
some possible alternative approaches that have been tried.
It is possible to extend the graphical lasso idea to the discrete MRF and CRF case. However, now
there is a set of parameters associated with each edge in the graph, so we have to use the graph
nodes, and node and edge potentials given by
ψt(yt, x) =
⎛
⎝
vT
t1x
vT
t2x
vT
t3x
⎞
⎠, ψst(ys, yt, x) =
⎛
⎝
wT
t11x
wT
st12x
wT
st13x
wT
st21x
wT
st22x
wT
st23x
wT
st31x
wT
st32x
wT
st33x
⎞
⎠
To learn sparse structure, we can minimize the following objective:
J
=
−
N


t
log ψt(yit, xi, vt) +
V

V

log ψst(yis, yit, xi, wst)

+λ1
V

V

||wst||p + λ2
V
baseball
games
league
players
bible
christian
god
jesus
question
car
dealer
drive
engine
card
driver
graphics
pc
problem
system
video
windows
case
course
evidence
fact
government
human
law
number
power
rights
state
world
children
president
religion
war
computer
data
email
program
science
software
university
memory
research
space
disk
files
display
image
dos
mac
scsi
earth
orbit
format
ftp
help
phone
jews
fans
hockey
team
version
nhl
season
win
gun
health
insurance
israel
launch
moon
nasa
shuttle
technology
won
Schmidt.
(The use of ℓ1 regularization for learning the structure of binary MRFs was proposed in (Lee
Although this objective is convex, it can be costly to evaluate, since we need to perform
should therefore use an optimizer that does not make too many calls to the objective function
an approximate objective and gradient more quickly. Another approach is to apply the group& )




&


)





&


)




: )


))












&ORXG\
6SULQNOHU
5DLQ
:HW
*UDVV
Water sprinkler DGM with corresponding binary CPTs. T and F stand for true and false.
Thin junction trees
So far, we have been concerned with learning “sparse” graphs, but these do not necessarily have
low treewidth. For example, a D × D grid is sparse, but has treewidth O(D). This means that
the models we learn may be intractable to use for inference purposes, which defeats one of the
two main reasons to learn graph structure in the ﬁrst place (the other reason being “knowledge
discovery”). There have been various attempts to learn graphical models with bounded treewidth
as thin junction trees, but the exact problem in general is hard.
An alternative approach is to learn a model with low circuit complexity (Gogate et al.
Exercises
a. Suppose I perform a perfect intervention and make the grass wet. What is the probability the sprinkler
is on, p(S = T|do(W = T))?
b. Suppose I perform a perfect intervention and make the grass dry. What is the probability the sprinkler
is on, p(S = T|do(W = F))?
c. Suppose I perform a perfect intervention and make the clouds “turn on” (e.g., by seeding them). What
is the probability the sprinkler is on, p(S = T|do(C = T))?Latent variable models for discrete data
Introduction
In this chapter, we are concerned with latent variable models for discrete data, such as bit vectors,
sequences of categorical variables, count vectors, graph structures, relational data, etc. These
images, movie ratings, etc. However, we will mostly focus on text analysis, and this will be
reﬂected in our terminology.
Since we will be dealing with so many different kinds of data, we need some precise notation
the identity of the l’th word in document i, where V is the number of possible words in
We will often ignore the word order, resulting in a bag of words. This can be reduced to
matrix N is often large but sparse, since we typically have many documents, but most words
do not occur in any given document.
In some cases, we might have multiple different bags of words, e.g., bags of text words and
bags of visual words. These correspond to different “channels” or types of features. We will
Out goal is to build joint probability models of p(yi) or p(ni) using latent variables to capture
the correlations. We will then try to interpret the latent variables, which provide a compressed
going into more detail in later sections.
Towards the end of the chapter, we will consider modeling graphs and relations, which can
also be represented as sparse discrete matrices. For example, we might want to model the graph
of which papers mycite which other papers. We will denote these relations by R, reserving the
symbol Y for any categorical data (e.g., text) associated with the nodes.Distributed state LVMs for discrete data
In this section, we summarize a variety of possible approaches for constructing models of the
integer counts.
Mixture models
clusters. We will use a discrete prior, qi ∼Cat(π). For variable length documents, we can
deﬁne p(yil|qi = k) = bkv, where bkv is the probability that cluster k generates word v. The
value of qi is known as a topic, and the vector bk is the k’th topic’s word distribution. That is,
the likelihood has the form
Li

Cat(yil|bk)
The induced distribution on the visible data is given by

k
πk
 Li

Cat(yil|bk)

The “generative story” which this encodes is as follows: for document i, pick a topic qi from
sophisticated generative models later in this chapter.
If we have a ﬁxed set of categorical observations, we can use a different topic matrix for each
output variable:
R

Cat(yil|b(r)
k )
This is an unsupervised analog of naive Bayes classiﬁcation.
We can also model count vectors.
If the sum Li = 
v niv is known, we can use a
multinomial:
p(ni|Li, qi = k) = Mu(ni|Li, bk)
p(ni|qi = k) =
V
Poi(niv|λvk)
In this case, Li|qi = k ∼Poi(Exponential family PCA
Unfortunately, ﬁnite mixture models are very limited in their expressive power. A more ﬂexible
This method can certainly be applied to discrete or count data. Indeed, the method known
matrix.
tion. We just have to change the likelihood to
Li

Cat(yil|S(Wzi))
of categorical responses, we can use
p(y1:R|zi) =
R

Cat(yir|S(Wrzi))
have counts, we can use a multinomial model
p(ni|Li, zi) = Mu(ni|Li, S(Wzi))
or a Poisson model
p(ni|zi) =
V
Poi(niv| exp(wT
v,:zi))
distribution on the visible variables has the form

  Li

p(yil|zi, W)

N(zi|μ, Σ)dzi
Fitting this model is tricky, due to the lack of conjugacy.
coordinate ascent method that alternates between estimating the zi and W.
This can be
regarded as a degenerate version of EM, that computes a point estimate of zi in the E step. The
problem with the degenerate approach is that it is very prone to overﬁtting, since the number
algorithm would marginalize out the latent variables zi. A way to do this for categorical PCA,γ
W R,K,V
. . .
yi,R
. . .
zi
N
μ
Σ
(a)
BK,V
. . .
ni,V
πi
α
Li
N
(b)
Two LVMs for discrete data. Circles are scalar nodes, ellipses are vector nodes, squares are
matrix nodes. (a) Categorical PCA. (b) Multinomial PCA.
LDA and mPCA
times it is more convenient to use the dual parameters. For example, for the multinomial, the
dual parameter is the probability vector, whereas the natural parameter is the vector of log odds.
If we want to use the dual parameters, we need to constrain the latent variables so they live
in the appropriate parameter space. In the case of categorical data, we will need to ensure the
we will denote such a latent vector by πi. In this case, the natural prior for the latent variables
When we have a count vector whose total sum is known, the likelihood is given by
p(ni|Li, πi) = Mu(ni|Li, Bπi)
k bvkπiv, this can be seen as a form
of matrix factorization for the count matrix. Note that we use bv,k to denote the parameter
The corresponding marginal distribution has the form
p(ni|Li) =


Mu(ni|Li, Bπi)Dir(πi|α)dπi
Unfortunately, this integral cannot be computed analytically.
If we have a variable length sequence (of known length), we can use
Li

Cat(yil|Bπi)much greater detail below. LDA can be thought of as a probabilistic extension of LSA, where the
which makes interpetation difficult.
A predecessor to LDA, known as probabilistic latent semantic indexing or PLSI (Hofmann
ePCA), rather than integrating it out. Thus in PLSI, there is no prior for πi.
We can modify LDA to handle a ﬁxed number of different categorical responses as follows:
R

Cat(yil|B(r)πi)
Now consider modeling count vectors where we do not constrain the sum to be observed. In
i . This
can be ensured by using a prior of the form
i ) =
K

ik|αk, βk)
The likelihood is given by
i ) =
V
Poi(niv|bT
i )
Li, reduces to the mPCA model. This follows since a set of Poisson random variables, when
i .
rithm, so suffers from overﬁtting. Some procedures to ﬁt the GaP model, which overcome these
as follows:
ik|αk, βk)
prior, although we will not go into detail here.γ
B
. . .
ni,V
. . .
i,K
N
α1 β1
. . .
αK βK
(a)
γ
B
yil
qil
Li
πi
N
α
(b)
Latent Dirichlet allocation (LDA)
detail.
Basics
distribution over topics, rather than a single topic, the model is called an admixture mixture
πi|α
∼
Dir(α1K)
qil|πi
∼
Cat(πi)
bk|γ
∼
Dir(γ1V )
yil|qil = k, B
∼
Cat(bk)
They typically use wnd for the identity of word n in document d, znd to represent the discrete indicator, θd as the
continuous latent vector for document d, and βk as the k’th topic vector.P(word3)
P(word1)
P(word2)
=   observed
document
=   generated
document
=   topic
(white dots), and each topic (black dots), is a point in the 3d simplex.
Used with kind permission of Tom Griffiths.
direct arc from πi to yil, with the following CPD:
p(yil = v|πi)
=

k
p(yil = v|qil = k)p(qil = k) =

k
πikbkv
As we mentioned in the introduction, this is very similar to the multinomial PCA model proposed
LDA has an interesting geometric interpretation. Each vector bk deﬁnes a distribution over
V words; each k is known as a topic. Each document vector πi deﬁnes a distribution over K
topics. So we model each document as an admixture over topics. Equivalently, we can think
of LDA as a form of dimensionality reduction (assuming K < V , as is usually the case), where
we project a point in the V -dimensional simplex (a normalized document count vector xi) onto
One advantage of using the simplex as our latent space rather than Euclidean space is that
the simplex can handle ambiguity. This is importance since in natural language, words can often
have multiple meanings, a phenomomen known as polysemy. For example, “play” might refer
to a verb (e.g., “to play ball” or “to play the coronet”), or to a noun (e.g., “Shakespeare’s play”).
In LDA, we can have multiple topics, each of which can generate the word “play”, as shown in
Given word l in document i, we can compute p(qil = k|yi, θ), and thus infer its most likely
topic. By looking at the word in isolation, it might be hard to know what sense of the word is
meant, but we can disambiguate this by looking at other words in the document. In particular,
given xi, we can infer the topic distribution πi for the document; this acts as a prior forword
prob.
word
prob.
word
prob.
MUSIC
LITERATURE
PLAY
DANCE
POEM
BALL
SONG
POETRY
GAME
PLAY
POET
PLAYING
SING
PLAYS
HIT
SINGING
POEMS
PLAYED
BAND
PLAY
BASEBALL
PLAYED
LITERARY
GAMES
SANG
WRITERS
BAT
SONGS
DRAMA
RUN
DANCING
WROTE
THROW
PIANO
POETS
BALLS
PLAYING
WRITER
TENNIS
RHYTHM
SHAKESPEARE
HOME
ALBERT
WRITTEN
CATCH
MUSICAL
STAGE
FIELD
Three topics related to the word play.
Used with kind permission of Tom Griffiths.
Bix beiderbecke, at age060 fifteen207, sat174 on the slope071 of a bluff055 overlooking027 the mississippi137 river137. He
was listening077 to music077 coming009 from a passing043 riverboat. The music077 had already captured006 his heart157
as well as his ear119. It was jazz077. Bix beiderbecke had already had music077 lessons077. He showed002 promise134 on
the piano077, and his parents035 hoped268 he might consider118 becoming a concert077 pianist077. But bix was
interested268 in another kind050 of music077. He wanted268 to play077 the cornet. And he wanted268 to play077 jazz077...  
There is a simple050 reason106 why there are so few periods078 of really great theater082 in our whole western046 world.
Too many things300 have to come right at the very same time. The dramatists must have the right actors082, the
actors082 must have the right playhouses, the playhouses must have the right audiences082. We must remember288 that
plays082 exist143 to be performed077, not merely050 to be read254. ( even when you read254 a play082 to yourself, try288 to
perform062 it, to put174 it on a stage078, as you go along.) as soon028 as a play082 has to be performed082, then some 
kind126 of theatrical082...
Jim296 has a game166 book254. Jim296 reads254 the book254. Jim296 sees081 a game166 for one. Jim296 plays166 the game166.
Jim296 likes081 the game166 for one. The game166 book254 helps081 jim296. Don180 comes040 into the house038. Don180 and
jim296 read254 the game166 book254. The boys020 see a game166 for two. The two boys020 play166 the game166. The
boys020 play166 the game166 for two. The boys020 like the game166. Meg282 comes040 into the house282. Meg282 and
don180 and jim296 read254 the book254. They see a game166 for three. Meg282 and don180 and jim296 play166 the game166.
They play166...
Three documents from the TASA corpus containing different senses of the word play. Grayed
out words were ignored by the model, because they correspond to uninteresting stop words (such as “and”,
“the”, etc.) or very low frequency words.
Used with
kind permission of Tom Griffiths.interpretation of “play” the most likely, as shown by the superscript. The second document
interprets play in the theatrical sense, and the third in the sports sense. Note that is crucial
that πi be a latent variable, so information can ﬂow between the qil’s, thus enabling local
disambiguation to use the full set of words.
Unsupervised discovery of topics
identiﬁability. In particular, it forces the topics to correspond to the tags, and then it learns a
distribution over words for each tag. This can make the results easier to interpret.
Quantitatively evaluating LDA as a language model
In order to evaluate LDA quantitatively, we can treat it as a language model, i.e., a probability
distribution over sequences of words. Of course, it is not a very good language model, since it
ignores word order and just looks at single words (unigrams), but it is interesting to compare
language models are sometimes useful for information retrieval purposes. The standard way to
measure the quality of a language model is to use perplexity, which we now deﬁne below.
Perplexity
The perplexity of language model q given a stochastic process3 p is deﬁned as
perplexity(p, q) ≜2H(p,q)
H(p, q) ≜lim
N

y1:N
p(y1:N) log q(y1:N)
The cross entropy (and hence perplexity) is minimized if q = p; in this case, the model can
predict as well as the “true” distribution.
We can approximate the stochastic process by using a single long test sequence (composed
it y∗
distribution (an approximation to the stochastic process) as
pemp(y1:N) = δy∗
collated by a company formerly known as Touchstone Applied Science Associates, but now known as Questar Assessment
can think of natural language as a stochastic process, since it can generate an inﬁnite stream of words.N log q(y∗
and the perplexity becomes
perplexity(pemp, q) = 2H(pemp,q) = q(y∗
N
:
:
;
N

q(y∗
i |y∗
We see that this is the geometric mean of the inverse predictive probabilities, which is the usual
In the case of unigram models, the cross entropy term is given by
N
N

Li
Li

log q(yil)
where N is the number of documents and Li is the number of words in document i. Hence
the perplexity of model q is given by
perplexity(pemp, p) = exp

N
N

Li
Li

log q(yil)


tive distribution. Suppose the model predicts that each symbol (letter, word, whatever) is equally
are more likely than others, and the model correctly reﬂects this, its perplexity will be lower
than K. Of course, H(p, p) = H(p) ≤H(p, q), so we can never reduce the perplexity below
the entropy of the underlying stochastic process.
Perplexity of LDA
The key quantity is p(v), the predictive distribution of the model over possible words. (It is
implicitly conditioned on the training set.) For LDA, this can be approximated by plugging in
B (e.g., the posterior mean estimate) and approximately integrating out q using mean ﬁeld
likelihood).
tion of a multinoulli, MAP estimation of a mixture of multinoullis, and pLSI. (When performing
MAP estimation, the same Dirichlet prior on B was used as in the LDA model.) The metric
these other methods.Number of Topics
Perplexity
 
 
Unigram
Mixtures of Unigrams
LDA
Fold in pLSI
Perplexity vs number of topics on the TREC AP corpus for various language models. Based
γ
b1 . . . bK
. . .y1,L1
. . .q1,L1
π1
. . .
. . .
. . .
. . .
. . .
yN,LN
. . .
qN,LN
πN
α
(a)
γ
. . .y1,L1
. . .q1,L1
. . .
. . .
. . .
yN,LN
. . .
qN,LN
α
(b)
(a) LDA unrolled for N documents. (b) Collapsed LDA, where we integrate out the πi and
the bk.
Fitting using (collapsed) Gibbs sampling
It is straightforward to derive a Gibbs sampling algorithm for LDA. The full conditionals are as
follows:
p(qil = k|·)
∝
exp[log πik + log bk,xil]
p(πi|·)
=
Dir({αk +

l
I(zil = k)})
p(bk|·)
=
Dir({γv +

i

l
I(xil = v, zil = k)})
However, one can get better performance by analytically integrating out the πi’s and the bk’s,both of which have a Dirichlet distribution, and just sampling the discrete qil’s. This approach
can sample them one at a time, as we explain below.
First, we need some notation. Let civk = Li
word v is assigned to topic k in document i. Let cik = 
v civk be the number of times any
word from document i has been assigned to topic k. Let cvk = 
i civk be the number of times
word v has been assigned to topic k in any document. Let niv = 
k civk be the number of
times word v occurs in document i; this is observed. Let ck = 
v cvk be the number of words
assigned to topic k. Finally, let Li = 
k cik be the number of words in document i; this is
observed.
p(q|α)
=

i

  Li

Cat(qil|πi)

Dir(πi|α1K)dπi
=
Γ(Kα)
Γ(α)K
	N
N

K
Γ(Li + Kα)
By similar reasoning, one can show
p(y|q, γ)
=

k

 ⎡
⎣
Cat(yil|bk)
⎤
⎦Dir(bk|γ1V )dbk
=
Γ(V β)
Γ(β)V
	K
K

V
Γ(ck + V β)
conditional for p(qil|q−i,l). Deﬁne c−
ivk to be the same as civk except it is compute by summing
over all locations in document i except for qil. Also, let yil = v. Then
p(qi,l = k|q−i,l, y, α, γ)
∝
c−
v,k + γ
c−
k + V γ
c−
i,k + α
Li + Kα
We see that a word in a document is assigned to a topic based both on how often that word is
generated by the topic (ﬁrst term), and also on how often that topic is used in that document
(second term).
a given word in the corpus, decrement the relevant counts, based on the topic assigned to the
This algorithm can be made efficient since the count matrices are very sparse.
ExampleRiver
Stream
Bank
Money
Loan
(a)
River
Stream
Bank
Money
Loan
(b)
steps of Gibbs sampling.
Used with kind permission of
Tom Griffiths.
other ﬁnancial terms. The right part of the ﬁgure shows the state of the Gibbs sampler after
based on the presence of the words “money” and “loan”. The posterior mean estimate of the
Fitting using batch variational inference
A faster alternative to MCMC is to use variational EM. (We cannot use exact EM since exact
inference of πi and qi is intractable.) We give the details below.
Sequence version
q(πi, qi) = Dir(πi|˜πi)

l
Cat(qil|˜qil)
We will follow the usual mean ﬁeld recipe. For q(qil), we use Bayes’ rule, but where we need to
take expectations over the prior:
˜qilk
∝
byi,l,k exp(E [log πik])
where
E [log πik] = ψk(˜πi.) ≜Ψ(˜πik) −Ψ(

k′
˜πik′)where Ψ is the digamma function. The update for q(πi) is obtained by adding up the expected
counts:
˜πik
=
αk +

l
˜qilk
The M step is obtained by adding up the expected counts and normalizing:
ˆbvk
∝
γv +
N

Li

˜qilkI(yil = v)
Count version
Note that the E step takes O((
i Li)V K) space to store the ˜qilk. It is much more space
efficient to perform inference in the mPCA version of the model, which works with counts; these
only take O(NV K) space, which is a big savings if documents are long. (By contrast, the
collapsed Gibbs sampler must work explicitly with the qil variables.)
We will focus on approximating p(πi, ci|ni, Li), where we write ci as shorthand for ci... We
will again use a fully factorized (mean ﬁeld) approximation of the form
q(πi, ci) = Dir(πi|˜πi)

v
Mu(civ.|niv, ˜civ.)
The new E step becomes
˜πik
=
αk +

v
niv˜civk
˜civk
∝
bvk exp(E [log πik])
The new M step becomes
ˆbvk
∝
γv +

i
niv˜civk
VB version
We now modify the algorithm to use VB instead of EM, so that we infer the parameters as
the online learning setting, as we discuss below.
Our new posterior approximation becomes
q(πi, ci, B) = Dir(πi|˜πi)

v
Mu(civ.|niv, ˜civ.)

k
Dir(b.k|˜b.k)
The update for ˜civk changes, to the following:
˜civk
∝
exp (E [log bvk] + E [log πik])// E step ;
(˜πi, ˜ci) = Estep(ni, ˜B, α);
// M step ;
˜bvk = γv + svk;
˜πold
i.
= ˜πi., ˜πik = αk;
˜civk = exp
!
ψk(˜bv.) + ψk(˜πold
i. )
#
;
˜civ. = normalize(˜civ.);
K

k |˜πik −˜πold
ik | < thresh;
Also, the M step becomes
˜bvk
=
γv +

i
˜civk
No normalization is required, since we are just updating the pseudcounts. The overall algorithm
Fitting using online variational inference
In the bathc version, the E step clearly takes O(NKV T) time, where T is the number of
inference, as we now explain.
usual way. We then compute the variational parameters for B treating the expected sufficient
statistics from the single data case as if the whole data set had those statistics. Finally, we makeSet step size ρt = (τ0 + t)−κ;
Pick document i = i(t); ;
(˜πi, ˜ci) = Estep(ni, ˜B, α);
˜bnew
vk
= γv + Nniv˜civk;
vk ;
Documents seen (log scale)
Perplexity
Batch 98K
Online 98K
a partial update for the variational parameters for B, putting weight ρt on the new estimate
steps), where the data is drawn from (English) Wikipedia. The ﬁgure shows that online variational
inference is much faster than offline inference, yet produces similar results.
Determining the number of topics
Choosing K, the number of topics, is a standard model selection problem.
Here are some
approaches that have been taken:
•
•
Cross validation, using the log likelihood on a test set.•
Use the variational lower bound as a proxy for log p(D|K).
•
Extensions of LDA
discuss a few of these below.
Correlated topic model
One weakness of LDA is that it cannot capture correlation between topics. For example, if a
The source of the problem is the use of a Dirichlet prior for πi. The problem with the Dirichelt
it that it is characterized by just a mean vector and a strength parameter, but its covariance is
ﬁxed (Σij = −αiαj), rather than being a free parameter.
One way around this is to replace the Dirichlet prior with the logistic normal distribution, as
bk|γ
∼
Dir(γ1V )
zi
∼
N(μ, Σ)
πi|zi
=
S(zi)
qil|πi
∼
Cat(πi)
yil|qil = k, B
∼
Cat(bk)
categorical PCA, but slightly different. To see the difference, let us marginalize out the qil and
πi. Then in the CTM we have
yil ∼Cat(BS(zi))
where B is a stochastic matrix. By contrast, in catPCA we have
yil ∼Cat(S(Wzi))
where W is an unconstrained matrix.
Fitting this model is tricky, since the prior for πi is no longer conjugate to the multinomial
we discussed Bayesian multiclass logistic regression. In the CTM case, things are even harder
since the categorical response variables qi are hidden, but we can handle this by using an
Having ﬁt the model, one can then convert ˆΣ to a sparse precision matrix ˆΣ
topic in the corpus. Edges represent signiﬁcant elements of the precision matrix.wild type
mutant
mutations
mutants
mutation
plants
plant
gene
genes
arabidopsis
p53
cell cycle
activity
cyclin
regulation
amino acids
cdna
sequence
isolated
protein
gene
disease
mutations
families
mutation
rna
dna
rna polymerase
cleavage
site
cells
cell
expression
cell lines
bone marrow
united states
women
universities
students
education
science
scientists
says
research
people
research
funding
support
nih
program
surface
tip
image
sample
device
laser
optical
light
electrons
quantum
materials
organic
polymer
polymers
molecules
volcanic
deposits
magma
eruption
volcanism
mantle
crust
upper mantle
meteorites
ratios
earthquake
earthquakes
fault
images
data
ancient
found
impact
million years ago
africa
climate
ocean
ice
changes
climate change
cells
proteins
researchers
protein
found
patients
disease
treatment
drugs
clinical
genetic
population
populations
differences
variation
fossil record
birds
fossils
dinosaurs
fossil
sequence
sequences
genome
dna
sequencing
bacteria
bacterial
host
resistance
parasite
development
embryos
drosophila
genes
expression
species
forest
forests
populations
ecosystems
synapses
ltp
glutamate
synaptic
neurons
neurons
stimulus
motor
visual
cortical
ozone
atmospheric
measurements
stratosphere
concentrations
sun
solar wind
earth
planets
planet
co2
carbon
carbon dioxide
methane
water
receptor
receptors
ligand
ligands
apoptosis
proteins
protein
binding
domain
domains
activated
tyrosine phosphorylation
activation
phosphorylation
kinase
magnetic
magnetic ﬁeld
spin
superconductivity
superconducting
physicists
particles
physics
particle
experiment
surface
liquid
surfaces
ﬂuid
model
reaction
reactions
molecule
molecules
transition state
enzyme
enzymes
iron
active site
reduction
pressure
high pressure
pressures
core
inner core
brain
memory
subjects
left
task
computer
problem
information
computers
problems
stars
astronomers
universe
galaxies
galaxy
virus
hiv
aids
infection
viruses
mice
antigen
t cells
antigens
immune response
size reﬂects overall prevalence of the topic.
Used with
kind permission of David Blei.
Dynamic topic model
In LDA, the topics (distributions over words) are assumed to be static. In some cases, it makes
sense to allow these distributions to evolve smoothly over time. For example, an article might
use the topic “neuroscience”, but if it was written in the 1900s, it is more likely to use words
like “nerve”, whereas if it was written in the 2000s, it is more likely to use words like “calcium
receptor” (this reﬂects the general trend of neuroscience towards molecular biology).
In particular, we assume the topic distributions evolve according to a Gaussian random walk,
and then we map these Gaussian vectors to probabilities via the softmax function:
∼
πt
i
∼
Dir(α1K)
qt
il|πt
i
∼
Cat(πt
i)
yt
il|qt
il = k, Bt
∼
Cat(S(bt
k))
This is known as a dynamic topic model (Blei and Lafferty 2006b).k
il
il
i
N
bt
k
yt
il
qt
il
πt
i
N
k
il
il
i
N
K
α
The dynamic topic model.
One can perform approximate infernece in this model using a structured mean ﬁeld method
details).
of some speciﬁc words belonging to this topic. On the bottom right, we list the titles of some
articles that contained this topic.
trieval.
That is, suppose we look for documents about the inheritance of disease.
Modern
articles will use words like “DNA”, but older articles (before the discovery of DNA) may use other
terms such as “heritable unit”. But both articles are likely to use the same topics. Similar ideas
The LDA model assumes words are exchangeable, which is clearly not true.
A simple way
to model sequential dependence between words is to use a hidden Markov model or HMM.
capture the overall gist of a document. Hence they can generate syntactically correct sentences"Neuroscience"
brain
movement
action
right
eye
hand
left
muscle
nerve
sound 
movement
eye
right
hand
brain
left
action
muscle
sound
experiment
brain
eye
movement
right
left
hand
nerve
vision
sound
muscle 
movement
brain
sound
nerve
active
muscle
left
eye
right
nervous 
movement
sound
muscle
active
nerve
stimulate
ﬁber
reaction
brain
response 
stimulate
muscle
sound
movement
response
nerve
frequency
ﬁber
active
brain 
record
nerve
stimulate
response
muscle
electrode
active
brain
ﬁber
potential 
respons
record
stimulate
nerve
muscle
active
frequency
electrode
potential
study 
response
stimulate
record
condition
active
potential
stimulus
nerve
subject
eye 
respons
cell
potential
stimul
neuron
active
nerve
eye
record
abstract 
cell
neuron
response
active
brain
stimul
muscle
system
nerve
receptor 
cell
channel
neuron
ca2
active
brain
receptor
muscle
respons
current 
neuron
active
brain
cell
ﬁg
response
channel
receptor
synapse
signal 
ca2
neuron
nerve
Part of the output of the dynamic topic model when applied to articles from Science. We
within this topic over time, and some articles that contained this topic.
Lafferty 2006b).
Used with kind permission of David Blei.
“however”, and uses the LDA to model content or semantic words, which are harder to predict.
There is a distinguished HMM state which speciﬁes when the LDA model should be used to
generate the word; the rest of the time, the HMM generates the word.
yil from the semantic LDA model, with topic speciﬁed by qil; otherwise we generate word yil
p(πi)
=
Dir(πi|α1K)
p(qil = k|πi)
=
πik
=
AHMM(c, c′)
p(yil = v|qil = k, zil = c)
=

BLDA(k, v)
BHMM(c, v)
Inference in this model can be done with collapsed Gibbs sampling, analytically integratingAHMM
BHMM
BLDA
zi,l
yi,l
. . .
qi,l
πi
α
N
In contrast to this approach, we study here how the overall network activity can
control
single cell
parameters such as input resistance, as well as time and space constants, parameters that are crucial for
excitability and spariotemporal (sic) integration.
The integrated architecture in this paper combines feed forward
control
and error feedback adaptive
control using neural networks.
In other words, for our proof of convergence, we require the softassign algorithm to return
a doubly
stochastic matrix as *sinkhorn theorem guarantees that it will instead of a matrix which is merely close
to being doubly stochastic based on some reasonable metric.
The aim is to construct a portfolio with a maximal expected
return
for a given risk level and time
horizon while simultaneously obeying *institutional or *legally required constraints.
The left graph is the standard experiment the right from a training with # samples.
The graph G is called the *guest graph, and H is called the host graph.
Graylevel indicates posterior probability of assignment to LDA component, with black being highest. The
boxed word appears as a function word in one sentence, and as a content word in another sentence.
Asterisked words had low frequency, and were treated as a single word type by the model. Source: Figure
Used with kind permission of Tom Griffiths.the
the
the
the
the
a
the
the
the
blood
,
,
of
a
the
,
,
,
,
and
and
,
of
of
of
a
a
of
of
of
to
,
,
a
of
in
body
a
in
in
in
in
and
and
game
heart
in
land
and
to
water
in
drink
ball
and
trees
to
classes
picture
is
story
alcohol
and
in
tree
farmers
government
ﬁlm
and
is
to
team
to
with
for
a
image
matter
to
bottle
to
is
on
farm
state
lens
are
as
in
play
blood
forest
farmers
government
light
water
story
drugs
ball
heart
trees
land
state
eye
matter
stories
drug
game
pressure
forests
crops
federal
lens
molecules
poem
alcohol
team
body
land
farm
public
image
liquid
characters
people
*
lungs
soil
food
local
mirror
particles
poetry
drinking
baseball
oxygen
areas
people
act
eyes
gas
character
person
players
vessels
park
farming
states
glass
solid
author
effects
football
arteries
wildlife
wheat
national
object
substance
poems
marijuana
player
*
area
farms
laws
objects
temperature
life
body
ﬁeld
breathing
rain
corn
department
lenses
changes
poet
use
basketball
the
in
he
*
be
said
can
time
,
a
for
it
new
have
made
would
way
;
his
to
you
other
see
used
will
years
(
this
on
they
ﬁrst
make
came
could
day
:
their
with
i
same
do
went
may
part
)
these
at
she
great
know
found
had
number
your
by
we
good
get
called
must
kind
her
from
there
small
go
do
place
my
as
this
little
take
have
some
into
who
old
ﬁnd
did
Upper row: Topics extracted by the LDA model when trained on the combined Brown and
Used with kind permission of Tom Griffiths.
responsible for syntactic words, and the LDA for semantics words. If we did not have the HMM,
the LDA topics would get “polluted” by function words (see top of ﬁgure), which is why such
words are normally removed during preprocessing.
The model can also help disambiguate when the same word is being used syntactically or
semantically.
return a matrix” (verb) vs “the maximal expected return” (noun). In principle, a part of speech
but different senses, e.g., “the left graph” (a synactic role) vs “the graph G” (a semantic role).
The topic of probabilistic models for syntax and semantics is a vast one, which we do notB
yil
qil
ci
¯qi
w
πi
N
α
(a)
B
A
yil
qil
ci
πi
N
α
(b)
(a) Supervised LDA. (b) Discriminative LDA.
Supervised LDA
In this section, we discuss extensions of LDA to handle side information of various kinds beyond
just words.
Generative supervised LDA
approaches, but most are direct mappings from the words to the class. In some cases, such
as sentiment analysis, we can get better performance by ﬁrst performing inference, to try
to disambiguate the meaning of words. For example, suppose the goal is to determine if a
document is a favorable review of a movie or not. If we encounter the phrase “Brad Pitt was
excellent until the middle of the movie”, the word “excellent” may lead us to think the review is
positive, but clearly the overall sentiment is negative.
One way to tackle such problems is to build a joint model of the form p(ci, yi|θ). (Blei
generated from the topics as follows:
p(ci|qi) = Ber(sigm(wT qi))
Here qi is the empirical topic distribution for document i:
Li
Li

qilkB
yil
qil
πi
xi
N
W
(a)
B
yil
qil
πi
xi
N
W i
μ
Σ
(b)
B
yil
qil
πi
αi
xi
N
W
(c)
a node that πi a deterministic function of its parents. (b) Mixture of experts with random effects. (c)
We can ﬁt this model using Monte Carlo EM: run the collapsed Gibbs sampler in the E step, to
compute E [qik], and then use this as the input feature to a standard logistic regression package.
Discriminative supervised LDA
regular LDA is that the topic prior becomes input dependent, as follows:
p(qil|πi, ci = c, θ) = Cat(Acπ)
where Ac is a K × K stochastic matrix.
So far, we have assumed the “side information” is a single categorical variable ci. Often we
The idea is that yil represent correlated tags or labels, which we want to predict given xi. We
now discuss several attempts to extend LDA so that it can generate tags given the inputs.
This is just like LDA except we replace the Dirichlet prior on πi with a deterministic function
of the input:
πi
=
S(Wxi)
the deterministic πi we have
p(qil|xi, W) = Cat(S(Wxi))
First ﬁt an unsupervised LDA model based only on yi; then treat the inferred πi as data, andﬁt a multinomial logistic regression model mapping xi to πi. Although this is fast, ﬁtting LDA
in an unsupervised fashion does not necessarily result in a discriminative set of latent variables,
There is a more subtle problem with this model. Since πi is a deterministic function of the
inputs, it is effectively observed, rendering the qil (and hence the tags yil) independent. In other
words,
p(yi|xi, θ) =
Li

p(yil|xi, θ) =
Li


k
p(yil|qil = k, B)p(qil = k|xi, W)
This means that if we observe the value of one tag, it will have no inﬂuence on any of the
over predicting each tag independently.
One way to induce correlations is to make W a random variable. The resulting model is
the correlated topic model. It is possible to extend this model by adding Markovian dynamics
A closely related approach, known as Dirichlet multinomial regression LDA (Mimno and
α a function of the input
αi
=
exp(Wxi)
where W is a K × D matrix. Eliminating the deterministic αi we have
πi ∼Dir(exp(Wxi))
A variant of this model, where xi corresponds to a bag of discrete labels and πi ∼Dir(α ⊙
correspondence with the latent topics, which makes the resulting topics much more interpretable.
model as special cases.
Discriminative categorical PCA
An alternative to using LDA is to expand the categorical PCA model with inputs, as shown in
p(zi|xi, V)
=
N(Vxi, Σ)
p(yi|zi, W)
=

l
Cat(yil|S(Wzi))
This model is essentially a probabilistic neural network with one hidden layer, as shown inyil
zi
xi
N
W
V
(a)
W
yi1
. . .
. . .
yiLi
zi1
. . .
ziK
xi1
. . .
. . .
xiD
N
v1
vK
(b)
(a) Categorical PCA with inputs and exchangeable outputs. (b) Same as (a), but with the
vector nodes expanded out.
key difference from a neural net is that information can ﬂow between the yil’s via the latent
bottleneck layer zi. This should work better than a conventional neural net when the output
labels are highly correlated, even after conditioning on the features; this problem frequently
arises in multi label classiﬁcation. Note that we could allow a direct xi to yi arc, but this would
If we use this model for regression, rather than classiﬁcation, we can perform the E step exactly,
converges faster than standard backpropagation.
dependent means. If the output is Gaussian, this corresponds to a mixture of discriminative
an (as yet unpublished) model, which we could call “discriminative mixtures of categorical factor
analyzers”.
Another source of discrete data is when modeling graph or network structures.
To see the
connection, recall that any graph on D nodes can be represented as a D × D adjacency
interaction networks, or patterns of disease transmission between people or animals. There are
usually two primary goals when analysing such data: ﬁrst, try to discover some “interesting(a)
(b)
the block structure more apparent.
(a)
R
z
η
(b)
permuted to show the block structure. We also sketch of how the stochastic block model can generate this
structure” in the graph, such as clusters or communities; second, try to predict which links
might occur in the future (e.g., who will make friends with whom). Below we summarize some
models that have been proposed for these tasks, some of which are related to LDA. Futher details
therein.
Stochastic block model
if we look more deeply, we see it is possible to partition the nodes into three groups or blocks,A
D
C
B
A
D
C
B
A
E
C D
A B
F G H
A
B
C
D
E
G
F
H
C
D
D
C
B
A
D
C
B
Relational system
Sorted matrix
E
D
C
B
A
E
D
C
B
A
B
D
A
E
C
E
G
H
F
B
A
C
A
D
B
C
D
B
A
Some examples of graphs generated using the stochastic block model with different kinds
of connectivity patterns between the blocks.
The abstract graph (between blocks) represent a ring, a
matrix for the graph with the nodes in their permtuted ordering. It is clear that there is block
structure.
We can make a generative model of block structured graphs as follows.
First, for every
node, sample a latent block qi ∼Cat(π), where πk is the probability of choosing block k, for
groups; let us denote this probability by ηa,b. This can come from a beta prior. Finally, generate
each edge Rij using the following model:
p(Rij = r|qi = a, qj = b, η) = Ber(r|ηa,b)
nodes in our example.
Note that this is quite different from a conventional clustering problem. For example, we
for generating many different kinds of graph structure. For example, some social networks have
hierarchical structure, which can be modeled by clustering people into different social strata,
whereas others consist of a set of cliques.
Unlike a standard mixture model, it is not possible to ﬁt this model using exact EM, because
all the latent qi variables become correlated. However, one can use variational EM (Airoldi et al.qi
ηa,b
Ri,j
I
π
qj
J
(a)
ηa,b
Ri,j
I
πi
qi←j
πj
α
J
(b)
(a) Stochastic block model. (b) Mixed membership stochastic block model.
to the LDA case).
If we have features associated with each node, we can make a discriminative version of this
model, for example by deﬁning
p(Rij = r|qi = a, qj = b, xi, xj, θ) = Ber(r|wT
a,bf(xi, xj))
where f(xi, xj) is some way of combining the feature vectors. For example, we could use
concatenation, [xi, xj], or elementwise product xi ⊗xj as in supervised LDA. The overall
model is like a relational extension of the mixture of experts model.
Mixed membership stochastic block model
stochastic block model, and is similar in spirit to fuzzy clustering or soft clustering. Note
that πik is not the same as p(zi = k|D); the former represents ontological uncertainty (to
what degree does each object belong to a cluster) wheras the latter represents epistemological
uncertainty (which cluster does an object belong to). If we want to combine epistemological
and ontological uncertainty, we can compute p(πi|D).
In more detail, the generative process is as follows. First, each node picks a distribution over
blocks, πi ∼Dir(α). Second, choose the probability of connecting group a to group b, for all
pairs of groups, ηa,b ∼β(α, β). Third, for each edge, sample two discrete variables, one for
each direction:
Finally, generate each edge Rij using the following model:(a)
Outcasts
Loyal
Opposition 
Young
Turks
Waverers
15Ramuald
(b)
Unlike the regular stochastic block model, each node can play a different role, depending on
who it is connecting to. As an illustration of this, we will consider a data set that is widely used
most of the monk belong to one of the three clusters, known as the “young turks”, the “outcasts”
Sampson called these monks the “waverers”. It is interesting to see that the model can recover
the same kinds of insights as Sampson derived by hand.
it may be due to the fact that person i and j have not had an opportunity to interact, or
that data is not available for that interaction, as opposed to the fact that these people don’t
want to interact. In other words, absence of evidence is not evidence of absence. We can model
In other words, we robustify the observation model to allow for outliers, as follows:
Relational topic model
In many cases, the nodes in our network have atttributes. For example, if the nodes represent
academic papers, and the edges represent citations, then the attributes include the text of the
document itself. It is therefore desirable to create a model that can explain the text and the link
structure concurrently. Such a model can predict links given text, or even vice versa.yil
qil
πi
¯qi
Rij
I
¯qj
qjl
yjl
πj
J
α
B
w
DGM for the relational topic model.
represents whether there is an edge between nodes i and j) is modeled as follows:
Recall that qi is the empirical topic distribution for document i, qik ≜
Li
Li
See
Note that it is important that Rij depend on the actual topics chosen, qi and qj, and not
on the topic distributions, πi and πj, otherwise predictive performance is not as good. The
intuitive reason for this is as follows: if Rij is a child of πi and πj, it will be treated as just
another word, similar to the qil’s and yil’s; but since there are many more words than edges,
the graph structure information will get “washed out”. By making Rij a child of qi and qj, the
graph information can inﬂuence the choice of topics more directly.
The method does better at predicting missing links than the simpler approach of ﬁrst ﬁtting an
LDA model, and then using the qi’s as inputs to a logistic regression problem. The reason is
namely that the RTM learns a latent space that is forced to be predictive of the graph structure
and words, whereas LDA might learn a latent space that is not useful for predicting the graph.
LVMs for relational data
Graphs can be used to represent data which represents the relation amongst variables of a
certain type, e.g., friendship relationships between people. But often we have multiple types of
between people and people, and one between people and movies.sex are attributes (unary functions) of the people class.
where Ti are sets or types. A binary, pairwise or dyadic relation is a relation deﬁned on pairs
of objects. For example, the seen relation between people and movies might be represented as
the set of movies that people have seen. We can either represent this explicitly as a set, such as
seen
= { (Bob, StarWars), (Bob, TombRaider), (Alice, Jaws)}
or implicitly, using an indicator function for the set:
but which can still be modelled by relations. For example, we might have a ternary relation,
when chemical k is present. This can be modelled by a 3d binary matrix. We will give some
Making probabilistic models of relational data is called statistical relational learning (Getoor
graphical models; this is known as probabilistic relational modeling. Another approach is to
use latent variable models, as we discuss below.
Inﬁnite relational model
It is straightforward to extend the stochastic block model to model relational data: we just
associate a latent variable qt
the probability of the relation holding between speciﬁc entities by looking up the probability of
we have
i = a, q1
j = b, q2
k = c, η) = ηa,b,c	

			
	

	



		

		
	






	


	
causes,
result of
affects,
complicates
disrupts
result of, manifestation of
affects, process of,
affects,
process of
associated with
causes
manifestation of
affects
affects,
process of,
result of
affects,
result of
complicates,
manifestation of
interact with
process of,
result of
affects,
process of
causes,
manifestation of,
associated with
complicates
affects,
causes
manifestation of,
complicates
affects,
Illustration of an ontology learned by IRM applied to the Uniﬁed Medical Language System.
identical model, under the name inﬁnite hidden relational model (IHRM), was concurrently
sketch some interesting applications.
Learning ontologies
An ontology refers to an organisation of knowledge. In AI, ontologies are often built by hand
set of binary predicates. The result is a 3d cube. We can then apply the IRM to partition the
function cluster, b represents the organism cluster, and c represents the affects cluster).
Clustering based on relations and features
We can also use IRM to cluster objects based on their relations and their features. For example,joint
membership
of IGOs
joint
membership
of NGOs
negative
behavior
negative
communications
accusations
protests
treaties
conferences
common bloc
membership
economic
aid
emigration
military
alliance
sends
tourists to
exports
books to
exports to
Brazil
Netherlands
UK
USA
Burma
Indonesia
Jordan
Egypt
India
Israel
China
Cuba
Poland
USSR
noncommunist
western bloc
constitutional govt
free elections
communist bloc
communists
totalitarian
elitist
high censorship
no free elections
illiteracy
domestic violence
purges
far from US
rainfall
religious books
govt education $
military personnel
seaborne goods
govt crisis
UN delinquent
neutral bloc
assassinations
govt revolution
num religions
intervening military
some censorship
energy consumed
telephone
population
defense $
threats
GNP
protests
Catholics
US aid taken
popn. density
land area
railroad length
foreign students
age of country
law NGOs
num languages
aid $ taken
female workers
foreign mail sent
protein in diet
investments
arts NGOs
monarchy
road length
arable
emigrants
unemployed
calories in diet
a)
c)
d)
e)
g)
h)
i)
b)
book
translations
f)
of Charles Kemp.
predicates representing interaction types between countries (e.g., “sends tourists to”, “economic
therefore combines aspects of both the biclustering model and the ontology discovery model.)
When given multiple relations, the IRM treats them as conditionally independent. In this case,
we have
p(R1, R2, q1, q2, q3|θ) = p(R1|q1, q2, θ)p(R2|q1, q3, θ)
ﬁrst of which contains “noncommunist”, which captures one of the most important aspects of
clusters, reﬂecting similar relationships (e.g., “negative behavior and “accusations”).Probabilistic matrix factorization for collaborative ﬁltering
we see that CF is a kind of relational learning problem (and one with particular commercial
importance).
Much of the work in this area makes use of the data that Netﬂix made available in their
observed, so the matrix is extremely sparse. In addition the data is quite imbalanced, with many
pairs, for which the ranking is known but withheld from contestants. The performance measure
is root mean square error:
RMSE =
:
:
N
N

where X(mi, ui) is the true rating of user ui on movie mi, and ˆX(mi, ui) is the prediction.
the winning team below.
observation model. However, this does not capture the fact that the ratings are ordered. Although
we could use an ordinal observation model, in practice people use a Gaussian observation model
for simplicity. One way to make the model better match the data is to pass the model’s predicted
transforming the data using Rij =
"
We could use the IRM for the CF task, by associating a discrete latent variable for each user
qu
i and for each movie or video qv
j , and then deﬁning
p(Rij = r|qu
i = a, qv
j = b, θ) = N(r|μa,b, σ2)
Another possibility is to replace the discrete latent variables with continuous latent variables
πu
p(Rij = r|ui, vj) = N(r|uT
i vj, σ2)
MovieLens and EachMovie. However, these datasets are much easier to predict, because there is less imbalance betweenVisualization of a small relational dataset, where we have one relation, likes(user, movie),
with kind permission of Zhao Xu.
Rij
ui
μu
Σu
vj
μv
Σv
σ2
(a)
Freddy Got Fingered
Freddy vs. Jason
Half Baked
Road Trip
The Sound of Music
Sophie’s Choice
Moonstruck
Maid in Manhattan
The Way We Were
Runaway Bride
Coyote Ugly
The Royal Tenenbaums
Punch−Drunk Love
I Heart Huckabees
Armageddon
Citizen Kane
Stepmom
Julien Donkey−Boy
Sister Act
The Fast and the Furious
The Wizard of Oz
Scarface
Natural Born Killers
Annie Hall
Belle de Jour
Lost in Translation
The Longest Yard
Being John Malkovich
Catwoman
(b)
(a) A DGM for probabilistic matrix factorization. (b) Visualization of the ﬁrst two factors in
the PMF model estimated from the Netﬂix challenge data. Each movie j is plotted at the location speciﬁed
right we have more serious dramas (Sophie’s Choice, Moonstruck). On the top we have critically acclaimed
Hollywood blockbusters (Armageddon, Runway Bride). The Wizard of Oz is right in the middle of these axes.
user is close to a movie in that space, they are likely to rate it highly. All of the best entries in
PMF is closely related to the SVD. In particular, if there is no missing data, then computing
the MLE for the ui’s and the vj’s is equivalent to ﬁnding a rank K approximation to R.Epochs
RMSE
PMF
Constrained 
PMF
Netflix 
Baseline Score
SVD
(a)
Millions of Parameters
RMSE
Plain
(b)
(a) RMSE on the validation set for different PMF variants vs number of passes through
version where the mean and diagonal covariance of the Gaussian prior were learned from data. From
on the test set (quiz portion) vs number of parameters for several different models. “Plain” is the baseline
“With temporal dynamics” allows the offset terms to change over time. The Netﬂix baseline system achieves
permission of Yehuda Koren.
The most straightforward way to ﬁt the PMF model is to minimize the overall NLL:
J(U, V) = −log p(R|U, V, O) = −log
⎛
⎝
N

M


N(Rij|uT
i vj, σ2)
⎞
⎠
dJ
dui
=
d
dui

ij

eijvj
where eij = Rij −uT
i vj is the error term. By stochastically sampling a single movie j that user
i has watched, the update takes the following simple form:
ui
=
ui + ηeijvj
where η is the learning rate. The update for vj is similar.We can regularize this by imposing Gaussian priors:
p(U, V) =

i
N(ui|μu, Σu)

j
N(vj|μv, Σv)
UIK, and Σv = σ2
V IK, the new objective becomes
J(U, V)
=
−log p(R, U, V|O, θ)
=

i

j
+λU

i

j
V . By varying the regularizers, we can
stochastic gradient descent. We can also compute approximate posteriors using variational Bayes
If we use diagonal covariances for the priors, we can penalize each latent dimension by a
Optimizing the prior parameters (μu, Σu, μv, Σv) at the same time as the model parameters
(U, V, σ2) is a way to create an adaptive prior. This avoids the need to search for the optimal
users give low scores for all types of movies. We can model this by allowing for user and movie
speciﬁc offset or bias terms as follows:
p(Rij = r|ui, vj, θ) = N(r|uT
i vj + μ + fi + gj, σ2)
i vj is the
interaction term. This is equivalent to applying PMF just to the residual matrix, and gives much
We can also allow the bias terms to evolve over time, to reﬂect the changing preferences of
users (Koren 2009b). This is important since in the Netﬂix competition, the test data was more
help a lot.
Often we also have side information of various kinds. In the Netﬂix competition, entrants
knew which movies the user had rated in the test set, even though they did not know the
values of these ratings. That is, they knew the value of the (dense) O matrix even on the
test set. If a user chooses to rate a movie, it is likely because they have seen it, which in
turns means they thought they would like it. Thus the very act of rating reveals information.
Conversely, if a user chooses not rate a movie, it suggests they knew they would not like it.
is not available. However, we often know which movies the user has watched or declined towatch, even if they did not rate them (this is called implicit feedback), and this can be used as
useful side information.
Another source of side information concerns the content of the movie, such as the movie
genre, the list of the actors, or a synopsis of the plot. This can be denoted by xv, the features
of the video. (In the case where we just have the id of the video, we can treat xv as a |V|-
dimensional bit vector with just one bit turned on.) We may also know features about the user,
which we can denote by xu. In some cases, we only know if the user clicked on the video or
not, that is, we may not have a numerical rating. We can then modify the model as follows:
p(R(u, v)|xu, xv, θ) = Ber(R(u, v)|(Uxu)T (Vxv))
where U is a |U| × K matrix, and V is a |V| × K matrix (we can incorporate an offset term
posterior p(U, V|D) in an online fashion, using ADF and EP, was described in (Stern et al.
on all the ads used by Bing.
Unfortunately, ﬁtting this model just from positive binary data can result in an over prediction
of links, since no negative examples are included. Better performance is obtained if one has
access to the set of all videos shown to the user, of which at most one was picked; data of this
form is known as an impression log. In this case, we can use a multinomial model instead of
To understand why, suppose some is presented with a choice of an action movie starring Arnold
Schwarzenegger, an action movie starring Vin Diesel, and a comedy starring Hugh Grant. If
the user picks Arnold Schwarzenegger, we learn not only that they like prefer action movies to
comedies, but also that they prefer Schwarzenegger to Diesel. This is more informative than just
knowing that they like Schwarzenegger and action movies.
Restricted Boltzmann machines (RBMs)
So far, all the models we have proposed in this chapter have been representable by directed
graphical models. But some models are better represented using undirected graphs. For example,
exact inference is intractable, and even approximate inference, using e.g., Gibbs sampling, can
be slow. However, suppose we restrict the architecture so that the nodes are arranged in layers,
Then the model has the form
p(h, v|θ) =
Z(θ)
R

K

ψrk(vr, hk)
where R is the number of visible (response) variables, K is the number of hidden variables, and
v plays the role of y earlier in this chapter. This model is known as a restricted Boltzmann
because we are multiplying together a set of “experts” (here, potential functions on each edge)(a)
H
V
(b)
(a) A general Boltzmann machine, with an arbitrary graph structure. The shaded (visible)
nodes are partitioned into input and output, although the model is actually symmetric and deﬁnes a joint
density on all the nodes. (b) A restricted Boltzmann machine with a bipartite structure. Note the lack of
and then normalizing, whereas in a mixture of experts, we take a convex combination of
normalized distributions. The intuitive reason why PoE models might work better than a mixture
in different ways we can create “sharp” distributions which predict data which satisﬁes the
A given document might have the topics “government”, “maﬁa” and “playboy”. If we “multiply”
the predictions of each topic together, the model may give very high probability to the word
Typically the hidden nodes in an RBM are binary, so h speciﬁes which constraints are active.
It is worth comparing this with the directed models we have discussed. In a mixture model, we
restriction that exactly one bit is on at a time. This is called a localist encoding, since only
one hidden unit is used to generate the response vector. This is analogous to the hypothetical
notion of grandmother cells in the brain, that are able to recognize only one kind of object.
By contrast, an RBM uses a distributed encoding, where many units are involved in generating
variables are conditionally independent given the visible variables, so the posterior factorizes:
p(h|v, θ) =

k
p(hk|v, θ)
This makes inference much simpler than in a directed model, since we can estimate each hkVisible
Hidden
Name
Reference
Binary
Binary
Binary RBM
Gaussian
Binary
Gaussian RBM
Categorical
Binary
Categorical RBM
Multiple categorical
Binary
Gaussian
Gaussian
Undirected PCA
Binary
Gaussian
Undirected binary PCA
Summary of different kinds of RBM.
independently and in parallel, as in a feedforward neural network. The disadvantage is that
training undirected models is much harder, as we discuss below.
Varieties of RBMs
In this section, we describe various forms of RBMs, by deﬁning different pairwise potential
Binary RBMs
The most common form of RBM has binary hidden nodes and binary visible nodes. The joint
distribution then has the following form:
p(v, h|θ)
=
Z(θ) exp(−E(v, h; θ))
E(v, h; θ)
≜
−
R

K

vrhkWrk −
R

vrbr −
K

hkck
=
−(vT Wh + vT b + hT c)
Z(θ)
=

v

h
exp(−E(v, h; θ))
where E is the energy function, W is a R×K weight matrix, b are the visible bias terms, c are
When using a binary RBM, the posterior can be computed as follows:
p(h|v, θ)
=
K

p(hk|v, θ) =

k
Ber(hk|sigm(wT
:,kv))
By symmetry, one can show that we can generate data given the hidden variables as follows:
p(v|h, θ) =

r
p(vr|h, θ) =

r
Ber(vr|sigm(wT
r,:h))=
sigm(WT v)
E [v|h, θ]
=
sigm(Wh)
The weights in W are called the generative weights, since they are used to generate the
observations, and the weights in WT are called the recognition weights, since they are used
to recognize the input.
input vector v “looks like” the weight vector w:,k (up to scaling factors). Thus each hidden node
captures certain features of the input, as encoded in its weight vector, similar to a feedforward
neural network.
Categorical RBM
where C is the number of states for each vir. We deﬁne a new energy function as follows
E(v, h; θ)
≜
−
R

K

C

vc
rhkW c
rk −
R

C

vc
rbc
r −
K

hkck
The full conditionals are given by
p(vr|h, θ)
=
Cat(S({bc
r +

k
hkW c
rk}C
=
sigm(ck +

r

c
vc
rW c
rk)
Gaussian RBM
following energy function:
E(v, h|θ) = −
R

K

R

K

akhk
2vT Λv)
where η = Λμ. We see that we have set Λ = I, and η = 
k hkw:,k. Thus the mean is
k hkw:,k. The full conditionals, which are needed for inference andlearning, are given by
p(vr|h, θ)
=
N(vr|br +

k
=
sigm

ck +

r
wrkvr


We see that each visible unit has a Gaussian distribution whose mean is a function of the
hidden bit vector. More powerful models, which make the (co)variance depend on the hidden
RBMs with Gaussian hidden units
If we use Gaussian latent variables and Gaussian visible variables, we get an undirected version
of factor analysis. However, it turns out that it is identical to the standard directed version
If we use Gaussian latent variables and categorical observed variables, we get an undirected
Netﬂix collaborative ﬁltering problem, but was found to be signiﬁcantly inferior to using binary
latent variables, which have more expressive power.
Learning RBMs
In this section, we discuss some ways to compute ML parameter estimates of RBMs, using
have many parameters and therefore need to be trained on very large datasets. In addition, it is
standard to use ℓ2 regularization, a technique that is often called weight decay in this context.
Deriving the gradient using p(h, v|θ)
ﬁt a generic latent variable maxent model. In the context of the Boltzmann machine, we have
one feature per edge, so the gradient is given by
∂ℓ
∂wrk
N
N

E [vrhk|vi, θ] −E [vrhk|θ]
∇w ℓ= Epemp(·|θ)

vhT 
−Ep(·|θ)

vhT 
where pemp(v, h|θ) ≜p(h|v, θ)pemp(v), and pemp(v) =
N
N
distribution.
The ﬁrst term on the gradient, when v is ﬁxed to a data case, is sometimes called the
clamped phase, and the second term, when v is free, is sometimes called the unclampedphase. When the model expectations match the empirical expectations, the two terms cancel
out, the gradient becomes zero and learning stops. This algorithm was ﬁrst proposed in (Ackley
to do this below.
Deriving the gradient using p(v|θ)
based models. First we marginalize out the hidden variables and write the RBM in the form
p(v|θ) =
Z(θ) exp(−F(v; θ)), where F(v; θ) is the free energy:
F(v)
≜

h
E(v, h) =

h
exp

 R

K

vrhkWrk


=

h
K

exp

 R

vrhkWrk


=
K


exp

 R

vrhrWrk


=
K


R

vrWrk)


N
N

N
N

F(vi|θ) −log Z(θ)
Using the fact that Z(θ) = 
v exp(−F(v; θ)) we have
∇ℓ(θ)
=
N
N

∇F(vi) −∇Z
Z
=
N
N

∇F(vi) +

v
∇F(v)exp(−F(v))
Z
=
N
N

∇F(vi) + E [∇F(v)]
∂
∂wrk
F(v) = −vrE [hk|v, θ] = −E [vrhk|v, θ]
Hence
∂
∂wrk
N
N

E [vrhk|v, θ] −E [vrhk|θ]+M
;L


LQILQLW\
GDWD
VWHS
UHFRQVWUXFWLRQV
HTXLOLEULXP
VDPSOHV
Illustration of Gibbs sampling in an RBM. The visible nodes are initialized at a datavector,
then we sample a hidden vector, then another visible vector, etc.
Eventually (at “inﬁnity”) we will be
producing samples from the joint distribution p(v, h|θ).
Approximating the expectations
We can approximate the expectations needed to evaluate the gradient by performing block
joint distribution p(v, h|θ) as follows: initialize the chain at vv1 (e.g. by setting v1 = vi for
some data vector), and then sample from h1 ∼p(h|v1), then from v2 ∼p(v|h1), then from
the Markov chain reaches equilibrium (i.e., until it has “burned in”) before we can interpret the
samples as coming from the joint distribution of interest, and this might take a long time.
A faster alternative is to use mean ﬁeld, where we make the approximation E [vrhk] ≈
more subtle reason not to use mean ﬁeld: since the gradient has the form E [vrhk|v]−E [vrhk],
we see that the negative sign in front means that the method will try to make the variational
Contrastive divergence
The problem with using Gibbs sampling to compute the gradient is that it is slow. We now
CD was
originally derived by approximating an objective function deﬁned as the difference of two KL
divergences, rather than trying to maximize the likelihood itself. However, from an algorithmic
mates the “unclamped” expectations with “brief” Gibbs sampling where we initialize each Markov
chain at the data vectors. That is, we approximate the gradient for one datavector as follows:
∇w ℓ≈E

vhT |vi

−Eq

vhT follows:
hi ∼p(h|vi, θ)
v′
i ∼p(v|hi, θ)
h′
i ∼p(h|v′
i, θ)
We then make the approximation
Eq

vhT 
≈vi(h′
i)T
Such samples are sometimes called fantasy data.
We can think of v′
i as the model’s best
attempt at reconstructing vi after being coded and then decoded by the model. This is similar
In practice, it is common to use E [h|v′
i] instead of a sampled value h′
i in the ﬁnal upwards
pass, since this reduces the variance. However, it is not valid to use E [h|vi] instead of sampling
hi ∼p(h|vi) in the earlier upwards passes, because then each hidden unit would be able to
since we are maximizing likelihood.) Various tricks can be used to speed this algorithm up, such
for each minibatch of size B do
for each case vi in the minibatch do
Sample hi ∼p(h|vi, W);
Sample v′
i ∼p(v|hi, W);
i = E [h|v′
i, W];
Compute gradient ∇W = (vi)(μi)T −(v′
i)(μ′
i)T ;
Accumulate g := g + ∇W;
Persistent CD
ﬁtting maxent models. This avoids the need to run MCMC to convergence at each iteration,by exploiting the fact that the parameters are changing slowly, so the Markov chains will not
two dynamical processes running at different time scales: the states change quickly, and the
// Mean ﬁeld updates ;
μik = sigm(vT
i w:,k)
// MCMC updates ;
Generate (vs, hs) by brief Gibbs sampling from old (vs, hs)
// Parameter updates ;
N
N
S
S
W := W + αtg;
Decrease αt
Applications of RBMs
The main application of RBMs is as a building block for deep generative models, which we
They are particularly useful in cases where inference of the hidden states at test time must be
fast. We give some examples below.
Language modeling and document retrieval
to LDA. One subtlety is that the partition function in an undirected models depends on how
big the graph is, and therefore on how long the document is. A solution to this was proposed
hidden activation bias terms ck by the document length L to compensate form the fact that the
E(v, h; θ)
≜
−
K

C

vchkW c
k −
C

vcbc
r −L
K

hkckData set
Number of docs
K
¯D
St. Dev.
Avg. Test perplexity per word (in nats)
Train
Test
Unigram
NIPS
Reuters
Comparison of RBM (replicated softmax) and LDA on three corpora. K is the number of
words in the vocabulary, D is the average document length, and St. Dev. is the standard deviation of the
document length.
Recall (%) 
Precision (%)
Replicated 
Recall (%) 
Precision (%)
Replicated 
Reuters
where vc = L
the r subscript) with C states, where C is the number of words in the vocabulary. This is
We can compare the modeling power of RBMs vs LDA by measuring the perplexity on a test
but that an RBM is signiﬁcantly better than LDA.
for RBMs and LDA on two different corpora. These curves were generated as follows: a query
document from the test set is taken, its similarity to all the training documents is computed,
where the similarity is deﬁned as the cosine of the angle between the two topic vectors, and
then the top M documents are returned for varying M. A retrieved document is considered
relevant if it has the same class label as that of the query’s (this is the only place where labels
are used).RBMs for collaborative ﬁltering
RBMs have been applied to the Netﬂix collaborative ﬁltering competition (Salakhutdinov et al.
outperform SVD. By combining the two methods, performance can be further improved. (The
winning entry in the challenge was an ensemble of many different types of model (Koren 2009a).)
Exercises
Show how to compute Z(θ) for an RBM with K binary hidden nodes and R binary observed nodes in
O(R2K) time, assuming K < R.Deep learning
Introduction
However, when we look at the brain, we seem many levels of processing. It is believed that each
level is learning features or representations at increasing levels of abstraction. For example, the
the brain might perform vision.)
This observation has inspired a recent trend in machine learning known as deep learning
problems as well, such as speech and language.)
In this chapter, we give a brief overview of this new ﬁeld. However, we caution the reader
that the topic of deep learning is currently evolving very quickly, so the material in this chapter
may soon be outdated.
Deep generative models
Deep models often have millions of parameters. Acquiring enough labeled data to train such
models is diffcult, despite crowd sourcing sites such as Mechanical Turk. In simple settings, such
To overcome the problem of needing labeled training data, we will focus on unsupervised
learning. The most natural way to perform this is to use generative models. In this section, we
discuss three different kinds of deep generative models: directed, undirected, and mixed.
complex scenes, and then to use this as training data for computer vision systems. However, often graphics programs
images.(a)
(b)
(c)
belief net).
Deep directed networks
Perhaps the most natural way to build a deep generative model is to construct a deep directed
notational simplicity. The number and size of layers is usually chosen by hand, although one
to infer the model structure.
We shall call models of this form deep directed networks or DDNs. If all the nodes are
this case, the model deﬁnes the following joint distribution:
p(h1, h2, h3, v|θ)
=

i
Ber(vi|sigm(hT

j
Ber(h1j|sigm(hT

k
Ber(h2k|sigm(hT

l
Ber(h3l|w3l)
Unfortunately, inference in directed models such as these is intractable because the posterior
since they approximate the correlated posterior with a factorial posterior. One can also use
are highly correlated. Slow inference also results in slow learning.
Deep Boltzmann machines
A natural alternative to a directed model is to construct a deep undirected model. For example,
layers, the model is deﬁned as follows:
p(h1, h2, h3, v|θ) =
Z(θ) exp
⎛
⎝
ij
vih1jW1ij +

jk
h1jh2jW2jk +

kl
h2kh3lW3kl
⎞where we are ignoring constant offset or bias terms.
wise) Gibbs sampling, or block mean ﬁeld, since all the nodes in each layer are conditionally
independent of each other given the layers above and below (Salakhutdinov and Larochelle
undirected models.
Deep belief networks
An interesting compromise is to use a model that is partially directed and partially undirected.
In particular, suppose we construct a layered model which has directed arrows, except at the
model is deﬁned as follows:
p(h1, h2, h3, v|θ)
=

i
Ber(vi|sigm(hT

j
Ber(h1j|sigm(hT
Z(θ) exp


kl
h2kh3lW3kl


Essentially the top two layers act as an associative memory, and the remaining layers then
generate the output.
The advantage of this peculiar architecture is that we can infer the hidden states in a
To see why, suppose we only have two hidden layers, and that
W2 = WT
This deﬁnes a model of the form p(h1, h2, v|W1).
One can show that the distribution
p(h1, v|W1) = 
h2 p(h1, h2, v|W1) has the form p(h1, v|W1) =
Z(W1) exp(vT W1h1),
which is equivalent to an RBM. Since the DBN is equivalent to the RBM as far as p(h1, v|W1)
is concerned, we can infer the posterior p(h1|v, W1) in the DBN exactly as in the RBM. This
posterior is exact, even though it is fully factorized.
Now the only way to get a factored posterior is if the prior p(h1|W1) is a complementary
prior. This is a prior which, when multiplied by the likelihood p(v|h1), results in a perfectly
factored posterior. Thus we see that the top level RBM in a DBN acts as a complementary prior
for the bottom level directed sigmoidal likelihood function.
between the DBN and the RBM does not hold exactly any more, but we can still use the factored
so DBNs are usually only used in a feedforward manner.
deep belief networks, has suggested the acronyms DeeBNs and DyBNs for these two different meanings. However, this(a)
(b)
(c)
(a) A DBN with two hidden layers and tied weights that is equivalent to an RBM. Source:
(b) A stack of RBMs trained greedily. (c) The corresponding DBN.
Used with kind permission of Ruslan Salakhutdinov.
The equivalence between DBNs and RBMs suggests the following strategy for learning a DBN.
•
•
directed weights W1 and let W2 be “untied” so it is no longer forced to be equal to WT
We will now learn a better prior for p(h1|W2) by ﬁtting a second RBM. The input data to
this new RBM is the activation of the hidden units E [h1|v, W1] which can be computed
using a factorial approximation.
•
Continue to add more hidden layers until some stopping criterion is satisiﬁed, e.g., you run
out of time or memory, or you start to overﬁt the validation set. Construct the DBN from
observed data likelihood.
Of course this procedure might result in overﬁtting, but that is a
different matter.
In practice, we want to be able to use any number of hidden units in each level. This means
we will not be able to initialize the weights so that Wℓ= WT
guarantee. Nevertheless the method works well in practice, as we will see. The method can also
using a technique called backﬁtting. This works as follows. Perform an upwards sampling pass
to the top. Then perform brief Gibbs sampling in the top level RBM, and perform a CD update
of the RBM parameters. Finally, perform a downwards ancestral sampling pass (which is an
approximate sample from the posterior), and update the logistic CPD parameters using a small
procedure is very slow.Deep neural networks
acting like neural networks. In view of this, it is natural to dispense with the generative story
and try to ﬁt deep neural networks directly, as we discuss below. The resulting training methods
are often simpler to implement, and can be faster.
Note, however, that performance with deep neural nets is sometimes not as good as with
of the signal.
It is interesting to note that in the mammalian visual cortex, there are many more feedback
The role of these feedback connections is not precisely understood, but they presumably provide
contextual prior information (e.g., coming from the previous “frame” or retinal glance) which
the models we discuss below do not do this.
Many decision problems can be reduced to classiﬁcation, e.g., predict which object (if any) is
present in an image patch, or predict which phoneme is present in a given acoustic feature
Unfortunately, this method does not work very well. One problem is that the gradient becomes
weaker the further we move away from the data; this is known as the “vanishing gradient”
Consequently early attempts to learn deep neural networks proved unsuccesful. Recently there
Below we discuss a way to initialize the parameters using unsupervised learning; this is called
than just predicting a scalar response.

 




 
 
 

 
 
 













	
	


	




	
	



	































	
























!"#
$



%
Training a deep autoencoder. (a) First we greedily train some RBMs. (b) Then we construct
that is trained to predict the input itself. To prevent the system from learning the trivial identity
mapping, the hidden layer in the middle is usually constrained to be a narrow bottleneck. The
system can minimize the reconstruction error by ensuring the hidden units capture the most
relevant aspects of the data.
suppose all the functions are linear. In this case, one can show that the weights to the K
hidden units will span the same subspace as the ﬁrst K principal components of the data
equivalent to PCA. However, by using nonlinear activation functions, one can discover nonlinear
representations of the data.
becomes too small as it passes back through multiple layers, and the learning algorithm often
gets stuck in poor local minima.
One solution to this problem is to greedily train a series of RBMs and to use these to initialize
backprop in the usual fashion. This approach, ﬁrst suggested in (Hinton and Salakhutdinovpixel 
image
This could be the 
top level of 
another sensory 
pathway
(a)
(b)
(a) A DBN architecture for classifying MNIST digits.
test cases of MNIST. Above each image is the estimated label.
weights.
visible layer. This prevents the model from learning the identity function. But there are other
One approach is to impose sparsity constraints on the activation of the hidden units (Ranzato
by setting them to zero, so the model has to learn to predict the missing entries. This can be
shown to be equivalent to a certain approximate form of maximum likelihood training (known
Of course, we can stack these models on top of each other to learn a deep stacked denoising
if desired.
Applications of deep networks
In this section, we mention a few applications of the models we have been discussing.
Handwritten digit classiﬁcation using DBNs
layer corresponds to binary images of handwritten digits from the MNIST data set. In addition,(a)
Leading          
Economic         
Indicators       
European Community 
Earnings 
Interbank Markets
Government 
Borrowings 
Disasters and 
Accidents     
Energy Markets
(b)
Used with kind permission of Ruslan Salakhutdinov.
layer was trained using as input the activations of the lower hidden layer, as well as the class
the input is an image. Generic methods work just as well on permuted versions of the input
Deep autoencoders can learn informative features from raw data. Such features are often used
as input to standard supervised learning methods.



	


	


	





















!

!








"
#

$
Used with kind permission of Ruslan Salakhutdinov.
This
enables very fast retrieval of related documents.
mapping codewords to documents. This approach is known as semantic hashing, since the
binary representation of semantically similar documents will be close in Hamming distance.
entry in the table. At test time, we compute the codeword for the query, and then simply retrieve
the relevant documents in constant time by looking up the contents of the relevant address in
memory. To ﬁnd other other related documents, we can compute all the codewords within ax1
x2
x3
x4
h1
h1
h1
h2
h2
h2
w1
w1
w1
w2
w2
w2
A small 1d convolutional RBM with two groups of hidden units, each associated with a ﬁlter
computed using the ﬁlter w1, the second view using ﬁlter w2. Similarly, h1
data in the second window, (x2, x3), computed using w1 and w2 respectively.
The key point is that the total time is independent of the size of the corpus.
Of course, there are other techniques for fast document retrieval, such as inverted indices.
These rely on the fact that individual words are quite informative, so we can simply intersect all
the documents that contain each word. However, when performing image retrieval, it is clear that
a deep autoencoder could learn a good semantic hashing function that outperformed previous
vector quantizing image patches).
Learning audio features using 1d convolutional DBNs
To apply DBNs to time series of unbounded length, it is necessary to use some form of parameter
The hidden activation vector for each group is computed by convolving the input vector with
that group’s ﬁlter (weight vector or matrix). In other words, each node within a hidden group
is a weighted combination of a subset of the inputs.
We compute the activaton of all the
hidden nodes by “sliding” this weight vector over the input. This allows us to model translation
invariance, since we use the same weights no matter where in the input vector the pattern

k

“nuisance variables” may be the variables of interest. For example if the task is to determine if a photograph was taken
in the morning or the evening, then lighting is one of the more salient features, and object identity may be less relevant.
As always, one task’s “signal” is another task’s “noise”, so it unwise to “throw away” apparently irrelevant informationMore formally, for binary 1d signals, we can deﬁne the full conditionals in a convolutional
p(hk
=
sigm((wk ⊗v)t + bt)
=
sigm(

k
(wk ⊗hk)s + cs)
where wk is the weight vector for group k, bt and cs are bias terms, and a ⊗b represents the
convolution of vectors a and b.
It is common to add a max pooling layer as well as a convolutional layer, which computes
a local maximum over the ﬁltered response.
This allows for a small amount of translation
ably. Deﬁning this for a neural network is simple, but deﬁning this in a way which allows for
information ﬂow backwards as well as forwards is a bit more involved. The basic idea is similar
much information.
consists of speech signals, the method recovers a representation that is similar to phonemes.
niques using standard features such as MFCC. (All features were fed into the same discriminative
classiﬁer.)
tion. In an interview, the tech lead of this project said “historically, there have been very few
Learning image features using 2d convolutional DBNs
have learned ﬁlters that look like grandmother cells, that are speciﬁc to individual object classes,
and in some cases, to individual objects.
Discussion
too early.(e.g., color planes).
Each input layer is passed through a different set of ﬁlters.
Each hidden unit is
obtained by convolving with the appropriate ﬁlter, and then summing over the input planes. The ﬁnal layer
is obtained by computing the local maximum within a small window.
faces, cars, airplanes, motorbikes
(a)
(b)
Visualization of the ﬁlters learned by a convolutional DBN in layers two and three.
Source:
Used with kind permission of Honglak Lee.
approach scale to more challenging problems, such as scene interpretation or natural language
understanding?
To put the problem in perspective, consider the DBN for handwritten digit classiﬁcation in
As Hinton says,
fMRI scan. This suggests that much bigger networks may be required to compete with
To scale up to more challenging problems, various groups are using GPUs (see e.g., (Raina
higher level of abstraction, where inference is done in the space of objects or their parts, ratherwhere by “symbol” we mean something atomic, that can be combined with other symbols in a
compositional way.
are associated with words in natural language, but it seems unlikely we can jump directly from
of symbolic or atomic parts.
vector quantization. This generates a set of visual words. These can then be modelled using
still quite “shallow”.
It is possible to deﬁne, and learn, deep models which use discrete latent parts. Here we just
combine RBMs with hierarchical latent Dirichlet allocation methods, trained in an unsupervised
machine learning.Notation
Introduction
It is very difficult to come up with a single, consistent notation to cover the wide variety of
data, models and algorithms that we discuss. Furthermore, conventions differ between machine
learning and statistics, and between different books and papers. Nevertheless, we have tried
to be as consistent as possible. Below we summarize most of the notation used in this book,
although individual sections may introduce new notation. Note also that the same symbol may
have different meanings depending on the context, although we try to avoid this where possible.
General math notation
Symbol
Meaning
⌊x⌋
Floor of x, i.e., round down to nearest integer
⌈x⌉
Ceiling of x, i.e., round up to nearest integer
x ⊗y
Convolution of x and y
x ⊙y
Hadamard (elementwise) product of x and y
a ∧b
logical AND
a ∨b
logical OR
¬a
logical NOT
I(x)
∞
Inﬁnity
→
∝
Proportional to, so y = ax can be written as y ∝x
|x|
Absolute value
|S|
Size (cardinality) of a set
n!
Factorial function
∇
Vector of ﬁrst derivatives
Hessian matrix of second derivatives
≜
Deﬁned as
O(·)
R
The real numbers
≈
Approximately equal to
argmaxx f(x)
Argmax: the value x that maximizes fNotation
B(a, b)
Beta function, B(a, b) = Γ(a)Γ(b)
B(α)
Multivariate beta function,

k Γ(αk)
Γ(
k αk)
n
k

n choose k, equal to n!/(k!(n −k)!)
δ(x)
δij
δx(y)
exp(x)
Exponential function ex
Γ(x)
Gamma function, Γ(x) =
 ∞
ux−1e−udu
Ψ(x)
Digamma function, Ψ(x) =
d
dx log Γ(x)
X
A set from which values are drawn (e.g., X = RD)
Linear algebra notation
We use boldface lowercase to denote vectors, such as a, and boldface uppercase to denote
matrices, such as A. Vectors are assumed to be column vectors, unless noted otherwise.
Symbol
Meaning
A is a positive deﬁnite matrix
tr(A)
Trace of a matrix
det(A)
Determinant of matrix A
|A|
Determinant of matrix A
Inverse of a matrix
A†
AT
Transpose of a matrix
aT
Transpose of a vector
diag(a)
Diagonal matrix made from vector a
diag(A)
Diagonal vector extracted from matrix A
I or Id
Identity matrix of size d × d (ones on diagonal, zeros off)
Vector of ones (of length d)
Vector of zeros (of length d)
Euclidean or ℓ2 norm
$d
j
ℓ1 norm d
A:,j
j’th column of matrix
Ai,:
transpose of i’th row of matrix (a column vector)
Aij
Element (i, j) of matrix A
x ⊗y
Tensor product of x and y
Probability notation
We denote random and ﬁxed scalars by lower case, random and ﬁxed vectors by bold lower case,
to denote scalar random variables. Also, we use p() for both discrete and continuous random
variables.Notation
Symbol
Meaning
X ⊥Y
X is independent of Y
X ̸⊥Y
X is not independent of Y
X ⊥Y |Z
X is conditionally independent of Y given Z
X ̸⊥Y |Z
X is not conditionally independent of Y given Z
X ∼p
X is distributed according to distribution p
α
Parameters of a Beta or Dirichlet distribution
cov [x]
Covariance of x
E [X]
Expected value of X
Eq [X]
Expected value of X wrt distribution q
H (X) or H (p)
Entropy of distribution p(X)
I (X; Y )
Mutual information between X and Y
KL (p||q)
KL divergence from distribution p to q
ℓ(θ)
L(θ, a)
λ
Λ
mode [X]
Most probable value of X
μ
Mean of a scalar distribution
μ
Mean of a multivariate distribution
p(x)
Probability density or mass function
p(x|y)
Conditional probability density of x given y
Φ
cdf of standard normal
φ
pdf of standard normal
π
multinomial parameter vector, Stationary distribution of Markov chain
ρ
Correlation coefficient
sigm(x)
Sigmoid (logistic) function,
σ2
Variance
Σ
Covariance matrix
var [x]
Variance of x
ν
Degrees of freedom parameter
Z
Normalization constant of a probability distribution
In general, we use upper case letters to denote constants, such as C, D, K, N, S, T, etc. We
We use x to represent an observed data vector. In a supervised problem, we use y or y to
represent the desired output label. We use z to represent a hidden variable. Sometimes we also
use q to represent a hidden discrete variable.Notation
Symbol
Meaning
C
Number of classes
D
Dimensionality of data vector (number of features)
R
Number of outputs (response variables)
D
Dtest
Test data
J(θ)
Cost function
K
Number of states or dimensions of a variable (often latent)
κ(x, y)
Kernel function
K
Kernel matrix
λ
Strength of ℓ2 or ℓ1 regularizer
N
Number of data cases
Nc
Number of examples of class c, Nc = N
φ(x)
Basis function expansion of feature vector x
Φ
Basis function expansion of design matrix X
q()
Approximate or proposal distribution
Q(θ, θold)
Auxiliary function in EM
S
Number of samples
T
Length of a sequence
T(D)
Test statistic for data
T
Transition matrix of Markov chain
θ
Parameter vector
θ(s)
s’th sample of parameter vector
ˆθ
ˆθML
ˆθMAP
θ
w
Vector of regression weights (called β in statistics)
W
Matrix of regression weights
xij
xi
X
Design matrix of size N × D
x
N
N
˜x
Future test case
x∗
Future test case
y
Vector of all training labels y = (y1, . . . , yN)
zij
Latent component j for case i
Graphical model notationNotation
Symbol
Meaning
s ∼t
Node s is connected to node t
bel
Belief function
C
Cliques of a graph
chj
Child of node j in a DAG
descj
Descendants of node j in a DAG
G
A graph
E
Edges of a graph
mbt
Markov blanket of node t
nbdt
Neighborhood of node t
pat
Parents of node t in a DAG
predt
Predecessors of node t in a DAG wrt some ordering
ψc(xc)
Potential function for clique c
S
Separators of a graph
θsjk
prob. node s is in state k given its parents are in states j
V
Nodes of a graphNotation
List of commonly used abbreviations
Abbreviation
Meaning
cdf
Cumulative distribution function
CPD
Conditional probability distribution
CPT
Conditional probability table
CRF
Conditional random ﬁeld
DAG
Directed acyclic graphic
DGM
Directed graphical model
EB
Empirical Bayes
EM
Expectation maximization algorithm
EP
Expectation propagation
GLM
Generalized linear model
GMM
Gaussian mixture model
HMM
Hidden Markov model
iid
Independent and identically distributed
iff
If and only if
KL
Kullback Leibler divergence
LDS
Linear dynamical system
LHS
Left hand side (of an equation)
MAP
Maximum A Posterior estimate
MCMC
Markov chain Monte Carlo
MH
Metropolis Hastings
MLE
Maximum likelihood estimate
MPM
Maximum of Posterior Marginals
MRF
Markov random ﬁeld
MSE
Mean squared error
NLL
Negative log likelihood
OLS
Ordinary least squares
pd
Positive deﬁnite (matrix)
pdf
Probability density function
pmf
Probability mass function
RBPF
RHS
Right hand side (of an equation)
RJMCMC
Reversible jump MCMC
RSS
Residual sum of squares
SLDS
Switching linear dynamical system
SSE
Sum of squared errors
UGM
Undirected graphical model
VB
Variational Bayes
wrt
With respect to